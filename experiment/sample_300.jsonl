{"repositoryUrl": "https://github.com/omair-sajid-confiz/adr-poc.git", "path": "doc/adr/0007-use-unity-for-dependency-injection.md", "template": "Nygard", "status": "Accepted Extends [6. use dependency injection](0006-use-dependency-injection.md) Amended By [8. Use Autofac for dependency injection](0008-use-autofac-for-dependency-injection.md)", "firstCommit": "2018-09-17T19:35:59Z", "lastCommit": "2018-09-17T19:35:59Z", "numberOfCommits": 1, "title": "7. Use Unity for dependency injection", "wordCount": 62, "authors": {"name1": 1}, "content": "# 7. Use Unity for dependency injection\n\nDate: 2018-09-17\n\n## Status\n\nAccepted\n\nExtends [6. use dependency injection](0006-use-dependency-injection.md)\n\nAmended By [8. Use Autofac for dependency injection](0008-use-autofac-for-dependency-injection.md)\n\n## Context\n\nThe issue motivating this decision, and any context that influences or constrains the decision.\n\n## Decision\n\nThe change that we're proposing or have agreed to implement.\n\n## Consequences\n\nWhat becomes easier or more difficult to do and any risks introduced by the change that will need to be mitigated.\n"}
{"repositoryUrl": "https://github.com/superwerker/superwerker.git", "path": "docs/adrs/notifications.md", "template": "unknown", "status": null, "firstCommit": "2021-03-10T12:46:36Z", "lastCommit": "2021-03-18T15:48:41Z", "numberOfCommits": 6, "title": "Notifications", "wordCount": 272, "authors": {"name1": 1, "name2": 4, "name3": 1}, "content": "# Notifications\n\n## Context\n\nsuperwerker creates an OpsItem in Systems Manager OpsCenter for each email received by the [RootMail](rootmail.md) feature. Without notifications for new OpsItems, users need to check the OpsCenter for new items manually and might miss important information regarding their AWS accounts and resources.\n\n## Decision\n\n- Use CloudWatch Events to trigger an AWS Lambda function whenever a new OpsItem is created.\n  - OpsCenter / OpsItem supports SNS notifications, but the desired SNS topic Arn needs to be provided explicitly whenever an OpsItem is created. Therefore, we decided against this native feature.\n- Publish a message to an SNS topic for every new OpsItem.\n - Use SNS, since subscriptions and email verification work out-of-the-box with CloudFormation tooling.\n - Use native email subscriptions for SNS to notify a specified email address about new messages.\n - We decided against using SES for email notifications since this would lead to several additional steps like verifying sender and recipient domains or email addresses.\n- We decided against (re-)using the existing root email address of the management AWS account since we would need to keep this in sync with the SNS subscription (because the management account root email adress can be changed). And we wanted to keep the notification feature simple.\n- If no email address is provided, no SNS topic is created.\n\n## Consequences\n\n- Users can provide an email address for notifications when creating the superwerker CloudFormation stack.\n- Users need to verify the SNS subscription for the provided email address.\n- Users will receive an email about every new OpsItem in OpsCenter.\n- Users need to take care of handling OpsItems (e.g., close them).\n- SNS email subscriptions do not allow the full range of email capabilities/customizations (which SES would).\n"}
{"repositoryUrl": "https://github.com/DanilSuits/dddsample-pelargir.git", "path": "doc/adr/0003-install-citerus-as-a-git-submodule.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-06-03T14:58:16Z", "lastCommit": "2018-06-03T14:58:16Z", "numberOfCommits": 1, "title": "3. Install citerus as a git submodule", "wordCount": 209, "authors": {"name1": 1}, "content": "# 3. Install citerus as a git submodule\n\nDate: 03/06/2018\n\n## Status\n\nAccepted\n\n## Context\n\nI can't be bothered to be installing the citerus library into my\nrepositories all the time.\n\nI want to keep my copy of that closely synchronized to the common\nproject, so that any progress that is made can be incorporated\ninto my work.\n\nI want a convenient way to introduce fixes in the existing implementation\nas I discover the need.\n\nI want to keep the touch on the existing code _light_; I expect\nto be making some fairly drastic changes in approach, and I don't\nwant to be debating my choices with the maintainers.\n\nI'm not currently comfortable working with maven projects where\nthe elements are not rooted under a common location.\n\n## Decision\n\nAdd the upstream as a git submodule, so that it's clear precisely\nwhich version of the upstream is in play at any given time.\n\nChanges that are of general interest can be in that space.\n\nMost radical changes (for a _gentle_ definition of radical) should\nbe maintained elsewhere.\n\n## Consequences\n\nWell, I have to learn how git submodules work.\n\nI'll need to be careful about how to make global changes, for\nconcerns like formatting, removing unused imports, and so on.\n\n"}
{"repositoryUrl": "https://github.com/SAP/cloud-sdk-js.git", "path": "knowledge-base/adr/0028-public-api-extraction.md", "template": "unknown", "status": null, "firstCommit": "2021-10-11T07:15:07Z", "lastCommit": "2021-11-11T08:40:55Z", "numberOfCommits": 2, "title": "Public API Generation", "wordCount": 693, "authors": {"name1": 2}, "content": "# Public API Generation\n\n## Status\n\naccepted\n\n## Context\n\nFor a typescript library it is common to use [barrels](https://basarat.gitbook.io/typescript/main-1/barrel) to make imports easy for consumers.\nA barrel is simply an `index.ts` re-exporting content of other source file.\nAs a consumer you can then import all parts of the public API via the root `index.ts`.\n\nIn the past we exported everything via multiple barrel file in each folder and a \\* in the barrels:\n\n```ts\nexport * from './some-file.ts';\n```\n\nThis was convenient for us, but we had no distinction between:\n\n- This is part of the `public` API where a consumer can rely on stability.\n- This is part of the `internal` API and not meant for direct usage of the consumer.\n  The convenience had the drawback that we were often blocked to do a refactoring.\n  This ADR proposes a way to improve the situation.\n\n## API Categories\n\nOur new strategy should provide the following options:\n\n- `Exported Public API`: This is a cautious decision and consumer can rely on stable contract for minor versions.\n  It contains only objects where we see (re)use potential for customers.\n  This API is reachable via the root level import e.g. `@sap-cloud-sdk/odata-v2`.\n- `Exported Internal API`: Exported for technical reasons but not meant to be used by consumer.\n  We keep this API stable for path versions.\n  This API is accessible via `@sap-cloud-sdk/odata-v2/internal`.\n- `Not exported`: Should be used wherever possible\n\nEach object like constant, function, interface, type, class **must** be in one of the categories.\n\n## How to Achieve it\n\nNote that the problem has two sides:\n\n- For TypeScript the `d.ts` files are the source of truth for the available types.\n  You could use the [stripInternal](https://www.typescriptlang.org/tsconfig#stripInternal) compiler flag to remove internal object from the `d.ts` files.\n  You could still use `export * from 'ABC\"` because the type definition are reduced.\n- For JavaScript the module exports in the transpiled `index.js` matter.\n  To have a minimal API exposed here one has to avoid `*` in the export statements.\n\nThere are tools like [barrelsby](https://github.com/bencoveney/barrelsby#readme) to create the barrels for you.\nHowever, these tools do not create named minimal exports.\nHence, we propose the following approach:\n\n- You go over the code and manually maintain the API:\n  1. Use `@internal` annotation and `stripInternal` compiler option for parts the internal API.\n   This removes the internal API from the `d.ts` files.\n  2. Maintain minimal named exports in the root `index.ts` pointing to the objects of the public API.\n   This creates also minimal module exports in the JavaScript usecase.\n  3. We create a `internal` export as we did for the [v4 case](https://github.com/SAP/cloud-sdk-js/tree/v1.28.0/packages/core) which can be imported via `@sap-cloud-sdk/odata-v2/internal`.\n   This internal folder contains all exports.\n\nThis is a large manual effort initially and seems to be redundant because the minimal `index.ts` alone would already do the trick.\nHowever, the double maintenance makes better check rules possible.\nWe plan to implement the following checks:\n\n- Have a check to avoid any `*` exports in the root `index.ts`.\n- Use an automatic tool like [barrelsby](https://github.com/bencoveney/barrelsby#readme) to create `index.ts` exporting everything to the `internal` folder.\n- Have a check to enforce exposed object from `d.ts` match the named exports of the root `index.ts`.\n  - A missing value in the `index.ts` denotes: You have exported something in the code but not added it to the relevant barrel or missed the `@internal`.\n  - A missing value in the `d.ts` denotes: You have violated the API contract and marked a previously exported object as `@internal`.\n- In the integration and E2E tests we enforce root level imports via eslint rule.\n- [Optiona] Have a check to enforce TsDoc on all exported objects.\n  Either a full doc if part of the public API or at least `@internal` as minimal value.\n  The [eslint-plugin-tsdoc](https://github.com/microsoft/tsdoc/issues/209) does not have a `require` rule.\n  The [eslint-plugin-jsdoc](https://github.com/gajus/eslint-plugin-jsdoc) does have it but does not recognize many TypeScript object as `interface`.\n\n## Internal Packages\n\nThe packages:\n\n- odata-common\n- generator-common\n\nare not meant for direct usage.\nHence, they do not export a public API (root level index is empty).\nAll object intended for the public are re-exported from `odata-v2`,`odata-v4` or the generator packages.\n\n## Consequences\n\nWe have a minimal API exposed to the consumer and most of the exported function are internal.\nWe are able to do refactoring on the internal API methods.\nWe have tooling enforcing to keep the public API minimal and well documented.\n"}
{"repositoryUrl": "https://github.com/huifenqi/arch.git", "path": "decisions/0030-capacity-evaluation-storage.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-05-25T03:59:26Z", "lastCommit": "2017-09-01T06:39:01Z", "numberOfCommits": 2, "title": "30. 容量评估 - 存储", "wordCount": 164, "authors": {"name1": 2}, "content": "# 30. 容量评估 - 存储\n\nDate: 2017-05-25\n\n## Status\n\nAccepted\n\n## Context\n\n1. 一些需要高 IOPS 的服务，磁盘使用的是普通云盘，如，数据库，备份服务，图片服务等。\n\n## Decision\n\n1. 明确存储的使用场景；\n2. 关注吞吐量，IOPS和数据首次获取时间。\n\n我们的存储都是基于 Aliyun 的，他有以下类别及特点：\n\n* nas(文件存储)\n\t* 使用场景\n\t\t* 负载均衡共享存储和高可用\n\t\t* 企业办公文件共享(**不支持本地挂载**)\n\t\t* 数据备份\n\t\t* 服务器日志共享\n\t* 价格及吞吐能力\n\t\t* 2/G/M SSD性能型 60M/s\n\t\t* **0.65/G/M 容量型 30M/s**\n* disk(块存储、云盘) \n\t* 使用场景\n\t\t* 普通云盘\n\t\t\t* 不被经常访问或者低 I/O 负载的应用场景\n\t\t* 高效云盘\n\t\t\t* 中小型数据库\n\t\t\t* 大型开发测试\n\t\t\t* Web 服务器日志\n\t\t* SSD 云盘\n\t\t\t* I/O 密集型应用\n\t\t\t* 中大型关系型数据库\n\t\t\t* NoSQL 数据库\n\t* 价格\n\t\t* 普通云盘 0.3/G/M \n\t\t* 高效云盘 0.35/G/M\n\t\t* SSD 云盘 1/G/M\n\t* 吞吐量\n\t\t* 普通云盘 30MBps\n\t\t* 高效云盘 **80MBps**\n\t\t* SSD 云盘 256MBps\n\t* IOPS\n\t\t* 普通云盘 数百\n\t\t* 高效云盘 3000\n\t\t* SSD 云盘 20000\n\t* 访问延迟\n\t\t* 普通云盘 5 - 10 ms\n\t\t* 高效云盘 1 - 3 ms\n\t\t* SSD 云盘 0.5 - 2 ms\n* oss(对象存储)\n\t* 使用场景\n\t\t* 图片和音视频等应用的海量存储\n\t\t* 网页或者移动应用的静态和动态资源分离\n\t\t* 云端数据处理\n\t* 价格及吞吐能力\n\t\t* **0.148/G/M 标准型 吞吐量大，热点文件、需要频繁访问的业务场景** 大概 50M/s，类似高效云盘\n\t\t* **0.08/G/M 低频访问型 数据访问实时，读取频率较低的业务场景** Object 存储最低 30 天\n\t\t* 0.06/G/M 归档型 数据恢复有等待时间，数据有存储时长要求 Object 存储最低 30 天\n* oas(归档存储)\n\t* 使用场景\n\t\t* 低成本备份\n\t\t* 数据归档\n\t\t* 取代磁带\n\t* 价格及吞吐能力\n\t\t* 0.07/G/M 用户提取数据时能够容忍0-4小时的时间延迟\n\n## Consequences\n\n1. 加粗的信息可以重点查看下，同类别里比较推荐；\n2. 我们选择时还要考虑价格及存储时长要求。\n\nRefs:\n\n* [Server request and upgrade: capacity evaluation][1]\n* 云盘参数和性能测试方法：[https://help.aliyun.com/document\\_detail/25382.html][2]\n\n[1]:\t0019-server-request-and-upgrade-capacity-evaluation.md\n[2]:\thttps://help.aliyun.com/document_detail/25382.html"}
{"repositoryUrl": "https://github.com/noushad-pp/base-react-typescript.git", "path": "doc/adr/0001-record-architecture-decisions.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-08-31T10:43:00Z", "lastCommit": "2019-08-31T10:43:00Z", "numberOfCommits": 1, "title": "1. Record architecture decisions", "wordCount": 45, "authors": {"name1": 1}, "content": "# 1. Record architecture decisions\n\nDate: 2019-09-25\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to record the architectural decisions made on this project.\n\n## Decision\n\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n\n## Consequences\n\nSee Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's [adr-tools](https://github.com/npryce/adr-tools).\n"}
{"repositoryUrl": "https://github.com/zooniverse/front-end-monorepo.git", "path": "docs/arch/adr-23.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-12-19T16:44:10Z", "lastCommit": "2019-12-19T16:44:10Z", "numberOfCommits": 1, "title": "ADR-23 Tasks as classifier plugins", "wordCount": 433, "authors": {"name1": 1}, "content": "# ADR-23 Tasks as classifier plugins\n\n## Context\nWe'd like the new classifier to be easily extensible. However, adding new tasks to the classifier involved updating the code in several places:\n- add new code in three places:\n  - [task views](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/components/Classifier/components/TaskArea/components/Tasks).\n  - [task models](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/tasks).\n  - [annotation models](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/annotations).\n- import the new modules by name in several places, and register them:\n  - [registered views](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/components/Classifier/components/TaskArea/components/Tasks/helpers/getTaskComponent.js).\n  - [import tasks models for workflow steps](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/WorkflowStepStore.js#L5-L18).\n  - [import all annotations to the classification model](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/Classification.js#L3).\n  - [register annotations with the classifications store](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/ClassificationStore.js#L111-L120).\n\nIt was easy to forget one of these steps and a lot of this could be automated in code.\n\n## Decision\n\n- Keep all the code together. Store task views and models next to each other in the filesystem. (#1212)\n- Import named modules to a registry object (or similar) then load them in to other code from that register. (#1212)\n- Delegate responsibility from the classification to individual tasks. (#1228)\n\n### Implementation\n\n- Task code was moved to `lib-classifier/src/plugins/tasks`. Each task has its own directory, with these subdirectories:\n  - _components_: React components to render the task.\n  - _models_: MobX State Tree models for the task. One Task model and one Annotation model.\n- a _taskRegistry_ object was added, which is described in the [tasks README](https://github.com/zooniverse/front-end-monorepo/blob/master/packages/lib-classifier/src/plugins/tasks/readme.md).\n- Responsibility for creating new annotations was removed from the classifications store, removing the need for the classifications store to know about different types of tasks and how to create an annotation for each. New methods were added to the task models to delegate responsibility and make tasks more flexible:\n  - _task.createAnnotation()_ creates a new annotation of the correct type for a specific task.\n  - _task.defaultAnnotation_ (read-only) returns the default annotation for a specific task.\n\n## Status\n\nAccepted\n\n## Consequences\n\n- A similar architecture could be used to register subject viewers with the classifier.\n- Tasks could be removed completely from the classifier. When a workflow loads, its tasks could be instantiated outside the classifier and only the tasks needed for the workflow could be passed in as props.\n- The classifier could make better use of the MobX State Tree. A classification could store the tasks used to generate that classification, each task holding a reference to its own annotation. This opens up the possibility of more flexible code for tracking workflow history and handling recursive workflows. We could also take advantage of the tree (via _getParent()_ or _getParentOfType()_) to easily reference the task that generated a specific annotation, or the classification that a task is currently doing work for.\n- the registry model has no equivalent for `import { SingleChoiceAnnotation, MultipleChoiceAnnotation, TextAnnotation } from '@plugins/tasks/models/annotations'`. It would be helpful, but not necessary, to be able to do this when setting up classifier stores.\n- the task registry is only available after the task models have been set up and initialised, limiting its usefulness when accessing models in order to set up other models, such as drawing tools.\n"}
{"repositoryUrl": "https://github.com/ASethi93/james.git", "path": "src/adr/0013-precompute-jmap-preview.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-12-10T20:05:48Z", "lastCommit": "2020-12-10T20:05:48Z", "numberOfCommits": 1, "title": "13. Precompute JMAP Email preview", "wordCount": 329, "authors": {"name1": 1}, "content": "# 13. Precompute JMAP Email preview\n\nDate: 2019-10-09\n\n## Status\n\nAccepted\n\n## Context\n\nJMAP messages have a handy preview property displaying the firsts 256 characters of meaningful test of a message.\n\nThis property is often displayed for message listing in JMAP clients, thus it is queried a lot.\n\nCurrently, to get the preview, James retrieves the full message body, parse it using MIME parsers, removes HTML and keep meaningful text.\n\n## Decision\n\nWe should pre-compute message preview.\n\nA MailboxListener will compute the preview and store it in a MessagePreviewStore.\n\nWe should have a Cassandra and memory implementation.\n\nWhen the preview is precomputed then for these messages we can consider the \"preview\" property as a metadata.\n\nWhen the preview is not precomputed then we should compute the preview for these messages, and save the result for later.\n\nWe should provide a webAdmin task allowing to rebuild the projection. The computing and storing in MessagePreviewStore \nis idempotent and the task can be run in live without any concurrency problem.\n\nSome performance tests will be run in order to evaluate the improvements.\n\n## Consequences\n\nGiven the following scenario played by 2500 users per hour (constant rate)\n - Authenticate\n - List mailboxes\n - List messages in one of their mailboxes\n - Get 8 times the properties expected to be fast to fetch with JMAP\n\nWe went from:\n - A 7% failure and timeout rate before this change to almost no failure\n - Mean time for GetMessages went from 9 710 ms to 434 ms (22 time improvment), for all operation from\n 12 802 ms to 407 ms (31 time improvment)\n - P99 is a metric that did not make sense because the initial simulation exceeded Gatling (the performance measuring tool \n we use) timeout (60s) at the p95 percentile. After this proposal p99 for the entire scenario is of 1 747 ms\n\nAs such, this changeset significantly increases the JMAP performance.\n\n## References\n\n - https://jmap.io/server.html#1-emails JMAP client guice states that preview needs to be quick to retrieve\n\n - Similar decision had been taken at FastMail: https://fastmail.blog/2014/12/15/dec-15-putting-the-fast-in-fastmail-loading-your-mailbox-quickly/\n\n - [JIRA](https://issues.apache.org/jira/browse/JAMES-2919)\n"}
{"repositoryUrl": "https://github.com/alphagov/paas-team-manual.git", "path": "source/architecture_decision_records/ADR039-aiven-metrics-for-users.html.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-05-15T10:28:20Z", "lastCommit": "2021-03-18T22:01:24Z", "numberOfCommits": 7, "title": "(sem título)", "wordCount": 437, "authors": {"name1": 1, "name2": 1, "name3": 1, "name4": 2, "name5": 1, "name6": 1}, "content": "---\ntitle: ADR039 - Aiven metrics for users\n---\n\n# ADR039: Aiven metrics for users\n\n## Context\n\nWe offer our users the following backing services through [Aiven](https://aiven.io):\n\n- Elasticsearch\n- InfluxDB (private beta)\n\nWe want to make sure our users can view metrics for their Aiven backing services so that our users can:\n\n- debug and respond to usage and service performance changes\n- understand the operational characteristics of their applications and services\n- make better capacity planning and budgeting decisions\n\nAiven has service integrations which add extra functionality to an Aiven service. Service integrations are useful for:\n\n- shipping logs to an Elasticsearch/Rsyslog\n- sending metrics to Datadog\n- sending metrics to Aiven Postgres/InfluxDB\n- exposing metrics in Prometheus exposition format\n\nWe currently run Prometheus for monitoring the platform, using the [Prometheus BOSH release](https://github.com/bosh-prometheus/prometheus-boshrelease) and have confidence and experience using it.\n\nWe will need to think about Prometheus failover. If we load balance Prometheus without sticky sessions, the metrics Prometheus reports will be erratic, as different instances report different metrics.\n\n## Decision\n\nWe will use Prometheus to scrape Aiven-provided services.\n\nWe will deploy new Prometheus in the Cloud Foundry BOSH deployment using the Prometheus BOSH release. This will reduce blast radius - tenant usage of metrics will not affect our ability to operate and monitor the platform using Prometheus.\n\nWe will need to automate the following tasks:\n\n1. Service discovery: make sure Prometheus has an updated list of Aiven services to scrape. We must colocate this automation with the Prometheus instance.\n2. Service integration: make sure every eligible Aiven-provided service uses the Aiven service integration for Prometheus.\n\n## Initial implementation\n\nIn the initial implementation we deploy multiple instances of Prometheus for high availability.\n\n![architecture](../images/adr450-prometheus-aiven-architecture.svg)\n\nThere will be three services on the instance:\n\n- Prometheus\n- Caddy\n- Aiven service discovery\n\nWe use Gorouter for exposing Prometheus publicly.\n\nWe use Caddy for failover and for authentication.\nCaddy does automatic failover. This means that during regular operation, it proxies all traffic to a single instance, but when the usual instance is unreachable, Caddy will proxy the request to the colocated Prometheus.\n\nThe Aiven service discovery process will regularly query the Aiven API to keep\nan updated list of Aiven Elasticsearch services for which we want to receive metrics.\n\nThe Prometheus server instances will be configured with this list and retrieve\nmetrics from the respective Aiven Elasticsearch service instances.\n\nThis architecture is temporary and we will review it when we look at how to expose the Prometheus API to all users.\n\n## Status\n\nAccepted\n\n## Consequences\n\nWe will provide Prometheus as the datastore and interface for (a subset of) tenant metrics.\n\nWe will deploy additional stateful instances to our Cloud Foundry BOSH deployment.\n\nWe will maintain software to automate the integration of Aiven services and Prometheus.\n"}
{"repositoryUrl": "https://github.com/nickvdyck/webtty.git", "path": "docs/adr/0003-coding-style-and-enforcement.md", "template": "Nygard", "status": "Proposed", "firstCommit": "2019-10-21T21:30:42Z", "lastCommit": "2020-04-16T15:40:07Z", "numberOfCommits": 2, "title": "3. Coding Style and Enforcement", "wordCount": 176, "authors": {"name1": 2}, "content": "# 3. Coding Style and Enforcement\n\nDate: 21/10/2019\n\n## Status\n\nProposed\n\n## Context\n\nThis codebase should be easily approachable by all those contributing to it current and future, consistency of coding style is thus an important aspect.\n\n## Decision\nFor TypeScript based projects we will use eslint to enforce consistent coding styles and prettier to enforce consistent code formatting.\n\nFor C# based projects I have not made a decision yet, this is something that should be addressed in a future extension.\n\nAn editorconfig file will be available where needed so that editors can pick up common settings.\n\nThese rules should not be written in stone. In the future, it should be possible given consensus to add/remove or override certain rules.\n\nCommits introducing code that does not adhere to the above settings should fail the CI build.\n\n## Consequences\nNot having linting an coding style automated setup for C# can haunt me in the future, because of all the styling dept that will be accrued. But I simply don't have the bandwidth or knowledge a the moment to get this setup.\n"}
{"repositoryUrl": "https://github.com/xuyuji9000/adr-playground.git", "path": "doc/architecture/decisions/0002-implement-as-unix-shell-scripts.md", "template": "Nygard", "status": "Superceded by [3. Remove Unix Scripts](0003-remove-unix-scripts.md)", "firstCommit": "2018-09-12T16:33:05Z", "lastCommit": "2018-09-12T16:34:23Z", "numberOfCommits": 2, "title": "2. Implement as Unix shell scripts", "wordCount": 60, "authors": {"name1": 2}, "content": "# 2. Implement as Unix shell scripts\n\nDate: 2018-09-13\n\n## Status\n\nSuperceded by [3. Remove Unix Scripts](0003-remove-unix-scripts.md)\n\n## Context\n\nThe issue motivating this decision, and any context that influences or constrains the decision.\n\n## Decision\n\nThe change that we're proposing or have agreed to implement.\n\n## Consequences\n\nWhat becomes easier or more difficult to do and any risks introduced by the change that will need to be mitigated.\n"}
{"repositoryUrl": "https://github.com/JulianG/bananatabs.git", "path": "doc/adr/0004-using-react-context.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-07-07T20:55:50Z", "lastCommit": "2019-07-07T23:11:54Z", "numberOfCommits": 3, "title": "4. Using React Context", "wordCount": 173, "authors": {"name1": 3}, "content": "# 4. Using React Context\n\nDate: 2019-07-01\n\n## Status\n\nAccepted\n\n## Context\n\nThere was a lot of prop-drilling in the component tree.\n\n### Old Render Tree -- A lot of prop-drilling\n\n```\n<App/>\n  <BananaTabs />\n  <MainView /> (✅session, ❌sessionMutator, ❌windowMutator, ❌tabMutator, ✅browserController)\n  <Title />\n  <WindowListView />  (✅windows, ✅sessionMutator, ❌windowMutator, ❌tabMutator)\n  <WindowView />  (✅window, ❌windowMutator, ❌tabMutator)\n  <WindowHeader />  (️️⚠️window️, ✅windowMutator, ❌tabMutator)\n  <DisclosureButton />  (✅window, ✅windowMutator)\n  <VisibilityIcon />  (✅window, ✅windowMutator, ✅tabMutator)\n  <WindowTitle />   (✅window, ✅windowMutator)\n  <TabList />   (✅window, ❌tabMutator)\n  <TabView />   (✅window, ✅tab, ✅tabMutator)\n  <TabToolsView />\n  <MainViewCmdButtons /> (none)\n\nLegend:\n✅prop: actually used by component\n⚠️prop: only reading id (e.g. window.id, tab.id)\n❌prop: only passing it down to children\n\n```\n\n## Decision\n\nI'm going to try to use React Context with the `useContext` hook to see if I can reduce or eliminate prop-drilling.\n\n## Consequences\n\n### New Render Tree -- No prop-drilling\n\n```\n<App/>\n  <BananaTabs />\n  <MainView /> (✅session, ✅browserController)\n  <Title />\n  <WindowListView />  (✅windows, ⚛️sessionMutator)\n  <WindowView />  (✅window)\n  <WindowHeader />  (️️⚠️window️, ⚛️windowMutator)\n  <DisclosureButton />  (✅window, ⚛️windowMutator)\n  <VisibilityIcon />  (✅window, ⚛️windowMutator, ⚛️tabMutator)\n  <WindowTitle />   (✅window, ⚛️windowMutator)\n  <TabList />   (✅window)\n  <TabView />   (✅window, ✅tab, ⚛️tabMutator)\n  <TabToolsView />\n  <MainViewCmdButtons /> (none)\n\nLegend:\n✅prop: used by component\n⚠️prop: only reading id (e.g. window.id, tab.id)\n❌prop: only passing it down to children\n⚛️context: using context hook\n```\n"}
{"repositoryUrl": "https://github.com/alphagov/monitoring-doc.git", "path": "documentation/architecture/decisions/0012-deploy-alertmanager-to-k8s.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-11-06T16:29:57Z", "lastCommit": "2018-11-06T16:29:57Z", "numberOfCommits": 1, "title": "12. Deploying alertmanager to the new platform", "wordCount": 549, "authors": {"name1": 1}, "content": "# 12. Deploying alertmanager to the new platform\n\nDate: 2018-11-06\n\n## Status\n\nAccepted\n\n## Context\n\nThe Observe team is a part of the new platform team, which is building out kubernetes capability in GDS.\n\nThere is a long-term goal that teams in GDS should avoid running bespoke infrastructure, specific to that team, so that any infrastructure we run is run in a common way and supportible by many people.\n\nWe also have a desire to migrate off ECS.  ECS is painful for running alertmanager because:\n\n  - ECS doesn't support dropping configuration files in place\n  - ECS doesn't support exposing multiple ports via load balancer for service discovery\n\nKubernetes does not have either of these limitations.\n\nCurrently, we have a plan to migrate everything to EC2, in order to get away from ECS.  We have quite a bit of outstanding pain from the old way of doing things:\n\n  - we have two different deploy processes; one using the Makefile and one using the deploy_enclave.sh\n  - we have two different code styles, related to the above\n  - we have two different types of infrastructure\n\nWe haven't fully planned out how we would migrate alertmanager to EC2, but we suspect it would involve at least the following tasks:\n\n  - create a way of provisioning an EC2 instance with alertmanager installed (probably a stock ubuntu AMI with cloud.conf to install software)\n  - create a way of deploying that instance with configuration added (probably a terraform module similar to what we have for prometheus)\n  - actually deploy some alertmanagers to EC2 in parallel with ECS\n  - migrate prometheus to start using both EC2 and ECS alertmanagers in parallel\n  - once we're confident, switch off the ECS alertmanagers\n  - tidy up the old ECS alertmanager code\n\nThis feels like a lot of work, especially if our longer-term goal is that we shouldn't run bespoke infrastructure and should instead run in some common way such as the new platform.\n\nNevertheless, we could leave alertmanager in ECS but still ease some of the pain by refactoring the terraform code to be the new module-style instead of the old project-and-Makefile style, even if we leave alertmanager itself in ECS.\n\n(Prometheus is different: we want to run prometheus the same way that non-PaaS teams such as Verify or Pay run it, so that we can offer guidance to them. The principle is the same: we want to run things the same way other GDS teams run them.)\n\n## Decision\n\n1. We will pause any work migrating alertmanager to EC2\n2. We will run an alertmanager in the new platform, leaving the remaining alertmanagers in ECS\n3. We will try to migrate as much of nginx out of ECS as possible; in particular, we want paas-proxy to move to the same network (possibly same EC2 instance) as prometheus.\n4. We will refactor our terraform for ECS to be module-based rather than the old project-and-Makefile style, so that we reduce the different types of code and deployment style.\n5. We will keep prometheus running in EC2 and not migrate it to the new platform (although new platform environments will each have a prometheus available to them)\n\n## Consequences\n\nWe will have to be careful to keep alertmanager configuration in sync between the old and new infrascture.\n\nWe will have to keep our ECS instances running longer than we might otherwise choose to.\n"}
{"repositoryUrl": "https://github.com/cosmos/cosmos-sdk.git", "path": "docs/architecture/adr-046-module-params.md", "template": "Nygard", "status": "Proposed", "firstCommit": "2021-10-14T19:18:07Z", "lastCommit": "2021-10-30T13:43:04Z", "numberOfCommits": 3, "title": "ADR 046: Module Params", "wordCount": 923, "authors": {"name1": 2, "name2": 1}, "content": "# ADR 046: Module Params\n\n## Changelog\n\n* Sep 22, 2021: Initial Draft\n\n## Status\n\nProposed\n\n## Abstract\n\nThis ADR describes an alternative approach to how Cosmos SDK modules use, interact,\nand store their respective parameters.\n\n## Context\n\nCurrently, in the Cosmos SDK, modules that require the use of parameters use the\n`x/params` module. The `x/params` works by having modules define parameters,\ntypically via a simple `Params` structure, and registering that structure in\nthe `x/params` module via a unique `Subspace` that belongs to the respective\nregistering module. The registering module then has unique access to its respective\n`Subspace`. Through this `Subspace`, the module can get and set its `Params`\nstructure.\n\nIn addition, the Cosmos SDK's `x/gov` module has direct support for changing\nparameters on-chain via a `ParamChangeProposal` governance proposal type, where\nstakeholders can vote on suggested parameter changes.\n\nThere are various tradeoffs to using the `x/params` module to manage individual\nmodule parameters. Namely, managing parameters essentially comes for \"free\" in\nthat developers only need to define the `Params` struct, the `Subspace`, and the\nvarious auxiliary functions, e.g. `ParamSetPairs`, on the `Params` type. However,\nthere are some notable drawbacks. These drawbacks include the fact that parameters\nare serialized in state via JSON which is extremely slow. In addition, parameter\nchanges via `ParamChangeProposal` governance proposals have no way of reading from\nor writing to state. In other words, it is currently not possible to have any\nstate transitions in the application during an attempt to change param(s).\n\n## Decision\n\nWe will build off of the alignment of `x/gov` and `x/authz` work per\n[#9810](https://github.com/cosmos/cosmos-sdk/pull/9810). Namely, module developers\nwill create one or more unique parameter data structures that must be serialized\nto state. The Param data structures must implement `sdk.Msg` interface with respective\nProtobuf Msg service method which will validate and update the parameters with all\nnecessary changes. The `x/gov` module via the work done in\n[#9810](https://github.com/cosmos/cosmos-sdk/pull/9810), will dispatch Param\nmessages, which will be handled by Protobuf Msg services.\n\nNote, it is up to developers to decide how to structure their parameters and\nthe respective `sdk.Msg` messages. Consider the parameters currently defined in\n`x/auth` using the `x/params` module for parameter management:\n\n```protobuf\nmessage Params {\n  uint64 max_memo_characters   = 1;\n  uint64 tx_sig_limit  = 2;\n  uint64 tx_size_cost_per_byte   = 3;\n  uint64 sig_verify_cost_ed25519   = 4;\n  uint64 sig_verify_cost_secp256k1 = 5;\n}\n```\n\nDevelopers can choose to either create a unique data structure for every field in\n`Params` or they can create a single `Params` structure as outlined above in the\ncase of `x/auth`.\n\nIn the former, `x/params`, approach, a `sdk.Msg` would need to be created for every single\nfield along with a handler. This can become burdensome if there are a lot of\nparameter fields. In the latter case, there is only a single data structure and\nthus only a single message handler, however, the message handler might have to be\nmore sophisticated in that it might need to understand what parameters are being\nchanged vs what parameters are untouched.\n\nParams change proposals are made using the `x/gov` module. Execution is done through\n`x/authz` authorization to the root `x/gov` module's account.\n\nContinuing to use `x/auth`, we demonstrate a more complete example:\n\n```go\ntype Params struct {\n\tMaxMemoCharacters  uint64\n\tTxSigLimit   uint64\n\tTxSizeCostPerByte  uint64\n\tSigVerifyCostED25519   uint64\n\tSigVerifyCostSecp256k1 uint64\n}\n\ntype MsgUpdateParams struct {\n\tMaxMemoCharacters  uint64\n\tTxSigLimit   uint64\n\tTxSizeCostPerByte  uint64\n\tSigVerifyCostED25519   uint64\n\tSigVerifyCostSecp256k1 uint64\n}\n\ntype MsgUpdateParamsResponse struct {}\n\nfunc (ms msgServer) UpdateParams(goCtx context.Context, msg *types.MsgUpdateParams) (*types.MsgUpdateParamsResponse, error) {\n  ctx := sdk.UnwrapSDKContext(goCtx)\n\n  // verification logic...\n\n  // persist params\n  params := ParamsFromMsg(msg)\n  ms.SaveParams(ctx, params)\n\n  return &types.MsgUpdateParamsResponse{}, nil\n}\n\nfunc ParamsFromMsg(msg *types.MsgUpdateParams) Params {\n  // ...\n}\n```\n\nA gRPC `Service` query should also be provided, for example:\n\n```protobuf\nservice Query {\n  // ...\n  \n  rpc Params(QueryParamsRequest) returns (QueryParamsResponse) {\n  option (google.api.http).get = \"/cosmos/<module>/v1beta1/params\";\n  }\n}\n\nmessage QueryParamsResponse {\n  Params params = 1 [(gogoproto.nullable) = false];\n}\n```\n\n## Consequences\n\nAs a result of implementing the module parameter methodology, we gain the ability\nfor module parameter changes to be stateful and extensible to fit nearly every\napplication's use case. We will be able to emit events (and trigger hooks registered\nto that events using the work proposed in [event hooks](https://github.com/cosmos/cosmos-sdk/discussions/9656)),\ncall other Msg service methods or perform migration.\nIn addition, there will be significant gains in performance when it comes to reading\nand writing parameters from and to state, especially if a specific set of parameters\nare read on a consistent basis.\n\nHowever, this methodology will require developers to implement more types and\nMsg service methods which can become burdensome if many parameters exist. In addition,\ndevelopers are required to implement persistence logics of module parameters.\nHowever, this should be trivial.\n\n### Backwards Compatibility\n\nThe new method for working with module parameters is naturally not backwards\ncompatible with the existing `x/params` module. However, the `x/params` will\nremain in the Cosmos SDK and will be marked as deprecated with no additional\nfunctionality being added apart from potential bug fixes. Note, the `x/params`\nmodule may be removed entirely in a future release.\n\n### Positive\n\n* Module parameters are serialized more efficiently\n* Modules are able to react on parameters changes and perform additional actions.\n* Special events can be emitted, allowing hooks to be triggered.\n\n### Negative\n\n* Module parameters become slightly more burdensome for module developers:\n  * Modules are now responsible for persisting and retrieving parameter state\n  * Modules are now required to have unique message handlers to handle parameter\n  changes per unique parameter data structure.\n\n### Neutral\n\n* Requires [#9810](https://github.com/cosmos/cosmos-sdk/pull/9810) to be reviewed\n  and merged.\n\n<!-- ## Further Discussions\n\nWhile an ADR is in the DRAFT or PROPOSED stage, this section should contain a summary of issues to be solved in future iterations (usually referencing comments from a pull-request discussion).\nLater, this section can optionally list ideas or improvements the author or reviewers found during the analysis of this ADR. -->\n\n## References\n\n* https://github.com/cosmos/cosmos-sdk/pull/9810\n* https://github.com/cosmos/cosmos-sdk/issues/9438\n* https://github.com/cosmos/cosmos-sdk/discussions/9913\n"}
{"repositoryUrl": "https://github.com/marylly/entusiasme-kotlin.git", "path": "doc/adr/000000_entusiasme_application.md", "template": "unknown", "status": null, "firstCommit": "2020-04-08T00:00:04Z", "lastCommit": "2020-04-08T00:00:04Z", "numberOfCommits": 1, "title": "ADR 000000: Entusiasme Application Development", "wordCount": 116, "authors": {"name1": 1}, "content": "# ADR 000000: Entusiasme Application Development\n\n### **Prologue/Summary**\n\nIn the context of software development professional market for women developers facing the difficult to choose a programming language to study we decided to choose kotlin because of the raise of projects and companies using this tecnologies in their products.\n__________\n\n### **Discussion/Context**\n\nIn all discussions of technical development communities groups appears the difficult for women to choose a technology to start learn software development.\n___________\n\n### **Decision**\n* Google's tecnology \n* Great company supporting and improving\n* Good documentation\n* Small learning curve for Java developer\n* Easy switch to other OOP language\n* SO Interoperability\n* Easy to learn\n* Sugar Syntax\n___________\n### **Status**\nAccepted\n___________\n### **Way/State/Version/Model**\nN/A\n___________\n### **Consequences**\nN/A\n___________\n### **Updates**\n| Information | From | To | Date |\n|---|---|---|---|\n| | | | |"}
{"repositoryUrl": "https://github.com/alphagov/app-performance-summary.git", "path": "doc/adr/0003-integrate-with-drive.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-03-06T16:57:49Z", "lastCommit": "2018-03-06T16:57:49Z", "numberOfCommits": 1, "title": "3. Integrate with google drive", "wordCount": 111, "authors": {"name1": 1}, "content": "# 3. Integrate with google drive\n\nDate: 2018-02-26\n\n## Status\n\nAccepted\n\n## Context\n\n- We're using [Google Data Studio](https://datastudio.google.com) to present KPIs to stakeholders.\n- We're calculating application metrics on a periodic basis\n- We need a way to get the data into a dashboard\n\n## Decision\n\nIntegrate with google drive so we can load KPI reports into a spreadsheet within the GOV.UK team drive.\n\nUse the [Pygsheets](http://pygsheets.readthedocs.io/) library for this.\n\nThe data in the sheet can then be used as a data source within data studio.\n\n## Consequences\n\n- We will need to manage another [service\n  account](http://pygsheets.readthedocs.io/en/latest/authorizing.html)\n- Some of our data infrastructure uses AWS, some of it uses google cloud\n- Anyone on GOV.UK can view measures of application performance\n"}
{"repositoryUrl": "https://github.com/vwt-digital/operational-data-hub.git", "path": "coding_guidelines/adr/0008-config-and-environment-variables.md", "template": "unknown", "status": null, "firstCommit": "2021-03-15T12:49:31Z", "lastCommit": "2021-03-15T13:28:55Z", "numberOfCommits": 2, "title": "8. Config and environment variables", "wordCount": 307, "authors": {"name1": 2}, "content": "# 8. Config and environment variables\n\nDate: 2021-03-15\n\n## Status\n\nAccepted\n\n## Context\n\nWe feel the need to create guidelines for the use of config variables and environment variables when using cloudbuild.\n\n## Decision\n\n**In Short**\n\nThe code that is going to be executed by the Cloud Function (aka the project that is deployed) should receive its configuration variables from a config file. \n\nThe cloudbuild steps use environment variables which can be stored in the cloudbuild.yaml or an external file.\n\n**Elaboration**\n\nWhen developing a Cloud Function, you should store the variables in a config file. The variables are easy to read and use, and other developers (and you) don’t need to build anything for the project to run (unless something other than variables need a build).\n\nThis moves over to when a project is deployed: The variables used in the code of the Cloud Function are all stored within the project: A config file. The project should not receive variables from a cloudbuild, but from a file that is merged into the project by the build (or the other way around).\n\nWhen building something, you might need variables that you are only going to use for building/deployment. Or you might use an external project that needs to get some variables. These variables should be given as environment variables or CLI flags. These variables could be stored in a file that is placed inside a repository for easy access, but can be put into the build as environment files.\n\n**Examples**\n\nConfiguration stored in a repository and merged into the project environment (or the other way around), like this example:\n\n`config.py`\n```python\nINDEX = 0\nTOKEN = 'AAAAaaaaBBBBbbbb'\n```\n`__main__.py`\n```python\nfrom config import INDEX, TOKEN\n```\n\n<br>\n\nConfiguration as an environment variable used to build in a specific step, like this example:\n```yaml\nsubstitutions:\n  _VENV: '/venv'\n  - name: 'cloud'\n  entrypoint: 'bash'\n  args:\n  - '-c'\n  - |\n  source ${_VENV}/bin/activate\n```"}
{"repositoryUrl": "https://github.com/kglazko/innovation-week.git", "path": "fftv-categories/docs/architecture/adr-0002-robot-pattern.md", "template": "unknown", "status": null, "firstCommit": "2019-03-27T23:05:13Z", "lastCommit": "2019-03-27T23:05:13Z", "numberOfCommits": 1, "title": "ADR 2: Robot Pattern in UI Tests", "wordCount": 483, "authors": {"name1": 1}, "content": "# ADR 2: Robot Pattern in UI Tests\n## Context\nUI tests are notoriously difficult to maintain. A few problems that UI tests have are that they often:\n1. Lack a clear architecture\n1. Repeat complex logic\n1. Are written imperatively\n\nAfter briefly searching, only one architecture for UI tests comes up: **the Robot pattern.** The Robot pattern separates the \"how\" and the \"what\" concerns of a UI test. The tests handle the what - click this button, assert the state, click that button, assert the state - while the robots handle the \"how\" - how to find the button view, how to click the button, and how to actually assert the state, i.e. the implementation details.\n\nThe Robot pattern results in declarative tests like:\n```kotlin\nnavigationOverlay {\n  assertCanGoBack()\n  goBack()\n  assertCanNotGoBack()\n\n}.enterUrlAndEnterToBrowser(\"https://mozilla.org\") {\n  assertBodyContent(\"Welcome to mozilla.org!\")\n}\n```\n\nIn the above example, the top-level functions are screen transitions while the inner scope functions are interactions on a given screen.\n\nTo learn more about the Robot pattern, see these resources:\n- [Brief introduction (missing screen transitions)](https://medium.com/android-bits/espresso-robot-pattern-in-kotlin-fc820ce250f7)\n- [Presentation introducing the pattern](https://academy.realm.io/posts/kau-jake-wharton-testing-robots/)\n- [Slide deck from presentation](https://jakewharton.com/testing-robots/)\n\nPros of the Robot pattern:\n- Declarative test files\n- Discourages repetition of complex \"how\" logic by centralizing in robots\n- Reduces the number of places test code needs to change for UX changes, e.g.:\n  - If only one screen changes, only one robot, and perhaps the reliant tests, needs to change\n  - Small UX changes (e.g. polish) generally only change the robot code\n- Clearly separates which interactions occur on which screens\n- Reduced scope when implementing functionality through robot abstractions\n\nCons of the Robot pattern:\n- To write robots and debug test failures, it's a pattern that must be learned\n- Relies on Kotlin features so probably won't be readable in Java\n- Test failure call stacks are less clear due to nested function calls to support the test DSL\n- Robot screen transition implementations use uncommon Kotlin syntax to support the test DSL\n- Hard to write generic re-usable code, as required by the robots\n\nNeutral notes on the Robot pattern:\n- Doesn't define how to handle interactions that don't fit into a screen (e.g. mutating internal state, clicking hardware remote buttons)\n\n## Decision\nWe will architect our UI tests using the Robot pattern: there are no obvious alternatives and the pros significantly outweigh the cons when addressing our specific problems.\n\nStatus: Accepted\n\n## Consequences\n- Tests written with the Robot pattern should be more maintainable but are expected to need more upfront design and brain power\n- All new UI tests will be written using the Robot pattern and Kotlin\n- All existing UI tests will be refactored to use the Robot pattern (to reduce code duplication) and Kotlin\n- Test developers need to learn about the robot pattern in order to debug tests and modify the robots\n- The QA teams, in order to maintain these tests, need to familiarize themselves with Kotlin\n- The test harness is not coupled to the robot pattern so if we have issues we can easily go back to writing tests without an architecture as before\n"}
{"repositoryUrl": "https://github.com/alphagov/publishing-api.git", "path": "docs/arch/adr-005-documents-and-editions.md", "template": "unknown", "status": null, "firstCommit": "2020-06-08T08:31:06Z", "lastCommit": "2020-06-09T15:17:06Z", "numberOfCommits": 2, "title": "Decision Record: Resolve consistency and uniqueness in content", "wordCount": 739, "authors": {"name1": 1, "name2": 1}, "content": "# Decision Record: Resolve consistency and uniqueness in content\n\n## Context\n\nPublishing API has adopted the open/closed principle where the domain model is\nbased around a central ContentItem model, where there are numerous entities\nthat store data related to the ContentItem - however the ContentItem does not\nhave knowledge about them.\n\nOver time this data structure revealed a number of problems to developers\nand users of the Publishing API. Such as:\n\n* Difficulty in maintaining uniqueness constraints, as these were in multiple\n  tables - particularly problematic with concurrent requests.\n* Long verbose queries that were inconsistent with how queries are normally\n  authored in Ruby-on-Rails.\n* Slow performing queries due to the need for many joins.\n* Large amounts of code to try resolve problems identified.\n\nA number of non-exclusive options were considered on how to resolve the issues:\n\n* **Option 1**: Merge user_facing_version and locale into ContentItem table, as\n  this would allow us to set a single uniqueness constraint that could raise an\n  error on a concurrent request;\n* **Option 2**: Merge user_facing_version, locale, base_path, and state into\n  ContentItem table, as this would allow multiple uniqueness constraints but\n  breaks from open/closed principal;\n* **Option 3**: Split ContentItems into Document and Edition models, with\n  Edition being a particular version of a Content Item and a Document spanning\n  all versions;\n* **Option 4**: Split a ContentItem model into two separate models, one that\n  focuses on the uniqueness and relationships, the latter on the content;\n* **Option 5**: ContentStore specified directly in the database rather than as a\n  byproduct of the state value;\n* **Option 6**: Store state history, this would change state from being a single\n  field that is updated to a collection that is stored for a ContentItem;\n* **Option 7**: Take a location centric approach to storing how base_path values are\n  stored for ContentItems. This would involve a table storing which items use a\n  particular base_paths and would open the door to storing other items (such as\n  redirects) that require a base_path.\n\n## Decisions\n\nEach of the options was considered and a selection of them were chosen to be\nimplemented:\n\n### Option 1\n\nThis was rejected in favour of Option 2 which effectively superseded it.\n\n### Option 2\n\nThis was accepted as it was felt the additional complexity introduced by using\nthe open/closed principal was a greater cost to us than the potential\ncomplexity of increasing the concerns of a ContentItem model.\n\nThere is concern that the model could become a \"god\" model as the initial\nproposal tried to avoid. We agreed that we should not use the model layer\nfor logic where possible and instead use supplementary classes. This would allow\nus to keep our models \"thin\".\n\n### Option 3\n\nThis option was accepted with some uncertainty of the naming that should be\nused. It was decided to use the original proposal of \"Documents\" and\n\"Editions\" as these are concepts already in use in GOV.UK publishing - with a\nsynonymous meaning. There was concern that not all content the Publishing API\nstores is considered a \"document\", however it was felt that the use of\nthe term \"document\" was already in use within Publishing and this would not be\nintroducing a fresh problem.\n\nThe key aspects that influenced choosing this was:\n* It offered a simple means to lock requests for concurrency\n* It provided a simpler interface for someone to look up which content is\n  in draft or in live\n* It made the distinctions between translations of a piece of content clearer.\n\n### Option 4\n\nThis was rejected. It was felt that this option was letting our application\nconcerns influence our schema too greatly. There was also not a clear\ndifference between which model would have responsibility for what data, which\nwe felt would make it a challenging abstraction to explain.\n\n### Option 5\n\nThis was initially delayed for further investigation. However it transpired\nthat to have a unique index between base_path and state in PostgreSQL we would\nneed this. Thus it was accepted and implemented.\n\n### Option 6\n\nThis was rejected due to it not being a current concern, it is an idea that\nmay be revisited as part of work to include workflow history and/or to support\na greater array of workflow states.\n\n### Option 7\n\nThis was rejected since we are not at a point where we are concerned with\ndifferent entities sharing the concept of base_path. It was felt that until\nthese are a concern this idea offered an increase in complexity without any\nclear benefits.\n\nThis will be reconsidered if we pursue ideas such as \"redirects as first-class\ncitizens\".\n\n"}
{"repositoryUrl": "https://github.com/cosmos/cosmos-sdk.git", "path": "docs/architecture/adr-008-dCERT-group.md", "template": "Nygard", "status": "> Proposed", "firstCommit": "2019-09-30T19:05:29Z", "lastCommit": "2021-05-27T15:31:04Z", "numberOfCommits": 2, "title": "ADR 008: Decentralized Computer Emergency Response Team (dCERT) Group", "wordCount": 1260, "authors": {"name1": 1, "name2": 1}, "content": "# ADR 008: Decentralized Computer Emergency Response Team (dCERT) Group\n\n## Changelog\n\n* 2019 Jul 31: Initial Draft\n\n## Context\n\nIn order to reduce the number of parties involved with handling sensitive\ninformation in an emergency scenario, we propose the creation of a\nspecialization group named The Decentralized Computer Emergency Response Team\n(dCERT).  Initially this group's role is intended to serve as coordinators\nbetween various actors within a blockchain community such as validators,\nbug-hunters, and developers.  During a time of crisis, the dCERT group would\naggregate and relay input from a variety of stakeholders to the developers who\nare actively devising a patch to the software, this way sensitive information\ndoes not need to be publicly disclosed while some input from the community can\nstill be gained.\n\nAdditionally, a special privilege is proposed for the dCERT group: the capacity\nto \"circuit-break\" (aka. temporarily disable)  a particular message path. Note\nthat this privilege should be enabled/disabled globally with a governance\nparameter such that this privilege could start disabled and later be enabled\nthrough a parameter change proposal, once a dCERT group has been established.\n\nIn the future it is foreseeable that the community may wish to expand the roles\nof dCERT with further responsibilities such as the capacity to \"pre-approve\" a\nsecurity update on behalf of the community prior to a full community\nwide vote whereby the sensitive information would be revealed prior to a\nvulnerability being patched on the live network.  \n\n## Decision\n\nThe dCERT group is proposed to include an implementation of a `SpecializationGroup`\nas defined in [ADR 007](./adr-007-specialization-groups.md). This will include the\nimplementation of:\n\n* continuous voting\n* slashing due to breach of soft contract\n* revoking a member due to breach of soft contract\n* emergency disband of the entire dCERT group (ex. for colluding maliciously)\n* compensation stipend from the community pool or other means decided by\n   governance\n\nThis system necessitates the following new parameters:\n\n* blockly stipend allowance per dCERT member\n* maximum number of dCERT members\n* required staked slashable tokens for each dCERT member\n* quorum for suspending a particular member\n* proposal wager for disbanding the dCERT group\n* stabilization period for dCERT member transition\n* circuit break dCERT privileges enabled\n\nThese parameters are expected to be implemented through the param keeper such\nthat governance may change them at any given point.\n\n### Continuous Voting Electionator\n\nAn `Electionator` object is to be implemented as continuous voting and with the\nfollowing specifications:\n\n* All delegation addresses may submit votes at any point which updates their\n   preferred representation on the dCERT group.\n* Preferred representation may be arbitrarily split between addresses (ex. 50%\n   to John, 25% to Sally, 25% to Carol)\n* In order for a new member to be added to the dCERT group they must\n   send a transaction accepting their admission at which point the validity of\n   their admission is to be confirmed.\n  * A sequence number is assigned when a member is added to dCERT group.\n   If a member leaves the dCERT group and then enters back, a new sequence number\n   is assigned.  \n* Addresses which control the greatest amount of preferred-representation are\n   eligible to join the dCERT group (up the _maximum number of dCERT members_).\n   If the dCERT group is already full and new member is admitted, the existing\n   dCERT member with the lowest amount of votes is kicked from the dCERT group.\n  * In the split situation where the dCERT group is full but a vying candidate\n   has the same amount of vote as an existing dCERT member, the existing\n   member should maintain its position.\n  * In the split situation where somebody must be kicked out but the two\n   addresses with the smallest number of votes have the same number of votes,\n   the address with the smallest sequence number maintains its position.  \n* A stabilization period can be optionally included to reduce the\n   \"flip-flopping\" of the dCERT membership tail members. If a stabilization\n   period is provided which is greater than 0, when members are kicked due to\n   insufficient support, a queue entry is created which documents which member is\n   to replace which other member. While this entry is in the queue, no new entries\n   to kick that same dCERT member can be made. When the entry matures at the\n   duration of the  stabilization period, the new member is instantiated, and old\n   member kicked.\n\n### Staking/Slashing\n\nAll members of the dCERT group must stake tokens _specifically_ to maintain\neligibility as a dCERT member. These tokens can be staked directly by the vying\ndCERT member or out of the good will of a 3rd party (who shall gain no on-chain\nbenefits for doing so). This staking mechanism should use the existing global\nunbonding time of tokens staked for network validator security. A dCERT member\ncan _only be_ a member if it has the required tokens staked under this\nmechanism. If those tokens are unbonded then the dCERT member must be\nautomatically kicked from the group.  \n\nSlashing of a particular dCERT member due to soft-contract breach should be\nperformed by governance on a per member basis based on the magnitude of the\nbreach.  The process flow is anticipated to be that a dCERT member is suspended\nby the dCERT group prior to being slashed by governance.  \n\nMembership suspension by the dCERT group takes place through a voting procedure\nby the dCERT group members. After this suspension has taken place, a governance\nproposal to slash the dCERT member must be submitted, if the proposal is not\napproved by the time the rescinding member has completed unbonding their\ntokens, then the tokens are no longer staked and unable to be slashed.\n\nAdditionally in the case of an emergency situation of a colluding and malicious\ndCERT group, the community needs the capability to disband the entire dCERT\ngroup and likely fully slash them. This could be achieved though a special new\nproposal type (implemented as a general governance proposal) which would halt\nthe functionality of the dCERT group until the proposal was concluded. This\nspecial proposal type would likely need to also have a fairly large wager which\ncould be slashed if the proposal creator was malicious. The reason a large\nwager should be required is because as soon as the proposal is made, the\ncapability of the dCERT group to halt message routes is put on temporarily\nsuspended, meaning that a malicious actor who created such a proposal could\nthen potentially exploit a bug during this period of time, with no dCERT group\ncapable of shutting down the exploitable message routes.\n\n### dCERT membership transactions\n\nActive dCERT members\n\n* change of the description of the dCERT group\n* circuit break a message route\n* vote to suspend a dCERT member.\n\nHere circuit-breaking refers to the capability to disable a groups of messages,\nThis could for instance mean: \"disable all staking-delegation messages\", or\n\"disable all distribution messages\". This could be accomplished by verifying\nthat the message route has not been \"circuit-broken\" at CheckTx time (in\n`baseapp/baseapp.go`).\n\n\"unbreaking\" a circuit is anticipated only to occur during a hard fork upgrade\nmeaning that no capability to unbreak a message route on a live chain is\nrequired.\n\nNote also, that if there was a problem with governance voting (for instance a\ncapability to vote many times) then governance would be broken and should be\nhalted with this mechanism, it would be then up to the validator set to\ncoordinate and hard-fork upgrade to a patched version of the software where\ngovernance is re-enabled (and fixed). If the dCERT group abuses this privilege\nthey should all be severely slashed.\n\n## Status\n\nProposed\n\n## Consequences\n\n### Positive\n\n* Potential to reduces the number of parties to coordinate with during an emergency\n* Reduction in possibility of disclosing sensitive information to malicious parties\n\n### Negative\n\n* Centralization risks\n\n### Neutral\n\n## References\n\n  [Specialization Groups ADR](./adr-007-specialization-groups.md)\n"}
{"repositoryUrl": "https://github.com/KIT-SOC4S/ftd-scratch3-offline.git", "path": "docs/architecture/decisions/0010-use-travis-ci-for-continuous-integration.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-10-20T19:19:01Z", "lastCommit": "2019-10-20T19:19:01Z", "numberOfCommits": 1, "title": "10. Use travis ci for continuous integration", "wordCount": 85, "authors": {"name1": 1}, "content": "# 10. Use travis ci for continuous integration\n\nDate: 2019-10-20\n\n## Status\n\nAccepted\n\n## Context\n\nWe want to use continuous integration to make sure that at any time the build is working.  \nCI will check every commit and PR.  \nPossible choices are: Travis CI, CircleCI or AppVeyor.  \nTravis CI offers Linux and Mac builds. Windows is in beta.  \nCircleCI supports all 3 platforms.  \nAppVeyor supports Linux and Windows.  \nThe authors have already used Travis CI.  \n\n## Decision\n\nWe will use Travis CI for continuous integration.  \n\n## Consequences\n\nPossible vendor lock-in.\n"}
{"repositoryUrl": "https://github.com/wikimedia/mediawiki-extensions-Popups.git", "path": "docs/adr/0009-utilize-browser-caching.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-06-22T06:12:49Z", "lastCommit": "2019-01-17T16:11:29Z", "numberOfCommits": 4, "title": "9. Utilize browser caching", "wordCount": 176, "authors": {"name1": 1, "name2": 2, "name3": 1}, "content": "# 9. Utilize browser caching\n\nDate: 2017-05-24\n\n## Status\n\nAccepted\n\n## Context\n\nWe needed to make sure we're not overloading the servers with excessive\nrequests. We wanted to find a way to serve fresh resources while keeping\nthe back-end happy.\n\n## Decision\n\nPage Previews will leverage the browser's cache rather than maintaining its own.\nWe rely on Grade A browsers implementing HTTP caching correctly and their\nvendors making accessing them as efficient as possible in order to avoid\nincurring the incidental complexity of writing our own cache in JavaScript.\n\nWe'll set appropriate `Cache-Control` HTTP headers for both the MediaWiki API,\nvia [the `maxage` and `smaxage` main module parameters][0], and the RESTBase page\nsummary endpoint with the help of the Services team.\n\n## Consequences\n\nResources fetched from the MediaWiki API [will be cached for 5 minutes in public\ncaches and the browsers's cache][1]. Unlike the MediaWiki API, resources fetched\nfrom the RESTBase endpoint, [will be cached for 14 days in public caches][2].\n\n[0]: https://www.mediawiki.org/wiki/API:Main_module#Parameters\n[1]: https://github.com/wikimedia/mediawiki-extensions-Popups/blob/86075fba/src/gateway/mediawiki.js#L15\n[2]: https://github.com/wikimedia/mediawiki-services-restbase-deploy/blob/9a86d4ce/scap/templates/config.yaml.j2#L100-L101\n"}
{"repositoryUrl": "https://github.com/akvo/akvo-product-design.git", "path": "FLOW/arch/ADR-003.md", "template": "Nygard", "status": "In progress.", "firstCommit": "2015-01-27T12:54:55Z", "lastCommit": "2015-02-10T10:50:00Z", "numberOfCommits": 3, "title": "ADR 003: Browser-based submission of survey form responses", "wordCount": 313, "authors": {"name1": 2, "name2": 1}, "content": "## ADR 003: Browser-based submission of survey form responses\n\n## Context\n\nIn certain cases organisations would like to take advantage of the AkvoFLOW data gathering capabilities but due to their (organistions') nature, they do not require the mobile app to fill out survey forms.  They instead require the possibility to fill out and submit survey forms through a web browser.\n\nThe implementation of this feature occurs at a moment in which we are in the process of migrating the entire Akvo FLOW backend functionality away from being hosted on GAE infrastructure.\n\nThe requirement of the possibility to fill in and submit responses through a browser-based interface is also applicable beyond AkvoFLOW and is relevant for other products developed by Akvo.\n\n## Decision\n\n* We will avoid as much as possible making any modification to the GAE component and  implement this functionality as a service that interacts with the GAE component.\n\n* We will split the implementation of the functionality into two components, a client and a backend server component.\n\n* The server component will read the XML representation of the form definition (as currently used by the Akvo FLOW mobile app), and expose an API that delivers the form definition in JSON format. \n\n* The client will read the flow representation from the server API, and render the forms in html.\n\n* The client will store responses locally until submit time. At submit time, the client delivers the responses to the server.\n\n* The server component will receive the responses submitted by the client, transform the responses to the correct format as used by GAE, and use the regular GAE data ingestion process, as used by the device.\n\n## Status\n\nIn progress.\n\n## Consequences\n\n* We will create minimal to no code modifications within the GAE component also avoiding the possible need for reimplementation at a later stage.\n\n* Storing the responses locally till submission implies the possibility of data loss in case a browser/computer crashes.\n"}
{"repositoryUrl": "https://github.com/alphagov/paas-team-manual.git", "path": "source/architecture_decision_records/ADR006-rds-broker.html.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-04-26T16:22:14Z", "lastCommit": "2021-03-18T22:01:24Z", "numberOfCommits": 5, "title": "(sem título)", "wordCount": 97, "authors": {"name1": 2, "name2": 1, "name3": 1, "name4": 1}, "content": "---\ntitle: ADR006 - RDS broker\n---\n\n# ADR006: RDS broker\n\n## Context\n\nWe need to provide tenants with the ability to provision databases for use in\ntheir applications. Our first iteration of this will be using RDS.\nWe investigated some implementations of a service broker which supported RDS\n\n - [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\n - [18F](https://github.com/18F/rds-service-broker)\n\n## Decision\n\nWe will use the [cf platform eng](https://github.com/cf-platform-eng/rds-broker)\nrds broker. As this is not a supported product, we will fork this and maintain\nthis and implement new features ourselves.\n\n## Status\n\nAccepted\n\n## Consequences\n\nWe will be maintaining a new service broker, but have a head start on creating\nit by basing it on an existing service broker.\n"}
{"repositoryUrl": "https://github.com/threefoldtech/js-sdk.git", "path": "docs/architecture/decisions/0007-payment-for-3bot-deployer.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-09-15T16:13:14Z", "lastCommit": "2020-09-15T20:04:09Z", "numberOfCommits": 2, "title": "7. payment_for_3Bot_deployer", "wordCount": 168, "authors": {"name1": 1, "name2": 1}, "content": "# 7. payment_for_3Bot_deployer\n\nDate: 2020-09-15\n\n## Status\n\nAccepted\n\n## Context\n\nThe deployment of 3Bot is not guaranteed to succeed due to network failures or misbehaving of nodes on the grid. Users who pay for a 3Bot reserve capacity and in case of failing to initialize the solution they lose their money (and that capacity won't be usable after payment)\n\n## Decision\n\n- 3Bot deployer to start with a funded wallet \n- when the user wants to create an instance, we create a pool for this instance using the funded wallet for 15 mins (that should be enough for the initialization step).\n- When we manage to deploy and initialize the 3Bot, we ask the user to extend the lifetime of the 3Bot in the same chatflow\n- In case of pool extension failure they will be refunded from the explorer\n\n\n## Consequences\n\n- Users will guarantee their payment won't be lost.\n- A small amount of tokens might be lost from our wallet but it guarantees the safety of users payments and successful deployment of their solutions.\n"}
{"repositoryUrl": "https://github.com/etienneleba/TDD-hexagonal-project.git", "path": "Docs/ADRS/2020_12_02_9_35_TECHNICAL.md", "template": "unknown", "status": null, "firstCommit": "2020-12-02T08:40:05Z", "lastCommit": "2020-12-02T08:40:05Z", "numberOfCommits": 1, "title": "Context", "wordCount": 52, "authors": {"name1": 1}, "content": "### Context\nThis is the beginning on the project and for the moment all the commit are done on the master branch\n\n### Decision\n\nUse Gitflow to handle code source and project management.\n\n### Consequences\n\nInstall the gitflow plugin, explain how to contribute to the project with gitflow in the how to contribute documentation file. \n\n\n"}
{"repositoryUrl": "https://github.com/contiamo/operational-visualizations.git", "path": "docs/adr/0001-about-cursors.md", "template": "Nygard", "status": "2019-07-08 proposed", "firstCommit": "2019-07-09T11:56:28Z", "lastCommit": "2019-08-01T09:56:25Z", "numberOfCommits": 3, "title": "1. about-cursors", "wordCount": 327, "authors": {"name1": 2, "name2": 1}, "content": "# 1. about-cursors\n\nDate: 2019-07-08\n\n## Status\n\n2019-07-08 proposed\n\n## Context\n\nIn the grid in cell function we recieve `FragmentFrame` which is vertical slice of original `DataFrame` and then when we render visualaisation we iterate through raw data with `mapRows` method of `IterableFrame` and then we need to access some exact value in raw row. One of the ideas on how to do this universally was to use some kind of **cursor**.\n\nCurrent implementation of **cursor** looks like this\n\n```tsx\nexport interface ColumnCursor<Name extends string, ValueInRawRow = any> {\n  (row: RowCursor): ValueInRawRow;\n  column: Name;\n  index: number;\n}\n```\n\nso we can access value like this `cursor(row)` or more hacky and optimised way `row[cursor.index]`. Second option is fragile, in case we would want to change implementation of `row` (for example, from row-oriented storage to column-orinted) it will brake in many places (at least we have types, which will make this refactoring easier).\n\nCursor at the moment can be recieved from \"root\" `DataFrame`, which we think of as source of all derivative `PivotFrame`s and `FragmentFrame`s. Cursors are cached for referential transparency, so if they got passed down to React components, they will not trigger rerender.\n\n**Question** raised in [one of PR](https://github.com/contiamo/operational-visualizations/pull/70/files#diff-2044c7c7ba6c3fbf04dd49cf3cfa68b9R38): Shall we provide `getCursor` functionality in `FragmentFrame`? From one side it may make code simpler, on the other side we need to make sure that `FragmentFrame`s are referentially transperent (which is not the case, for example, for `cell` method) and it will create a lot of copies of the same cursors. We can as well pass reference to \"root\" `DataFrame` along all derivative structures and \"proxy\" `getCursor` method call to it.\n\n## Decision\n\nAt the moment we decided to use proposed version of cursors and adjust it in the future depending if we like DX or not. We will provide `getCursor` method only for `DataFrame` for now, but we can chnage this in the future.\n\n## Consequences\n\nThis decision mainly affects DX. And it is hard to foresee, without trying out in real project\n"}
{"repositoryUrl": "https://github.com/psu-libraries/scholarsphere.git", "path": "doc/architecture/decisions/0001-record-architecture-decisions.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-11-04T20:11:06Z", "lastCommit": "2019-11-04T20:11:06Z", "numberOfCommits": 1, "title": "1. Record architecture decisions", "wordCount": 45, "authors": {"name1": 1}, "content": "# 1. Record architecture decisions\n\nDate: 2019-11-04\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to record the architectural decisions made on this project.\n\n## Decision\n\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n\n## Consequences\n\nSee Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's [adr-tools](https://github.com/npryce/adr-tools).\n"}
{"repositoryUrl": "https://github.com/alphagov/content-data-api.git", "path": "docs/arch/adr-007-etl-publishing-api-content-store.md", "template": "unknown", "status": null, "firstCommit": "2021-03-25T09:10:12Z", "lastCommit": "2021-03-25T09:16:46Z", "numberOfCommits": 2, "title": "ADR 007: ETL to populate Content Items' dimension.", "wordCount": 212, "authors": {"name1": 1, "name2": 1}, "content": "# ADR 007: ETL to populate Content Items' dimension.\n\n26-01-2018\n\n## Context\n\nAs [per this Trello card][1], we want to populate the Content Items' dimension with the latest changes that result of editing content.\n   \n \n## Decision\n\nAddressing the ETL process for Content Items this way:\n\n1. The first time, we load all the content items via [publishing-api][3]. We retrieve all the content-ids and paths of all the content items that are live.\n2. For each Content Item we will get the JSON from the [Content Store][2], and we will extract the attributes that we need to fulfil the immediate needs. We will also persist all the JSON to be able to extract other attributes in the future.\n3. On a daily basis, we will be listening to [publishing-api][3] events, in the same way that [Rummager][4] or [Email Alert Service][5] do. Once we receive a change for the content item, we will automatically update the content items dimension with the new approach.\n\n### Benefits:\n\n1. This is more aligned with GOV.UK architecture.\n1. This is very light and efficient. It also embrace simple code as the ETL process for Content Items is almost trivial.\n\n## Status\n\nAccepted.\n\n[1]: https://trello.com/c/zqcU0x3s/28-3-content-items-find-source-for-content-items\n[2]: http://github.com/alphagov/content-store\n[3]: http://github.com/alphagov/publishing-api\n[4]: http://github.com/alphagov/rummager\n[5]: https://github.com/alphagov/email-alert-service\n"}
{"repositoryUrl": "https://github.com/mahanhz/clean-architecture-example.git", "path": "doc/adr/0003-use-spring-framework.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-07-30T16:13:08Z", "lastCommit": "2017-08-01T16:37:44Z", "numberOfCommits": 3, "title": "3. Use Spring framework", "wordCount": 68, "authors": {"name1": 3}, "content": "# 3. Use Spring framework\n\nDate: 20-July-2017\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to build the how part of the application\n\n## Decision\n\nWe will use Springs ecosystem to implement how our application works.\n\nMore explicitly we will use Spring Boot, Spring Data and Spring WebMvc.\n\n## Consequences\n\nSince developers are already familiar with the technology and like ot use it, there is no delay in being productive.\n\n## Made by\n\nMahan Hashemizadeh"}
{"repositoryUrl": "https://github.com/miquecg/heroes-board-game.git", "path": "docs/adr/0003-broadcast-messages.md", "template": "Madr", "status": null, "firstCommit": "2020-05-04T10:38:16Z", "lastCommit": "2020-05-04T10:38:16Z", "numberOfCommits": 1, "title": "Broadcast Messages", "wordCount": 206, "authors": {"name1": 1}, "content": "# Broadcast Messages\n\n## Context and Problem Statement\n\nHeroes can kill nearby enemies on a certain range, but there is no central place in the game that knows every hero position on the board at any time. Each one of them is the source of truth.\nIn order to do that a hero must broadcast the intent to the rest of players.\nHow to send messages to all heroes at once?\n\n## Decision Drivers\n\n* Less possible changes\n* Performance is not a concern\n\n## Considered Options\n\n* `:pg2` - Implement broadcast on top of an `:all_heroes` group\n* Registry for PubSub\n* Manager GenServer - Stores and monitors heroes PID\n* `Supervisor.which_children/1`\n\n## Decision Outcome\n\nChosen option: \"`Supervisor.which_children/1`\", because it comes out best.\n\n## Pros and Cons of the Options\n\n### `:pg2`\n\n* Good, because it's designed for such use cases\n* Good, because it's optimized for speed\n* Bad, because it's an extra application in the release\n\n### Registry for PubSub\n\nExample: https://hexdocs.pm/elixir/Registry.html#module-using-as-a-pubsub\n\n* Good, because it allows multiple use cases\n* Bad, because of some extra complexity\n\n### Manager GenServer\n\n* Good, because it's simple and focused\n* Bad, because it reimplements existing solutions\n\n### `Supervisor.which_children/1`\n\nDocs: https://hexdocs.pm/elixir/Supervisor.html#which_children/1\n\n* Good, because it can be done with existing parts of the system\n* Bad, because of uncertainties if system was to be scaled\n"}
{"repositoryUrl": "https://github.com/guttih/island.is-glosur.git", "path": "docs/adr/0001-use-nx.md", "template": "Madr", "status": "accepted", "firstCommit": "2020-09-04T09:55:11Z", "lastCommit": "2020-09-04T09:55:11Z", "numberOfCommits": 1, "title": "Use NX", "wordCount": 307, "authors": {"name1": 1}, "content": "# Use NX\n\n- Status: accepted\n- Deciders: devs\n- Date: 03.05.2020\n\n## Context and Problem Statement\n\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI/CD.\n\n## Decision Drivers\n\n* Low complexity and overhead in development.\n* Fit for our stack.\n* Optimize CI/CD with dependency graphs and/or caching.\n* Flexible.\n\n## Considered Options\n\n* [Bazel]\n* [Nx]\n* [Lerna]\n\n## Decision Outcome\n\nChosen option: \"Nx\", because:\n\n* It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\n* It's relatively easy to learn with focused documentation.\n* It has schematics to generate apps, libraries and components that includes all of our tools.\n* It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\n\n## Pros and Cons of the Options\n\n### [Bazel]\n\n* Good, because it's created and maintained by Google\n* Good, because it supports multiple programming languages and platforms.\n* Bad, because it's difficult to learn, with a custom BUILD files.\n* Bad, because JS support is hacky.\n\n### [Nx]\n\n* Good, because it has built in support for our stack.\n* Good, because it's been around for a while, originating in the Angular world.\n* Good, because it's designed with best practices for large scale web applications.\n* Good, because it supports elaborate code generation.\n* Good, because it helps optimise CI/CD for large projects.\n* Bad, because it's fairly opinionated.\n* Bad, because it's an open source project maintained by an agency.\n\n### [Lerna]\n\n* Good, because it integrates with NPM and Yarn.\n* Good, because it's used by a lot of open source projects.\n* Bad, because it's primarily designed for managing and publishing open source projects, not building and deploying large scale applications.\n\n## Links\n\n* [Why you should switch from Lerna to Nx](https://blog.nrwl.io/why-you-should-switch-from-lerna-to-nx-463bcaf6821)\n\n[Pants]: https://www.pantsbuild.org/\n[Bazel]: https://bazel.build/\n[Nx]: https://nx.dev/\n[Lerna]: https://lerna.js.org/\n"}
{"repositoryUrl": "https://github.com/ebi-uniprot/uniprot-rest-api.git", "path": "doc/architecture/decisions/0003-spring-framework.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-03-08T16:50:33Z", "lastCommit": "2019-03-08T16:50:33Z", "numberOfCommits": 1, "title": "3. Spring Framework", "wordCount": 100, "authors": {"name1": 1}, "content": "# 3. Spring Framework\n\nDate: 2018-08-02\n\n## Status\n\nAccepted\n\n## Context\n\nProgramming frameworks can promote the productivity of a project; producing smaller code bases, added reliability,\nadditional features (than one would otherwise write themselves), etc. \n\n## Decision\n\nWe have used the [Spring framework](https://spring.io/) within the team for a number of years, and its recent advances in the domain of REST applications makes it an ideal choice.\n\n## Consequences\n\nAs stated by Spring themselves, they are an opinionated framework that guides the design of certain aspects of a codebase.\nHowever, these 'opinions' are generally there for the benefit of a project, making it possible to achieve your goal.\n"}
{"repositoryUrl": "https://github.com/alphagov/content-data-api.git", "path": "docs/arch/adr-008-focus-on-english-content.md", "template": "unknown", "status": null, "firstCommit": "2021-03-25T09:10:12Z", "lastCommit": "2021-03-25T09:16:46Z", "numberOfCommits": 2, "title": "ADR 008: Focus on english content in the first iterations.", "wordCount": 93, "authors": {"name1": 1, "name2": 1}, "content": "# ADR 008: Focus on english content in the first iterations.\n\n28-01-2018\n\n## Context\n\nSome Content Items are written in different languages, so [Publishing-api][1] will return the `content_id` along with all locales assigned to the Content Item.  \n\n## Decision\n\nFocus on content written in English.\n\nThe main reason is that we would need different algorithms and libraries to make our application consistent among all the languages / locales. \nIf this is a real need, we will support it in future iterations of the Data Warehouse.\n\n### Benefits:\n\nThis makes the codebase simpler.   \n\n## Status\n\nAccepted.\n\n[1]: http://github.com/alphagov/publishing-api\n"}
{"repositoryUrl": "https://github.com/alphagov/email-alert-api.git", "path": "docs/adr/adr-005-record-architecture-decisions.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-10-22T07:56:12Z", "lastCommit": "2020-10-22T07:56:12Z", "numberOfCommits": 1, "title": "5. Record architecture decisions", "wordCount": 118, "authors": {"name1": 1}, "content": "# 5. Record architecture decisions\n\nDate: 2020-10-21\n\n## Context\n\nWhile we have already recorded architectural decisions for this project, there is no guidance to follow for writing new ones. Recently we found that creating a new ADR was [hindered](https://github.com/alphagov/email-alert-api/pull/1441#discussion_r508729384) by wanting to be consistent with the structure of previous ADRs.\n\n## Decision\n\nWe will continue to use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions.\n\nWe will adopt a structure for future ADRs that matches this document - the exemplar in [our Rails app conventions](https://docs.publishing.service.gov.uk/manual/conventions-for-rails-applications.html#documenting-your-decisions). We will not change how the files are named, to avoid breaking links that were never expected to change.\n\n## Status\n\nAccepted\n\n## Consequences\n\nADRs preceding this one will have an inconsistent format to the ones that follow this.\n"}
{"repositoryUrl": "https://github.com/globtec/phpadr.git", "path": "docs/arch/0001-documenting-architecture-decisions.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-02-23T07:41:04Z", "lastCommit": "2018-02-23T07:41:04Z", "numberOfCommits": 1, "title": "1. Documenting architecture decisions", "wordCount": 61, "authors": {"name1": 1}, "content": "# 1. Documenting architecture decisions\n\nDate: 2018-02-11\n\n## Status\n\nAccepted\n\n## Context\n\nRecord certain design decisions for the benefit of future team members as well as for external oversight.\n\n## Decision\n\nUse Architecture Decision Records (ADR), that is a technique for capturing important architectural decisions, along with their context and consequences as described by [Michael Nygard](https://twitter.com/mtnygard) in his article: [Documenting Architecture Decisions](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n\n## Consequences\n\nSee Michael Nygard's article, linked above."}
{"repositoryUrl": "https://github.com/ministryofjustice/hmpps-interventions-ui.git", "path": "doc/architecture/decisions/0012-use-feature-flags-for-in-progress-features.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2021-09-20T10:49:51Z", "lastCommit": "2021-11-30T09:34:27Z", "numberOfCommits": 2, "title": "12. Use feature flags for in-progress features", "wordCount": 457, "authors": {"name1": 1, "name2": 1}, "content": "# 12. Use feature flags for in-progress features\n\nDate: 2021-09-13\n\n## Status\n\nAccepted\n\n## Context\n\nThere are a few reasons we might not want to put a new feature in front of users as soon as it's been merged into the `main` branch:\n\n- The API might not be up-to-date with the latest version of the UI (and vice versa) - because we're building the UI and API side of the service independently, there are times when the two may be out of sync: an endpoint may not yet be providing all the data we need; the backend functionality may not be finished at the time of writing the UI code.\n- We want to satisfy the Pact contracts between the two sides of the service but not use the new data structure until the UI has been updated.\n- The new functionality may need to be further tested (either with users by developers) and iterated upon before release.\n- We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes - this means we'd want to merge smaller chunks of work at a time, which might not be ready for users.\n- We want to test interactions between systems (e.g. the Community API) on the Development environment but not release these changes to the public.\n\n## Decision\n\nAny features or behaviour that isn't ready to be interacted with by users will be placed behind a config-based feature flag, configured in `server/config.ts`, e.g. as below:\n```\nfeatures: {\n  previouslyApprovedActionPlans: get('FEATURE_PREVIOUSLY_APPROVED_ACTION_PLANS', 'false') === 'true',\n}\n```\n\nThis can then be turned on for each environment by adding the environment variable (e.g. `FEATURE_PREVIOUSLY_APPROVED_ACTION_PLANS`) to the intended environment.\n\nWe'll usually want to enable this for the development environment and test environment, possibly the pre-prod environment but not the production environment.\n\nBefore deploying the changes to the production environment, it's a good idea to double check the configuration is as expected e.g. by checking it's hidden in the pre-production environment.\n\nOnce the feature is ready to be interacted with by users, we'll remove the feature flag from the UI configuration.\n\n## Consequences\n\n- Pull requests can be kept small and self-contained, as we don't need to release a whole feature at once, and there's no risk of it being visible to users.\n- We can test functionality on the development environment without these changes being deployed to production.\n- We can release changes to the contracts between the UI and API but keep existing functionality working until both are in sync.\n- We can test functionality in User Research sessions without it being on the production environment.\n\nThere's a small overhead involved in setting up the config and testing it's visible or not on the desired environment.\n"}
{"repositoryUrl": "https://github.com/ASethi93/james.git", "path": "src/adr/0002-make-taskmanager-distributed.md", "template": "Nygard", "status": "Accepted (lazy consensus)", "firstCommit": "2020-12-10T20:05:48Z", "lastCommit": "2020-12-10T20:05:48Z", "numberOfCommits": 1, "title": "2. Make TaskManager Distributed", "wordCount": 136, "authors": {"name1": 1}, "content": "# 2. Make TaskManager Distributed\n\nDate: 2019-10-02\n\n## Status\n\nAccepted (lazy consensus)\n\n## Context\n\nIn order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.\n\nCurrently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level.\nTasks are scheduled and ran on the same node they are scheduled.\n\nWe are also unable to list or access to the details of all the `Task`s of a cluster.\n\n## Decision\n\nCreate a distribution-aware implementation of `TaskManager`.\n\n## Consequences\n\n * Split the `TaskManager` part dealing with the coordination (`Task` management and view) and the `Task` execution (located in `TaskManagerWorker`)\n * The distributed `TaskManager` will rely on RabbitMQ to coordinate and the event system to synchronize states\n"}
{"repositoryUrl": "https://github.com/scheduleonce/once-ui.git", "path": "docs/adr/0005-publish-the-library-on-npm.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-12-21T21:23:55Z", "lastCommit": "2020-03-19T08:35:44Z", "numberOfCommits": 3, "title": "5. Publish the library on npm", "wordCount": 55, "authors": {"name1": 2, "name2": 1}, "content": "# 5. Publish the library on npm\n\nDate: 2018-12-21\n\n## Status\n\nAccepted\n\n## Context\n\nNeed to find a way for developers to import this library into their own projects.\n\n## Decision\n\nWe will use npm to publish the library privately in [MyGet](https://www.myget.org) (a 3rd party npm compatible registry)\n\n## Consequences\n\nImporting the library is as easy as running `npm install @oncehub/ui`\n"}
{"repositoryUrl": "https://github.com/dante-ev/docker-texlive.git", "path": "docs/adr/0002-provide-all-packages.md", "template": "Madr", "status": null, "firstCommit": "2019-09-25T19:47:38Z", "lastCommit": "2019-09-25T21:42:27Z", "numberOfCommits": 2, "title": "Provide all packages", "wordCount": 70, "authors": {"name1": 2}, "content": "# Provide all packages\n\n## Context and Problem Statement\n\nShould the Docker image include all packages or a subset of the packages?\n\n## Considered Options\n\n* Provide all packages\n* Provide a subset of packages\n* Provide a minimal set of packages and use [texliveonfly](https://ctan.org/pkg/texliveonfly)\n\n## Decision Outcome\n\nChosen option: \"Provide all packages\", because \n\n* texliveonfly does not work on all packages\n* speeds-up compilation time (because no additional download)\n\nWe accept that the final image is ~2GB of size.\n"}
{"repositoryUrl": "https://github.com/avniproject/openchs-adr.git", "path": "decisions/0012-create-a-generic-relationship-framework-to-link-between-mother-and-child.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-05-28T09:03:57Z", "lastCommit": "2018-05-28T09:03:57Z", "numberOfCommits": 1, "title": "12. Create a generic relationship framework to link between mother and child", "wordCount": 228, "authors": {"name1": 1}, "content": "# 12. Create a generic relationship framework to link between mother and child\n\nDate: 2018-05-28\n\n## Status\n\nAccepted\n\n## Context\n\nDuring a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy. \n\nAt the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship. \n\nWe need the modeling of a relationship to be a generic structure that can support both these use cases. \n\n## Decision\n\nCreate an option to map relationships between individuals, with relationship being a concept orthogonal to families. Relationships between individuals can be anything (family relationships, or even relationships to ASHA worker etc if required). \n\nRelationships will be two-way with different values between the two. We will not build (atleast for now) the ability to automatically deduce transitive relationships. \n\n## Consequences\n\nSupport for family can be turned off if it is not required for an organisation. \nDevelopment for the family feature and mother-child linking can happen in parallel without dependencies. \nThere will be individuals that are related, but are not part of the same family. It is possible that some of them are not part of any family. The user is expected to do this manually. \n"}
{"repositoryUrl": "https://github.com/openmrs/openmrs-rfc-frontend.git", "path": "text/0030-link-system.md", "template": "unknown", "status": null, "firstCommit": "2021-11-09T21:32:33Z", "lastCommit": "2021-11-09T21:32:33Z", "numberOfCommits": 1, "title": "Link System", "wordCount": 643, "authors": {"name1": 1}, "content": "# Link System\n- Start Date: 2021/10/21\n- RFC PR: https://github.com/openmrs/openmrs-rfc-frontend/pull/30\n\n## Decision, including impact on distributions\n\nWe will extend the extension system to have support specific to links.\n\nWe will allow a new property of the `setupOpenMRS` return object, called `links`.\nIt will be an array of objects which contain\n- `id`: Identifies the link\n- `label`: The text that should appear on the link\n- `to` (optional): The target path or URL. Can include `${variables}` which will be interpolated from\n`openmrsBase`, `openmrsSpaBase`, or the extension props\n- `onClick` (optional): A click handler. Receives extension props as its second argument\n- `menu` or `menus` (optional): Names of menus to attach the link to\n- `offline`, `online`, and `meta` (optional): As in extensions.\n\n```ts\nfunction setupOpenMRS() {\n  return {\n  pages: [],\n  extensions: [],\n  links: [{\n  id: \"Patient list link\", \n  label: () => t(\"patientList\", \"Patient List\"),\n  to: \"${openmrsSpaBase}/patient-list\",\n  onClick: (e) => { console.log(\"Clicked button \" + e.button); }\n  menu: 'App menu',\n  }]\n  }\n}\n```\n\nWe will define a new React component, `Menu` (along with\nframework-independent functions that would support it). `Menu` would\nwork exactly like `ExtensionSlot`, but only for Links.\n\n```ts\nfunction Menu(props: { name: string, state?: object });\n```\n\nWe will extend the configuration system so that every Menu supports\nthe same configuration options as ExtensionSlots.\n\nThe `configure` value will support only the key `openInNewTab`.\n`openInNewTab` will default to `true` for external links, `false` otherwise.\nWhen `true`, it will add `target=\"_blank\"` to the link. If the link is an\nexternal link, it will also add `rel=\"noopener noreferrer\"`.\n\nMenu configuration will also support one additional key, `links`, which would\nallow an implementer to add arbitrary links to the Menu:\n\n```json\n{\n  \"@openmrs/esm-primary-navigation-app\": {\n  \"menus\": {\n  \"App menu\": {\n  \"add\": [\"Patient list link\"],\n  \"remove\": [\"Provider management link\"],\n  \"order\": [\"Home page link\", \"Patient list link\"],\n  \"links\": [{\n  \"label\": \"Home\",\n  \"to\": \"${openmrsSpaBase}/home\"\n  }],\n  \"configure\": {\n  \"Patient list link\": {\n  \"openInNewTab\": true\n  }\n  }\n  }\n  }\n  }\n}\n```\n\nFor links created using `links`, the link label is the ID.\n\n## Reason for decision\n\n- We have a proliferation of trivial \"link\" extensions, which all share\n  approximately the same simple structure.\n- Link styling should be left up to the slot/menu. This enforces that\n  links are not bringing their own CSS classes.\n- We already have an extension system wiring things together, so this\n  won't add much complexity to it.\n- All menu-like UI elements should be configurable. At present, developers\n  add configurability to menus in various ad-hoc ways, or they leave the\n  menu unconfigurable.\n\n## Alternatives\n\n- Continue making link extensions and solving the configurability problem\n  each time.\n- To solve the configurability problem, you could create a feature in\n  a module (or a new module) where you can create new\n  link-like extensions via the config schema. In the config, give it a\n  label, a target, and a slot (or slots) to attach to, and it will create\n  the link as an extension and attach it to that slot. This solves the\n  configurability problem for menu-like slots. It does not, however,\n  create a common abstraction for defining link-like extensions in code.\n  It also would be defining new extensions based on the config, which\n  would create interdependency between config and extensions in the load\n  process. This may have consequences for performance.\n- To create a common abstraction for defining link-like extensions in code,\n  you could simply define a generic link-like extension generator in\n  esm-framework, which produces the expected type of extension.\n- Another solution to solve the configurability problem would be to create\n  some abstraction for use at the slot level, which both generates part of a\n  config schema, and wraps the slot. This would probably not be very elegant,\n  and config schema generation is a kind of abstraction and complexity that\n  we have not yet broached.\n\n## Common practices (not enforced)\n\nIn the patient chart and the offline tools, we create pages and links together\nas extensions. The link is the extension component, and the page is created\nusing the extension meta. In this paradigm, those extensions would use the\n`createLinkExtension` function to create their link components, and would\notherwise be unaffected.\n"}
{"repositoryUrl": "https://github.com/wangyyovo/oklog.git", "path": "doc/arch/adr-001-multitenancy.md", "template": "Nygard", "status": "April 3, 2018: Accepted.", "firstCommit": "2020-11-12T13:30:04Z", "lastCommit": "2020-11-12T13:30:04Z", "numberOfCommits": 1, "title": "ADR 1: Multitenancy", "wordCount": 215, "authors": {"name1": 1}, "content": "# ADR 1: Multitenancy\r\n\r\nWe propose to add features OK Log to make it suitable for multi-tenant\r\nenvironments.\r\n\r\n## Context\r\n\r\nSystem operators often run shared infrastructure for different workloads. For\r\nexample, a single Kubernetes cluster may serve multiple departments in an\r\norganization, none of whom should need to know about the others.\r\n\r\nOK Log as it exists today (v0.3.2) assumes all users (both producers and\r\nconsumers) are part of the same global namespace, and provides no way to\r\nsegment ingestion or querying.\r\n\r\nWe have at least one interesting use case, with one potential user, where\r\nmulti-tenanancy would be a requirement.\r\n\r\n## Decision\r\n\r\nMotivated by this new use case, we judge that adding multi-tenant features, in\r\naddition to a handful of other longstanding feature requests, would push OK Log\r\nin a useful direction.\r\n\r\nThe initial set of issues include:\r\n\r\n- [Add first-class concept of topics](https://github.com/oklog/oklog/issues/113)\r\n- Separate indexer layer for faster queries\r\n- [Move to length-delimited records](https://github.com/oklog/oklog/issues/112)\r\n- [Extend record identifier](https://github.com/oklog/oklog/issues/114)\r\n- [Long-term storage](https://github.com/oklog/oklog/issues/115)\r\n\r\nAdditional issues may be filed, and these issues may be refactored or dropped\r\noutright depending on the result of experimentation.\r\n\r\n## Status\r\n\r\nApril 3, 2018: Accepted.\r\n\r\n## Consequences\r\n\r\nWe hope adding these features will make OK Log more broadly useful, even in\r\nnon-multi-tenant environments. However, some features will likely introduce\r\nincompatibility between older and newer versions of OK Log, making in-place\r\nupgrades difficult or impossible. We will mark any such changes with a major\r\nversion bump.\r\n"}
{"repositoryUrl": "https://github.com/guardian/dotcom-rendering.git", "path": "dotcom-rendering/docs/architecture/004-emotion.md", "template": "unknown", "status": null, "firstCommit": "2021-08-17T09:46:02Z", "lastCommit": "2021-08-17T10:02:57Z", "numberOfCommits": 2, "title": "Emotion", "wordCount": 104, "authors": {"name1": 1, "name2": 1}, "content": "# Emotion\n\n## Context\n\nUsing a CSS-in-JS library will allow us to scope styles to a component. Popular CSS-in-JS libraries include Styled Components, Emotion and Styletron.\n\nStyletron generates atomic CSS, guaranteeing the smallest possible CSS output. However, it does not support language features that cannot be atomised, such as nesting and sibling selectors.\n\nStyled Components and Emotion support more complex features and have a similar API (composing styled components using the `styled()` method). Emotion additionally supports adding styles to the `css` prop on React components. It is also smaller (5.66KB Gzipped vs 16.27KB for styled-components).\n\n## Decision\n\nWe will use Emotion as our CSS-in-JS library.\n\n## Status\n\nApproved\n"}
{"repositoryUrl": "https://github.com/jmoratilla/devops-challenge.git", "path": "doc/adr/0010-feat-ci-cd-with-circleci.md", "template": "Nygard", "status": "Draft", "firstCommit": "2020-02-25T17:57:50Z", "lastCommit": "2020-02-25T17:57:50Z", "numberOfCommits": 1, "title": "10. feat-ci-cd-with-circleci", "wordCount": 236, "authors": {"name1": 1}, "content": "# 10. feat-ci-cd-with-circleci\n\n# 8. feat-about-cicd\n\nDate: 2020-02-20\n\n## Status\n\nDraft\n\n## Context\n\nTime to work on the CI/CD solution.\n\nI need a job manager to build, test and deploy the apps to the kubernetes\n cluster.\n\nI know there is a lot of documentation about jenkins, but I have been working\nin the last years with other solutions like:\n\n* SolanoCI (now closed)\n* CircleCI\n\nSo I need to see if there is a way to use my knowledge in CircleCI or not.\n  Besides, CircleCI has a free plan very useful for testing.\n\nI know there is a jenkins-x product, but I don't have a clear idea about\n it.  I installed it and spent couple of hours to make it work (jx) but I got\n an error downloading kops, and I couldn't go further. \n\n\n## Decision\n\nTry first with CircleCI to see if I can deploy the apps to kubernetes.\n\nAs I'm using a monorepo, all the apps are within the same repository, so we\n cannot separate the building process of each microservice.  All them will be\n build and deployed as one.\n\nBut if nothing has change in the app, then the building process will be faster.\n\nTesting will be peformed on all apps secuentially, and some tests could be\n performed on all services without implementing mockups.\n\nThis way, when a event is received by the CI, a script will be executed to \n execute the actions on each app.\n\n \n\n## Consequences\n\n"}
{"repositoryUrl": "https://github.com/jdanil/skunkworks.git", "path": "docs/decision-log/fonts.md", "template": "unknown", "status": null, "firstCommit": "2020-01-16T00:54:22Z", "lastCommit": "2021-02-13T13:18:39Z", "numberOfCommits": 2, "title": "Fonts", "wordCount": 15, "authors": {"name1": 1, "name2": 1}, "content": "# Fonts\n\n## Decision\n\nPrefer system fonts, otherwise use variable fonts.\n\n```css\n@font-face {\n  font-display: fallback;\n}\n```\n\n## Rationale\n\n## Resources\n\n- [Controlling Font Performance with Font Display](https://developers.google.com/web/updates/2016/02/font-display)\n"}
{"repositoryUrl": "https://github.com/open-apparel-registry/open-apparel-registry.git", "path": "doc/arch/adr-002-decide-how-to-display-more-facilities.md", "template": "unknown", "status": null, "firstCommit": "2019-08-12T15:04:59Z", "lastCommit": "2019-08-12T15:04:59Z", "numberOfCommits": 1, "title": "Determine How to Display All Facilities on the Map", "wordCount": 1663, "authors": {"name1": 1}, "content": "# Determine How to Display All Facilities on the Map\n\n## Context\n\nThe Open Apparel Registry currently includes more than 18,000 facilities. For\nperformance reasons, we have paginated the facilities data API endpoint data so\nthat it will [return a maximum of 500 results][pagination-pr] for any single\nrequest. In turn this means that the frontend client will only ever display a\nmaximum of 500 facilities at a time, rendered as clustered Leaflet markers via\nReact-Leaflet. Facilities API requests are currently filtered using Django\nquerysets whose inputs are querystring parameters included in the API requests.\n\nTo enable users to view all of the OAR's facilities on the map simultaneously,\nwe'll need to update how the API returns facilities for display and how the\nclient renders them on the map. At present this means updating the application\nso that it can display 18,000+ facilities simultaneously. Following upcoming MSI\nintegration work, we anticipate that the number of OAR facilities will increase\nto around 100,000 -- which the application should be able to map. In addition,\nwe also want users to be able to filter these vector tiles by query parameters\nlike contributor, facility name, and country, along with the map bounding box.\n\nTo accomplish this we have decided to use vector tiles generated, ultimately,\nby PostGIS's [`ST_AsMVT`][st-asmvt] function, rendering them in the frontend\nwith [Leaflet Vector Grid][leaflet-vector-grid] (possibly via\n[react-leaflet-vector-grid][react-leaflet-vector-grid]). We've decided to have\nthe vector tiles cluster facilities by zoom level, which would limit the number\nof actual points the frontend needs to display at any given time.\n\nThis ADR documents a subsequent decision between setting up a dedicated\n`ST_AsMVT`-based vector tile server, like [Martin][martin] or adding a new\nvector tile endpoint to the existing Django web application which would make\nthe `ST_AsMVT` query.\n\n## Four Rejected Options\n\nBefore landing on an `ST_AsMVT`-based solution, we did consider a few other\noptions which we ultimately rejected outright:\n\n### Reusing Existing /facilities API Endpoint\n\nIn theory we could remove the `MAX_PAGE_SIZE` limit on the `/facilities` API\nendpoint. In practice this would cause performance problems as the size of the\nGeoJSON response -- and the number of Leaflet markers -- increased. The web app\nwould have to serialize tens of thousands of markers for each request, the\nGeoJSON payload for each request could be several megabytes in size, and the\nclient would have to put tens of thousands of Leaflet markers in browser\nmemory.\n\n### Using Windshaft\n\nWhile we could potentially use a combination of [Windshaft][windshaft] and\n[Leaflet.utfgrid][leaflet-utfgrid] to render facilities, there wasn't much\nenthusiasm for setting up and maintaining a Windshaft tiler for a few specific\nreasons:\n\n- using Windshaft requires adding another service\n- Windshaft is pretty costly to configure and maintain\n- Windshaft's documentation isn't great\n\n### Creating Static Vector Tiles\n\nWe ruled out the idea of creating a static set of vector tiles because the\nOAR's facilities data changes frequently.\n\n### Using a Lambda Function Tiler\n\nAzavea has undertaken some research work to determine the viability of using\na tiler based on a Lambda function which can connect to PostGIS and call\n`ST_AsMVT`. However, the research has discovered a few limits on this approach,\nsuch as dealing with function warmup times and handling concurrent database\nconnections.\n\n## Two `ST_AsMVT`-Based Approaches\n\nAn `ST_AsMVT`-based approach to generate vector tiles dynamically seemed to be\npromising. While the vector tiles working group's report did note some\nuncertainty around how performant it would be to generate tiles in PostGIS,\nOAR's traffic is such that it may not encounter performance problems which\ncould emerge for a higher traffic site.\n\nWe considered two ways to generate tiles using `ST_AsMVT`:\n\n- using a dedicated vector tile server like [Martin][martin], [t-rex][trex], or\n[tegola][tegola]\n- adding a vector tiles endpoint to the existing Django web app\n\n### Using Martin (or t-rex or Tegola) as a Vector Tile Server\n\nMartin, t-rex, and Tegola are open-source vector tile servers which can connect\ndirectly to PostGIS and render vector tiles. Judging by their documentation,\neach of them appear to be fairly straightforward to configure and operate. Each\nhas a slightly different API.\n\nWe considered Martin most seriously as an option in part because it had good\ndocumentation around how to write PL/pgSQL functions for requesting tiles with\ndata filtered by a set of query parameters. Here's [Martin's function sources example][martin-function-sources]:\n\n```plpgsql\nCREATE OR REPLACE FUNCTION public.function_source(z integer, x integer, y integer, query_params json) RETURNS BYTEA AS $$\nDECLARE\n  bounds GEOMETRY(POLYGON, 3857) := TileBBox(z, x, y, 3857);\n  mvt BYTEA;\nBEGIN\n  SELECT INTO mvt ST_AsMVT(tile, 'public.function_source', 4096, 'geom') FROM (\n  SELECT\n  ST_AsMVTGeom(geom, bounds, 4096, 64, true) AS geom\n  FROM public.table_source\n  WHERE geom && bounds\n  ) as tile WHERE geom IS NOT NULL;\n\n  RETURN mvt;\nEND\n$$ LANGUAGE plpgsql IMMUTABLE STRICT PARALLEL SAFE;\n```\n\n#### Pros\n\n##### Configuration\n\nMartin appears fairly straightforward to configure and its documentation\nencompassed most of what we'd want to do.\n\n##### Performance\n\nMartin touts being \"suitable for large databases\" which indicates that it\nmight obviate some of the performance concerns around using `ST_AsMVT`.\n\n#### Cons\n\n##### PL/pgSQL Function Sources\n\nSince Martin uses PL/pgSQL functions for its filtering, we would have to\nrewrite some facility filtering logic that currently exists in Django querysets\nin the web app to work in PL/pgSQL. Moreover, each time we added a new filter\nor search option to the web application, we'd have to write a version of the\nsame query in PL/pgSQL for the tiler.\n\n##### Security\n\nMartin's `query_params` appear to be passed in to the database as strings, which\nopens a security hole. While we could create a PostGIS role or user with a\nlimited, readonly set of permissions to use solely for the Martin instance,\ndoing so requires taking on some additional risk and complexity.\n\nLikewise, adding PostGIS-based security just for the tiler may also compel us to\nhave to figure out how to duplicate features like API key authentication or\nfacilities-data request logging -- which we've already written once in Django.\n\n##### Unfamiliarity\n\nWe don't have any experience running Martin in production. We've also got\nlimited experience using Rust, the language in which Martin is written.\nTogether this means a Martin-based tile server may be difficult to operate and\ndebug.\n\n### Adding a Vector Tile Endpoint to the Existing Django Web App\n\nAdding a vector tile endpoint to the existing web app seemed like a promising\napproach, since it would enable trying out `ST_AsMVT` while reusing the app's\ndatabase connection and Django's querysets for filtering. In this approach we\nwould add a `/tile/{layer}/{z}/{x}/{y}/` endpoint to the Django application,\nthen update the client to make tile requests there rather than rendering the\n`/facilities` GeoJSON response as Leaflet markers.\n\n#### Pros\n\n##### Provides Access to the Existing Django Queryset Apparatus\n\nWhile using Martin (or a similar solution) would compel writing new versions of\nthe facilities queries in PL/pgSQL, placing a vector tile endpoint in Django\nlets us reuse some of the existing query code and also provides access to\nDjango models and querysets. Likewise, we would not have to write new code for\nnew filter and search options in two different languages.\n\n##### Already Has a Secure Database Connection\n\nThe Django web application already has a secure database connection, so we\nwould not have to create a solution for securing Martin or another PostGIS-backed\ntile server.\n\n##### Enables Switching from `ST_AsMVT` to Another Python Vector Tile Option\n\nThere remains some question about the viability of using `ST_AsMVT`. If it turns\nout that this is not a performant solution, having the tile endpoint in Django\nmakes it possible to drop out of using `ST_AsMVT` altogether and to switch to\nan alternate Python library for generating vector tiles.\n\n##### Doesn't Require Creating & Deploying a Different Service\n\nAdding Martin or another vector tile server would increase the number of\ndifferent kinds of services running as part of the OAR, which adds to the\napplication's complexity. Keeping the tile endpoint in Django does not require\nadding a new service.\n\n##### Allows Scaling by Increasing the Number of App Instances\n\nAdding a tile endpoint to the Django app also enables continuing to scale the\napplication in the usual way: by increasing the number of app instances\navailable to serve requests.\n\n#### Cons\n\n##### Mingles Tile Request Traffic with Other App Traffic\n\nThe biggest downside of adding a vector tile endpoint to the Django app is\nthat it would mean mingling tile request traffic with other app traffic.\nWhile we plan to limit the size of tile request responses by clusting facilities\nat different zoom levels, tile request traffic will likely be more frequent and\nsustained than requests to the current `/facilities` endpoint.\n\n## Decision\n\nWe have decided to add a vector tile endpoint to the existing Django app.\n\nWhile Martin, in particular, seemed like a compelling solution, we had enough\nopen questions about it to discourage us from taking on the complexity of\nusing it here.\n\nOur main apprehension about adding a tile endpoint to the existing web app is\nthat it'll mingle tile requests with other requests in a way that could cause\nperformance problems. However, given the size of the OAR's traffic and the\npossibility of addressing traffic increases by scaling the number of app\ninstances, this seemed like an acceptable tradeoff.\n\n## Consequences\n\nAs a consequence of this decision, we will need to:\n\n- add a new `/tile` endpoint to the API.\n- determine an aggregation strategy for clustering facilities at\ndifferent zoom levels\n- adjust the Leaflet map to use this tile endpoint and determine symbology\n- make necessary adjustments to the frontend for sending selected filters\nand searches to the `/tile` endpoint.\n- determine whether to adjust the Gunicorn configuration to change the number of\nworkers per instance or the worker type\n- decide whether to add caching HTTP headers to `/tile` and to replicate any\nchanges in CloudFront\n\n[pagination-pr]: https://github.com/open-apparel-registry/open-apparel-registry/pull/509\n[st-asmvt]: https://postgis.net/docs/ST_AsMVT.html\n[leaflet-vector-grid]: https://github.com/Leaflet/Leaflet.VectorGrid\n[react-leaflet-vector-grid]: https://github.com/mhasbie/react-leaflet-vectorgrid\n[windshaft]: https://github.com/CartoDB/Windshaft\n[leaflet-utfgrid]: https://github.com/danzel/Leaflet.utfgrid\n[trex]: https://github.com/t-rex-tileserver/t-rex\n[tegola]: https://tegola.io/\n[martin]: https://github.com/urbica/martin\n[martin-function-sources]: https://github.com/urbica/martin#function-sources\n"}
{"repositoryUrl": "https://github.com/joejag/wikiindex.git", "path": "doc/arch/adr-003-testing_library.md", "template": "unknown", "status": null, "firstCommit": "2015-02-28T15:44:23Z", "lastCommit": "2015-02-28T15:46:52Z", "numberOfCommits": 2, "title": "Testing library", "wordCount": 62, "authors": {"name1": 2}, "content": "# Testing library\n\n## Context\n\n* We want to write programmer tests to support a TDD workflow.\n* We want to be able to mock out functions.\n\n## Decision\n\n* We will use Midje to test our code.\n* Despite it's heavy macro design, it allows you to write expressive code and easily makes mocks\n\n## Alternatives Considered\n\n* The inbuilt (deftest). We felt it was less expressive than Midje.\n"}
{"repositoryUrl": "https://github.com/ministryofjustice/opg-digideps.git", "path": "docs/architecture/decisions/0002-use-amazon-aurora-for-application-database.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-02-20T09:56:20Z", "lastCommit": "2020-07-23T14:48:15Z", "numberOfCommits": 2, "title": "2. Use Amazon Aurora Serverless for ephemeral environments", "wordCount": 235, "authors": {"name1": 1, "name2": 1}, "content": "# 2. Use Amazon Aurora Serverless for ephemeral environments\n\nDate: 2020-02-14\n\n## Status\n\nAccepted\n\n## Context\n\nDigiDeps uses a Postgres database to store persistent information. Since the project was first set up, we have been using an Amazon RDS hosted instance of Postgres for this. However, this hosting option lacks scalability so we have to pay for a full database host 24/7 for each environment.\n\nWe have several environments which do not need to be highly available. This includes \"ephemeral\" development environments, the \"main\" environment used in CI, and the training environment. We do not need to run a database for these environments outside of working hours, and often inside of them too.\n\n## Decision\n\nWe will use Amazon Aurora Serverless for environments which do not need to always be on. Aurora automatically scales with usage, including pausing completely if the database isn't in use.\n\n## Consequences\n\nWe will need to stop running regular healthchecks in these environments, since this prevents the database from pausing.\n\nOur database infrastructure will vary between accounts, meaning we cannot be certain that code which worked in development will work in production. Smoke tests in preproduction will indicate any infrastructure failures before release to production.\n\nWe will consider upgrading our production database to a matching Postgres version (10.7) and/or hosting it with Provisioned Aurora to further align in the future.\n\nAs Aurora identifies itself as Postgres, no application changes are needed to support this.\n"}
{"repositoryUrl": "https://github.com/openkfw/TruBudget.git", "path": "docs/developer/architecture/0005-workflowitem-ordering.md", "template": "Nygard", "status": "Draft", "firstCommit": "2021-06-30T15:51:32Z", "lastCommit": "2021-06-30T15:51:32Z", "numberOfCommits": 1, "title": "(sem título)", "wordCount": 381, "authors": {"name1": 1}, "content": "---\nsidebar_position: 5\n---\n# Workflowitem-ordering\n\nDate: 04/05/2018\n\n## Status\n\nDraft\n\n## Context\n\nWorkflowitems are sorted by their creation time by default, but there needs to be some mechanism that allows for manual sorting as well (mainly relevant for the UI). Previously, each workflowitem would hold a pointer to the previous item in the list. However, this approach cannot prevent an inconsistent state if there is a data race between two concurrent requests: it may happen that two workflowitems share the same pointer (turning the list into a tree).\n\n## Decision\n\nWe solve this by maintaining the ordering as a list, stored with the subproject the workflowitems belong to:\n\n```plain\nsubproject stream:\n  stream item \"workflowitem_ordering\" => { data: [id1, id2, ...], log: [], permissions: {}}\n```\n\nNote that we use the resource structure here simply to be able to treat the record like any other, but `log` and `permissions` have no meaning at the time of writing.\n\nSince Multichain doesn't offer transactions for stream operations, we cannot guarantee that a newly created workflowitem would always be recorded in the list, so we apply the following trick when computing the ordering:\n\n- Workflowitems that are included in the workflowitem-ordering are included in the result exactly in that ordering;\n- all remaining workflowitems are sorted by their creation time and appended to the result.\n\nBecause workflowitems are sorted by their creation time by default, newly created items _do not_ have to be added to the ordering, so no inconsistencies can occur.\n\n## Consequences\n\nUsing this approach, we get the following properties:\n\n- Without setting an ordering through an API call, the ordering list is empty and all items are sorted by their creation time.\n- When an ordering is set, it is respected when returning workflowitems.\n- The case of concurrent requests:\n  - Concurrent creation causes both items to be appended to the list, ordered by their creation time, or in arbitrary order in case the creation times are equal.\n  - Concurrent updates of the ordering is a race with all-or-nothing semantics: whoever finished the update last wins, and there can never be any inconsistencies.\n- If an update to the ordering does not include a workflowitem that was not present when the request was issued, when returning the ordered list of workflowitems, the missing workflowitem is simply set as the last element (which makes sense: it is the newest workflowitem, after all).\n"}
{"repositoryUrl": "https://github.com/buildit/bookit-api.git", "path": "docs/architecture/decisions/0009-use-jpa.md", "template": "Nygard", "status": "Accepted Amends [8. Database](0008-database.md)", "firstCommit": "2017-12-27T18:53:03Z", "lastCommit": "2017-12-27T18:53:03Z", "numberOfCommits": 1, "title": "9. Use JPA", "wordCount": 239, "authors": {"name1": 1}, "content": "# 9. Use JPA\n\nDate: 2017-12-27\n\n## Status\n\nAccepted\n\nAmends [8. Database](0008-database.md)\n\n## Context\n\nOriginally, we utilized a Spring's JdbcTemplate to quickly CRUD our entities against the data store.  While this was quick and easy, we did most of our filtering in the application as opposed to SQL WHERE clauses.  As we continued, each addition to our entities required a lot of boilerplate code.  \n\nSpring has great JPA support and Boot uses Hibernate out of the box.  Our entity models are still relatively simple, but using JPA reduces a lot of the boilerplate code, and opens up a lot of additional features \"for free.\"  Specifically, we can utilize JPA to manage our schema updates (naively, later if we need something more robust we can look to Liquibase or Flyway).  It also simplifies joins, where clauses, and gives us more database independence.\n\n## Decision\n\n* Use JPA to map objects into database tables\n  * Use Hibernate as the JPA implementation - Spring Boot's default\n* Leverage Spring Data's JPA support to implement queries via Repository interface patterns\n\n## Consequences\n\n* While the tests will ensure the code is correct, we will need to ensure the code performs appropriately and does not encounter N+1 or combinatorial query explosion.\n* Will still need to figure out the best approach to support our local datetime overlapping interval logic in WHERE clauses\n* Consider utilizing Spring Data's RestRepository support to expose repositories directly (very fast time to market, should have started with this)\n"}
{"repositoryUrl": "https://github.com/crypto-com/thaler.git", "path": "architecture-docs/adr-001.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-12-11T09:53:44Z", "lastCommit": "2020-11-09T04:06:38Z", "numberOfCommits": 3, "title": "ADR 001: Storage Consolidation", "wordCount": 1122, "authors": {"name1": 2, "name2": 1}, "content": "# ADR 001: Storage Consolidation\n\n## Changelog\n* 10-12-2019: Initial Draft\n* 11-12-2019: Extra comments\n* 03-01-2020: Refinement of the decision based on early experiments + discussion on Slack\n\n## Context\nCurrently (not counting Tendermint's internal storage or wallets), two processes maintain their internal storage:\n\n* chain-abci: stores the node state, Merkle trie of staking states, transaction metadata (whether spent or not),\nvalidator tracking, etc.\n* tx-validation enclave (TVE): sealed transaction data (of valid obfuscated transactions that have outputs)\n\nThe reason for having two processes is that SGX SDK compilation is different and needs Intel SGX SDK tooling\n(and the subsequent process execution requires Intel SGX PSW tooling, such as AESM service),\nso for the development convenience, the transaction validation code that needs to execute in an enclave\nis isolated. (For example, one can build and run chain-abci on any platform (e.g. macOS), \nand run the enclave parts inside a docker container or on a remote Linux host.)\nThe inter-process communication is over a simple REQ-REP 0MQ socket.\n\n*Problem 1: These two storage locations need to be \"in sync\"*:\n\nwhen an obfuscated transaction arrives that spends some transaction outputs, chain-abci will do a basic validation and check if they are unspent and forward it to TVE (assuming its storage contains\nsealed transaction data of respective outputs). There is currently a naive check that TVE\nstores the latest app hash provided by chain-abci; and upon a startup, chain-abci cross-checks if TVE is in sync with it. This leads to various errors and annoyances that are usually resolved by removing all storage and syncing from scratch (in certain cases, there may be a better mechanism, but wasn't implemented).\n\n*Problem 2: Transaction querying*:\n\nAs wallet / client-* may be lightweight client and not have access to TEE directly, it will connect to one remotely.\nFor this purpose, there is transaction query enclave (TQE). See [this document](https://github.com/crypto-com/thaler-docs/blob/master/plan.md#transaction-query-enclave-tqe-optional--for-client-infrastructure) for more details.\n\nThere are two flows (over an attested secure channel):\n\n1. retrieving transactions: client submits transaction identifiers signed by its view key, and TQE replies with matching transaction data. For this workflow, TQE contacts TVE over REQ-REP 0MQ socket to retrieve data.\n\n2. submitting new transactions: client submits a new transaction, TQE forwards it to TVE that checks it (so that it doesn't obfuscate random / invalid data) and if it's valid, it encrypts it with the obfuscation key (currently compile-time mock, but planned to be periodically regenerated\nby another type of enclave) and returns the obfuscated transaction to TQE that forwards it to the client.\n\nIn the first flow, TQE only talks to the TVE's application wrapper that handles the persistence -- it can unseal\nthe transaction data, because the key policy is MRSIGNER. \n\nIn the second flow, TVE holds the obfuscation key inside the enclave memory, so the payload goes to TVE.\nCurrently, TVE cannot check everything, e.g. staked state or if a transaction output was spent or not\n-- in the future, it may internally have app hash components and at least require some lightweight proofs\nfor these things.\n\nFor the first flow, it's unnecessary for TQE to talk to TVE. For the second flow, it'll be desirable\nto do a more complete verification (currently there are a few hacks and workarounds).\n\n## Decision\nThis will be a bit big change, so it can be done in several steps:\n\n* separate out the storage functionality from chain-abci into chain-storage crate\nhttps://github.com/crypto-com/chain/issues/753\n  * This should decouple the existing storage functionality from state machine logic\n  * The result should be more high-level APIs that can then be used or extended by the embedded tx-validation enclave app wrapper\n  (getSealedTx, insertSealedTx...) to encapsulate some of the low-level storage choices\n\n* embed tx-validation enclave app wrapper in chain-abci\nBased on early experiments in: https://github.com/crypto-com/chain/pull/738\nIt will include several changes (in several sub-PRs):\n  * make more general SGX library loader (currently, it's been fixed to enclave.signed.so)\n  * extract out enclave-only tx-validation core functionality into a separate crate that would allow *optional* mock version (replacing the enclave-bridge)\n  * build process modifications: consolidating common functionality, making chain-abci's build process to expose URTS to tx-validation enclave\n  * chain-abci starting up tx-validation's zMQ server for the sole purpose of preserving current tx-query workflows (tx-query changes are out of scope of this ADR,\ne.g. having \"insecure\" test-only connection)\n  * replace the \"embedded\" tx-validation's sled storage with chain-storage (*addressing Problem 1*):\n  * store sealed transaction payloads in `COL_BODIES` (or a dedicated column if desired)\n  * for serving tx-query requests, just use chain-abci's latest committed state (for last block's time etc.)\n  * remove the redundant enclave-protocol variants of zMQ inter-communication message payloads (unused by tx-query):\n  * CheckChain: no need for the latest app hash checking (only one storage), can do the sanity check (the mainnet/testnet *.signed.so will be different) with direct call\n  * VerifyTx: can call directly (IntraEnclave)\n  * EndBlock: can call directly \n  * CommitBlock: no need (both information stored / handled during normal chain-abci execution)\n  * modifications in tx-query protocol and client to allow complete verification verification of encryption requests (*addressing Problem 2*):\n  * `EncryptionRequest::WithdrawStake` doesn't need to include the StakedState information:\n  * tx-query will obtain the address from the signature payload and pass that to chain-abci / embedded tx-validation\n  * chain-abci (before calling the tx-validation ecall) should look up the staked state (if it doesn't exist, it'll return an error)\n  * move `SGX_TEST` tx-validation's SGX unit test (unfortunately the normal Rust unit tests don't work in Apache SGX SDK) under an optional flag in chain-abci\n  * along the way, update documentation, integration tests and integration test environment set ups\n  \n## Status\n\nAccepted\n\n## Consequences\n\n### Positive\n\n* Only one place to store transaction data -- no need to keep storage of two processes in sync\n* Decoupling state machine logic (chain-abci) from the storage\n* Full validation of (existing) TQE requests\n* As TQE (and other non-yet-implemented enclave logic) evolves, it'll be beneficial to have one canonical Chain storage place (chain-storage / chain-abci)\nwhich TQE etc. can rely on\n\n### Negative\n* More complex chain-abci building process \n\n### Neutral\n* A few more crates (storage crate + some of the enclave functionality may be extracted and abstracted out into existing or new crates)\n* Coupling TQE process to chain-abci\n* Storage space shared between chain-abci and \"sub-abci\" enclave applications: perhaps an extra column in RocksDB-like storage\n* Depending on how the tx-validation embedding into chain-abci and mocking is done, it may be a source of \"silent\" enclave only errors\na developer would discover only after the full integration test, or worse at longer runs in SGX env (as it's not every edge case is covered by integration test and there's no fuzzing yet)\n* many documentation and script changes (as tx-validation wrapper app has been there for quite some time)\n\n## References\n\n* moving app wrapers to chain-abci: https://github.com/crypto-com/chain/pull/665#discussion_r356377869\n* https://github.com/libra/libra/tree/master/storage/storage-service\n* more discussion pointing out other concerns in tx-query: https://github.com/crypto-com/chain/pull/741\n* early embedding tx-validation into chain-abci experiment: https://github.com/crypto-com/chain/pull/738\n"}
{"repositoryUrl": "https://github.com/Tripwire/octagon.git", "path": "doc/architecture/decisions/0003-we-test-via-visual-screenshots.md", "template": "Nygard", "status": "accepted", "firstCommit": "2018-01-25T21:23:02Z", "lastCommit": "2018-01-25T21:23:02Z", "numberOfCommits": 1, "title": "3. we test via visual screenshots", "wordCount": 153, "authors": {"name1": 1}, "content": "# 3. we test via visual screenshots\n\ndate: 2018-01-25\n\n## status\n\naccepted\n\n## context\n\nhow to test UI code is a long, complicated, and contested topic.  unit test code is welcome in this project, but outside the scope of this decision.\n\n- visual changes are subtle & difficult to detect\n- small CSS changes can have large, cascading effects across a web app\n\ndue to high risks in subtle changes, we test our components visually, and track changes.\n\n## decision\n\n- we shall take screenhots of components we develop and components we import\n- we shall check in our screenshots into change control\n- we shall test screenshot changes onchange\n- we shall capture screenshots out of real browser engines, not mock browsers\n\ncompleteness of component coverage is not defined in this decision.  tooling used to realize this decision is not defined in this decision.\n\n## consequences\n\n- cost of ownership is high\n  - tooling is generally complicated\n  - tests are often slow\n  - browser coverage may be limited\n\n"}
{"repositoryUrl": "https://github.com/nationalarchives/tdr-dev-documentation.git", "path": "architecture-decision-records/0001-multi-account-terraform.md", "template": "unknown", "status": null, "firstCommit": "2020-03-11T14:52:41Z", "lastCommit": "2020-03-11T14:52:41Z", "numberOfCommits": 1, "title": "1. Use Terraform across multiple AWS accounts", "wordCount": 407, "authors": {"name1": 1}, "content": "# 1. Use Terraform across multiple AWS accounts\n\n**Date:** 2020-02-20\n\nThe actual decision was made earlier than this, between December 2019 and\nJanuary 2020.\n\n## Context\n\nWe have created multiple AWS accounts to host the different TDR environments,\nand we're happy with the decision we made during the Alpha to use Terraform to\nconfigure the infrastructure (see [terraform.md][alpha-terraform] and\n[terraform_vs_cloudformation.md][tf-vs-cf]).\n\nThe Alpha only ran in one AWS account. We used Terraform workspaces so that we\ncould create multiple environments if we wanted, but in the end we only used one\nenvironment for prototyping and user testing.\n\nIn Alpha, we ran Terraform scripts from our dev machines. For Beta, we'd prefer\nto run Terraform in a controlled environment like Jenkins. This has a few\nadvantages:\n\n- Makes it easier to control access to production infrastructure: a developer\n  can have permission to run Jenkins jobs (and therefore make\n  version-controlled, peer-reviewed changes to production) without having\n  credentials giving them full access to the production account\n- Makes it easier to deploy changes to each environment in turn\n- Provides a record of (recent) Terraform runs, and who started them\n\nBut there's a chicken-and-egg problem with running Terraform from a hosted\nsystem like Jenkins, because it would be helpful if we could use Terraform to\ncreate the Jenkins infrastructure itself.\n\n[alpha-terraform]: ../technology-considerations/terraform.md\n[tf-vs-cf]: ../technology-considerations/terraform_vs_cloudformation.md\n\n## Decision\n\nCreate multiple Terraform projects, which are run in different stages:\n\n- A backend project which is run against the management account and bootstraps\n  the rest of the infrastructure. It is run from a development machine and\n  creates the IAM roles and Terraform state storage (S3 and DynamoDB) that will\n  be used by the environment-specific Terraform scripts. This project's own\n  Terraform state is stored in a manually-created S3 bucket and DynamoDB table.\n- A CI project which is run against the management account when the backend\n  bootstrap script has been run. It is also run from a development machine, but\n  the Terraform state is saved in the storage set up by the bootstrap script.\n- An environment project which is run against each TDR environment account\n  (integration, staging, production). This script is run from Jenkins.\n\nSince the environment-specific Terraform is run from Jenkins, which is in AWS,\nwe can use IAM roles to give Jenkins permission to run Terraform. This means\nthat we don't need to generate an AWS secret key (which could be used from\noutside our environment if it was stolen) for Jenkins to run Terraform.\n"}
{"repositoryUrl": "https://github.com/mateuszmidor/ArchStudy.git", "path": "ADR/decisions/0003-stop-recording-architecture-decisions.md", "template": "Nygard", "status": "Accepted Supercedes [1. Record architecture decisions](0001-record-architecture-decisions.md)", "firstCommit": "2021-01-16T20:12:03Z", "lastCommit": "2021-01-16T20:12:03Z", "numberOfCommits": 1, "title": "3. Stop recording architecture decisions", "wordCount": 59, "authors": {"name1": 1}, "content": "# 3. Stop recording architecture decisions\n\nDate: 2021-01-16\n\n## Status\n\nAccepted\n\nSupercedes [1. Record architecture decisions](0001-record-architecture-decisions.md)\n\n## Context\n\nThe issue motivating this decision, and any context that influences or constrains the decision.\n\n## Decision\n\nThe change that we're proposing or have agreed to implement.\n\n## Consequences\n\nWhat becomes easier or more difficult to do and any risks introduced by the change that will need to be mitigated.\n"}
{"repositoryUrl": "https://github.com/archivematica/archivematica-architectural-decisions.git", "path": "/0000-use-markdown-architectural-decision-records.md", "template": "Madr", "status": null, "firstCommit": "2019-02-23T23:05:33Z", "lastCommit": "2021-06-09T06:05:30Z", "numberOfCommits": 4, "title": "(sem título)", "wordCount": 169, "authors": {"name1": 1, "name2": 3}, "content": "---\nlayout: page\nstatus: accepted\nadr: \"0000\"\ntitle: Use Markdown Architectural Decision Records\ndeciders:\ndate:\n---\n\n## Context and problem statement\n\nWe want to record architectural decisions made in this project.\nWhich format and structure should these records follow?\n\n## Considered options\n\n* [MADR][0] 2.1.2 – The Markdown Architectural Decision Records\n* [Michael Nygard's template][1] – The first incarnation of the term \"ADR\"\n* [Sustainable Architectural Decisions][2] – The Y-Statements\n* Other templates listed at @joelparkerhenderson's [repository][3].\n* Formless – No conventions for file format and structure\n\n## Decision outcome\n\nChosen option: \"MADR 2.1.2\", because\n\n* Implicit assumptions should be made explicit.\n  Design documentation is important to enable people understanding the decisions\n  later on. See also [A rational design process: How and why to fake it][4].\n* The MADR format is lean and fits our development style.\n* The MADR structure is comprehensible and facilitates usage & maintenance.\n* The MADR project is vivid.\n* Version 2.1.2 is the latest one available when starting to document ADRs.\n\n[0]: https://adr.github.io/madr/\n[1]: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n[2]: https://www.infoq.com/articles/sustainable-architectural-design-decisions\n[3]: https://github.com/joelparkerhenderson/architecture_decision_record\n[4]: https://doi.org/10.1109/TSE.1986.6312940\n"}
{"repositoryUrl": "https://github.com/dxw/dalmatian-frontend.git", "path": "doc/architecture/decisions/0005-use-brakeman-for-security-analysis.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-08-19T10:29:07Z", "lastCommit": "2020-08-19T10:29:07Z", "numberOfCommits": 1, "title": "5. use-brakeman-for-security-analysis", "wordCount": 112, "authors": {"name1": 1}, "content": "# 5. use-brakeman-for-security-analysis\n\nDate: 2020-04-03\n\n## Status\n\nAccepted\n\n## Context\n\nWe need a mechanism for highlighting security vulnerabilities in our code before it reaches production environments\n\n## Decision\n\nUse the [Brakeman](https://brakemanscanner.org/) static security analysis tool to find vulnerabilities in development and test\n\n## Consequences\n\n- Brakeman will be run as part of CI and fail the build if there are any warnings\n- Brakeman can also be run in the development environment to allow developers to address issues before committing code to the repository\n- Brakeman will help developers learn about common vulnerabilities and develop a more defensive coding style\n- Use of Brakeman in development & test environments should reduce or eliminate code vulnerabilities that would be exposed in a penetration test\n"}
{"repositoryUrl": "https://github.com/mozilla/fxa.git", "path": "docs/adr/0001-isolating-payment-content-with-third-party-widgets-from-general-account-management.md", "template": "Madr", "status": null, "firstCommit": "2019-05-07T16:44:09Z", "lastCommit": "2019-05-07T16:44:09Z", "numberOfCommits": 1, "title": "Isolating payment content with third-party widgets from general account management", "wordCount": 816, "authors": {"name1": 1}, "content": "# Isolating payment content with third-party widgets from general account management\n\n* Deciders: Ben Bangert, Ian Bicking, Wil Clouser, Les Orchard, Shane Tomlinson\n* Date: 2019-05-06\n\n## Context and Problem Statement\n\nIn the implementation of payment features for subscription services, we've\ndecided to use third-party JavaScript for payment widgets.\n\nBest practices established by our security team indicate that third-party JS\nshould not be included on the highly-sensitive pages - i.e. such as those used\nfor general account management on Firefox Accounts.\n\nSo, we need a way to isolate the pages responsible for subscription sign-up and\nmanagement from the rest of Firefox Accounts.\n\n## Decision Drivers\n\n* Security when dealing with financial transactions.\n* Security when including with third-party JS code for payment widgets.\n* Simplicity in user experience flows.\n* Delivering against the subscription services deadline.\n\n## Considered Options\n\n* Option A - Payment pages as separate app supplied with pre-generated access\n  token\n* Option B - Payment pages as separate app and normal OAuth Relying Party\n* Option C - Payment-related content embedded in iframes with pre-generated\n  access token passed via postMessage\n* Option D - Payment pages as normal FxA content using iframes to isolate\n  third-party widgets\n\n## Decision Outcome\n\nChosen option: \"Option A - Payment pages as separate app supplied with\npre-generated access token\", because\n\n* Further refinements to access token delivery mechanism in Option A do not\n  significantly affect the rest of the payments app.\n* Doesn't preclude an upgrade to Option B in the future - i.e. once [Issue\n  #640](https://github.com/mozilla/fxa/issues/640) is resolved.\n* Doesn't preclude Option C as a future option - e.g. offering embedded\n  subscription widgets to third-parties. \n* Fastest practical option given existing record of reviews by security & UX and\n  work completed so far.\n* Fresh start with a more modern web stack (i.e. React).\n\n## Pros and Cons of the Options\n\n### Option A - Payment pages as separate app supplied with pre-generated access token\n\n* Description\n  * Payments pages as standalone web app (i.e. payments.firefox.com)\n  * Access token generated via fxa-content-server (i.e. accounts.firefox.com) to\n  access fxa-auth-server subscription APIs.\n  * Access token conveyed to payments pages directly via URL parameter\n  * Alternatively, access token conveyed indirectly via code exchange or other\n  more secure mechanism.\n* Pros\n  * Can effectively isolate third-party widgets by virtue of living on a\n  separate origin and receiving a scoped access token.\n  * Our original plan of record reviewed by security and UX teams - i.e. time\n  already spent.\n  * Allows us to build something from scratch using a more modern framework like\n  React.\n* Cons\n  * Building something with React is novel for the project overall.\n  * We need to stand up yet another server.\n\n### Option B - Payment pages as separate app and normal OAuth Relying Party\n\n* Description:\n  * Payments pages as standalone web app (i.e. payments.firefox.com).\n  * Payments pages as full OAuth Relying Party with the usual login flow\n  requesting scope to access fxa-auth-server subscription APIs.\n  * Access token acquired via standard OAuth mechanisms.\n* Pros\n  * Can effectively isolate third-party widgets by virtue of living on a\n  separate origin and receiving a scoped access token.\n  * Allows us to build something from scratch using a more modern framework like\n  React.\n* Cons\n  * There is [an open issue to support\n  `?prompt=none`](https://github.com/mozilla/fxa/issues/640) for OpenID\n  Connect login. Left unresolved, this issue means accessing payment pages can\n  result in a redundant login prompt in UX flows.\n  * Building something with React is novel for the project overall.\n  * We need to stand up yet another server.\n\n### Option C - Payment-related content embedded in iframes with pre-generated access token passed via postMessage\n\n* Description\n  * Payments content hosted on separate domain (i.e. payments.firefox.com)\n  embedded in iframes on fxa-content-server pages (i.e. accounts.firefox.com)\n  * Access token generated via fxa-content-server (i.e. accounts.firefox.com)\n  * `iframe.postMessage()` API used to pass access token to payments content\n* Pros\n  * This can effectively isolate third-party widgets by virtue of living on a\n  separate origin and receiving a scoped access token.\n* Cons\n  * We need to stand up yet another server - i.e. at least for the separate\n  origin.\n  * Security story around iframes on the same origin as user management pages\n  has not been reviewed - i.e. additional time needed.\n  * Seems an awkward fit with UX flows drafted so far, for both subscription\n  management and sign-up.\n  * Would most likely be more content built using Backbone requiring\n  modernization later.\n  * (unless we combined React & Backbone on fxa-content-server, which is not\n  as clean as a separate app)\n\n### Option D - Payment pages as normal FxA content using iframes to isolate third-party widgets\n\n* Description\n  * Payments pages hosted within fxa-content-server\n  * (i.e. only accounts.firefox.com involved - no payments.firefox.com host\n  created)\n  * iframes used to embed and isolate third-party widgets\n  * Session token used as usual for API authentication, no access token\n  required.\n* Pros\n  * Requires the fewest novel technology choices.\n  * Can reuse the existing fxa-content-server.\n* Cons\n  * Security story around iframes on the same origin as user management pages\n  has not been reviewed - i.e. additional time needed.\n  * Would most likely be more content built using Backbone requiring\n  modernization later.\n  * (unless we combined React & Backbone on fxa-content-server, which is not\n  as clean as a separate app)\n\n## Links\n\n* Shane's earlier \"[Securing the payment page][securing]\" Google Doc.\n\n[securing]:\nhttps://docs.google.com/document/d/17NItC2sWtMH4iGfyaxo_WLxWmKAKpfWOw3N14tQY7I8/edit?usp=sharing"}
{"repositoryUrl": "https://github.com/kgrzybek/modular-monolith-with-ddd.git", "path": "docs/architecture-decision-log/0017-implement-archictecture-tests.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-11-24T22:07:17Z", "lastCommit": "2019-11-24T22:07:17Z", "numberOfCommits": 1, "title": "17. Implement Architecture Tests", "wordCount": 160, "authors": {"name1": 1}, "content": "# 17. Implement Architecture Tests\n\nDate: 2019-11-16\n\n## Status\n\nAccepted\n\n## Context\n\nIn some cases it is not possible to enforce the application architecture, design or established conventions using compiler (compile-time). For this reason, code implementations can diverge from the original design and architecture. We want to minimize this behavior, not only by code review.\n\n## Decision\n\nWe decided to implement Unit Tests for our architecture. </br>\nWe will implement tests for each module separately and one tests library for general architecture. We will use _NetArchTest_ library which was created exactly for this purpose.\n\n## Consequences\n- We will have quick feedback about breaking the design rules\n- Unit tests for architecture are documenting our architecture to some level\n- We will have dependency to external library\n- We need to implement some _\"reflection-based\"_ code to check some rules, because library does not provide everything what we need\n- This kind of tests are a bit slower than normal unit tests (because of reflection)\n- More tests to maintain"}
{"repositoryUrl": "https://github.com/serienator/serienator.git", "path": "adr/cicd-tool.md", "template": "unknown", "status": null, "firstCommit": "2019-10-08T20:29:02Z", "lastCommit": "2019-10-08T20:29:02Z", "numberOfCommits": 1, "title": "Tool for CI/CD", "wordCount": 18, "authors": {"name1": 1}, "content": "# Tool for CI/CD\n\n## Issue\n\nWe would like to have a tool for allowing our ci/cd process. \n\n## Decision\n\nJenkins\n"}
{"repositoryUrl": "https://github.com/apache/james-project.git", "path": "src/adr/0043-avoid-elasticsearch-on-critical-reads.md", "template": "Nygard", "status": "Accepted (lazy consensus) & implemented Scope: Distributed James", "firstCommit": "2020-11-18T02:27:43Z", "lastCommit": "2020-12-14T06:49:38Z", "numberOfCommits": 3, "title": "43. Avoid ElasticSearch on critical reads", "wordCount": 802, "authors": {"name1": 3}, "content": "# 43. Avoid ElasticSearch on critical reads\n\nDate: 2020-11-11\n\n## Status\n\nAccepted (lazy consensus) & implemented\n\nScope: Distributed James\n\n## Context\n\nA user willing to use a webmail powered by the JMAP protocol will end up doing the following operations:\n - `Mailbox/get` to retrieve the mailboxes. This call is resolved against metadata stored in Cassandra.\n - `Email/query` to retrieve the list of emails. This call is nowadays resolved on ElasticSearch for Email search after\n a right resolution pass against Cassandra.\n - `Email/get` to retrieve various levels of details. Depending on requested properties, this is either\n retrieved from Cassandra alone or from ObjectStorage.\n\nSo, ElasticSearch is queried on every JMAP interaction for listing emails. Administrators thus need to enforce availability and good performance\nfor this component.\n\nRelying on more services for every read also harms our resiliency as ElasticSearch outages have major impacts.\n\nAlso we should mention our ElasticSearch implementation in Distributed James suffers the following flaws:\n - Updates of flags lead to updates of the all Email object, leading to sparse segments\n - We currently rely on scrolling for JMAP (in order to ensure messageId uniqueness in the response while respecting limit & position)\n - We noticed some very slow traces against ElasticSearch, even for simple queries.\n\nRegarding Distributed James data-stores responsibilities:\n - Cassandra is the source of truth for metadata, its storage needs to be adapted to known access patterns.\n - ElasticSearch allows resolution of arbitrary queries, and performs full text search.\n\n## Decision\n\nProvide an optional view for most common `Email/query` requests both on Draft and RFC-8621 implementations.\nThis includes filters and sorts on 'sentAt'.\n\nThis view will be stored into Cassandra, and updated asynchronously via a MailboxListener.\n\n## Consequences\n\nA migration task will be provided for new adopters.\n\nAdministrators would be offered a configuration option to turn this view on and off as needed.\n\nIf enabled, given clients following well defined Email/query requests, administrators would no longer need\nto ensure high availability and good performances for ElasticSearch to ensure availability of basic usages\n(mailbox content listing).\n\nGiven these pre-requisites, we thus expect a decrease in overall ElasticSearch load, allowing savings compared\nto actual deployments. Furthermore, we expect better performances by resolving such queries against Cassandra.\n\nThe expected added load to Cassandra is low, as the search is a simple Cassandra read. As we only store messageId,\nCassandra dataset size will only grow of a few percents if enabled.\n\n## Alternatives\n\nThose not willing to adopt this view will not be affected. By disabling the listener and the view usage, they will keep\nresolving all `Email/query` against ElasticSearch.\n\nAnother solution is to implement the projecting using a in-memory datagrid such as infinispan. The projection\nwould be computed using a MailboxListener and the data would be first fetched from this cache and fallback to\nElasticSearch. We did not choose it as Cassandra is already there, well mastered, as disk storage is cheaper than\nmemory. InfiniSpan would moreover need additional datastore to allow a persistent state. Infinispan on the other hand\nwould be faster and would have less restrictions on data filtering and sorting. Also this would require one more software dependency.\n\n## Example of optimized JMAP requests\n\n### A: Email list sorted by sentAt, with limit\n\nRFC-8621:\n\n```\n[\"Email/query\",\n {\n   \"accountId\": \"29883977c13473ae7cb7678ef767cbfbaffc8a44a6e463d971d23a65c1dc4af6\",\n   \"filter: {\n   \"inMailbox\":\"abcd\"\n   }\n   \"comparator\": [{\n   \"property\":\"sentAt\",\n   \"isAscending\": false\n   }],\n   \"position\": 30,\n   \"limit\": 30\n },\n \"c1\"]\n```\n\nDraft:\n\n```\n[[\"getMessageList\", {\"filter\":{\"inMailboxes\": [\"abcd\"]}, \"sort\": [\"date desc\"]}, \"#0\"]]\n```\n\n### B: Email list sorted by sentAt, with limit, after a given receivedAt date\n\nRFC-8621:\n\n```\n[\"Email/query\",\n {\n   \"accountId\": \"29883977c13473ae7cb7678ef767cbfbaffc8a44a6e463d971d23a65c1dc4af6\",\n   \"filter: {\n   \"inMailbox\":\"abcd\",\n   \"after\": \"aDate\"\n   }\n   \"comparator\": [{\n   \"property\":\"sentAt\",\n   \"isAscending\": false\n   }],\n   \"position\": 30,\n   \"limit\": 30\n },\n \"c1\"]\n```\n\nDraft: Draft do only expose a single date property thus do not differenciate sentAt from receivedAt. Draft adopts sentAt\nto back the date property up, thus the above request cannot be written using draft syntax.\n\n### C: Email list sorted by sentAt, with limit, after a given sentAt date\n\nDraft:\n\n```\n[[\"getMessageList\", {\"filter\":{\"after\":\"aDate\", \"inMailboxes\": [\"abcd\"]}, \"sort\": [\"date desc\"]}, \"#0\"]]\n```\n\nRFC-8621: There is no filter properties targeting \"sentAt\" thus the above request cannot be written.\n\n## Cassandra table structure\n\nSeveral tables are required in order to implement this view on top of Cassandra.\n\nEventual denormalization consistency can be enforced by using BATCH statements.\n\nA table allows sorting messages of a mailbox by sentAt, allows answering A and C:\n\n```\nTABLE email_query_view_sent_at\nPRIMARY KEY mailboxId\nCLUSTERING COLUMN sentAt\nCLUSTERING COLUMN messageId\nORDERED BY sentAt\n```\n\nA table allows filtering emails after a receivedAt date. Given a limited number of results, soft sorting and limits can\nbe applied using the sentAt column. This allows answering B:\n\n```\nTABLE email_query_view_sent_at\nPRIMARY KEY mailboxId\nCLUSTERING COLUMN receivedAt\nCLUSTERING COLUMN messageId\nCOLUMN sentAt\nORDERED BY receivedAt\n```\n\nFinally upon deletes, receivedAt and sentAt should be known. Thus we need to provide a lookup table:\n\n```\nTABLE email_query_view_date_lookup\nPRIMARY KEY mailboxId\nCLUSTERING COLUMN messageId\nCOLUMN sentAt\nCOLUMN receivedAt\n```\n\nNote that to handle position & limit, we need to fetch `position + limit` ordered items then removing `position` firsts items.\n\n## References\n\n* [JIRA](https://issues.apache.org/jira/browse/JAMES-3440)\n* [PR discussing this ADR](https://github.com/apache/james-project/pull/259)"}
{"repositoryUrl": "https://github.com/vwt-digital/operational-data-hub.git", "path": "architecture/adr/0021-messages-are-in-json-format.md", "template": "Nygard", "status": "Accepted Implements [16. Pub/Sub implements Event sourcing](0016-pub-sub-implements-event-sourcing.md) Implemented by [19. Single schema per topic](0019-single-schema-per-topic.md)", "firstCommit": "2020-09-21T12:48:18Z", "lastCommit": "2020-10-08T12:19:54Z", "numberOfCommits": 3, "title": "21. Messages are in JSON format", "wordCount": 89, "authors": {"name1": 1, "name2": 2}, "content": "# 21. Messages are in JSON format\n\nDate: 2020-09-21\n\n## Status\n\nAccepted\n\nImplements [16. Pub/Sub implements Event sourcing](0016-pub-sub-implements-event-sourcing.md)\n\nImplemented by [19. Single schema per topic](0019-single-schema-per-topic.md)\n\n## Context\n\nIt is preferred to use a single message type for the business events. This makes it easier to handle messages on the pub/sub system in a standerdized way.\n\n## Decision\n\nAll business events on the ODH platform topics are formatted as [JSON](https://tools.ietf.org/html/rfc7159)\n\n## Consequences\n\n### Disadvantages\n\nMessages deliverd to the ODH or messages for systems conneted to the ODH might need other message formats. In these cases the messages need to be transformed from one layout to the other.\n"}
{"repositoryUrl": "https://github.com/customcommander/project-blueprint.git", "path": "adr/0003-test-distributed-files-only.md", "template": "Nygard", "status": "Accepted Relates [2. Use Google Closure Compiler](0002-use-google-closure-compiler.md)", "firstCommit": "2021-02-08T22:37:13Z", "lastCommit": "2021-02-08T22:37:13Z", "numberOfCommits": 1, "title": "3. Test Distributed Files Only", "wordCount": 119, "authors": {"name1": 1}, "content": "# 3. Test Distributed Files Only\n\nDate: 2021-02-08\n\n## Status\n\nAccepted\n\nRelates [2. Use Google Closure Compiler](0002-use-google-closure-compiler.md)\n\n## Context\n\nIt is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.\n\n## Decision\n\nTesting will be made against the production bundle to catch compilation errors before they reach our users.\n\n## Consequences\n\n- Longer testing feedback loop as sources must be recompiled before each test run.\n- Stronger focus on testing the public interface as internal functions and helpers are out of reach."}
{"repositoryUrl": "https://github.com/buildit/bookit-api.git", "path": "docs/architecture/decisions/0010-jpa-manages-schema.md", "template": "Nygard", "status": "Accepted Amends [8. Database](0008-database.md)", "firstCommit": "2017-12-27T18:53:03Z", "lastCommit": "2017-12-27T19:45:05Z", "numberOfCommits": 3, "title": "10. JPA manages schema", "wordCount": 218, "authors": {"name1": 3}, "content": "# 10. JPA manages schema\n\nDate: 2017-12-27\n\n## Status\n\nAccepted\n\nAmends [8. Database](0008-database.md)\n\n## Context\n\nOriginally, we used Spring Boot's Database Initialization support to automatically create and intialize our database via schema.sql and data.sql scripts.  Each deployment (application initialization) would execute these scripts.  Our implementation would drop the database and recreate it each time.  While this accelerated our development (avoid data migrations), it's not sustainable\n\n## Decision\n\n* Leverage Hibernate's (our JPA implementation) ddl-auto feature to update the staging/production databases (we will continue to drop/recreate all other databases....local, integration).\n  * recreating in integration ensures a clean database for each run.  In addition, it validates that we can recreate a database from scratch\n\n## Consequences\n\n* Hibernate does warn against utilizing this feature \"in production.\"\n* We can \"test\" the update in staging.  There is a risk that Hibernate updates a schema naively which may result in dropped data\n* This approach won't address how we can add static data (locations & bookables for now) over time.\n* Hibernate won't drop columns that are no longer used.  Add only.  \n* Also won't migrate data for you.  Schema only.\n* If any of the above prove onerous, we can address by introducing a database migration tool such as Liquibase or Flyway.\n* This did cause us to hit a memory limit (previously set to 256MB).  Had to bump to 512MB.\n"}
{"repositoryUrl": "https://github.com/vwt-digital/operational-data-hub.git", "path": "coding_guidelines/adr/0002-repo-naming-conventions.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2021-02-03T08:14:37Z", "lastCommit": "2021-02-04T09:23:07Z", "numberOfCommits": 3, "title": "2. repo naming conventions", "wordCount": 259, "authors": {"name1": 3}, "content": "# 2. repo naming conventions\n\nDate: 2021-02-01\n\n## Status\n\nAccepted\n\n## Context\n\nWe feel the need to use a naming convention for github repos.\n\n## Decision\n\nWe identify three kinds of repositories:\n\n### 1. General rules:\n* Use hyphens ('-') between words in the name because:\n  * words written together without something inbetween are unclear.\n  * \"\\_\" is harder to type than \"-\"  [stack overflow](ttps://stackoverflow.com/a/11947816).\n* Make repo names not longer than needed. (Because GCP project names are also limited in length).\n\n### 2. config VWT DAT repositories\nConfig repositories are repositories containing configurations of a specific Google Cloud Project (GCP) project.\n\n* Should have the same name as the GCP project they are connected to minus the customer, environment and location.\n* Name ends with `-config`.\n\n### 3.  solutions VWT DAT repositories\nSolutions repositories are repositories containing solutions, they can belong to multiple domains.\n* Their names should always start with the domain they belong to.\n* If the repository will handle multiple facets of the service, the name should end in `-handlers`\n* Sometimes, two repositories are connected because they are the frontend and backend of a service. Their names should be the same except for the ending. Frontend repositories should end in `-tool` and backend repositories should end in `-api`.\n\n### 4. \"normal\" VWT DAT repositories\n\"Normal\" repositories are repositories not belonging to a solution. They contain code used specifically for the Operational Data Hub (ODH).\n* Repository naming is equal to naming convention for solution repositories. Domains for these reposiitories is limited to `dat` and `odh`.\n* If the repository is forked from another repository, its name should contain the name of the repository it forked from.\n\n## Consequences\n"}
{"repositoryUrl": "https://github.com/ASethi93/james.git", "path": "src/adr/0031-distributed-mail-queue.md", "template": "Nygard", "status": "Accepted (lazy consensus)", "firstCommit": "2020-12-10T20:05:48Z", "lastCommit": "2020-12-10T20:05:48Z", "numberOfCommits": 1, "title": "31. Distributed Mail Queue", "wordCount": 1184, "authors": {"name1": 1}, "content": "# 31. Distributed Mail Queue\n\nDate: 2020-04-13\n\n## Status\n\nAccepted (lazy consensus)\n\n## Context\n\nMailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short \nSMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload\nto not overload a server. \n\nFurthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements\nallows, among others:\n\n - Delaying retries upon MX delivery failure to a remote site.\n - Throttling, which could be helpful for not being considered a spammer.\n\nA mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait \ndelays, purging the queue, etc.\n\nSpring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue. \nEmails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need \nto interact with all its James servers, which is not friendly in a distributed setup.\n\nDistributed James relies on the following third party softwares (among other):\n\n - **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be \nimplemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.\n - **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.\n - **ObjectStorage** (Swift or S3) holds byte content.\n\n## Decision\n\nDistributed James should ship a distributed MailQueue composing the following softwares with the following \nresponsibilities:\n\n - **RabbitMQ** for messaging. A rabbitMQ consumer will trigger dequeue operations.\n - A time series projection of the queue content (order by time list of mail metadata) will be maintained in **Cassandra** (see later). Time series avoid the \naforementioned tombstone anti-pattern, and no polling is performed on this projection.\n - **ObjectStorage** (Swift or S3) holds large byte content. This avoids overwhelming other softwares which do not scale\n as well in term of Input/Output operation per seconds.\n \nHere are details of the tables composing Cassandra MailQueue View data-model:\n\n - **enqueuedMailsV3** holds the time series. The primary key holds the queue name, the (rounded) time of enqueue \ndesigned as a slice, and a bucketCount. Slicing enables listing a large amount of items from a given point in time, in an \nfashion that is not achievable with a classic partition approach. The bucketCount enables sharding and avoids all writes \nat a given point in time to go to the same Cassandra partition. The clustering key is composed of an enqueueId - a \nunique identifier. The content holds the metadata of the email. This table enables, from a starting date, to load all of\nthe emails that have ever been in the mailQueue. Its content is never deleted.\n - **deletedMailsV2** tells wether a mail stored in *enqueuedMailsV3* had been deleted or not. The queueName and \nenqueueId are used as primary key. This table is updated upon dequeue and deletes. This table is queried upon dequeue \nto filter out deleted/purged items. \n - **browseStart** store the latest known point in time from which all previous emails had been deleted/dequeued. It \nenables to skip most deleted items upon browsing/deleting queue content. Its update is probability based and \nasynchronously piggy backed on dequeue.\n \nHere are the main mail operation sequences:\n\n - Upon **enqueue** mail content is stored in the *object storage*, an entry is added in *enqueuedMailsV3* and a message \n is fired on *rabbitMQ*.\n - **dequeue** is triggered by a rabbitMQ message to be received. *deletedMailsV2* is queried to know if the message had\nalready been deleted. If not, the mail content is retrieved from the *object storage*, then an entry is added in \n*deletedMailsV2* to notice the email had been dequeued. A dequeue has a random probability to trigger a browse start\nupdate. If so, from current browse start, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*\nuntil the first non deleted / dequeued email is found. This point becomes the new browse start. BrowseStart can never \npoint after the start of the current slice. A grace period upon browse start update is left to tolerate clock skew.\nUpdate of the browse start is done randomly as it is a simple way to avoid synchronisation in a distributed system: we\nensure liveness while uneeded browseStart updates being triggered would simply waste a few resources.\n - Upon **browse**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*, starting from the \ncurrent browse start.\n - Upon **delete/purge**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*. Mails matching \nthe condition are marked as deleted in *enqueuedMailsV3*.\n - Upon **getSize**, we perform a browse and count the returned elements.\n \nThe distributed mail queue requires a fine tuned configuration, which mostly depends of the count of Cassandra servers, \nand of the mailQueue throughput:\n - **sliceWindow** is the time period of a slice. All the elements of **enqueuedMailsV3** sharing the same slice are \nretrieved at once. The bigger, the more elements are going to be read at once, the less frequent browse start update \nwill be. Lower values might result in many almost empty slices to be read, generating higher read load. We recommend \n**sliceWindow** to be chosen from users maximum throughput so that approximately 10.000 emails be contained in a slice.\nOnly values dividing the current *sliceWindow* are allowed as new values (otherwize previous slices might not be found).\n - **bucketCount** enables spreading the writes in your Cassandra cluster using a bucketting strategy. Low values will \nlead to workload not to be spread evenly, higher values might result in uneeded reads upon browse. The count of Cassandra \nservers should be a good starting value. Only increasing the count of buckets is supported as a configuration update as\ndecreasing the bucket count might result in some buckets to be lost.\n - **updateBrowseStartPace** governs the probability of updating browseStart upon dequeue/deletes. We recommend choosing \na value guarantying a reasonable probability of updating the browse start every few slices. Too big values will lead to\nuneeded update of not yet finished slices. Too low values will end up in a more expensive browseStart update and browse\niterating through slices with all their content deleted. This value can be changed freely.\n\nWe rely on eventSourcing to validate the mailQueue configuration changes upon James start following the aforementioned rules.\n\n## Limitations\n\nDelays are not supported. This mail queue implementation is thus not suited for a Mail Exchange (MX) implementation.\nThe [following proposal](https://issues.apache.org/jira/browse/JAMES-2896) could be a solution to support delays.\n\n**enqueuedMailsV3** and **deletedMailsV2** is never cleaned up and the corresponding blobs are always referenced. This is not\nideal both from a privacy and space storage costs point of view.\n\n**getSize** operation is sub-optimal and thus not efficient. Combined with metric reporting of mail queue size being \nperiodically performed by all James servers this can lead, upon increasing throughput to a Cassandra overload. A configuration\nparameter allows to disable mail queue size reporting as a temporary solution. Some alternatives had been presented like \n[an eventually consistent per slice counters approach](https://github.com/linagora/james-project/pull/2565). An other \nproposed solution is [to rely on RabbitMQ management API to retrieve mail queue size](https://github.com/linagora/james-project/pull/2325)\nhowever by design it cannot take into account purge/delete operations. Read \n[the corresponding JIRA](https://issues.apache.org/jira/browse/JAMES-2733).\n\n## Consequences\n\nDistributed mail queue allows a better spreading of Mail processing workload. It enables a centralized mailQueue\nmanagement for all James servers.\n\nYet some additional work is required to use it as a Mail Exchange scenario.\n"}
{"repositoryUrl": "https://github.com/dante-ev/docker-texlive.git", "path": "docs/adr/0001-do-not-base-on-any-tex-image.md", "template": "Madr", "status": null, "firstCommit": "2018-01-10T00:17:43Z", "lastCommit": "2020-06-14T19:42:28Z", "numberOfCommits": 10, "title": "Do not base on any tex image", "wordCount": 319, "authors": {"name1": 10}, "content": "# Do not base on any tex image\n\n## Context and Problem Statement\n\nThe Docker image can be made from scratch or base (`FROM`) on an existing one.\nWhen choosing an existing one, which one should be taken?\n\n## Decision Drivers\n\n* Self-maintaining an image (from scratch) is hard and one repeats the mistakes, others have done\n* Patching an existing image (via PRs) might lead to rejections\n\n## Considered Options\n\n* Do not base on any tex image\n* Use <https://github.com/scottkosty/install-tl-ubuntu> - Ubuntu and TeXLive 2019\n* base on <https://github.com/reitzig/texlive-docker> - alpine different flavours can be selected\n* base on https://github.com/janweinschenker/docker-texlive/blob/master/Dockerfile - Ubuntu 17.10, texlive-full\n* base on https://github.com/thomasWeise/docker-texlive/blob/master/image/Dockerfile - Ubuntu 16.04, small installation\n* <s>base on https://github.com/sumandoc/TeXLive-2017/blob/master/Dockerfile - Debian sid, install-tl-unx.tar.gz, LaTeXML</s> - does not exist anymore\n* base on https://github.com/rchurchley/docker-texlive/blob/latest/Dockerfile - Debian latest, texlive.iso; fetches config files via wget\n* base on https://github.com/cdlm/docker-texlive - some more tools\n* base on https://github.com/mtneug/texlive-docker/blob/master/Dockerfile - Ubuntu 17.04, texlive-full; nice badges; build.sh\n* base on https://github.com/shuichiro-makigaki/docker-texlive-2017/blob/master/Dockerfile - Fedora latest\n* base on https://hub.docker.com/r/ctarwater/docker-texlive/~/dockerfile/ - debian Jessie\n* base on https://github.com/adinriv/docker-texlive/blob/master/Dockerfile - tlmgr -> based on docker-texlive-minimal - switched from install-tl-ubuntu to custom solution at https://github.com/adinriv/docker-texlive/commit/4c573e09bafff8da1ac121dd769a17bbbe0ca53b\n* base on https://github.com/adinriv/docker-minimal-texlive/blob/master/Dockerfile - ubuntu:rolling, install-tl-unx, minimal setup\n* base on https://github.com/harshjv/docker-texlive-2015/blob/master/Dockerfile - ubuntu 14.04\n* base on https://github.com/chrisanthropic/docker-TeXlive/blob/master/Dockerfile - debian:jessie, texlive 2015, ISO\n* base on https://github.com/camilstaps/docker-texlive/blob/master/Dockerfile - debian:jessie, config file in repo; `latexmk` does not work there. https://github.com/camilstaps/docker-texlive/issues/1\n* base on https://github.com/dc-uba/docker-alpine-texlive - [alpine linux](https://hub.docker.com/_/alpine/), minimal texlive 2016\n* base on https://github.com/blang/latex-docker (blog entry: https://ljvmiranda921.github.io/notebook/2018/04/23/postmortem-shift-to-docker/) - not maintained any more\n\n## Decision Outcome\n\nChosen option: \"Do not base on any tex image\", because\n\n* breaking changes on \"base images\" could come in\n* could become unmaintained (e.g., <https://github.com/adinriv/docker-texlive>)\n* the `install-tl-ubuntu` script could get unmaintained\n\nWe accept that\n\n* We are based on debian sid, which constantly changes\n* We will have to monitor the upstream repository if texlive 2019 is released and possibly adapt our Dockerfile.\n* We get a large image - more than 4 GB.\n* We have to install font packages separatetly for each font (e.g., [fonts-texgyre](https://packages.debian.org/sid/fonts/fonts-texgyre))\n"}
{"repositoryUrl": "https://github.com/alphagov/content-data-api.git", "path": "docs/arch/adr-014-track-attribute-changes-per-basepath.md", "template": "unknown", "status": null, "firstCommit": "2021-03-25T09:10:12Z", "lastCommit": "2021-03-25T09:16:46Z", "numberOfCommits": 2, "title": "Track attribute changes through time for basepaths", "wordCount": 622, "authors": {"name1": 1, "name2": 1}, "content": "# Track attribute changes through time for basepaths\n\n21-09-2018\n\n## Context:\n\nThe Data Warehouse tracks changes to Content Items through time so, for\nexample, if we change the organisation attached to a Content Item, we should\nknow WHEN that change happened and WHAT changed.\n\nTracking changes not only applies to other entities associated to the Dimension\nItems, but also to core attributes like the `title`, `description`,\n`rendering-app`, or any other attribute. This is the foundation of our Data\nWarehouse: track changes and track performance at the same time.\n\nIn GOV.UK we can identify a Content Item on a unique way by their `content_id`.\n\nIt is the combination of a `content_id` and a `locale` what makes a Content\nItem unique. So on our first iteration of the DW, when we decided to use the\nContent Item as the `grain`, hoping to be able to track all our changes by\n`content_id`.\n\nShortly after we noticed that most of our metrics are not related to a Content\nItem but the `base_path` of the item; the is due to some Content Items like\nGuides or Travel Advice, has multiple `base_paths` and we need to track metrics\nat that level (sub item). This was a business requirement driven by user\nresearch.\n\nSo we decided to reduce the scope of the `grain` to the `base_path` of the\nContent Item, which simplified and fulfilled our user needs. It was accepted by\nthe business that if the `base_path` changes, then we won't be able to track\nthe series backwards. We decided to postpone dealing with this issue when we\nhad more information about it.\n\nAs of today, we have learned more about our real needs for querying metrics.\nAggregating metrics through time for a Content Item with a base_path, need to\nbe done via its `content_id`, and there some edge cases to handle to be\naccurate. Unfortunately, to address this issues, we need to make our queries\nmore complex, and the codebase is way more complicated to maintain and reason\nabout.\n\nThe underlying problem is that we don't have a way to track our grain\n(base_path) through time because all the attributes can be changed, and\novercoming this limitation using `content_id` and `locales` it is just not\nworth the effort.\n\nIn summary, we would need to provide an efficient way to:\n\nGiven a `base_path` in their latest version, select all the metrics through\ntime, regardless of the changes to the item. \n\nAt the very end, what we are asking for is for a real unique identifier for the\nitem, because we actually don't have it, because the `content_id` is not\nplaying that role.\n\n## Decision\n\nAssociate a unique identifier to each item that is tracked in the dimensions\nitem. \n\nEach `base_path`, which represents the grain of Data Warehouse, will have a\nunique identifier that will never change. This ID will allow us to perform\nqueries through time in an efficient way, and also to be accurate with our\nresults.\n\nOn the same go, all the Contnet Items that share a locale will also be uniquely\nreferenced in the Data Warehouse.\n\n### Note\n\nWe would not need to add this unique identifier if \n\n1. The sub-items for Content with multiple paths have a unique UID for each `base_path`. \nUnfortunately, to implement this feature we would need to modify each publishing \napplication; which would impact our immediate delivery, so it needs a wider conversation.\n\n2. All the content items that have different locales but same ID have a different\n`content_id`.\n\n## Consequences\n\nIt has a low impact in our codebase; we only need to generate it once (on\ncreation), then it will be cloned with each version, so at the very end, a few\nlines of code with an important impact in our delivery.\n\n~\n\n"}
{"repositoryUrl": "https://github.com/eclipse/winery.git", "path": "docs/adr/0008-no-support-for-local-git-source-clones.md", "template": "unknown", "status": null, "firstCommit": "2017-09-29T17:24:02Z", "lastCommit": "2017-11-01T21:49:43Z", "numberOfCommits": 2, "title": "No support for local git source clones", "wordCount": 448, "authors": {"name1": 1, "name2": 1}, "content": "# No support for local git source clones\n\nA user wants to edit source files locally in his favourite IDE.\nTherefore, he wants to use the usual ways to retrieve source files.\nTypically, this is a `git clone` from a git repository having the respective source files.\n\nA user does not want to clone the whole winery repository, as this might a) be too large b) not focused enough.\nIt would be beneficial to have the source of an artifact template available as git checkout.\n\nThe source files of an artifact implementation are currently directly editable in the winery once they are uploaded. \nThe only way to edit sources locally is to download and upload them again.\nThe solution for the user should be:\n- easy to use\n- scalable in terms of storage required in Winery's repository\n\n\n## Considered Alternatives\n\n* No support for local clones\n* Git repositories as submodules\n* Using filter-branch (https://help.github.com/articles/splitting-a-subfolder-out-into-a-new-repository/)\n* Using git sparse-checkout to create a local clone (https://gist.github.com/sumardi/5559896)\n\n## Decision Outcome\n\n* Chosen Alternative: no support for local edit\n\nSince all alternatives require either too many additional git repositories or are very inconvenient to apply for the user,\nwe decided to not support any clone/push functionality.\n\n\n## Pros and Cons of the Alternatives \n\n### No support for local edit\n* `+` no changes needed\n* `-` no local edit support\n\n### Git repositories as submodules\n* `+` simple git cloning possible\n* `+` additional repositories can be cloned into winery-repository as submodules\n* `+` separate version history\n* `-` one repository for each implementation\n* `-` each separate repository has to be created on the git remote of winery (e.g., GitHub)\n\n### Using filter-branch on sever's side\n* `+` no changes needed in the existing repositories\n* `+` `git filter-branch --prune-empty --subdirectory-filter` allows to skip any subdirectory\n* `-` server needs to execute very large filter commands for each user for each requested artifact template\n* `-` the mapping back from the filtered repository to the full repository is cumbersome.\n* `-` merge conflicts are not resolved by git tooling automatically\n\n### Using filter-branch on user's side\n* `+` no changes needed in the existing repositories\n* `+` `git filter-branch --prune-empty --subdirectory-filter` allows to skip any subdirectory\n* `-` user needs to execute very large filter commands\n* `-` the mapping back from the filtered repository to the full repository is cumbersome.\n* `-` requires the user to type the commands manually\n\n### git sparse checkout\n* `+` no changes needed in the existing repositories\n* `-` requires the user to type the commands manually\n\n\n## License\n\nCopyright (c) 2017 Contributors to the Eclipse Foundation\n\nSee the NOTICE file(s) distributed with this work for additional\ninformation regarding copyright ownership.\n\nThis program and the accompanying materials are made available under the\nterms of the Eclipse Public License 2.0 which is available at\nhttp://www.eclipse.org/legal/epl-2.0, or the Apache Software License 2.0\nwhich is available at https://www.apache.org/licenses/LICENSE-2.0.\n\nSPDX-License-Identifier: EPL-2.0 OR Apache-2.0\n"}
{"repositoryUrl": "https://github.com/dxw/react-template.git", "path": "docs/adr/0008-use-styled-jsx.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-10-20T09:27:51Z", "lastCommit": "2019-10-20T09:27:51Z", "numberOfCommits": 1, "title": "8. Use styled-jsx", "wordCount": 123, "authors": {"name1": 1}, "content": "# 8. Use styled-jsx\n\nDate: 2019-10-10\n\n## Status\n\nAccepted\n\n## Context\n\nWe want to be able to style our components in a way that reduces chances of\nclashes, or unwanted styles.\n\nWe could use a convention like [BEM](http://getbem.com/), but they are generally\nhard to enforce.\n\n[`styled-jsx`](https://github.com/zeit/styled-jsx) is an alternative that scopes\nstyles to individual components, and is integrated into Next.js by default. We\ncan still fall back to global stylesheets if required (for instance if we are\nusing GOV.UK styles).\n\n## Decision\n\nWe will use `styled-jsx` for internal component styling.\n\n## Consequences\n\nUsing `styled-jsx` means styles for components are contained within those\ncomponents, ensuring they never leak out unintentionally. We can share styles\nbetween components via\n[`styled-jsx/css`](https://github.com/zeit/styled-jsx#external-css-and-styles-outside-of-the-component)\nas needed.\n\n`styled-jsx` also allows styles to be dynamic without dealing with stateful\nclasses.\n"}
{"repositoryUrl": "https://github.com/hmrc/play-frontend-hmrc.git", "path": "docs/maintainers/adr/0004-add-contact-and-welsh-information-links-into-footer.md", "template": "Madr", "status": "accepted", "firstCommit": "2020-12-11T08:59:48Z", "lastCommit": "2020-12-11T08:59:48Z", "numberOfCommits": 1, "title": "Add contact HMRC and Welsh information links to standard footer", "wordCount": 553, "authors": {"name1": 1}, "content": "# Add contact HMRC and Welsh information links to standard footer\n\n* Status: accepted\n* Date: 2020-12-04\n\nTechnical Story: PLATUI-854\n\n## Context and Problem Statement\n\nIn the context of classic services' requirement for contact HMRC and Welsh information links in their footer, facing the\n fact that these links are missing from hmrcStandardFooter, should we add them?\n\nThe additional links needed are:\n\n* \"Contact\", linking to https://www.gov.uk/government/organisations/hm-revenue-customs/contact\n* \"Rhestr o Wasanaethau Cymraeg\", linking to https://www.gov.uk/cymraeg\n\n## Decision Drivers\n\n* The need for consistency across HMRC services.\n* Our belief that including them is likely to improve the user experience for tax users.\n* We can see no good reason for not including them as standard because they are applicable across HMRC services.\n* We have a time sensitive opportunity of acting now while teams are in the process of\nuplifting their frontend libraries.\n* The HMRC design community have been consulted on multiple public Slack channels and two\nsuccessive design system working group meetings, with no objections noted.\n* Classic services support multiple live services. Not including these links as standard would mean their having to\nduplicate these links, and associated English and Welsh content, across tens of repositories.\n\n## Considered Options\n\n* Add the links to hmrcStandardFooter\n* Create an hmrcExtendedFooter component containing the additional links\n* Do nothing\n\n## Decision Outcome\n\nChosen option: \"Add the links to hmrcStandardFooter\", because this \nwill benefit tax users, and we have a unique window of opportunity to act now.\n\n### Positive Consequences\n\n* Tax users have better information provided to them\n* Teams do not need to duplicate content and URLs across hundreds of repositories\n* We can more easily maintain the content and links in a central repository\n\n### Negative Consequences\n\n* Teams currently using a Welsh link as their language toggle will likely need to switch to using one of the standard components\nfor language switching e.g. hmrcLanguageSelect.\n* Teams already including a contact link manually will need to remove it when upgrading. \n\n## Pros and Cons of the Options\n\n### Add the links to hmrcStandardFooter\n\n* Good, because it's more likely the additional links will become standard benefiting\nusers\n* Good, because we can standardise the content for English and Welsh\n* Good, because hyperlinks can be maintained in a central place and do not have to be corrected in 100s of separate services\n* Bad, because it's possible some teams may not want a contact or Welsh link for valid business reasons\n\n### Create an hmrcExtendedFooter component containing the additional links\n\n* Good, because new links will be added consistently for teams adopting it\n* Good, because we can standardise the text for English and Welsh\n* Good, because the hyperlinks can be maintained in a central repository\n* Bad, because uptake unlikely to be as high as making the links standard\n* Bad, because teams may become confused about which footer to use, increasing delivery friction\n\n### Do nothing\n\n* Good, because teams have more flexibility\n* Bad, because tax users will not benefit from these additional information links\n* Bad, because teams may add the additional links inconsistently with different text, different URLs, or \n in a different order\n* Bad, because there is no standardisation of the content in English and Welsh\n* Bad, because if changes need to be made, they will need to be made in 100s of individual services rather\nthan in one library\n* Bad, because teams will have to rollback these changes if they adopt a new standard footer with the contact and Welsh links\n"}
{"repositoryUrl": "https://github.com/nulib/meadow.git", "path": "doc/architecture/decisions/0017-preservation-strategy.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-10-11T20:14:40Z", "lastCommit": "2019-10-11T20:14:40Z", "numberOfCommits": 1, "title": "17. Preservation Strategy", "wordCount": 163, "authors": {"name1": 1}, "content": "# 17. Preservation Strategy\n\nDate: 2019-10-11\n\n## Status\n\nAccepted\n\n## Context\n\nHaving a \"preservation first\" mindset has been decided upoan as a stated goal for Meadow. Generally, this means that digital preservation\npolicies, processes and deliverables should be planned and implemented in tandem with development of the applications features, processeses and infrastructure.\n\n## Decision\n\nThe digital preservation lifecycle for a Work and its FileSets begin as soon as an Ingest Sheet is \"approved\". Actions in the ingest pipeline are used to\nadd digital preservation \"artifacts\" to Work and FileSet metadata (such as checksum and timestamps) and move objects to preservation storage buckets in S3. Additionally, the\nsuccess and failure outcomes of these actions are added as AuditEntries that can be used verification, future audits and problem resolution.\n\n## Consequences\n\nThe riskiest aspect of this strategy is that the application is rapidly evolving and we may have to alter aspects of our digital preservation strategy\nas we learn more about our chosen infrastructure's offerings and limitations over time.\n"}
{"repositoryUrl": "https://github.com/rhurkes/sware-server.git", "path": "docs/adr/0002-misc.md", "template": "unknown", "status": null, "firstCommit": "2020-04-18T01:34:52Z", "lastCommit": "2020-04-18T01:34:52Z", "numberOfCommits": 1, "title": "Miscellaneous small decisions", "wordCount": 512, "authors": {"name1": 1}, "content": "# Miscellaneous small decisions\n\n* Status: accepted\n* Date: 2020-04-14\n\n## Context and Problem Statements\n\nThere are multiple small decisions that need to be made:\n- What process should be used to create keys?\n  - Whatever method, having the store own the key generation is preferred as it allows individual loaders to not care about key generation.\n  - We used to use ingestion time in microseconds as the ID. In practice this was monotonic, and I was never able to simulate duplicate system times or times that jumped backwards in sequence. Thread-safety came for free when using ZMQ, but would require some extra work in a simplified solution. This work could have been a Mutex or using an MPSC queue (channels), but didn't provide a lot of value as the ingest time was metadata that is only useful in evaluating lag between ingestion and event dissemination.\n  - Based on the previous statement, I tried implementing using an AtomicUsize as an ID. This worked well, except it made the API contract a little more gross, and made the store considerably more complicated. Keyless fetches had to iterate backwards through all keys, deserializing the data, and checking if the ingest time was less than the threshold. Having this performance constraint on reads seemed bad. I ended up using a mutex and a while loop to prevent collisions of ingest timestamps as the ID. A mutex and db.get() aren't ideal, but write performance shouldn't be an issue.\n  - It would have been nice to leverage some sort of CAS/transaction in RocksDB, but the Rust bindings didn't allow for it.\n  - Tested the write performance in release builds at 6.4μs/put for the mutex/CAS-free code, and 9.3μs/put for the final implementation. The mutex performance hit was negligable, so most of the hit came from the CAS behavior. Even on the busiest of weather days, you're still going to be limited in your writes by the time over-the-wire from the sources which will measure in the dozens of milliseconds if not more. A 3μs difference would not be noticeable. sware v1 did writes in 30μs, so this is considerably faster.\n  - Can I create a collision? I took 4 threads and put 1M events with each using no throttling. I expected to see 4M keys in RocksDB, and saw 2216421 - almost a 50% collision rate.\n- I used to roll up all errors into a WxError type, which essentially just persisted the message from each. It was boilerplate that didn't add a ton of value, so I changed most of these functions to return `()` as an `Err` and log errors where they occur. I also switched from `slog` to `log` as I never really used all the structured logging features.\n- On 64 test events, gzipping shrunk the payload down to 14% of its original size. There were further savings by creating an optimized Event struct that doesn't serialize None values, but they were only about 5% smaller. I'll leave it for now, as it's not that much extra work, and compression won't be available in `warp` for a little while.\n"}
{"repositoryUrl": "https://github.com/ensemblejs/ensemblejs.git", "path": "doc/decisions/0003-use-immutablejs-for-immutability.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2016-09-05T12:53:08Z", "lastCommit": "2016-09-05T12:53:08Z", "numberOfCommits": 1, "title": "1. Use ImmutableJS for immutability", "wordCount": 130, "authors": {"name1": 1}, "content": "# 1. Use ImmutableJS for immutability\n\nDate: 15/06/2016\n\n## Status\n\nAccepted\n\n## Context\n\nThe cost of cloning JSON, while fast degrades with object size and as we're doing it serveral times per frame the cost is too much.\n\n## Decision\n\nImplement ImmutableJS to avoid the need to clone data. I'm hoping the internal behaviour of ImmutableJS is smart enough to avoid the cost of cloning by cleverly moving references around. The data has to be created once, but I am hopeful that that is the only time.\n\n## Consequences\n\n`JSON.parse(JSON.stringify(data))` is easy to reason about as well as being fast enough for most uses. The use of standard JavaScript objects is also easy to reason about. ImmutableJS enforces it's own API and it's another thing for someone to learn."}
{"repositoryUrl": "https://github.com/island-is/handbook.git", "path": "docs/adr/0001-use-nx.md", "template": "Madr", "status": "accepted", "firstCommit": "2020-05-03T23:14:19Z", "lastCommit": "2020-07-06T09:25:43Z", "numberOfCommits": 4, "title": "Use NX", "wordCount": 307, "authors": {"name1": 3, "name2": 1}, "content": "# Use NX\n\n- Status: accepted\n- Deciders: devs\n- Date: 03.05.2020\n\n## Context and Problem Statement\n\nWe want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI/CD.\n\n## Decision Drivers\n\n* Low complexity and overhead in development.\n* Fit for our stack.\n* Optimize CI/CD with dependency graphs and/or caching.\n* Flexible.\n\n## Considered Options\n\n* [Bazel]\n* [Nx]\n* [Lerna]\n\n## Decision Outcome\n\nChosen option: \"Nx\", because:\n\n* It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS).\n* It's relatively easy to learn with focused documentation.\n* It has schematics to generate apps, libraries and components that includes all of our tools.\n* It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.\n\n## Pros and Cons of the Options\n\n### [Bazel]\n\n* Good, because it's created and maintained by Google\n* Good, because it supports multiple programming languages and platforms.\n* Bad, because it's difficult to learn, with a custom BUILD files.\n* Bad, because JS support is hacky.\n\n### [Nx]\n\n* Good, because it has built in support for our stack.\n* Good, because it's been around for a while, originating in the Angular world.\n* Good, because it's designed with best practices for large scale web applications.\n* Good, because it supports elaborate code generation.\n* Good, because it helps optimise CI/CD for large projects.\n* Bad, because it's fairly opinionated.\n* Bad, because it's an open source project maintained by an agency.\n\n### [Lerna]\n\n* Good, because it integrates with NPM and Yarn.\n* Good, because it's used by a lot of open source projects.\n* Bad, because it's primarily designed for managing and publishing open source projects, not building and deploying large scale applications.\n\n## Links\n\n* [Why you should switch from Lerna to Nx](https://blog.nrwl.io/why-you-should-switch-from-lerna-to-nx-463bcaf6821)\n\n[Pants]: https://www.pantsbuild.org/\n[Bazel]: https://bazel.build/\n[Nx]: https://nx.dev/\n[Lerna]: https://lerna.js.org/\n"}
{"repositoryUrl": "https://github.com/ec-europa/europa-component-library.git", "path": "docs/decisions/004-datepicker.md", "template": "unknown", "status": null, "firstCommit": "2020-01-23T12:34:17Z", "lastCommit": "2020-01-23T12:34:17Z", "numberOfCommits": 1, "title": "Datepicker component", "wordCount": 252, "authors": {"name1": 1}, "content": "# Datepicker component\n\n| Status  | proposed   |\n| --- | -- |\n| **Proposed**  | 25/11/2019   |\n| **Accepted**  | (the date the proposal was accepted/rejected)  |\n| **Driver**  | @emeryro   |\n| **Approver**  | (who will make the decision and merge the PR)  |\n| **Consulted** | (who you worked with on this decision)   |\n| **Informed**  | (who should be informed if the proposal is accepted) |\n\n## Decision\n\n(Describe the decision that you propose, ideally in a single sentence)\n\n## Context\n\nA datepicker component has been requested for ECL2.  \nThis component was available on ECL1 but has not been ported yet.  \nWe have to find a good way to provide such feature.\n\nThe component has to:\n\n- respect the styling provided [in the specifications](https://webgate.ec.europa.eu/CITnet/confluence/x/fqvBN)\n- be translatable\n- offer a no-js behavior (still to be defined)\n- propose different date format\n- be as accessible as possible\n\n## Consequences\n\n(Describe the pros and cons of the proposed decision. Think about the people in the **Informed** line of the DACI table above. How will this decision affect them?)\n\n## Alternatives Considered\n\n### External library with custom style\n\n#### Pikaday\n\nOn ECL1 we used [Pikaday](https://github.com/Pikaday/Pikaday) to handle datepicker.  \nWe could rely on it again, as it seems to follow most of the requirements.\n\n**Pros**\n\n- quick to implement\n- similar to ECL1\n- code maintained by the library owner\n\n**Cons**\n\n- less control over the styling and behavior. Some specs may not be applied fully\n- not updated for more than a year\n- extra dependency for ECL2\n\n### Home made script and style\n\nWe could write a script from sctrach to handle datepicker.\n\n**Pros**\n\n- full control over the behavior, styling and accessibility\n\n**Cons**\n\n- will require far more time to implement\n- code has to be maintained on our side\n"}
{"repositoryUrl": "https://github.com/yext/edward.git", "path": "doc/adr/0002-implement-in-go.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-08-19T21:28:33Z", "lastCommit": "2018-08-19T21:28:33Z", "numberOfCommits": 1, "title": "2. Implement in Go", "wordCount": 91, "authors": {"name1": 1}, "content": "# 2. Implement in Go\n\nDate: 2016-03-02\n\n## Status\n\nAccepted\n\n## Context\n\nEdward will be provided as a command-line tool, ideally across multiple operating systems. It will need a simple means of installation and updating.\n\n## Decision\n\nEdward shall be implemented using Go.\n\n## Consequences\n\nGo applications are cross-platform and can be distributed using `go get`. This means that there is no additional configuration or scripting required. Alternate distribution mechanisms can be built and enabled separately from this repo.\n\nGo also provides built-in support for executing other processes and a basic system for providing command-line interfaces.\n"}
{"repositoryUrl": "https://github.com/raster-foundry/raster-foundry.git", "path": "docs/architecture/adr-0000-architecture-documentation.md", "template": "unknown", "status": null, "firstCommit": "2016-07-18T16:09:24Z", "lastCommit": "2016-07-19T12:43:13Z", "numberOfCommits": 2, "title": "0000 - Architecture Documentation", "wordCount": 167, "authors": {"name1": 1, "name2": 1}, "content": "# 0000 - Architecture Documentation\n\n## Context\nWe need a way to document major architecture decisions; in the past we have used the [Architecture Decision Record (ADR) format](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). On past projects, we have found the ADR format to be a useful way to write and manage architecture decisions.\n\nWe have written ADRs using both reStructuredText and Markdown formats on past projects. Certain documentation generators, such as Sphinx, can only use one of RST / Markdown. It is currently unknown which documentation generators we are likely to use for this project. The team is somewhat more comfortable writing in Markdown than RST.\n\n## Decision\nWe will continue to use the ADR format for writing architecture decisions for this project. We will use Markdown for formatting ADR documents.\n\n## Consequences\nMajor architectural decisions will need to be documented; changes to architectural decisions made via ADR will need to be documented in a superseding ADR.\n\nIf we choose to use a documentation generator that does not support Markdown, we may need to convert existing ADRs to that tool's preferred format.\n"}
{"repositoryUrl": "https://github.com/alphagov/gsp.git", "path": "docs/architecture/adr/ADR024-soft-multitenancy.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-07-05T16:23:08Z", "lastCommit": "2019-07-05T16:23:08Z", "numberOfCommits": 1, "title": "ADR024: Soft Multi-tenancy", "wordCount": 192, "authors": {"name1": 1}, "content": "# ADR024: Soft Multi-tenancy\n\n## Status\n\nAccepted\n\n## Context\n\nOne Programme has many Service Teams.\n\nOne Service Team has many Environments.\n\nSome Service Teams have separate AWS accounts for separate environments. (i.e. Staging, Production)\n\nMany Service Teams have micro-service architectures that run on many machines.\n\nSome Service Teams have unique programme specific network isolation requirements that may be hard to implement in a shared environment.\n\nSeparate programme level accounts would enable separation of billing.\n\nSharing the infrastructure within a programme will lower hosting costs.\n\nTo ensure network/compute isolation between Service Teams it may be necessary to isolate resources.\n\n\n## Decision\n\nWe will design for a \"soft multi-tenancy\" model where each programme shares a single GSP cluster with service teams within that programme.\n\nThis will:\n\n* Maintain clear separation of billing at the programme level by isolating cluster to programme's own AWS account\n* Maintain clear separation of programme specific policies and risk assessments by not forcing all users to adhere to the strictest rules?\n* Minimize costs by sharing infrastructure, control plane and tooling between teams/environments\n* Minimize support burden by reducing the amount of configuration\n\n## Consequences\n\n* Less efficient than one big cluster\n* Less isolated than millions of clusters\n"}
{"repositoryUrl": "https://github.com/jvdub/disc-golf-statistics.git", "path": "doc/architecture/decisions/0002-use-es2016-modules.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-12-01T18:22:09Z", "lastCommit": "2017-12-01T18:22:09Z", "numberOfCommits": 1, "title": "2. Use ES2016 Modules", "wordCount": 94, "authors": {"name1": 1}, "content": "# 2. Use ES2016 Modules\n\nDate: 2017-12-01\n\n## Status\n\nAccepted\n\n## Context\n\nES2016 introduced native support for the concept of modules. These are scoped files that expose some public functions. Modules are a way of organizing and sharing code.\n\n## Decision\n\nWe will use ES2016 modules to organize and share code. More information can be found here: http://exploringjs.com/es6/ch_modules.html#sec_modules-in-javascript\n\n## Consequences\n\nBecause of the nature of this project, this decision is made in lieu of choosing a framework. I want to be able to learn some of the new ES2016 features without a framework getting in the way.\n"}
{"repositoryUrl": "https://github.com/UKHomeOffice/drt-v2.git", "path": "doc/architecture/decisions/0013-use-lihaoyi-s-autowire.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-04-03T08:48:48Z", "lastCommit": "2017-04-03T08:48:48Z", "numberOfCommits": 1, "title": "10. use lihaoyi's autowire", "wordCount": 124, "authors": {"name1": 1}, "content": "# 10. use lihaoyi's autowire\n\nDate: 31/06/2016\n\n## Status\n\nAccepted\n\n## Context\n\nWe've got a Single Page app, it needs to talk to the server. Our use of scala and scalajs means we can use [lihaoyi's autowire\nmacros](https://github.com/lihaoyi/autowire)\nAlthough this is essentially a 0 on the [Richardson maturity model](https://martinfowler.com/articles/richardsonMaturityModel.html)\nit has huge benefits in terms of speed of change. We also (at the moment) only have the one client of the SPA so we can afford the tight coupling. \nIt doesn't preclude moving toward something more restful, as we can just add routes when we recognise a need.\n\n## Decision\n\nUse autowire for now. \n\n\n## Consequences\n\n+ \\+ Compile time safety between front and back. \n- \\- macro magic that has cause a few headaches here and there (beware immutable.Seq)\n"}
{"repositoryUrl": "https://github.com/tsobe/lobiani.git", "path": "doc/adr/0009-develop-admin-tool-as-spa.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2021-01-12T17:53:12Z", "lastCommit": "2021-03-31T21:48:29Z", "numberOfCommits": 6, "title": "9. Develop admin tool as SPA", "wordCount": 280, "authors": {"name1": 6}, "content": "# 9. Develop admin tool as SPA\n\nDate: 2021-01-11\n\n## Status\n\nAccepted\n\n## Context\n\nAs part of the functional requirements, we need to have a means of managing the content through an intuitive web \ninterface. There are a couple of options to achieve this\n\n### 1. As a \"traditional\" web application in the MVC architectural style, embedded within the backend (monolith)\n#### Pros\n\n- Server-side rendering sometimes can provide better user experience since content is immediately visible once \nthe page is loaded\n- Compatibility issues between the frontend and backend can be detected at the earlier stage\n\n#### Cons\n\n- Tools and libraries for the backend and frontend are mixed in the same project, making it a bit messy and mentally harder to grasp\n- Strong coupling between web layer and backend discourages us to design general-purpose API for other types of potential consumers\n- Provides limited level of interactivity\n\n### 2. As a Single Page Application in the MVVM architectural style, packaged and deployed separately from the backend\n#### Pros\n\n- Frontend is more decoupled from backend technologies since they interact with each other via API\n- Backend API can be potentially used by other consumers too: CLI, Native & Mobile (as long as it is general-purpose)\n- Frontend and backend can be delivered independently (and hence faster) from each other\n- Provides greater level of interactivity\n\n#### Cons\n\n- More pipelines need to be maintained in CI\n- Compatibility issues between the frontend and backend may be detected later, during the integration stage\n- Complete client-side rendering may degrade the user experience a bit\n\n## Decision\n\nWe will go for the SPA approach\n\n## Consequences\n\n- Initial development pace might be slower, since we will have to learn the new frontend technologies as we go\n- We will explore modern frontend technologies and approaches\n"}
{"repositoryUrl": "https://github.com/huifenqi/arch.git", "path": "decisions/0006-replace-svn-with-git.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-04-08T04:12:27Z", "lastCommit": "2018-03-23T10:45:22Z", "numberOfCommits": 4, "title": "6. SVN 迁移至 Git", "wordCount": 153, "authors": {"name1": 4}, "content": "# 6. SVN 迁移至 Git\n\nDate: 06/04/2017\n\n## Status\n\nAccepted\n\n## Context\n\n当前我们用的是 SVN 做代码、产品文档、UI 设计图的管理，在此说说其中的代码管理\n\n1. 代码仓库过大，新分支，新Tag 代码都是一份完整的拷贝;\n2. 必须联网提交；\n3. SVN 代码合并方式低效，目前使用 Beyond Compare 做代码合并，分支使用方式落后；\n4. 无法很好的做 code review（只能 patch 或第三方工具）；\n5. 面试者看到是这么落后，以此类别其他技术栈，综合理解就是，我能学到啥。\n\n## Decision\n\n使用全球最流行的分布式管理工具 Git 及平台 Github，其特点为分布式，目录结构简单，代码无冗余，可 review with PR。  \n\n### 方式一：  \n\ngit svn clone `svn project url` -T trunk\n\n将 SVN 项目的 trunk 转为 git 项目的 master，仅保留了 trunk 分支的提交记录，此方式适用于所有代码都规整到了一个分支，并且不需要其他分支的提交记录。\n\n### 方式二：\n\n使用命令 `https://github.com/nirvdrum/svn2git`，它可以保留所有branch, tags，以及所有分支的提交历史。\n\nsvn2git http://svn.example.com/path/to/repo --trunk trunk --tags tag --branches branch\n\ngit push --all origin\n\ngit push --tags\n\nuse `--revision number` to reduce the commit history.\n\n目前生产环境使用的 centos 版本过低，导致 git 也无法升级的处理方法：\n\nyum install http://opensource.wandisco.com/centos/6/git/x86\\_64/wandisco-git-release-6-1.noarch.rpm\n\nyum update git\n\n## Consequences\n\n* 工作流的调整\n\nRefs:\n\n* https://help.github.com/articles/what-are-the-differences-between-subversion-and-git/\n* http://stackoverflow.com/questions/871/why-is-git-better-than-subversion\n* [https://www.atlassian.com/git/tutorials/migrating-overview][1]\n* [https://www.atlassian.com/git/tutorials/svn-to-git-prepping-your-team-migration][2]\n* [https://www.git-tower.com/learn/git/ebook/cn/command-line/appendix/from-subversion-to-git][3]\n* [https://tecadmin.net/how-to-upgrade-git-version-1-7-10-on-centos-6/][4]\n\n[1]:\thttps://www.atlassian.com/git/tutorials/migrating-overview\n[2]:\thttps://www.atlassian.com/git/tutorials/svn-to-git-prepping-your-team-migration\n[3]:\thttps://www.git-tower.com/learn/git/ebook/cn/command-line/appendix/from-subversion-to-git\n[4]:\thttps://tecadmin.net/how-to-upgrade-git-version-1-7-10-on-centos-6/"}
{"repositoryUrl": "https://github.com/nulib/meadow.git", "path": "doc/architecture/decisions/0016-ingest-pipeline-spec.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-09-17T15:24:15Z", "lastCommit": "2019-09-17T15:24:15Z", "numberOfCommits": 1, "title": "16. ingest-pipeline-spec", "wordCount": 84, "authors": {"name1": 1}, "content": "# 16. ingest-pipeline-spec\n\nDate: 2019-09-17\n\n## Status\n\nAccepted\n\n## Context\n\nPer [issue #1104](https://github.com/nulib/next-generation-repository/issues/1104): Developers need a (basic/nothing fancy) general, conceptual/overall understanding/plan of what form the ingest pipeline will take so that different pieces may effectively be worked on by different people.\n\n## Decision\n\nWe developed a specification for a flexible, message-driven [Ingest Pipeline](../specs/ingest_pipeline.md).\n\n## Consequences\n\nUsing this spec, we will be able to break down the ingest process into a series of atomic actions, with consistent progress tracking and error reporting. Further refinements may be required, and will be handled by subsequent ADRs.\n"}
{"repositoryUrl": "https://github.com/dxw/support-rota.git", "path": "doc/architecture/decisions/0003-use-dotenv-for-managing-environment-variables.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-08-19T15:12:23Z", "lastCommit": "2020-08-19T15:12:23Z", "numberOfCommits": 1, "title": "3. use-dotenv-for-managing-environment-variables", "wordCount": 338, "authors": {"name1": 1}, "content": "# 3. use-dotenv-for-managing-environment-variables\n\nDate: 2019-09-19\n\n## Status\n\nAccepted\n\n## Context\n\nAccessing ENV directly without a wrapper is limited and can introduce problems.\n\nWe want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.\n\nWe have previously used Figaro for this purpose but it was deprecated in 2016 https://github.com/laserlemon/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.\n\nWe also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(\"1234\")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.\n\n## Decision\n\nUse DotEnv to load our environment variables.\n\n## Consequences\n\nShould Docker and Docker Compose be added to the project the environment variables will need to be loaded with `docker-compose up --env-file=.env.development` rather than `docker-compose.env` which is a pattern we have used. Having 2 files for managing environment variables such as `.env*` and `docker-compose.env*` is undesirable due to the overhead in keeping these in sync.\n\nDotEnv loads environment variables but doesn't offer an interface as Figaro did. For DotEnv you'd access by writing `ENV['foo']` rather than `DotEnv.foo`. We will need to make a supporting decision to use [Climate Control](https://thoughtbot.com/blog/testing-and-environment-variables#use-climate-control) to support testing.\n"}
{"repositoryUrl": "https://github.com/openkfw/TruBudget.git", "path": "docs/developer/architecture/0001-record-architecture-decisions.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2021-06-30T15:51:32Z", "lastCommit": "2021-06-30T15:51:32Z", "numberOfCommits": 1, "title": "(sem título)", "wordCount": 46, "authors": {"name1": 1}, "content": "---\nsidebar_position: 1\n---\n\n# Record architecture decisions\n\nDate: 03/04/2018\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to record the architectural decisions made on this project.\n\n## Decision\n\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n\n## Consequences\n\nSee Michael Nygard's article, linked above.\n"}
{"repositoryUrl": "https://github.com/commercetools/commercetools-adyen-integration.git", "path": "docs/adr/0002-matching-adyen-notification.md", "template": "Nygard", "status": "[Accepted](https://github.com/commercetools/commercetools-adyen-integration/pull/395)", "firstCommit": "2021-01-04T19:10:24Z", "lastCommit": "2021-05-05T11:37:13Z", "numberOfCommits": 3, "title": "2. Matching Adyen Notification with commercetools payment.", "wordCount": 234, "authors": {"name1": 1, "name2": 2}, "content": "# 2. Matching Adyen Notification with commercetools payment.\n\nDate: 2020-12-18\n\n## Status\n\n[Deprecated](https://github.com/commercetools/commercetools-adyen-integration/pull/395)\n\n## Context\n\nThe Adyen notification needs to be matched by its commercetools payment equivalent.\nWe are using the custom field for the merchantReference and fetching the commercetools payment object with query `custom(fields(merchantReference=\"${merchantReference}\"))`.\nThe alternative for that is the native payment `key` field.\n\n## Decision\n\n- We will use the native payment key for matching payment for notification.\n- The extension module will validate the reference field makePaymentRequest#reference before handling the payment to avoid unnecessary calls to Adyen.\n- The payment key will be set by the make payment handler, also makePaymentRequest#reference should be validated to avoid mismatches.\n- The notification will use the native payment key to fetch payment. It first finds the payment by `key` where `key=${merchantReference}` and then it finds in this payment the corresponding transaction\nby `interactionId` where `interactionId=${pspReference}`. \n\n## Consequences\n\n- It is easier to fetch a key rather than using a custom field, also a key is an indexed field, so with a key, it's more performant.\n- The payment key is unique for all payments.\n- It's not possible to set key with my-payments endpoint. This prevents by default changing/removing the key accidentally. It is more secure than custom fields as the custom field might be changed with my-payment endpoint. Check for more details: https://docs.commercetools.com/api/projects/me-payments\n\nPlease refer to the [0011-matching-adyen-notification](./0011-matching-adyen-notification.md) for the latest change regarding matching Notification with payment."}
{"repositoryUrl": "https://github.com/omair-sajid-confiz/adr-poc.git", "path": "doc/adr/0003-write-help-file.md", "template": "Nygard", "status": "Superceded by [4. Generate Help file](0004-generate-help-file.md)", "firstCommit": "2018-09-17T18:41:40Z", "lastCommit": "2018-09-17T18:41:40Z", "numberOfCommits": 1, "title": "3. write help file", "wordCount": 58, "authors": {"name1": 1}, "content": "# 3. write help file\n\nDate: 2018-09-17\n\n## Status\n\nSuperceded by [4. Generate Help file](0004-generate-help-file.md)\n\n## Context\n\nThe issue motivating this decision, and any context that influences or constrains the decision.\n\n## Decision\n\nThe change that we're proposing or have agreed to implement.\n\n## Consequences\n\nWhat becomes easier or more difficult to do and any risks introduced by the change that will need to be mitigated.\n"}
{"repositoryUrl": "https://github.com/ijmccallum/life-dashboard.git", "path": "docs/arch decisions/20191028 api spec.md", "template": "unknown", "status": null, "firstCommit": "2019-11-02T10:07:40Z", "lastCommit": "2019-12-07T09:42:16Z", "numberOfCommits": 2, "title": "Which API Spec to use", "wordCount": 132, "authors": {"name1": 1, "name2": 1}, "content": "# Which API Spec to use\n\nGoing to try an use some kind of spec standard to define and generate the api, this doc is to decide which spec to use.\n\n## Choices - pros cons\n\n - OpenAPI 3.0 (Yaml or JSON)\n   - Pros: Turns out this _is_ swagger, just newer\n   - cons: is there much point thinking about cons - haven't come across any competitors.\n - Swagger (Yaml or JSON)\n   - Pros: More stuff works with it maybe?\n   - Cons: it's the older one!\n\n - JSON: \n   - Pros: My code can read it directly (assuming I'm in JS)\n   - Cons: More syntax to write\n - YAML:\n   - Pros: It's not as tempting to try generating my own & it's more concice\n   - Cons: It's not able to be read nativly by my code.\n\n## Decision\n\nOpenAPI.YAML\n\nNewer spec, yaml is prettier. Simple as!"}
{"repositoryUrl": "https://github.com/guardian/editions.git", "path": "docs/04-✅-paths.md", "template": "Nygard", "status": "", "firstCommit": "2019-08-05T12:09:02Z", "lastCommit": "2019-08-05T12:09:02Z", "numberOfCommits": 1, "title": "Issue Paths", "wordCount": 96, "authors": {"name1": 1}, "content": "# Issue Paths\n\n## Status: ?\n\n## Context\n\nThe editions lambda needs to be able to identify specific versions of an issue.\n\n## Decision\n\nTo have two deployments of the backend, one for previewing, and a second for published issues.\n\nThe published issues deployment will replace the issueid path parameter with source/issueid.\n\n`source` will identify which file in the published bucket will be retreived to form the issue on.\n\n## Alternatives\n\nThe issue could be identified with a get parameter and they could both run in the same deployment.\n\n## Consequences\n\nThe published issues lambda will not be directly callable from the app.\n"}
{"repositoryUrl": "https://github.com/phpactor/amp-fswatch.git", "path": "adr/0002-do-not-use-callback.md", "template": "unknown", "status": null, "firstCommit": "2020-03-27T20:18:53Z", "lastCommit": "2020-03-29T08:31:43Z", "numberOfCommits": 2, "title": "(sem título)", "wordCount": 195, "authors": {"name1": 2}, "content": "Do not use a callback in the public API\n===\n\nContext\n---\n\nIn 0001 it was decided to use a callback to handle modified files. When used\nwith real code:\n\n```php\n$this->watcher->watch($this->paths, function (ModifiedFile $file) {\n  asyncCall(function () use ($file) {\n  $job = $this->indexer->getJob($file->path());\n\n  foreach ($job->generator() as $file) {\n  yield new Delayed(1);\n  }\n  });\n});\n```\n\nWhich is just odd. The callback is not part of the co-routine. Using a promise\nimproves this:\n\n```php\nwhile (null !== $file = yield $watcher->wait())\n  $job = $this->indexer->getJob($file->path());\n\n  foreach ($job->generator() as $file) {\n  yield new Delayed(1);\n  }\n});\n```\n\nDecision\n--\n\nRefactor the code to yield promises for modified files.\n\nWhilst initially I thought this would be quite difficult, it didn't take long.\nEach watcher has an async co-routing which builds a queue of modified files\nwhich are then subsequently yieled when `->wait()` is called on the `Watcher`\n(if there are no files, then we pause the co-routine for some milliseconds\nthen try again).\n\nConsequences\n--\n\nIt should be easier to integrate this library into Amp projects. On the\ndownside it does mean coupling Amp to the public API - but seeing as this\npackage is called AmpFsWatch, that's acceptable.\n"}
{"repositoryUrl": "https://github.com/PakkuDon/pixel-art-gallery.git", "path": "doc/adr/0002-store-pixel-art-in-github-repository.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-11-07T12:56:22Z", "lastCommit": "2020-11-16T10:06:17Z", "numberOfCommits": 2, "title": "2. Store pixel art in Github repository", "wordCount": 157, "authors": {"name1": 2}, "content": "# 2. Store pixel art in Github repository\n\nDate: 2020-11-06\n\n## Status\n\nAccepted\n\n## Context\n\nI've recently gotten into pixel art. I wanted a place to host these so I could share them later if I felt so inclined. But I didn't want to create a new account or use an existing account that had any PII associated with it.\n\nI also figured I could make another side project out of this.\n\n## Decision\n\n- Pixel art will be stored in a git repository that will be hosted on Github.\n- Link to image and associated details will be stored in a text file so they can be displayed in a UI later.\n\n## Consequences\n\n- Images and their associated metadata will be stored under version control so that they can be revised or reverted with relative ease.\n- As new images will be added in git commits my Github contribution graph may still appear green even if I haven't coded at all.\n"}
{"repositoryUrl": "https://github.com/vwt-digital/operational-data-hub.git", "path": "architecture/adr/0054-coding-guidelines.md", "template": "Nygard", "status": "Accepted Implements [4. Create software defined everything](0004-create-software-defined-everything.md)", "firstCommit": "2020-09-21T14:31:15Z", "lastCommit": "2020-11-03T08:29:40Z", "numberOfCommits": 2, "title": "54. Coding guidelines", "wordCount": 163, "authors": {"name1": 1, "name2": 1}, "content": "# 54. Coding guidelines\n\nDate: 2020-09-21\n\n## Status\n\nAccepted\n\nImplements [4. Create software defined everything](0004-create-software-defined-everything.md)\n\n## Context\n\nCoding conventions are a set of guidelines for a specific programming language that recommend programming style, practices, and methods for each aspect of a program written in that language. These conventions usually cover file organization, indentation, comments, declarations, statements, white space, naming conventions, programming practices, programming principles, programming rules of thumb, architectural best practices, etc. These are guidelines for software structural quality. Software programmers are highly recommended to follow these guidelines to help improve the readability of their source code and make software maintenance easier.\n\nCoding guidelines result in enhanced efficiency, reduced risk, mininized complexity, maintainability, bug rectification, comprehensive look and cost efficiency.\n\n## Decision\n\nWe will use coding guidelines for all languages used on the platform.\n\n## Consequences\n\nCoding guidelines have to be defined, learned and maintained. However, a lot of common standardized coding guidelines exist, coming with tools to simplify implementation.\n\n## References\n\n* https://en.wikipedia.org/wiki/Coding_conventions, retrieved 3 November 2020\n* https://www.multidots.com/importance-of-code-quality-and-coding-standard-in-software-development, retrieved 3 November 2020\n"}
{"repositoryUrl": "https://github.com/alphagov/monitoring-doc.git", "path": "documentation/architecture/decisions/0009-use-cloud-init-to-build-prometheus-server.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-08-15T11:04:12Z", "lastCommit": "2018-11-06T16:16:27Z", "numberOfCommits": 3, "title": "9. Use Cloud Init to build Prometheus Server", "wordCount": 142, "authors": {"name1": 1, "name2": 2}, "content": "# 9. Use Cloud Init to build Prometheus Server\n\nDate: 2018-08-15\n\n## Status\n\nAccepted\n\n## Context\n\nThe Prometheus Server needs to be built in a reproducible way within AWS.\nReproducible in this context means that the server can be built, destroyed and rebuilt.\nThe rebuilt server will be identical to the original server and the is no external intervention required (i.e. logging into the server to make changes to configuration)\n\n## Decision\n\nCloud init will be used to build a reproducible server.\n\n## Consequences\n\nThe cloud init was chosen over other strategies such as creating a machine image because there is prior art for building a Prometheus server with cloud init and building machine images requires additional tools.\nIt was felt that cloud init will be the fastest way to achieve the short term goals.\nThe use of cloud init should be reviewed at the earliest opportunity.\n"}
{"repositoryUrl": "https://github.com/alphagov/verify-matching-service-adapter.git", "path": "doc/adr/0001-record-architecture-decisions.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-10-31T14:03:56Z", "lastCommit": "2017-10-31T14:03:56Z", "numberOfCommits": 1, "title": "1. Record architecture decisions", "wordCount": 45, "authors": {"name1": 1}, "content": "# 1. Record architecture decisions\n\nDate: 15/05/2017\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to record the architectural decisions made on this project.\n\n## Decision\n\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n\n## Consequences\n\nSee Michael Nygard's article, linked above.\n"}
{"repositoryUrl": "https://github.com/thomvaill/log4brains.git", "path": "docs/adr/20200925-use-prettier-eslint-airbnb-for-the-code-style.md", "template": "Madr", "status": "accepted", "firstCommit": "2020-12-10T08:48:42Z", "lastCommit": "2020-12-10T08:48:42Z", "numberOfCommits": 1, "title": "Use Prettier-ESLint Airbnb for the code style", "wordCount": 155, "authors": {"name1": 1}, "content": "# Use Prettier-ESLint Airbnb for the code style\n\n- Status: accepted\n- Date: 2020-09-25\n\n## Context and Problem Statement\n\nWe have to choose our lint and format tools, and the code style to enforce as well.\n\n## Considered Options\n\n- Prettier only\n- ESLint only\n- ESLint with Airbnb code style\n- ESLint with StandardJS code style\n- ESLint with Google code style\n- Prettier-ESLint with Airbnb code style\n- Prettier-ESLint with StandardJS code style\n- Prettier-ESLint with Google code style\n\n## Decision Outcome\n\nChosen option: \"Prettier-ESLint with Airbnb code style\", because\n\n- Airbnb code style is widely used (see [npm trends](https://www.npmtrends.com/eslint-config-airbnb-vs-eslint-config-google-vs-standard-vs-eslint-config-standard))\n- Prettier-ESLint enforce some additional code style. We like it because the more opinionated the code style is, the less debates there will be :-)\n\nIn addition, we use also Prettier to format json and markdown files.\n\n### Positive Consequences <!-- optional -->\n\n- Developers are encouraged to use the [Prettier ESLint](https://marketplace.visualstudio.com/items?itemName=rvest.vs-code-prettier-eslint) and [Prettier](https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode) VSCode extensions while developing to auto-format the files on save\n- And they are encouraged to use the [ESLint VS Code extension](https://marketplace.visualstudio.com/items?itemName=dbaeumer.vscode-eslint) as well to highlight linting issues while developing\n"}
{"repositoryUrl": "https://github.com/Accenture/sfpowerscripts.git", "path": "decision records/002-parallel-development-process.md", "template": "unknown", "status": null, "firstCommit": "2021-05-27T22:12:15Z", "lastCommit": "2021-05-27T22:12:15Z", "numberOfCommits": 1, "title": "Process for creating parallel development streams", "wordCount": 217, "authors": {"name1": 1}, "content": "# Process for creating parallel development streams\n\n* Status: Rejected  <!-- optional -->\n\n\n## Context and Problem Statement\n\nWhen creating a parallel development streams (e.g. release), there are a list of manual steps that must be performed:\n\n- package versions may need to be updated so that packages between streams do not share the same version-space\n- a new artifact feed or npm tags need to be created\n- a set of artifacts needs to be created for the new development stream\n\n## Options\n1. **CLI helper tool for creating parallel dev streams**\n  - Create a CLI tool that automates the tasks required when creating parallel development streams:\n  - fetch last set of artifacts from source feed and publish them to the target feed; or\n  - create NPM tags that point to the latest artifact versions from parent stream\n  - increments package versions in the parent stream\n  - create a new git branch\n2. **Document the process**\n  - Document the process for creating parallel development streams in Gitbook\n\n## Decision\n\nAll this issues arised from the fact that with the assumption of using multiple feeds. As we are moving to ask users to utilize a single feed/artifact repository and how it is in most platforms like GitHub or GitLab, there is no specific need for a helper tool. Users should be versioning their artifacts using semantic version when dealing with multiple development streams\n"}
{"repositoryUrl": "https://github.com/hmrc/tech-radar.git", "path": "architecture-decision-records/adr-1_create-hybrid-tech-radar.md", "template": "Nygard", "status": "Accepted.", "firstCommit": "2019-02-01T16:42:10Z", "lastCommit": "2019-02-01T16:42:10Z", "numberOfCommits": 1, "title": "ADR 1: Create Hybrid Tech Radar", "wordCount": 270, "authors": {"name1": 1}, "content": "# ADR 1: Create Hybrid Tech Radar\n\n## Context\n\nTechnical material is not unified across the different Architecture spaces.  Additionally material is not consistently broken down and represented even within a single one of those spaces.  We need to increase consistency, accuracy and re-usability, and remove ambiguity to permit a better technical toolkit to be created to decentralise architecture, and implementation across the delivery centres.  There has been no shared understanding on what the pieces that comprise a technical toolkit should be.\n\nSome material has not been maintained/updated and the work has not been opened to a large enough pool of contributors.\n\nThere is not a high enough level of engagement with technical material across many distributed teams to achieve a high enough level of quality according to a set of standards designed to support operational strategies.\n\n## Decision\n\nWe will create a hybrid Tech Radar, fully describing a set of lego building blocks for forming architectural material usable across the entire architecture.  We will reach a shared understanding on the level of granularity is with our building blocks, and in terms of the definitions of what those building blocks are.\n\nThe tech radar will be dropped periodically with updates to increase and maintain engagement with the material.\n\nMaterial will be labelled across several axes (TBC) to provide indications on maturity of content, and suitability or relevance to different audiences etc.\n\n## Status\n\nAccepted.\n\n## Consequences\n\nRequires some complex activity in de-duplicating artefacts across the different areas of the architecture.  Requires a decision making process to reach consensus on how to edit material so it is compatible as we collide it in a single repository.\n"}
{"repositoryUrl": "https://github.com/learnitmyway/my-notes.git", "path": "adr/create-react-app.md", "template": "unknown", "status": null, "firstCommit": "2019-04-13T09:21:32Z", "lastCommit": "2019-04-13T09:21:32Z", "numberOfCommits": 1, "title": "Fork and extend Create React App", "wordCount": 100, "authors": {"name1": 1}, "content": "# Fork and extend Create React App\n\n## April 13 2019\n\n### Context\n\nIt would be nice to have `babel-plugin-jsx-remove-data-test-id` but I don't know of a way to add it to Create React App.\nit would be possible to fork CRA and adjust the config there. However, that seems to involved the following:\n\n1. Fork CRA\n2. Add plugin to babel-preset-react-app\n3. Publish developerdavo-babel-preset-react-app\n4. Point to developerdavo-babel-preset-react-app in react-scripts\n5. Publish developerdavo-react-scripts\n\nNot to mention, local development requires package linking and updating dependencies in developerdavo-babel-preset-react-app would require publishing two packages.\n\n### Decision\n\nIt doesn't seem to be worth the maintenance cost\n"}
{"repositoryUrl": "https://github.com/yldio/asap-hub.git", "path": "docs/decision/02-email-provider.md", "template": "unknown", "status": null, "firstCommit": "2020-05-19T10:43:10Z", "lastCommit": "2020-05-27T17:21:26Z", "numberOfCommits": 2, "title": "Email provider", "wordCount": 253, "authors": {"name1": 1, "name2": 1}, "content": "# Email provider\n\nStatus: Final\n\nDate: 2020-05-19\n\nAuthor: Filipe Pinheiro <filipe@yld.io>\n\nReviewed-by: Tim Seckinger <tim.seckinger@yld.io>\n\n## Context\n\nThe majority of web applications uses email as a communication channel. The ASAP Hub is no different, and emails are a way to keep the communication with users.\n\nThe requirements for an email service are the following:\n\n- **HTTP API**. The API allows us to send emails to our users programmatically. The API also works as the integration point and an excellent candidate to decouple sending emails from the other responsibilities of the service.\n- Provide a **template** engine. To create a consistent experience and mitigate misconfigured and poorly formatted emails, it's a good practice to have a template and send the data to fill the model to the email provider. The creation of the templates also enables us to decouple their production and test them in isolation.\n- Provide an **SMTP** configuration. [Auth0](../spike/0016-auth0.md) integrates seamlessly with several email services, but SMTP is also an option for the others.\n- (Optional) Track events about the emails sent\n\n## Options\n\nThe providers we looked into that fit he previous requirements were:\n\n- [Amazon SES](https://aws.amazon.com/ses/)\n- [Postmark](https://postmarkapp.com/)\n- [SendGrid](https://sendgrid.com/)\n- [Mailchimp](https://mailchimp.com/)\n- [Mailgun](https://www.mailgun.com/)\n- [SparkPost](https://www.sparkpost.com/)\n\n## Decision\n\nAt this stage of development, we decide to use Amazon SES. The disadvantage of using Amazon SES is the lack of a dashboard and the visibility of the emails sent. Still, we believe that the ease of integration at this stage of the project is important to deliver the first user stories quickly. We will make sure that the email provider can be changed with reasonable effort in the future.\n"}
{"repositoryUrl": "https://github.com/dennisseidel/saas-platform-frontend.git", "path": "adr/0010-use-aws-amplify-coginito-for-login-and-the-identity-provider.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-03-03T12:02:03Z", "lastCommit": "2019-06-19T12:13:49Z", "numberOfCommits": 2, "title": "10. Use aws amplify and coginito for login and the Identity Provider", "wordCount": 124, "authors": {"name1": 1, "name2": 1}, "content": "# 10. Use aws amplify and coginito for login and the Identity Provider\n\nDate: 2019-03-02\n\n## Status\n\nAccepted\n\n## Context\n\nA user expect that he can create an account and login into our application. This should be standard compliant, economical (cheap) and easy to implement.\n\n## Decision\n\nWe use AWS Amplify (as a frontend lib) and AWS Cognito as the Identity Provider.\n\n## Consequences\n\nThis makes it easy to setup within minuites in the app with the react pre defined components, as well as an mostly standard compliant IDP with AWS Amplify that has a ok pricing model - that is better then Auth0. I have have to check if the application gets more users if it is not more economic to switch to a self hosted version like Keyloak."}
{"repositoryUrl": "https://github.com/theaiscope/GDD-app.git", "path": "doc/adr/0003-use-bitrise-for-ci.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-06-27T10:19:10Z", "lastCommit": "2019-06-27T20:44:53Z", "numberOfCommits": 2, "title": "3. Use Bitrise for CI", "wordCount": 123, "authors": {"name1": 1, "name2": 1}, "content": "# 3. Use Bitrise for CI\n\nDate: 2019-06-04\n\n## Status\n\nAccepted\n\n## Context\n\nWe need an easy way to integrate and test out code that is fast and reliable.\n\n## Decision\n\nWe choose Bitrise because it came from a suggestion from more senior devs and provides an easy interface to manage the workflows with good support for Android apps and testing.\nIt also allows us to notify users in a easy way and with different roles. \n\n## Consequences\n\nBitrise does not allow us to use Pipelines-as-code 100% of the way. There is some configuration that needs to be done of the web workflow editor.\nInstructions followed to setup: https://devcenter.bitrise.io/tips-and-tricks/use-bitrise-yml-from-repository/\n\nIt was very easy to setup UI testing as the default configurations of the steps are already working \n"}
{"repositoryUrl": "https://github.com/benjamminj/portfolio.git", "path": "docs/architecture/decisions/0002-use-nextjs.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-09-04T04:31:03Z", "lastCommit": "2020-09-04T04:31:03Z", "numberOfCommits": 1, "title": "2. Use NextJS", "wordCount": 322, "authors": {"name1": 1}, "content": "# 2. Use NextJS\n\nDate: 2020-09-03\n\n## Status\n\nSuperceded by [14. Migrate to SvelteKit](0014-migrate-to-sveltekit.md)\n\nSuperceded by [13. Migrate to SvelteKit](0013-migrate-to-sveltekit.md)\n\n## Context\n\nThere's a few large architectural decisions that were made before I started using ADRs, so I'm backfilling them a little.\n\n[NextJS](https://nextjs.org/) is a fantastic framework for building scalable, enterprise-grade applications with React.\n\nNextJS has a few notable contrasts to [Gatsby](https://www.gatsbyjs.com/) (which this blog was previously built on) which factored into the decision to use it here. Some of these are captured below, some of my thoughts can be found in [this article by Jared Palmer](https://jaredpalmer.com/gatsby-vs-nextjs).\n\nHere's some loose bullet points behind my reasoning behind using NextJS for this site.\n\n- NextJS comes baked in with routing, webpack configurations, TypeScript support, and a whole host of other things. Less fiddling with tools, more writing meaningful code.\n- In general, Gatsby is more \"plugin-oriented\", NextJS is more \"recipe-oriented\". While plugins are nice to get a ton of functionality, each plugin introduces another dependency that you have to maintain, upgrade. Each new dependency introduces another point of failure.\n- NextJS doesn't have a GraphQL layer baked into it. For a website of this size I felt like being required to use the GraphQL layer to access markdown files was overkill.\n- NextJS allows you to build \"hybrid\" applications—ones where there's a combination of server-rendered (SSR) pages, static-rendered pages, and prerendered pages. There's not likely to be any SSR pages in this website, but NextJS sets you up for flexibility as time goes on in regards to rendering strategies.\n- NextJS is a super in-demand, \"hot\" skill in the React community, so having my website built on top of it both keeps me fresh on the latest patterns and showcases my abilities.\n- I like working with NextJS! I like the framework and how they've chosen the APIs!\n\n## Decision\n\nI will use [NextJS](https://nextjs.org/) as the framework on top of React to build this website\n\n## Consequences\n\nThis will influence some of the patterns throughout the application. Stuff like routing, file structure, etc.\n"}
{"repositoryUrl": "https://github.com/theupdateframework/python-tuf.git", "path": "docs/adr/0009-what-is-a-reference-implementation.md", "template": "Madr", "status": "accepted", "firstCommit": "2021-08-30T17:27:45Z", "lastCommit": "2021-08-30T17:27:45Z", "numberOfCommits": 1, "title": "Primary purpose of the reference implementation", "wordCount": 357, "authors": {"name1": 1}, "content": "# Primary purpose of the reference implementation\n\n* Status: accepted\n* Date: 2021-08-25\n\n## Context and Problem Statement\n\nThe original goal for the reference implementation refactor was to provide an\nimplementation which is both an aid to understanding the specification and a\ngood architecture for other implementations to mimic.\n\nDuring refactoring efforts on the metadata API and ngclient, several friction\npoints have arisen where a safe object-oriented API would result in a less\ndirect mapping to the [Document formats] in the specification.\n\nThe archetypal example friction point is that [Timestamp] lists snapshot _only_\nin a `meta` dictionary of `METAPATH` -> attribute fields. The dictionary will\nonly ever contain one value and creates an extra level of indirection for\nimplementations which try to map to the file format.\n\nWhen presented with such cases, we have considered multiple options:\n* Strict mapping to the [Document formats]\n* Simple and safe API in preference to mapping to the [Document formats]\n* Strict mapping to the [Document formats] with additional convenience API\n  which is documented as the preferred interface for users\n\nSo far implementation has tended towards the final option, but this is\nunsatisfying because:\n* the API contains traps for the unsuspecting users\n* two code paths to achieve the same goal is likely to result in inconsistent\n  behaviour and bugs\n\nTherefore, we would like to define our primary purpose so that we can make\nconsistent decisions.\n\n[Document formats]: https://theupdateframework.github.io/specification/latest/#document-formats\n[Timestamp]: https://theupdateframework.github.io/specification/latest/#file-formats-timestamp\n\n## Decision Drivers\n\n* The reference implementation is often the starting point for new\n  implementations, porting architecture of the reference implementation to new\n  languages/frameworks\n* Reading reference implementation code is a common way to learn about TUF\n* The TUF formats include non-intuitive JSON object formats when mapping to OOP\n  objects\n* Multiple code paths/API for the same feature is a common source of bugs\n\n## Considered Options\n\nPrimary purpose of the reference implementation is:\n* a learning resource to aid understanding of the specification (pedagogical reference)\n* a good architecture for other implementations to mimic (exemplary reference)\n\n## Decision Outcome\n\nPrimary purpose of the reference implementation is as an exemplary reference:\nproviding a safe, consistent API for users and a good architecture for other\nimplementations to mimic.\n\n## Links\n\n* Discussed [on Slack](https://cloud-native.slack.com/archives/C01GT17AC5D/p1629357567021600)\n* Discussed in the [August 2021 TUF community meeting](https://hackmd.io/jdAk9rmPSpOYUdstbIvbjw#August-25-2021-Meeting)\n"}
{"repositoryUrl": "https://github.com/linagora/linshare-mobile-android-app.git", "path": "adr/0002-image-loading-with-glide.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-01-03T07:03:06Z", "lastCommit": "2020-01-03T07:03:06Z", "numberOfCommits": 1, "title": "7. Image Loading with Glide", "wordCount": 182, "authors": {"name1": 1}, "content": "# 7. Image Loading with Glide\n\nDate: 2019-12-17\n\n## Status\n\nAccepted\n\n## Context\n\nIn the android linshare application, we implement the list file from the space of user. Proposed user interface design includes the thumbnail of file preview when user wants to show\nIt is necessary to have an library to process first part is image thumbnail and preview.\nOver the best practice of image processing there are 2 libraries is very commons by android developer community: Glide and Picasso\nTo compare between them, the commonly usage is the same, but Glide have much more strengthen rather than Picasso.\nIt process the image source and have method to generate the thumbnail natively, it consume less the memory than Picasso and the library is have smaller packer with much more APIs to help process image, witch could be useful later when we implement more functionality in the application\n\n## Decision\n\nWe decided to use Glide instead of Picasso.\n\n## Consequences\n\nBase on our implementation, the application is working well and for the developer it should be also save time to develop the feature base on Glide lib\n\n## References\n\n[Comparison](https://medium.com/@multidots/glide-vs-picasso-930eed42b81d).\n[Source code](https://github.com/bumptech/glide).\n"}
{"repositoryUrl": "https://github.com/NERC-CEH/datalab.git", "path": "architecture/decisions/0002-ansible-for-provisioning-tool.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-11-04T19:42:27Z", "lastCommit": "2017-11-04T19:42:27Z", "numberOfCommits": 1, "title": "2. Ansible for provisioning tool", "wordCount": 86, "authors": {"name1": 1}, "content": "# 2. Ansible for provisioning tool\n\nDate: 2017-11-04\n\n## Status\n\nAccepted\n\n## Context\n\nWe need a tool to provision servers and software for the datalabs project.\n\n## Decision\n\nWe will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella\nteam have experience using it.\n\n## Consequences\n\nAnsible does not support Windows meaning that additional work will be required to allow\nadministration from Windows laptops. This is not seen to be an issue as the project\nteam have been provided with Linux workstations on the client network.\n"}
{"repositoryUrl": "https://github.com/umd-lib/caia.git", "path": "docs/adr/0011-pin-mbtest-library-to-v2.5.1.md", "template": "unknown", "status": null, "firstCommit": "2021-03-26T13:44:40Z", "lastCommit": "2021-03-26T13:44:40Z", "numberOfCommits": 1, "title": "0011 - Pin \"mbtest\" library to v2.5.1", "wordCount": 272, "authors": {"name1": 1}, "content": "# 0011 - Pin \"mbtest\" library to v2.5.1\n\nDate: March 26, 2021\n\n## Context\n\nOn March 22, 2021, it was discovered that the \"caia\" Jenkins builds were\nfailing. All the failing tests were failing with the following error, related\nto a \"get_actual_requests\" method call:\n\n```\nTypeError: 'generator' object is not subscriptable\n```\n\nThe \"caia\" build was last successful in Jenkins on October 7, 2020. No builds\nwere performed again until March 22, 2021, as there was no development work\nbeing done on the project.\n\nBuilds were made on March 22, 2021 because of a move to the\n\"GitHub organization\" pipeline in LIBITD-1880, which triggered rebuilds in all\nexisting projects.\n\nWhen the last successful build was made in October, the \"mbtest\" library\n([https://github.com/brunns/mbtest](mbtest)) was at v2.5.1. In v2.5.2, the\n\"src/mbtest/server.py\" file was modified, changing the \"get_actual_requests\"\nmethod signature (see [this commit e398f2f1f32420](mbtest_commit)). from:\n\n```\ndef get_actual_requests(self) -> Mapping[int, JsonStructure]:\n```\n\nto\n\n```\ndef get_actual_requests(self) -> Iterable[Request]:\n```\n\nThe change from a Mapping to an Iterable is the cause of the error in the tests.\n\n## Decision\n\nThe simplest solution for the moment is to \"pin\" the version of the \"mbtest\"\nlibrary to v2.5.1 in the \"setup.py\" file. This will preserve the current\nbehavior, until further \"caia\" development warrants additional testing.\n\n## Consequences\n\nThis decision improves the stability of the \"caia\" builds by pinning the\n\"mbtest\" dependency to a specific version. Since this library is only used\nfor testing, keeping up with the latest updates (i.e. for security fixes) is\nnot a concern.\n\nIf significant additional development of the \"caia\" project is performed, it\nwould likely be worthwhile to update to the lastest \"mbtest\" version, and update\nthe tests appropriately.\n\n[mbtest]: https://github.com/brunns/mbtest\n[mbtest_commit]: https://github.com/brunns/mbtest/commit/e398f2f1f324209500506cc72fa0a045b2d420f4#diff-f6d8bc80c4ba5a033a4d011f675c4b43767a86fcd51b4463bdad275911ef95b6L159-R161\n"}
{"repositoryUrl": "https://github.com/ministryofjustice/modernisation-platform.git", "path": "architecture-decision-record/0010-terraform-module-strategy.md", "template": "Nygard", "status": "✅ Accepted", "firstCommit": "2021-06-18T14:42:17Z", "lastCommit": "2021-11-02T12:19:41Z", "numberOfCommits": 3, "title": "10. Terraform Module Strategy", "wordCount": 278, "authors": {"name1": 1, "name2": 2}, "content": "# 10. Terraform Module Strategy\n\nDate: 2021-06-18\n\n## Status\n\n✅ Accepted\n\n## Context\n\nThe Modernisation Platform uses [Terraform](https://www.terraform.io/) for its infrastructure as code. To make infrastructure reusable, or to simply tidy up code you can use [Terraform Modules](https://developer.hashicorp.com/terraform/language/modules). There are different use cases in the platform for using modules, and this ADR outlines how we plan to use them.\n\n## Decision\n\nModules used only by the Modernisation Platform core infrastructure will remain in the [terraform/modules](https://github.com/ministryofjustice/modernisation-platform/tree/main/terraform/modules) folder where they are currently located. These modules are mainly single use modules but created to keep the code tidier and easier to maintain. Modules used only by the core which currently have their own repository will remain where they are.\n\nModules used by users will have their own repository per module which we link to from the main repo. These modules will be versioned with GitHub releases, and tested with [Terratest](https://terratest.gruntwork.io/) against a test AWS account.\n\n## Consequences\n\n### General consequences\n\n* new user modules will be created in their own repository\n* a new template for module repositories should be created for consistency, similar to [Cloud Platform Module Template](https://github.com/ministryofjustice/cloud-platform-terraform-template)\n* we will name modules using the format `modernisation-platform-terraform-module-name`\n* core platform modules can stay where they are currently located\n* modules should be tested with Terratest\n* we will need to create a testing environment for Terratest to run\n\n### Advantages\n\n* we can version user modules with GitHub releases to avoid breaking existing infrastructure when updating modules\n* testing gives us confidence\n* keeping our core modules together makes it easier for us to navigate the platform code\n* we don't need to re-version core modules whenever we make a change to them\n\n### Disadvantages\n\n* user modules are scattered in various repositories (we will signpost them from the main repo to make it easier to find them)\n"}
{"repositoryUrl": "https://github.com/jmoratilla/devops-challenge.git", "path": "doc/adr/0006-feat-add-autoscaling-policy.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-02-20T10:48:15Z", "lastCommit": "2020-02-20T10:48:15Z", "numberOfCommits": 1, "title": "6. feat-add-autoscaling-policy", "wordCount": 225, "authors": {"name1": 1}, "content": "# 6. feat-add-autoscaling-policy\n\nDate: 2020-02-20\n\n## Status\n\nAccepted\n\n## Context\n\nGoal: The platform must be scalable according to the load\n\nWe need to autoscale nodes in case we need to cope with a high load.\n\nI have researching, and I have found that we have to scale the nodes\n as well as the pods.\n\nFor some loads, scaling pods can be enough, but for other workloads it\n can be better to scale the cluster nodes.\n\nIssues then:\n\n1. Autoscaling pods\n2. Autoscaling nodes\n\n\n\n\n## Decision\n\nFor the first issue, I'm going to use:\n\n* [Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)\n\nThis object is a controller and an API resource that is included in kubectl.  It\n allows to set a period of time to check the load of a pod (cpu or custom metrics)\n and increase the number of pods running.\n\nFor the second issue, I'm going to use:\n\n* [Cluster Autoscaler Addon](https://github.com/kubernetes/kops/tree/master/addons/cluster-autoscaler)\n\nWith this addon, you can set a policy in AWS IAM and attach it to the previously\n defined autoscaling group for nodes.\n\nI haven't found another way to set the AutoScale of the nodes instanceGroup \n\n\n## Consequences\n\nThis is complex to me, as I use to manually manage the load of the system and\n plan the scaling based on schedules.\n\nA problem it can occurs is that by any mean, somebody attacks the services so\n the cluster begins to grow uncontrolled and spend money and resources.\n\n"}
{"repositoryUrl": "https://github.com/elastic/cloud-on-k8s.git", "path": "docs/design/adr-template.md", "template": "Madr", "status": "[proposed | rejected | accepted | deprecated | … | superseded by [ADR-0005](0005-example.md)] <!-- optional -->", "firstCommit": "2019-02-12T14:01:40Z", "lastCommit": "2019-02-12T14:01:40Z", "numberOfCommits": 1, "title": "[short title of solved problem and solution]", "wordCount": 248, "authors": {"name1": 1}, "content": "# [short title of solved problem and solution]\n\n* Status: [proposed | rejected | accepted | deprecated | … | superseded by [ADR-0005](0005-example.md)] <!-- optional -->\n* Deciders: [list everyone involved in the decision] <!-- optional -->\n* Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->\n\nTechnical Story: [description | ticket/issue URL] <!-- optional -->\n\n## Context and Problem Statement\n\n[Describe the context and problem statement, for example, in free form using two to three sentences. You may want to articulate the problem in form of a question.]\n\n## Decision Drivers <!-- optional -->\n\n* [driver 1, for example, a force, facing concern, …]\n* [driver 2, for example, a force, facing concern, …]\n* … <!-- numbers of drivers can vary -->\n\n## Considered Options\n\n* [option 1]\n* [option 2]\n* [option 3]\n* … <!-- numbers of options can vary -->\n\n## Decision Outcome\n\nChosen option: \"[option 1]\", because [justification. For example, only option, which meets k.o. criterion decision driver | which resolves force force | … | comes out best].\n\n### Positive Consequences <!-- optional -->\n\n* [For example, improvement of quality attribute satisfaction, follow-up decisions required, …]\n* …\n\n### Negative Consequences <!-- optional -->\n\n* [For example, compromising quality attribute, follow-up decisions required, …]\n* …\n\n## Pros and Cons of the Options <!-- optional -->\n\n### [option 1]\n\n[example | description | pointer to more information | …] <!-- optional -->\n\n* Good, because [argument a]\n* Good, because [argument b]\n* Bad, because [argument c]\n* … <!-- numbers of pros and cons can vary -->\n\n### [option 2]\n\n[example | description | pointer to more information | …] <!-- optional -->\n\n* Good, because [argument a]\n* Good, because [argument b]\n* Bad, because [argument c]\n* … <!-- numbers of pros and cons can vary -->\n\n### [option 3]\n\n[example | description | pointer to more information | …] <!-- optional -->\n\n* Good, because [argument a]\n* Good, because [argument b]\n* Bad, because [argument c]\n* … <!-- numbers of pros and cons can vary -->\n\n## Links <!-- optional -->\n\n* [Link type] [Link to ADR] <!-- example: Refined by [ADR-0005](0005-example.md) -->\n* … <!-- numbers of links can vary -->\n"}
{"repositoryUrl": "https://github.com/KIT-SOC4S/ftd-scratch3-offline.git", "path": "docs/architecture/decisions/0008-use-scratch-saves-as-input-for-conversion-to-c.md", "template": "Nygard", "status": "Accepted Amended by [19. Allow direct input of project.json without a .sb3 file](0019-allow-direct-input-of-project-json-without-a-sb3-file.md)", "firstCommit": "2019-10-20T19:19:01Z", "lastCommit": "2019-10-31T23:21:32Z", "numberOfCommits": 2, "title": "8. Use scratch saves as input for conversion to C", "wordCount": 126, "authors": {"name1": 2}, "content": "# 8. Use scratch saves as input for conversion to C\n\nDate: 2019-10-20\n\n## Status\n\nAccepted\n\nAmended by [19. Allow direct input of project.json without a .sb3 file](0019-allow-direct-input-of-project-json-without-a-sb3-file.md)\n\n## Context\n\nWe need a way to get the used scratch blocks from a scratch program.  \nWe could directly use the format scratch internally (when running in the browser) uses to represent the program.  \nThis would mean that more code would have to be written in Javascript to interface with scratch.  \nThis would also couple us tightly to the internal representation scratch uses.  \nWe could use the format scratch uses to save the program.  \nIts format is defined [here](https://en.scratch-wiki.info/wiki/Scratch_File_Format) and it is stable for scratch3.  \nUsing the scratch saves as input is also more flexible than using an internal representation.  \n\n## Decision\n\nWe will use scratch's .sb3 save files as input.\n\n## Consequences\n\n-\n"}
{"repositoryUrl": "https://github.com/pulibrary/figgy.git", "path": "architecture-decisions/0003-preservation.md", "template": "unknown", "status": null, "firstCommit": "2019-04-11T19:19:17Z", "lastCommit": "2019-04-11T19:19:17Z", "numberOfCommits": 1, "title": "3. Preservation", "wordCount": 856, "authors": {"name1": 1}, "content": "# 3. Preservation\n\nDate: 2019-04-02\n\n## Status\n\nAccepted\n\n## Context\n\nWe have agreed that we will preserve digital objects by saving resources in\nGoogle Cloud Storage in a directory structure which preserves both the binaries\nthe resource is made up of as well as the JSON serialization of the resource\nitself.\n\n## Decisions\n\n1. Preserving\n   1. We will preserve materials in Google Cloud Coldline Storage with\n  `versioning` enabled. Versions will be kept indefinitely and without\n  limit. All files will go in a single bucket.\n  - Staging bucket is configured with the following command:\n  ```\n  gsutil mb -c regional -l us-west1 -p pulibrary-figgy-storage-1 gs://figgy-staging-preservation\n  echo '{\"rule\": [{\"action\": {\"type\": \"Delete\"}, \"condition\": {\"age\": 2}}]}' > lifecycle.json\n  gsutil lifecycle set lifecycle.json gs://figgy-staging-preservation\n  rm lifecycle.json\n  gsutil bucketpolicyonly set on gs://figgy-staging-preservation\n  gsutil iam ch serviceAccount:figgy-staging@pulibrary-figgy-storage-1.iam.gserviceaccount.com:objectAdmin gs://figgy-staging-preservation\n  ```\n  - Production bucket is configured with the following command:\n  ```\n  gsutil mb -c coldline -l us-west1 -p pulibrary-figgy-storage-1 gs://figgy-preservation\n  gsutil bucketpolicyonly set on gs://figgy-preservation\n  gsutil iam ch serviceAccount:figgy-preservation-production@pulibrary-figgy-storage-1.iam.gserviceaccount.com:objectAdmin gs://figgy-preservation\n  gsutil versioning set on gs://figgy-preservation\n  ```\n   1. When a resource is `complete` and marked with the `cloud` preservation\n  policy it will save itself and all resources contained in `member_ids` in\n  a directory structure in Google Cloud Storage that looks like the\n  following:\n  ```\n  - <resource-id>\n  - data\n  - <child-id>\n  - <child-id>.json\n  - <binary.tif>\n  - <resource-id>.json\n  ```\n   1. Children are preserved on save if their parents are preserved.\n   1. Related objects such as collections, Ephemera Terms, etc. will not be\n  packaged inside the preserved object. If it's important they be preserved,\n  those objects should be marked with the `cloud` preservation policy.\n   1. When a FileSet is added to a resource which is already complete and marked\n  with the `cloud` preservation policy, it will upload the new binary\n  content to both the repository and to Google Cloud Storage.\n   1. If a child is marked to be preserved, but its parent is not, it will still\n  save in a nested directory structure, but will not automatically create\n  backups of its parents.\n   1. This behavior will be attached to the ChangeSetPersister.\n1. Packaging Details\n   1. When preserved a `PreservationObject` will be created in Figgy with a\n  `preserved_object_id` property which points to the object it's preserving.\n   1. Each `PreservationObject` will contain `FileMetadata` for the binary\n  object as well as a serialized JSON file of the resource it's preserving.\n  On upload to preservation, those items' checksums will be calculated and\n  stored on the `PreservationObject`.\n   1. JSON metadata will have the use `pcdm:PreservedMetadata` and binary\n  content will have the use `pcdm:PreservationCopy`\n   1. We will only keep the most recent version of any file, overwriting any\n  files which match the same file name, but relying on versioning to go back\n  if necessary.\n   1. When a preserved resource is deleted, we will delete its directory from\n  preservation storage. If we need to get it again, we will look at Google\n  Cloud Storage's stored versions.\n   1. If a child's hierarchy changes (it moves parents), we will move the\n  content in the preservation storage to match.\n   1. When a file's binary content is replaced on disk, we will upload a new\n  copy of the file to preservation and calculate a new checksum.\n2. Fixity Checks\n   1. Technical details of fixity checking will occur in a later ADR.\n   1. A random subset of the preserved copies will have their files pulled down\n  from preservation storage, their checksums calculated as they're streamed,\n  and then compared to the checksum of the object stored locally.\n   1. In the case of a failure it will be reported to Figgy and displayed in a\n  dashboard for further follow-up and repair.\n\n## Consequences\n\n1. Structure\n   1. The resultant structure will not be in a format that is expected by\n  outside vendors. However, we have a BagIt packager for those use cases,\n  and this structure can be easily converted to a BagIt bag by creating\n  manifests using the checksums in the metadata files.\n1. \"State at Time of Preservation\"\n   1. As we are not preserving \"related\" resources as part of the resource, we\n  are not preserving the values of controlled vocabularies at time of\n  preservation. As of now, we do not have this use case, and are more\n  concerned with our material not being lost in the event of a technical\n  failure.\n1. Storage Format\n   1. Storing items in individual files means we will be unlikely to move to\n  another cloud storage with a delay on reads, like AWS Glacier.\n1. Finding a child resource\n   1. Storing in a nested structure means if somebody needs to find a child\n  resource we need information about its parent in order to find it. We\n  expect this to not be a problem - requests are often \"can I get page 6 of\n  X\", not \"can I get the file with this ID.\" However, if necessary we can\n  iterate over the file listing in cloud storage to find it.\n1. Versioning\n   1. Versioning everything may mean keeping copies of things that are never\n  used, and wasting space. We don't expect this to be a big problem as files\n  don't move around a lot post-complete. If it is, we can re-evaluate our\n  versioning strategy.\n"}
{"repositoryUrl": "https://github.com/xebia-france/xebikart-infra.git", "path": "doc/adr/002-use-rabbitmq-with-mqtt-plugin-to-make-devices-communicate-with-each-other.md", "template": "Madr", "status": "Accepted", "firstCommit": "2019-03-19T10:02:07Z", "lastCommit": "2019-03-19T10:02:07Z", "numberOfCommits": 1, "title": "Use RabbitMQ with MQTT plugin as message broker for devices so they can communicate with each other", "wordCount": 383, "authors": {"name1": 1}, "content": "# Use RabbitMQ with MQTT plugin as message broker for devices so they can communicate with each other\n\n* Status: Accepted\n- Date: 2019-03-11\n- Deciders:\n  - achotard\n  - blacroix\n  - jmartinsanchez\n\n## Context and Problem Statement\n\nWe want a message broker so the devices and other applications can communicate with the backend.\n\nWhat broker and protocol should we use?\n\n## Decision Drivers <!-- optional -->\n\n- Applicability regarding IoT projects : low-resources clients, etc\n- Possibility to use it to stream frames/images coming from cars cameras\n- Ease of deployment on Kubernetes\n- Existing knowledge of the team\n\n## Considered Options\n\n- [RabbitMQ](https://www.rabbitmq.com/), optionally with [MQTT plugin](https://www.rabbitmq.com/mqtt.html)\n- [VerneMQ](https://vernemq.com/)\n- Non-MQTT brokers\n  - [Kafka](https://kafka.apache.org/)\n  - [NATS](https://nats.io/)\n  - [AWS Kinesis](https://aws.amazon.com/kinesis/)\n  - [Google Cloud Pub/Sub](https://cloud.google.com/pubsub/)\n\n## Decision Outcome\n\nChosen option: **[RabbitMQ](TODO) with [MQTT plugin](https://www.rabbitmq.com/mqtt.html)**, because:\n\n- It is already well-known among the team\n- It has some [existing \"official\" Helm chart](https://github.com/helm/charts/tree/master/stable/rabbitmq)\n- It seems like a good fit to iterate fast\n\nWe **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.\n\nWe also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.\n\n## Pros and Cons of the Options <!-- optional -->\n\n### RabbitMQ\n\nPros:\n\n- Existing [\"official\" Helm chart](https://github.com/helm/charts/tree/master/stable/rabbitmq) ready to be deployed on Kubernetes\n- Another [\"official\" Helm chart supporting High\n  Availability](https://github.com/helm/charts/tree/master/stable/rabbitmq-ha/)\n  RabbitMQ clusters on Kubernetes\n- Official [MQTT plugin](https://www.rabbitmq.com/mqtt.html) supported in core distribution\n\nCons:\n\n- Too obvious :D\n- Doesn't scale as well as alternatives\n\n### VerneMQ\n\nPros:\n\n- Is the most performant MQTT broker out there\n- Is known to be scalable\n- Is pretty cool\n\nCons:\n\n- No \"official\" Helm chart ready to be deployed on Kubernetes\n- Not existing knowledge about it\n\n### Non-MQTT brokers\n\nNon-MQTT message brokers such as NATS, Kafka, AWS Kinesis and Google Cloud Pub/Sub were quickly put aside due to their lack of support for the MQTT protocol.\n\n**Managed services** such as Kinesis or Pub/Sub are also eliminated because they would lock us with a given provider and we want to be able to host the entire XebiKart infrastructure anywhere (envisionning going multi-cloud). This is also the reason behind the lack of consideration for things such as **AWS IoT** or equivalent MQTT managed solutions.\n\n**Kafka** didn't look like a good candidate for our use-case (also not supporting MQTT), as well as native NATS even if this one is the most ready to be distributed on Kubernetes. **We are still considering both for a later stage** when we will be deeper in the project and when we will be able to split brokers according to their usage.\n"}
{"repositoryUrl": "https://github.com/Alfresco/SearchServices.git", "path": "search-services/alfresco-search/doc/architecture/decisions/0008-content-store-replication.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-09-24T01:37:03Z", "lastCommit": "2019-09-25T14:30:53Z", "numberOfCommits": 3, "title": "1. (Solr)ContentStore Replication", "wordCount": 1936, "authors": {"name1": 3}, "content": "# 1. (Solr)ContentStore Replication\n\nDate: 2019-24-09\n\n## Status\nAccepted\n\n## Context\nSolr Index Replication distributes a copy of a master index to one or more slave servers. \nThe master server is in charge to manage updates to the index while all querying is handled by the slaves. \nThis division of labor enables Solr to scale to provide adequate responsiveness to queries against large search volumes.  \n  \n![Solr Replication](https://lucene.apache.org/solr/guide/6_6/images/index-replication/worddav2b7e14725d898b4104cdd9c502fc77cd.png)\n\n_(Source: Apache Solr Reference Guide)_  \n\nSearchServices, being a set of extensions written on top of Apache Solr, leverages this mechanism for replicating the index. \nHowever, there's an external part of data which is managed out of the plain Lucene index: the ContentStore.   \n   \nA ContentStore is composed by several files, specifically one for each indexed document; each of them is a SolrInputDocument instance stored on filesystem using a \nserialized and compressed form.   \n\n![ContentStore - Filesystem view](images/filesystem.png)\n\nThe ContentStore is where SearchServices maintains the input data in its original form, before entering in Solr and therefore before \napplying any text analysis. This source of data is needed for managing some search-related capabilities like: \n\n- fingerprinting\n- highlighting\n- clustering \n- stored (i.e. original) field value retrieval at query time \n\nIn a master/slave(s) infrastructure, the ContentStore must be managed on the master and it must be replicated on each slaves.  \nUnfortunately, the default built-in Solr replication mechanism considers only: \n\n- the Lucene index datafiles\n- the configuration files under the core \"conf\" folder (e.g. schema.xml. stopwords.txt) \n\nAs consequence of that, if we use the default Solr ReplicationHandler the ContentStore won't be replicated on slaves.  \n\n### How the built-in Solr ReplicationHandler works\nThe index replication mechanism is provided by org.apache.solr.handler.ReplicationHandler. \nThe component is quite complex because it contains the logic needed on both sides (master and slave) and therefore plays a different workflow depending on the role of the Solr node:\n\n- **MASTER**: the node is responsible to manage the index as consequence of deletes, adds, updates. The replication handler doesn't play an active role here; it just takes care about incrementing the index version, other than providing the required endpoints for retrieving the index datafiles \n- **SLAVE**:  the node maintains a local index version and (when the replication is enabled) periodically polls the master in order to detect index changes that need to be replicated\n- **REPEATER**: a node that acts at the same time as a MASTER and SLAVE. It is used as an intermediate node for replicating the index when slaves are on a different datacenter  \n\nThe _CommitVersionInfo_, which drives the replication process, is composed by two different values: \n\n- **index version**: a number indicating the index version; it is timestamp-based and it changes every time an update (i.e. add, delete) is applied to the index\n- **generation**: a lucene index is composed by several segments. Each of them has a progressive identification number called \"generation\". The one with the largest generation is the active one; from a replication perspective, the generation number doesn't actually indicate the \"active\" segment, but instead the current 'replicateable' one.\n\n#### Available services\nThere is only one single endpoint (/replication) provided by the ReplicationHandler.  \nThen, through a \"command\" request parameter we can indicate the logical \"endpoint\" we want to execute.  \nThe available commands are listed in the following table; an additional column reports if the customisation we are going to implement has some impact on it. \n\nThe following table lists the available sharding methods and the associated mnemonic codes (i.e. the value of the \"shard_method\" attribute we need to configure in solrcore.properties).\n\n|Command|Description|\n|--|---|\n|indexversion|Returns the version (index version + generation) of the latest replicatable index on the specified master or slave.|\n|filecontent|Returns the stream reference for a given file. It is used for replicating a file that needs to be synched. |\n|filelist|Returns the list of index files associated with a given generation and all configuration files included in the replication |\n|backup|An asynchronous call, which will backup data from the latest index commit point.|\n|restore|Restores an index snapshot into the current core. |\n|status|Backup status|\n|restorestatus|Restore status|\n|deletebackup|Deletes an index snapshot previously created. |\n|fetchindex|Forces the specified slave to fetch a copy of the index from its master.|\n|enablepoll|(Slave only) enables the polling on a slave node|\n|disablepoll|(Slave only) disables the polling on a slave node|\n|enablereplication|(Master only) Enables replication on the master for all its slaves.|\n|disablereplication|(Master only) Disables replication on the master for all its slaves.|\n|details|Returns the replication status and the current node configuration|\n|commits|Gets the commit points for the index.|\n\n#### Workflow\n\nAs briefly mentioned, the runtime logic of the ReplicationHandler depends on the role of the hosting Solr node.\n\n##### Master\n\nWhen the node is a master, the main responsibilities, from a replication perspective, are: \n * **update the index version and generation**: The index version is the actual trigger of the replication mechanism. The index version is incremented / updated each time a change (or a set of changes) is applied to the index, while the generation follows a similar path but it directly maintained at low level by Lucene. \n * **expose the relevant endpoints** needed and called by the slaves for replicating the data.\n \n##### Slave \n\nIn the \"Slave\" mode, other than providing more or less the same endpoints seen above (with a slightly different semantic, indeed), the main difference is the \"active\" role played by a background thread which, simplifying, performs the following operations: \n * get the latest index version and generation (command=indexversion) from the master and from the slave\n * if the versions are the same then no replication happens, otherwise\n * a temporary directory which will hold the downloaded data is created (index.<timestamp>)\n * the list of files to be replicated is fetched (command=filelist); this includes datafiles and configuration files\n * for each file that needs to be replicated retrieve and download the content (command=filecontent)\n\nInternally, the set of downloaded data files could change, because Solr could decide to download only a part of the index or a full copy. \n\n## Decision\nThe SearchServices replication mechanism needs to include also the ContentStore as part of the synchronization workflow.\nIn order to do that, the additional behaviour requires a customisation of the workflow provided by the default Solr ReplicationHandler, as explained below. \n\n### Alfresco Replication Handler Design\nThe Alfresco ReplicationHandler is, as the name suggests, a customisation of the default Solr ReplicationHandler described above. \nThe customisation injects some additional logic on both side of the replication mechanism in order to efficiently replicate the content store.   \n\n#### Master\nThe master maintains, for each core, a versioning history of the owned data which consists of:\n\n- the underlying Lucene index\n- configuration files (if they have to be versioned/replicated)\n- content store (if the content store replication is enabled)\n   \nThe master replication handler configuration will declare the new ReplicationHandler class in the solrconfig.xml and it will have an additional attribute \nwhich indicates the absolute path of the content store root folder.\n\nThe \"version\" number associated with a set of changes is incremented after a persistent change (i.e. hard commit) in the Lucene index.  \nNote this behaviour is already implemented in the default ReplicationHandler, so that means we will inject the additional logic required for the content store as part of such mechanism.  \nSpecifically, we will use the version number (aka indexversion*) also for storing content store changes.   \n\nThe content store changes will be stored in a separate Lucene Index, with must respect the following requirements:\n\n- it must be present only on master nodes \n- it must be initialised on-demand, even from scratch, in case of absence  \n- it must associate the indexversion to a set of content store changes (deletes and adds)\n- it must answer efficiently to the following query: given two index versions S and M, where S < M, give me all documents (i.e. content store changes) between S and M, sorted by commit timestamp asc\n \nThe ideal candidate class where this index should be facaded is SolrContentStore. The class, other than dealing with filesystem changes, would also manage the Lucene index by executing RW operations.\nThe SolrContentStore would execute and collect all changes that will be flushed in a new document of that internal index when the CommitTracker executes a commit.   \n\nThe existing endpoints on the master side described in SEARCH-1838 would be slightly changed in the following way:\n  \n|Command|Notes|Impact|\n|--|--|--|\n|indexversion|Returns the version (index version + generation) of the latest replicatable index on the specified master or slave.|NO|\n|filecontent|The command itself won't change. The difference will be in the additional content store stream handler|YES|\n|filelist|There will be an additional \"content store\" section which would list the content store changes that must be synchronized. Specifically, the \"deletes\" section will be a list of items, while the \"upserts\" would be a virtual file (something like contentstore://blalbalba) that corresponds on the master side to the optimized stream used for transferring upserts (new adds or updates)   |YES|\n|backup| |NO*|\n|restore| | NO* |\n|status| | NO*|\n|restorestatus| |NO*|\n|deletebackup| |NO*|\n|fetchindex|We will include also a configurable option where the fetch will include also the content store.  |YES|\n|enablepoll| |NO|\n|disablepoll| |NO|\n|enablereplication| |NO|\n|disablereplication| |NO|\n|details|We will include some detail about the content store.|NO|\n|commits| |NO|\n\nThe content store is an Alfresco concept and it is currently manually backed up.   \nIn terms of design we would want it to the backup as well; however, the design described in this document is not related with that set of functionalities; \nalthough the backup management consists of a set of services provided by the ReplicationHandler, we believe it's better to create a dedicated issue that will focus on that complex aspect.   \n \n#### Slave\n   \nThe slave configuration (from the Alfresco replication perspective) would be exactly the same on slave nodes.\nOther than providing the same capabilities of the built-in handler, it will also indicate the absolute path of the content store root folder.\n\nThe workflow on the slave node would follow the existing path: \n * a slave should persist the latest applied content store version somewhere (e.g. a file called .version). If that version doesn't exist then the slave needs to get the whole content store.\n * at a scheduled intervals (see the \"pollInterval\" above) the slave polls the master using the \"indexversion\" command, in order to get the latest \"replicateable\" version of the master data\n * if the version on the slave is equal to the version on the master then nothing happens\n * if the slave version is lesser than the master version, then the replication starts**\n * the slave issues a \"filelist\" commands for retrieving the list of changes (i.e. index changes, configuration files changes and content store changes)\n * the slave asks and downloads index files\n * the slave asks and downloads configuration files\n * the slave downloads and applies the content store adds \n * the slave executes the deletes on the content store\n * the content store version number is updated in the slave  \n \n_* the Solr replication handler actually uses 2 numbers for indicating the version: the first is called \"indexversion\" and it is mainly responsible for triggering the replication process. The second is the Lucene generation, that is: the number of the last active segment in the lucene index. For our purposes, we can ignore the generation and use only the indexversion._    \n ** _if the master version is lesser than the slave version, then something strange happened between the two indexes: the only thing we can do is to replicate the whole content store (the same thing happens with the Lucene index)_  \n\n## Consequences\nThe new replication mechanism will be able to properly synchronize, from master to slave(s), the whole logical \"index\" belonging to \nthe SearchServices master node. As explained above, that includes:\n\n- the Lucene index datafiles \n- the configuration files under the \"conf\" folder\n- the ContentStore  "}
{"repositoryUrl": "https://github.com/hmcts/cnp-design-documentation.git", "path": "doc/adr/0012-pipeline-metrics.md", "template": "unknown", "status": null, "firstCommit": "2017-12-05T10:53:20Z", "lastCommit": "2017-12-11T15:44:20Z", "numberOfCommits": 4, "title": "Pipeline Metrics", "wordCount": 430, "authors": {"name1": 4}, "content": "# Pipeline Metrics\n\n## Status\n\nProposed\n\n## Context\n\nTo determine the success and health of the pipeline, the following metrics will be used.\n\n## Key Metrics\n\n### Deployment Stability\n\nTracks how effective the Pipeline is in supporting recovery from issues via deployment of remediating changes.\nThis value is a factor of the pipeline's entire processing time and should be kept as low as possible whilst maintaining trust.\n\nA combination of:\n\n* Production Deployment Failure Rate\n  * Track the number of production changes which require remediation\n  * Sourced from issue system at HMCTS (what is this?)\n* Production Recovery Time\n  * The time it takes to remediate a Production a failed change\n  * Sourced from issue system at HMCTS (what is this?)\n\n![Deployment Stability](../../img/deploy-stability.png)\n\n### Deployment Throughput\n\nTracks how effective the pipeline is at supporting frequent releases (and therefore reduction of unreleased code and increased ability to react to change.\nThis value is an indicator of the ability to deploy to production frequently. It should be kept low and ideally less than a day.\n\nA combination of:\n\n* Deployment Lead Time\n  * Time from code landing on the non-live blue/green and the switch to release\n  * Source: deploy to non-live and blue/green switch from jenkins (or wherever we trigger this)\n* Deployment Interval\n  * Time between blue/green switch releases\n  * Sourced from the tool making the blue/green switch\n\n![Deployment Throughput](../../img/deploy-throughput.png)\n\n### Build Stability\n\nIndicates the team's ability and diligence of maintaining the application in a “potentially deployable” state from the master branch - a core tenant of Continuous Delivery.\nThis value should be kept low.\n\nA combination of:\n\n* Build Failure Rate\n  * Percentage of builds that fail to reach deployment to non-live (due to compilation, testing, any checks in the pipeline)\n  * Sourced from Jenkins\n* Build Failure Recovery Time\n  * Average time between a failed build and the next successful build that deploys to pre-release\n  * Sourced from Jenkins\n\n![Build Stability](../../img/build-stability.png)\n\n### Build Throughput\n\nIndicates the frequency of change in an application.  In an actively developed application this value should be relatively high - frequent changes being built and deployed to pre-release - at least once or twice a day.\nAs artefacts aren't used, we'll approximate to the deploy to non-live of blue/green.\n\nA combination of:\n\n* Build Lead Time\n  * Time from “merge to master” to deploy to non-live, i.e. before blue/green switch\n  * Sourced from Jenkins\n* Build Interval\n  * Time between deploys to pre-release\n  * Sourced from Jenkins\n\n![Build Throughput](../../img/build-throughput.png)\n\n### Mainline (master branch) Throughput\n\nTracks the rate at which code is committed to master - an indicator of Continuous Integration and whether code is hanging around in unmerged branches.\n\nA combination of:\n\n* Mainline Lead Time\n  * Time between a commit and its merge to master\n  * Sourced from github\n* Mainline Interval\n  * Average between master commits\n  * Sourced from Github\n\n![Mainline Throughput](../../img/mainline-throughput.png)\n"}
{"repositoryUrl": "https://github.com/cloudfoundry/cloud_controller_ng.git", "path": "decisions/0009-services-orphan-mitigation.md", "template": "unknown", "status": null, "firstCommit": "2020-12-17T10:53:24Z", "lastCommit": "2021-03-23T15:36:02Z", "numberOfCommits": 5, "title": "Context:", "wordCount": 1179, "authors": {"name1": 4, "name2": 1}, "content": "# Context:\n\nThe aim of this document is to record how Orphan Mitigation (OM) for service instances and service bindings is implemented \nand the reasoning behind the decisions taken.\n\n[OSBAPI](https://github.com/openservicebrokerapi/servicebroker/blob/v2.15/spec.md#orphan-mitigation) defines \nOrphan Mitigation as the attempt(s) made by the platform (Cloud Controller) to clear up any resources that may have been \ncreated by a Service Broker during an operation that eventually failed. Consequences of having lingering resources in the \nbroker may include higher costs, resource quota consumption, etc.\n\nIn this scenario, one of two things can happen:\n1. The platform has no record of such resources.\n\n   In this case there is no way for the platform to list failed operations of resources that are not tracked by it. \n   As a result, it is possible the operator ignores such resources may have been created. \n   Even if the operator realizes the failed operation actually created some resources in the service broker, \n   destroying such resources would include direct interaction with the service broker as the platform does not \n   provide tools to delete resources it doesn't track.\n   \n1.  The platform still keeps a record of failed service resources.\n\n  In this case the platform can choose whether to perform OM or defer the choice of when to clean up failed resources \n  to the operator. Deferring to the operator can be beneficial as it allows for more troubleshooting.\n\nAs of this document, the Cloud Controller aims to comply with the OSBAPI 2.15 specification. \nThe specification states the platform may choose to leave the decision of when OM happens to the operator in case they \nneed to troubleshoot for any scenario. So whether Cloud Foundry performs OM in the scenarios OSBAPI outlines as requiring \nit or not is our choice, as long as there is another mechanism in the platform that the operator can use to remove failed \nresources at a later stage (i.e. `DELETE /v3/service_instances` or `DELETE /v3/service_credential_bindings`). \nCloud Foundry shouldn’t perform OM in the scenarios OSBAPI says it is not needed.\n\n## V2\n\nv2 is not fully compliant with OSBAPI 2.15 version regarding OM. Changes needed to be compliant are not backwards compatible \nso it is difficult to introduce them without releasing a major version. The OM spec changed on OSBAPI v2.15 and thus, even \nwhen v2 was made compatible with that same version, not all scenarios behave the same. Also, in some scenarios the choice \nwas made to give the opportunity to the operator to troubleshoot and not automatically mitigate orphan resources. In other \nscenarios the choice was taken not to implement complicated OM logic in places where commands where available to delete \nresources and so clean up was deferred to the user.\n\nThere is also [this document](https://docs.google.com/document/d/11iXxAciCIQpCvrnzmGoEqQIbIVxpn6VDYlm_SVuq9TU/edit?usp=sharing) \nthat specifies what CC is doing in v2 and what it should be doing that has been shared with the community. \nAlthough effort has been put over time to keep it accurate, it is not 100% a reflection of what V2 really does.\n\n## V3\n\nIn v3 API all operations that require broker communication are done asynchronously. This is one of the main differences \nfrom v2 API. As a result of this approach CF always has a record of the resource that is being created or deleted. \nWhen the broker operation fails, CF still keeps a record of the resource e.g. if a create binding operation fails the CF\nwill have keep a record of that resource and set the state to `create failed`. This allows the operator to manually \nremove the failed resource and it means that there should not be any resources that the service broker has created and \nCF does not have a record of.\n\nHowever, we have chosen to keep performing orphan mitigation in many cases in order to keep some level of consistency \nwith the expected behaviour in v2 and to satisfy requirements and issues raised by users.\n\n\n# Provisioning\n\n## Scenarios when CF will perform OM:\n\nStatus code | Response Body |  OSBAPI advices OM |Notes\n:--:| --|:--:| --\n201 | malformed |  Yes |\n202 | malformed | No | If this happens, CF would not be able to record the broker response that might include important properties for continuing the async flow (e.g. operation_id).\n2xx | - |  Yes |\n422 | unexpected error | No | No resource should have been created, however attempting OM does not have any risks.\n5xx | - | Yes |\nClient Timeout | - | Yes |\n\n## Scenarios when CF will NOT perform OM:\n\nStatus code | Response Body |  OSBAPI advices OM |Notes\n:--:| --|:--:| --\n200 | Malformed | No |\n201 | Other (not malformed) | No |\n202 | Other (not malformed) | No |\n422 | Requires app/Async/Concurrency error | No |\n4xx | - | No |\n\n# Binding\n \n## Scenarios when CF will perform OM:\n\nStatus code | Response Body |  OSBAPI advices OM |Notes\n:--:| --|:--:| --\n 200 | bad data | No | If this happens, CF would not be able to record the broker response. Safest assumption is to delete the resource from the broker and allow the operator to start over.\n 201 | malformed |  Yes |\n 201 | bad data | No | If this happens, CF would not be able to record the broker response. Safest assumption is to delete the resource from the broker and allow the operator to start over.\n 202 | malformed | No | If this happens, CF would not be able to record the broker response that might include important properties for continuing the async flow (e.g. operation_id).\n 2xx | - |  Yes |\n 410 | - | No | This is not a valid error code for a `POST` request. No resource should have been created, however attempting OM does not have any risks.\n 422 | unexpected error | No | No resource should have been created, however attempting OM does not have any risks.\n 5xx | - | Yes |\n Client Timeout | - | Yes |\n\n## Scenarios when CF will NOT perform OM:\n\nStatus code | Response Body |  OSBAPI advices OM |Notes\n:--:| --|:--:| --\n 200 | Malformed | No |\n 201 | Other (not Malformed or Bad data) | No |\n 202 | Other (not Malformed or Bad data) | No |\n 422 | Requires app/Async/Concurrency error | No |\n 4xx | - | No |\n \n\nIn v3, in the case of any other CF internal error not related to the Broker response, CF does not perform OM. \nEven in case of failure, there is a record of the resource in the DB and the user is able to delete the resource after failure. \n\n# Handling service instances and bindings last operation broker responses\n\nCloud Foundry will not attempt any OM for any of the responses from instances or bindings Last Operation requests. \nWe decided to give the operator the possibility of troubleshooting and delete the resource when they see fit, in line with \n[SI behaviour](https://github.com/cloudfoundry/cloud_controller_ng/issues/1842) our users depend on.\n\n## Deprovisioning and Unbinding\n\nIn event of failure, Cloud Foundry will keep the record of the resource and the user can attempt to delete again. \nThere is not a clear benefit of implementing any OM logic for such straightforward scenario. \n\n## Changes from v2 to v3\nWhen possible we have kept the same OM implementation in v2 and v3. Cases when we have diverged have been documented in this doc.\n\nIn v3 all types of bindings, including service credentials bindings for apps and keys and service route bindings, \nhave the same OM behaviour.\n\n# Status\nAccepted\n\n# Consequences:\nThis document is a description of our reasoning about OM and its current implementation at the time of writing. \nThe behaviour for each use-case might change if OSBAPI advices new behaviour or our customers request other changes.\n"}
{"repositoryUrl": "https://github.com/MITLibraries/timdex.git", "path": "docs/architecture-decisions/0003-follow-twelve-factor-methodology.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-10-16T20:22:09Z", "lastCommit": "2018-10-16T20:22:09Z", "numberOfCommits": 1, "title": "3. Follow Twelve Factor methodology", "wordCount": 81, "authors": {"name1": 1}, "content": "# 3. Follow Twelve Factor methodology\n\nDate: 2018-10-16\n\n## Status\n\nAccepted\n\n## Context\n\nDesigning modern scalable cloud based applications requires intentionally\ndesigning the architecture to take advantage of the cloud.\n\nOne leading way to do that is\n[The Twelve Factor](https://12factor.net) methodology.\n\n## Decision\n\nWe will follow Twelve Factor methodology.\n\n## Consequences\n\nOur application will be deployable in the cloud in a scalable efficient manner.\n\nWe will leverage services for some aspects of applications that\npreviously would have relied on a Virtual Machine, such as storage for files\nand logs.\n"}
{"repositoryUrl": "https://github.com/DFE-Digital/buy-for-your-school.git", "path": "doc/architecture/decisions/0008-use-brakeman-for-security-analysis.md", "template": "Nygard", "status": "![Accepted](https://img.shields.io/badge/adr-accepted-green)", "firstCommit": "2021-06-23T10:15:01Z", "lastCommit": "2021-07-12T07:15:13Z", "numberOfCommits": 3, "title": "8. use-brakeman-for-security-analysis", "wordCount": 111, "authors": {"name1": 1, "name2": 2}, "content": "# 8. use-brakeman-for-security-analysis\n\nDate: 2020-04-03\n\n## Status\n\n![Accepted](https://img.shields.io/badge/adr-accepted-green)\n\n## Context\n\nWe need a mechanism for highlighting security vulnerabilities in our code before it reaches production environments\n\n## Decision\n\nUse the [Brakeman](https://brakemanscanner.org/) static security analysis tool to find vulnerabilities in development and test\n\n## Consequences\n\n- Brakeman will be run as part of CI and fail the build if there are any warnings\n- Brakeman can also be run in the development environment to allow developers to address issues before committing code to the repository\n- Brakeman will help developers learn about common vulnerabilities and develop a more defensive coding style\n- Use of Brakeman in development & test environments should reduce or eliminate code vulnerabilities that would be exposed in a penetration test\n"}
{"repositoryUrl": "https://github.com/guardian/editions.git", "path": "docs/00-✅-adr.md", "template": "unknown", "status": null, "firstCommit": "2019-05-08T12:46:20Z", "lastCommit": "2019-05-08T13:13:49Z", "numberOfCommits": 4, "title": "To record decisions as Architectural Decision Records", "wordCount": 51, "authors": {"name1": 4}, "content": "# To record decisions as Architectural Decision Records\n\n## Status: Busy\n\n## Context\n\nWe need to document our decisions on this new project.\n\n## Decision\n\nTo write ADRs for all major technical decisions. Naming each with a monotonically increasing counter.\n\nRejected ADRs will have -🚯- after the counter.\n\nAccepted ADRS will have -✅- after the counter.\n\n## Consequences\n\n🎉🎉🎉\n"}
{"repositoryUrl": "https://github.com/zooniverse/front-end-monorepo.git", "path": "docs/arch/adr-25.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-01-24T21:02:30Z", "lastCommit": "2020-05-01T19:17:50Z", "numberOfCommits": 2, "title": "ADR 25: Drawing Sub-task", "wordCount": 1320, "authors": {"name1": 2}, "content": "# ADR 25: Drawing Sub-task \n\nCreated: January 6, 2020\nUpdated: April 30, 2020\n\n## Context\n\nA drawing mark's sub-task is designed to support volunteers answering additional questions for each drawing mark annotation. It allows single choice or multiple choice question task, text task, dropdown task, and slider task.\n\n### Annotation JSON structure\n\nThe current sub-task annotation JSON structure is:\n\n```json\n// a point with sub-task consisting of a question task and a dropdown task\n{\n  \"annotations\": [\n  {\n  \"task\": \"T0\",\n  \"value\": [\n  {\n  \"frame\": 0,\n  \"tool\": 0,\n  \"x\": 452.18341064453125,\n  \"y\": 202.87478637695312,\n  \"details\": [\n  {\"value\": 0},\n  {\"value\": [\n  {\"value\": \"option-1\"},\n  {\"value\": \"option-2\"},\n  {\"value\": null}\n  ]}\n  ]\n  },\n  {\n  \"frame\": 0,\n  \"tool\": 0,\n  \"x\": 374.23454574576868,\n  \"y\": 455.23453656547428,\n  \"details\": [\n  {\"value\": 1},\n  {\"value\": [\n  {\"value\": \"option-3\"},\n  {\"value\": \"option-4\"},\n  {\"value\": \"option-5\"}\n  ]}\n  ]\n  },\n  {\n  \"frame\": 0,\n  \"tool\": 1,\n  \"x\": 404.61279296875,\n  \"y\": 583.4398803710938,\n  \"details\": [\n  {\"value\": 1},\n  {\"value\": [\n  {\"value\": \"option-3\"},\n  {\"value\": \"option-4\"},\n  {\"value\": \"option-5\"}\n  ]}\n  ]\n  }\n  ]\n  }\n  ]\n}\n```\n\nThe annotation structure for the sub-task, under `details`, has a few issues because it solely relies on an array index to relate back to the original sub-task. This makes it difficult to make downstream analysis and aggregation scripts. The aggregation code now has to parse the details array and make a \"mock annotation\" of the correct structure to be passed along to the next reducer.\n\n### Sub-task UI\n\nThe sub-task UI positioned itself fixed below relative to the position of the mark. Notably transcription projects have commented that this interferes with being able to transcribe successfully since the dialog may cover up part of the subject and cannot be moved without moving the drawing mark.\n\n## Decision\n\nFor initial support, we will support the single and multiple choice question tasks and the text task in the sub-task. Slider task may be deprecated and dropdown task may be changing in ways we do not have a plan for yet, so they can be supported later if it makes sense to add them.\n\n### Annotation JSON structure\n\nThe annotations in the details array will be updated to be an object that just contains a reference to the sub-task's unique identifier. The task annotation itself will be stored in the classification's annotations array flattened. \n\nThe main benefit of this reorganization will be with downstream analysis and aggregation. When aggregating drawn shapes the first step is clustering. Once the clusters are found the subtasks need to be aggregated within each cluster. This will be easier to do if the structure of each subtask annotation is the same as if that task was asked on its own. The code can just take all subtask annotations within a cluster and just pass it to the reducer as if it is a list of main task annotations without having to reshape them.\n\nAn addition of `markIndex` is being added for the sub-task annotations for the purpose of having an identifier relating it back to the parent drawing task annotation value array which represents marks. \n\nAn example of the new sub-task annotation JSON structure at classification submission:\n\n```json\n{\n  \"annotations\": [\n  {\n  \"task\": \"T0\",\n  \"taskType\": \"drawing\",\n  \"value\": [\n  {\n  \"frame\": 0,\n  \"toolIndex\": 0,\n  \"toolType\": \"point\",\n  \"x\": 452.18341064453125,\n  \"y\": 202.87478637695312,\n  \"details\": [\n  {\"task\": \"T0.0.0\"},\n  {\"task\": \"T0.0.1\"}\n  ]\n  },\n  {\n  \"frame\": 0,\n  \"toolIndex\": 0,\n  \"toolType\": \"point\",\n  \"x\": 374.23454574576868,\n  \"y\": 455.23453656547428,\n  \"details\": [\n  {\"task\": \"T0.0.0\"},\n  {\"task\": \"T0.0.1\"}\n  ]\n  },\n  {\n  \"frame\": 0,\n  \"toolIndex\": 1,\n  \"toolType\": \"point\",\n  \"x\": 404.61279296875,\n  \"y\": 583.4398803710938,\n  \"details\": [\n  {\"task\": \"T0.1.0\"},\n  {\"task\": \"T0.1.1\"}\n  ]\n  }\n  ]\n  },\n  {\n  \"task\": \"T0.0.0\",\n  \"taskType\": \"single\",\n  \"markIndex\": 0,\n  \"value\": 0\n  },\n  {\n  \"task\": \"T0.0.1\",\n  \"taskType\": \"dropdown\",\n  \"markIndex\": 0,\n  \"value\": [\n  {\"value\": \"option-1\"},\n  {\"value\": \"option-2\"},\n  {\"value\": null}\n  ]\n  },\n  {\n  \"task\": \"T0.0.0\",\n  \"taskType\": \"single\",\n  \"markIndex\": 1,\n  \"value\": 1\n  },\n  {\n  \"task\": \"T0.0.1\",\n  \"taskType\": \"dropdown\",\n  \"markIndex\": 1,\n  \"value\": [\n  {\"value\": \"option-3\"},\n  {\"value\": \"option-4\"},\n  {\"value\": \"option-5\"}\n  ]\n  },\n  {\n  \"task\": \"T0.1.0\",\n  \"markIndex\": 2,\n  \"taskType\": \"single\",\n  \"value\": 1\n  },\n  {\n  \"task\": \"T0.1.1\",\n  \"markIndex\": 2,\n  \"taskType\": \"dropdown\",\n  \"value\": [\n  {\"value\": \"option-3\"},\n  {\"value\": \"option-4\"},\n  {\"value\": \"option-5\"}\n  ]\n  }\n  ],\n  \"metadata\": {\n  \"classifier_version\": \"2.0\"\n  }\n}\n```\n\nThe sub-task identifiers follow a convention of `TASK_KEY.TOOL_INDEX.DETAILS_INDEX`.\n\nNote that this is the structure at classification submission. The classifier's internal store models may have differences for the purposes of keeping track of in-progress annotations and marks being made.\n\n### Drawing sub-task UI\n\nThe UI will change to adopt the design of Anti-Slavery Manuscripts (ASM) with this [generalized design](https://projects.invisionapp.com/d/main#/console/12923997/396381420/preview). It will be a pseudo-modal, but with a few notable differences from a true modal:\n\n- The initial position will be near the associated mark made\n- Interactions will be allowed with the image toolbar to allow zoom, rotate, as well as opening of the tutorial, field guide, and task help. Submission of the classification should not be allowed.\n  - If the sub-tasks are required, the modal should not be closeable until the required annotations are made or the mark is deleted if cancelled\n- The dialog can be moved and resized\n\nTo support movability and resizing, we will leverage [react-rnd](https://github.com/bokuweb/react-rnd) which is the same library ASM used. Grommet's `Layer` cannot be used since it is intended for actual modal or side panel use and cannot be arbitrarily positioned or moved.\n\n## Status\n\nAccepted\n\n## Consequences\n\nFlattening the annotations array for drawing task sub-tasks is conceptually consistent with the move to using workflow steps to flatten the combo task. It is however a breaking change and this change will have to be communicated to project builders. As with other classifications from the new classifier, this can be checked by the presence of `classifier_version: 2.0` in the classification metadata. In the future, we would also like to include a link to JSON schema for each annotation type as recommended in [ADR 07](adr-07.md).\n\nFlattening also has added benefits for Panoptes when generating classification exports. It can parse through a flattened array to convert machine readable strings to human readable strings for each task without having to check for values in a nested `details` array and then traverse it. \n\nIn the raw classification export, this also benefits researchers that want to analyze the outputted CSV directly and prefer a flatter JSON structure. Flat structures facilitate research teams being able to load data without JSON-based manipulation. There are many teams who would benefit from the ability to read-in a CSV to Excel and start analyzing their results, as opposed to needing to first parse JSON. There will still be some JSON structure in CSV exports, but this will contribute toward minimizing it. \n\nThe transcription task is a automatically configured drawing task with a sub-task and will be using a new variant of a text task that includes auto-suggestions from caesar reductions. Its sub-task should use the suggested changes from this ADR as well.\n\n### Aggregation \n\nThe aggregation for caesar code needed to be updated to accommodate the new annotation structure for sub-tasks. PR [289](https://github.com/zooniverse/aggregation-for-caesar/pull/289) implements these changes. Here is a sample extractor and reducer using the new code:\n\n#### Setting up the extractor\nTo set up and extractor you need to URL encode the keywords\n\n```json\n{\n  \"task\": \"T0\",\n  \"shape\": \"point\",\n  \"details\": {\n  \"T0_toolIndex0_subtask0\": \"question_extractor\",\n  \"T0_toolIndex0_subtask1\": \"dropdown_extractor\",\n  \"T0_toolIndex1_subtask0\": \"question_extractor\",\n  \"T0_toolIndex1_subtask1\": \"dropdown_extractor\"\n  }\n}\n```\n\nand that looks like\n`https://aggregation-caesar.zooniverse.org/extractors/shape_extractor?task=T0&shape=point&details=%7B%27T0_toolIndex0_subtask0%27%3A+%27question_extractor%27%2C+%27T0_toolIndex0_subtask1%27%3A+%27dropdown_extractor%27%2C+%27T0_toolIndex1_subtask0%27%3A+%27question_extractor%27%2C+%27T0_toolIndex1_subtask1%27%3A+%27dropdown_extractor%27%7D`\n\nalthough I expect the decoded URL would also work (not tested)\n`https://aggregation-caesar.zooniverse.org/extractors/shape_extractor?task=T0&shape=point&details={'T0_toolIndex0_subtask0':+'question_extractor',+'T0_toolIndex0_subtask1':+'dropdown_extractor',+'T0_toolIndex1_subtask0':+'question_extractor',+'T0_toolIndex1_subtask1':+'dropdown_extractor'}`\n\nThese keywords define the task ID, the shape used for the drawing tool, and the extractors to use for each of the subtasks. In this example there are two point tools on task `T0` and they each have a question subtask followed by a dropdown subtask.\n\nAny subtasks not explicitly defined in this `details` keyword are ignored an will not be extracted.\n\n#### Setting up the reducer\n\nThe reducer's URL prams also have the `details` section in the same format as the extractor. As an example for the dbscan reducer the keywords would look like (using default cluster params):\n\n``` json\n{\n  \"shape\": \"point\",\n  \"details\": {\n  \"T0_toolIndex0_subtask0\": \"question_reducer\",\n  \"T0_toolIndex0_subtask1\": \"dropdown_reducer\",\n  \"T0_toolIndex1_subtask0\": \"question_reducer\",\n  \"T0_toolIndex1_subtask1\": \"dropdown_reducer\"\n  }\n}\n```\n\nThe encoded URL would be\n`https://aggregation-caesar.zooniverse.org/reducers/shape_reducer_dbscan?shape=point&details=%7B%27T0_toolIndex0_subtask0%27%3A+%27question_reducer%27%2C+%27T0_toolIndex0_subtask1%27%3A+%27dropdown_reducer%27%2C+%27T0_toolIndex1_subtask0%27%3A+%27question_reducer%27%2C+%27T0_toolIndex1_subtask1%27%3A+%27dropdown_reducer%27%7D`\n\nor the decoded URL\n`https://aggregation-caesar.zooniverse.org/reducers/shape_reducer_dbscan?shape=point&details={'T0_toolIndex0_subtask0':+'question_reducer',+'T0_toolIndex0_subtask1':+'dropdown_reducer',+'T0_toolIndex1_subtask0':+'question_reducer',+'T0_toolIndex1_subtask1':+'dropdown_reducer'}`\n"}
{"repositoryUrl": "https://github.com/embvm/embvm-core.git", "path": "docs/architecture/decisions/0019-virtual-platform-takes-in-thwplatform-type.md", "template": "Nygard", "status": "Accepted Caused by [18. Driver Registration in HW Platform](0018-driver-registration-in-hw-platform.md)", "firstCommit": "2020-10-20T18:48:02Z", "lastCommit": "2020-10-20T18:48:02Z", "numberOfCommits": 1, "title": "19. Virtual Platform Takes in THWPlatform Type", "wordCount": 214, "authors": {"name1": 1}, "content": "# 19. Virtual Platform Takes in THWPlatform Type\n\nDate: 2020-10-20\n\n## Status\n\nAccepted\n\nCaused by [18. Driver Registration in HW Platform](0018-driver-registration-in-hw-platform.md)\n\n## Context\n\nAs a consequence of [ADR 0018](0018-driver-registration-in-hw-platform.md), we moved the [Driver Registry](../components/core/driver_registry.md) definition to the hardware platform and removed the global singleton from the platform. We also want the platform APIs to forward to the hw platform. However, we needed a way to access the hw platform object for successful forwarding. This requires the platform base class to know about the type.\n\n## Decision\n\nThe hardware platform type is now a template parameter for the Virtual Platform base class. A local variable will be declared (`hw_platform_`), and a `protected` API will be provided to access that variable as well.\n\n## Consequences\n\n- Users do not have control over how the hardware platform variable is named\n- Users may not know that they need to access the hardware platform through specific variables/functions: now there is an education challenge\n  + We can show example code to mitigate this\n- Only one hardware platform can be used with a platform, which would *potentially* break board ID and board revision selection for instantiating one of many hardware platforms. \n  + However, this can be handled in the hardware platform logic itself, if multiple revisions need to be supported.\n\n## Related Documents\n\nPlease see the [associated development log](../../development/logs/20201020_driver_registry_redesign.md) for detailed information about the changes that were made as part of this effort.\n"}
{"repositoryUrl": "https://github.com/Informatievlaanderen/publicservice-registry.git", "path": "docs/adr/0004-sqlstreamstore.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-03-25T11:23:43Z", "lastCommit": "2019-03-25T11:23:43Z", "numberOfCommits": 1, "title": "4. Use `SqlStreamStore`", "wordCount": 298, "authors": {"name1": 1}, "content": "# 4. Use `SqlStreamStore`\n\nDate: 2017-09-12\n\n## Status\n\nAccepted\n\n## Context\n\nSince we decided to use event sourcing, we need a way to store events in our database.\n\nIn `Wegwijs` we stored events in `MSSQL`, which allows easy debugging of events. All sql statements to save/read events were hand-written.\n\n**However**, since we decided on async event handlers in a previous ADR, we would benefit a lot from having catch-up subscriptions for our event handlers. Catch-up subscriptions allow event handlers to be in charge of what events they are interested in, and give event handlers more autonomy over their own rebuilds.\n\nWhile `GetEventStore` supports this, and is most likely a top-notch choice for storing events, this would require us to take care of hosting this. We also have doubts about the support for storing business-critical data outside of `MSSQL` in `AIV`.\n\nWe currently host no VMs for business-critical concerns, and we feel that hosting `GetEventStore` ourselves, would add a significant burden.\n\nAs an alternative, `SqlStreamStore` is an OSS library on GitHub which supports storing events into `MSSQL`, and has support for catch-up subscriptions. It has an active community, and has been used in several production systems successfully according to that community.\n\n## Decision\n\nWe will use the `SqlStreamStore` library as our event store. We will keep an eye on ongoing developments from `SqlStreamStore`.\n\n## Consequences\n\nWe will no longer have to implement saving/reading events ourselves.\n\nWe will have the option to use catch-up subscriptions, giving event handlers more autonomy.\n\nWe will need to keep an eye on `SqlStreamStore` developments, through github updates and the `#`sqlstreamstore` `channel on https://ddd-cqrs-es.slack.com.\n\nWe will be able to use constantly updated best practices from the community.\n\nIt will be harder to customize saving/reading the event store, though we don't see the need for that at this point.\n"}
{"repositoryUrl": "https://github.com/cfe84/roster.git", "path": "adr/general-004.password-derivation.md", "template": "unknown", "status": null, "firstCommit": "2019-12-17T06:48:31Z", "lastCommit": "2019-12-17T06:48:31Z", "numberOfCommits": 1, "title": "Decision", "wordCount": 14, "authors": {"name1": 1}, "content": "# Decision\n\nUsing a simple derivation function for now\n\n# Reason\n\nHave something sufficiently good rapidly."}
{"repositoryUrl": "https://github.com/MITLibraries/dos-server.git", "path": "docs/adr/adr-1-metadata.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-01-07T17:47:03Z", "lastCommit": "2020-01-13T15:31:29Z", "numberOfCommits": 2, "title": "Metadata", "wordCount": 114, "authors": {"name1": 2}, "content": "# Metadata\n\n## Context\n\nDigital objects need to have associated metadata for various use cases (refer to\n the requirements documentation for details). Metadata of these objects can be descriptive, administrative, and structural.\n To avoid \"duplication\" of descriptive metadata, it is desired that DOS not store descriptive metadata.\n\n## Decision\n\nDescriptive metadata will not be stored by DOS.\n\n## Status\n\nAccepted\n\n## Consequences\n\nSome use cases or features (such as bulk download of objects) that were originally gathered in DOS business analysis Phase 1\nmay not be fulfilled by DOS. It is possible that further business analysis may need to be done for these use cases and \nto see if any systems are already fulfilling that role in some capacity."}
{"repositoryUrl": "https://github.com/commonality/archetypes-rules.git", "path": "docs/adr/adr-0001-architecture-decision-record-use-adrs.md", "template": "Nygard", "status": "[![adr: accepted][adr-accepted-badge]][adr-0001]", "firstCommit": "2019-04-24T06:21:36Z", "lastCommit": "2019-05-02T21:04:48Z", "numberOfCommits": 3, "title": "Architecture Decision Record: Use ADRs", "wordCount": 779, "authors": {"name1": 3}, "content": "# Architecture Decision Record: Use ADRs\n\n[![Decided on][octicon-calendar]][adr-0001]\n<time datetime=\"2019-03-08\">2019-03-08</time>\n\n## Status\n\n[![adr: accepted][adr-accepted-badge]][adr-0001]\n\n## Context\n\nThe **archetypes-rules** team has several explicit goals that make the practice\nand discipline of architecture very important:\n\n-   We want to think deeply about all our architectural decisions, exploring all\n  alternatives and making a careful, considered, well-researched choice.\n\n-   We want to be as transparent as possible in our decision-making process.\n\n-   We don't want decisions to be made unilaterally in a vacuum. Specifically,\n  should **archetypes-rules** ever fall under the purview of a Technical\n  Steering Committee (TSC), we would want to give our TSC colleagues the\n  opportunity to review every major decision.\n\n-   Despite being a geographically and temporally distributed team, we want our\n  contributors to have a strong shared understanding of the technical rationale\n  behind decisions.\n\n-   We want to be able to revisit prior decisions to determine fairly whether they\n  still make sense, and if the motivating circumstances or conditions have\n  changed.\n\n## Decision\n\nWe will document every architecture-level decision for **archetypes-rules** and\nits core modules with an Architecture Decision Record. These are a well\nstructured, relatively lightweight way to capture architectural proposals. They\ncan serve as an artifact for discussion, and remain as an enduring record of the\ncontext and motivation of past decisions.\n\n### Our Workflow for Making and Recording Architecture Decisions[^1]\n\n```mermaid\ngraph TD\n\nsubgraph ADR Workflow\n\ntitle[\"ℹ️  High-level flow chart showing how we distribute design authority with architecture decision records\"]\ntitle-->A\nstyle title fill:#FFF,stroke:#FFF\nlinkStyle 0 stroke:#FFF,stroke-width:0;\n\n  A((\"start\")) --> B(Significant requirement, challenge, or change)\n  B --> C(\"Create GitLab Issue (with the ADR Template)\")\n  C --> D(\"Field Comments and Discussions\")\n  D --> E(\"Vote (lazy consensus)\")\n  E --> F{\"Tally 👍  👎  reactions\"}\n  F --> |\"👍 majority 'accepted'\"| G(\"Apply badge 'ADR accepted'\")\n  F --> |\"👎 majority 'rejected'\"| H(\"Apply badge 'ADR rejected'\")\n  G --> I\n  H --> I(\"Merge ADR into `master`\")\n  I --> J((\"stop\"))\nend\n```\n\n1.  A contributor creates an ADR document outlining an approach for a particular\n  question or problem. All ADRs have an initial status of \"proposed.\"\n  Maintainers and Trusted Committers must apply the <kbd>adr: proposed</kbd>\n  Issue label.\n\n1.  We track architecture decisions on the \"[ADR (Architecture Decision\n  Records)][adr-issue-board]\" Issue Board.\n\n1.  The Maintainer, Trusted Committer, Contributors, and Consumers consider the\n  ADR through _public_ GitLab Discussions. These GitLab Discussions must be\n  resolved by (lazy) consensus.\n\n1.  During the discussion period, the ADR should be updated to reflect\n  additional context, concerns raised, and proposed changes. Maintainers and\n  Trusted Committers must apply the <kbd>adr: discussion-underway</kbd> label\n  the associated Issue or Pull Request.\n\n1.  Once consensus is reached, the ADR will be marked as either an \"accepted\" or\n  \"rejected\". The Maintainer or Trusted Committer must likewise apply the\n  label \"adr: accepted\" or \"adr: rejected\" to the original issue.\n\n1.  The Maintainer or Trusted Committer will also update the **\"Status\"**\n  section of the ADR with the badge that reflects the ADR's state.[^2] \\(The\n  Style Guide for Images has\n  [\"how-to\" instructions for badges](https://github.com/archetypes-rules/signatures/wikis/Style-Guides/Images).)\n\n1.  Only after an ADR is accepted should implementing code be committed to the\n  `master` branch of a relevant application, library, or module.\n\n1.  All ADRs should be merged into the `master` branch, no matter what their\n  ultimate status is.\n\n1.  If a decision is revisited and a different conclusion is reached, a new ADR\n  should be created documenting the context and rationale for the change. The\n  new ADR should reference the old one, and once the new one is accepted, the\n  old one should (in its \"status\" section) be updated to point to the new one.\n  The old ADR should not be removed or otherwise modified except for the\n  annotation pointing to the new ADR.\n\n## Consequences\n\n1.  Contributors must write an ADR and submit it for review before selecting an\n  approach to any architectural decision; that is, any decision that affects\n  the way **archetypes-rules** or an **archetypes-rules** application is put\n  together at a high level.\n\n1.  We will have a concrete artifact around which to focus discussion, before\n  finalizing decisions.\n\n1.  If we follow the process, decisions will be made deliberately, as a group.\n\n1.  The master branch of our repositories will reflect the high-level consensus\n  of the steering group.\n\n1.  We will have a useful persistent record of why the system is the way it is.\n\n## References\n\n[^1]: Swindle, G. (2019) _Architecture Decisions · Wiki ·\n  archetypes-rules/signatures · GitLab_. Retrieved March 08, 2019, from\n  <https://github.com/archetypes-rules/signatures/wikis/Governance/Architecture-Decisions>\n\n[^2] Swindle, G. (2019) _Images · Wiki · archetypes-rules/signatures · GitLab_.\n  Retrieved March 08, 2019, from\n  <https://github.com/archetypes-rules/signatures/wikis/Style-Guides/Images>\n\n<!-- Do not remove this line or anything under it. -->\n\n[adr-0001]: docs/adr/adr-0001-architecture-decision-record-use-adrs.md\n\n[adr-issue-board]: https://github.com/archetypes-rules/signatures/boards/980468?&label_name[]=type%3A%20adr\n\n[adr-accepted-badge]: https://flat.badgen.net/badge/ADR/accepted/44AD8E\n\n[adr-proposed-badge]: https://flat.badgen.net/badge/ADR/proposed/AC900D\n\n[adr-rejected-badge]: https://flat.badgen.net/badge/ADR/rejected/D9534F\n\n[adr-deprecated-badge]: https://flat.badgen.net/badge/ADR/deprecated/7F8C8D\n\n<!-- Octicon image definition list -->\n\n[octicon-calendar]: https://cdnjs.cloudflare.com/ajax/libs/octicons/8.3.0/svg/calendar.svg\n"}
{"repositoryUrl": "https://github.com/UKGovernmentBEIS/beis-report-official-development-assistance.git", "path": "doc/architecture/decisions/0002-use-bullet-to-catch-nplus1-queries.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-10-21T11:21:23Z", "lastCommit": "2019-10-21T11:21:23Z", "numberOfCommits": 1, "title": "2. use-bullet-to-catch-nplus1-queries", "wordCount": 104, "authors": {"name1": 1}, "content": "# 2. use-bullet-to-catch-nplus1-queries\n\nDate: 2019-09-19\n\n## Status\n\nAccepted\n\n## Context\n\nIt can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.\n\n## Decision\n\nAdd an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.\n\n## Consequences\n\n- Code reviews are depended upon less to catch this type of error, removing the risk of human error\n- Application response times should no longer be affected by this type of issue\n- Requires protected branch configuration to ensure that CI must succeed before being able to be merged\n"}
{"repositoryUrl": "https://github.com/cloudfoundry-attic/copilot.git", "path": "docs/decisions/0001-record-architecture-decisions.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-07-25T21:12:32Z", "lastCommit": "2018-07-25T21:12:32Z", "numberOfCommits": 1, "title": "1. Record architecture decisions", "wordCount": 45, "authors": {"name1": 1}, "content": "# 1. Record architecture decisions\n\nDate: 2018-07-25\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to record the architectural decisions made on this project.\n\n## Decision\n\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n\n## Consequences\n\nSee Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's [adr-tools](https://github.com/npryce/adr-tools).\n"}
{"repositoryUrl": "https://github.com/katarzyna-starachowicz/my_budgeting.git", "path": "doc/adl/0003_rails_event_store.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-09-28T16:29:21Z", "lastCommit": "2020-09-28T16:29:21Z", "numberOfCommits": 1, "title": "3. Rails Event Storm", "wordCount": 72, "authors": {"name1": 1}, "content": "# 3. Rails Event Storm\n\n## Status\n\nAccepted\n\n## Context\n\nAs I'm learning DDD with Arkency course, I'm going to start with solution propsed by them.\n\n## Decision\n\n[Rails Event Storm](https://railseventstore.org/) is a library for publishing, consuming, storing and retrieving events. According to its creators from Arkency – it's your best companion for going with an Event-Driven Architecture for your Rails application. \n\n## Consequences\n\nIt is going to be the base for event-driven architecture of My Budgeting app.\n"}
{"repositoryUrl": "https://github.com/JulianG/bananatabs.git", "path": "doc/adr/0001-record-architecture-decisions.md", "template": "Nygard", "status": "Approved", "firstCommit": "2019-05-09T12:59:34Z", "lastCommit": "2019-06-10T13:01:18Z", "numberOfCommits": 2, "title": "1. Record architecture decisions", "wordCount": 249, "authors": {"name1": 2}, "content": "# 1. Record architecture decisions\n\nDate: 2019-05-09\n\n## Status\n\nApproved\n\n## Context\n\nWe need to record the architectural decisions made on this project.\n\n## Decision\n\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n\n## Consequences\n\nSee Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's [adr-tools](https://github.com/npryce/adr-tools).\n\nUse the `adr` command to manage ADRs.  Try running `adr help`.\n\nADRs are stored in a subdirectory of your project as Markdown files. \nThe default directory is `doc/adr`, but you can specify the directory\nwhen you initialise the ADR log.\n\n1. Create an ADR directory in the root of your project:\n\n  adr init doc/architecture/decisions\n\n  This will create a directory named `doc/architecture/decisions' \n  containing the first ADR, which records that you are using ADRs\n  to record architectural decisions and links to \n  [Michael Nygard's article on the subject][ADRs].\n\n2. Create Architecture Decision Records\n\n  adr new Implement as Unix shell scripts\n\n  This will create a new, numbered ADR file and open it in your\n  editor of choice (as specified by the VISUAL or EDITOR environment\n  variable).\n\n  To create a new ADR that supercedes a previous one (ADR 9, for example),\n  use the -s option.\n\n  adr new -s 9 Use Rust for performance-critical functionality\n\n  This will create a new ADR file that is flagged as superceding\n  ADR 9, and changes the status of ADR 9 to indicate that it is\n  superceded by the new ADR.  It then opens the new ADR in your\n  editor of choice.\n  \n3. For further information, use the built in help:\n\n  adr help\n"}
{"repositoryUrl": "https://github.com/WikiWatershed/model-my-watershed.git", "path": "doc/arch/adr-002-geoprocessing-caching.md", "template": "unknown", "status": null, "firstCommit": "2017-04-28T13:37:28Z", "lastCommit": "2017-04-28T13:37:28Z", "numberOfCommits": 1, "title": "002 - Geoprocessing Caching", "wordCount": 1253, "authors": {"name1": 1}, "content": "# 002 - Geoprocessing Caching\n\n## Context\n\nGeoprocessing calls for large shapes can take a long time to complete. While users can draw custom shapes, they can also pick from a list of predefined shapes in the system. Geoprocessing the same shape over and over when multiple users select it is wasteful and unnecessary. By caching the geoprocessing results of predefined shapes, or Well Known Areas of Interest (WKAoIs), we can improve user experience and application performance.\n\nOutside of geoprocessing, there are also [some](https://github.com/WikiWatershed/model-my-watershed/blob/dea12ff9a6fb234e30978e97ed1d0f6266a406c9/src/mmw/apps/modeling/calcs.py#L28) [database](https://github.com/WikiWatershed/model-my-watershed/blob/dea12ff9a6fb234e30978e97ed1d0f6266a406c9/src/mmw/apps/modeling/calcs.py#L51) [calls](https://github.com/WikiWatershed/model-my-watershed/blob/dea12ff9a6fb234e30978e97ed1d0f6266a406c9/src/mmw/apps/modeling/calcs.py#L97), but these are much faster in comparison and are not likely to see significant improvement by caching, especially since the cache could be a table in the database itself.\n\nIn this ADR we consider the following questions:\n\n * What will be cached?\n * How will it be cached?\n * Where will it be cached?\n * When will it be cached?\n * How will the cache be invalidated?\n\nWe also sketch out an implementation plan, and consider consequences and side-effects.\n\n## Decisions\n\n### What will be cached?\n\nThe output of the geoprocessing service for WKAoIs will be cached. This is almost always a JSON blob of key-value pairs, where the key is a combination of overlaid cell values in a set of rasters, and the value is a count of such cells. The inputs are a GeoJSON shape and a [set of related arguments](https://github.com/WikiWatershed/model-my-watershed/blob/develop/src/mmw/mmw/settings/base.py#L412) that specify rasters, operation type, CRS, etc.\n\nWhile we could cache the entire output of a MapShed or TR-55 run, an update to some of the constituent raster or vector data would force recalculation of the whole. By caching only the time-consuming geoprocessing results, we ensure that any updates to constituent rasters would invalidate only that specific cached result, leaving the others still current. And since vector data results would never be cached, they will always be current upon update as well.\n\nIn case of MapShed, the modifications do not change any of the geoprocessing queries, thus those requests can be cached easily. For TR-55, the modifications _do_ change the geoprocessing queries. For these cases, we will cache only the Current Conditions (modification-less) run, not other Scenarios, since storing arbitrary shapes can balloon the size of the cache very quickly. This decision may be revisited in the future.\n\nIn case we do not foresee updating the rasters very often, it may be beneficial to consider caching the entire JSON response of the API.\n\n### How will it be cached?\n\nThe current stack already has [Django Caching](https://github.com/WikiWatershed/model-my-watershed/blob/a49cc115d18d2c67cbd0721cf62faa08e98d13fc/src/mmw/mmw/settings/base.py#L76-L90) setup. This allows us to cache with a single line of code:\n\n```python\nfrom django.core.cache import cache\n\ncache.set(key, value, None)\n```\n\nwhere the `key` is a unique identified consisting of WKAoI id and the geoprocessing operation, `value` is the result of the geoprocessing operation, and `None` is the timeout value which ensures the values don't ever expire.\n\nRetrieval is as simple as:\n\n```python\nvalue = cache.get(key)\nif not value:\n  # Calculate and cache the value\n\nreturn value\n```\n\nThe `key` should be prefixed with `geop` to namespace it from other cache entries in the app, composed of the WKAoI id, which consists of a table name and an integer id, and the [geoprocessing operation name](https://github.com/WikiWatershed/model-my-watershed/blob/a49cc115d18d2c67cbd0721cf62faa08e98d13fc/src/mmw/mmw/settings/base.py#L387). For example, `geop__boundary_huc08__1__nlcd_soils`. In practice, the `key` may actually be something like `:1:geop__boundary_huc08__1__nlcd_soils`, with a configurable app-wide prefix (which defaults to `''`) and a version number are prefixed to our key. However, as long as we always access the cache via `django.core.cache.cache`, we shouldn't need to worry about that.\n\n### Where will it be cached?\n\nDjango Caching Framework can be configured to use a number of cache backends, including Redis and Database. It is currently setup to use Redis / [ElastiCache](https://aws.amazon.com/elasticache/redis/) in production.\n\nThe advantages of using Redis are:\n\n * It is already configured\n * It allows us to take advantage of the Django Caching Framework which is configured to use Redis\n * Redis is really fast, and a good candidate for storing key/value pairs like we intend to\n * It is designed to be a cache, and thus comes with mechanisms for timeout, LRU, and cache misses out of the box\n\nThe disadvantages of using Redis are:\n\n * In the case of system failure, the cached values will be lost, and will need to be cached again\n * If it purges least recently used values, it might not be a good candidate for storing hard-to-process large WKAoIs that are rarely used\n\n### When will it be cached?\n\nWhen a user selects a WKAoI, and a request is made to `/analyze` or `/modeling`, if the WKAoI results haven't already been cached, we will run and cache them. Over time, we will build up the cache, so that when new users request the same WKAoIs, they will get the cached results. These will be cached in the Celery Tasks where calculation would otherwise happen.\n\nFor a select few WKAoIs which are too large to process in the production infrastructure, such as HUC-8s which time-out during MapShed gathering phase, we can run their geoprocessing steps on more powerful infrastructure with longer timeouts in a batch process, cache the results, and then decommission it. This will make them available to regular users of the production app without needing the extra power to render them.\n\nFor this purpose, we'll need a pair of new Django management commands. The first should run the geoprocessing steps for given WKAoIs and save the results to a file. This will be run in the super environment. The second should take a file of pre-processed results for given shapes and operations, and add them to the cache. This will be run in the production environment.\n\n### How will the cache be invalidated?\n\nSince every cache entry is tagged with its type, if a certain raster is updated, we can remove all related cache entries. For example, if there are updates to `us-percent-slope-30m-epsg5070` raster, which is used in the [`nlcd_slope`](https://github.com/WikiWatershed/model-my-watershed/blob/a49cc115d18d2c67cbd0721cf62faa08e98d13fc/src/mmw/mmw/settings/base.py#L449) and [`slope`](https://github.com/WikiWatershed/model-my-watershed/blob/a49cc115d18d2c67cbd0721cf62faa08e98d13fc/src/mmw/mmw/settings/base.py#L462) requests, we could simply run\n\n```python\ncache.delete_pattern('geop__*__nlcd_slope')\ncache.delete_pattern('geop__*__slope')\n```\n\nTo refresh a specific shape we could do\n\n```python\ncache.delete_pattern('geop__boundary_huc08__1__*')\n```\n\nThese could be management commands as well.\n\n## Implementation Plan\n\n1. See if TR-55 geoprocessing, currently done via [`SummaryJob`](https://github.com/WikiWatershed/mmw-geoprocessing/blob/develop/summary/src/main/scala/SummaryJob.scala), can be done via [`MapshedJob`](https://github.com/WikiWatershed/mmw-geoprocessing/blob/develop/summary/src/main/scala/MapshedJob.scala), for consistency. And if so, rewrite the geoprocessing bits to use `MapshedJob` instead.\n2. Split [`nlcdSoilCensus`](https://github.com/WikiWatershed/model-my-watershed/blob/develop/src/mmw/mmw/settings/base.py#L388) into three requests: `nlcd`, `soil`, and `nlcd_soil`. The first two can be used for `/analyze` and will be much faster, while the second can be used for `/modeling/tr-55`.\n3. Update the geoprocessing submodule to take GeoJSON shape or WKAoI id, and geoprocessing type, and return the output. Update the Celery Tasks or Django Views which use the geoprocessing submodule to use the new interface.\n4. Add caching support to the geoprocessing submodule. All geoprocessing is done in two parts: `_start` and `_finish`. In the case of a cache hit, there should be a signal passed from `_start` to `_finish` that instructs it to fetch the value from the cache instead of `sjs_retrieve`.\n5. Update the UI to send WKAoI id instead of GeoJSON for predefined shapes. RWD and user defined shapes should still be sent as GeoJSON.\n\n## Consequences\n\nThis will make the worst case scenario, a cache miss, slightly longer than it currently is, because we'll be checking the cache before doing the actual geoprocessing.\n\nIn case of ElastiCache failures, the cache would have to be rebuilt.\n\nLarge WKAoIs that are processed out-of-band may get pushed out of the cache if they are not used and the cache exceeds its maximum size.\n\nUser defined shapes will still not be cached, and their runtime will not be improved at all by this. Since the longest time taking activity in the geoprocessing is fetching tiles from S3, adding a network cache there may help improve those runtimes.\n"}
{"repositoryUrl": "https://github.com/actions/runner.git", "path": "docs/adrs/0274-step-outcome-and-conclusion.md", "template": "unknown", "status": null, "firstCommit": "2020-03-16T18:56:07Z", "lastCommit": "2020-03-16T18:56:07Z", "numberOfCommits": 1, "title": "ADR 0274: Step outcome and conclusion", "wordCount": 165, "authors": {"name1": 1}, "content": "# ADR 0274: Step outcome and conclusion\n\n**Date**: 2020-01-13\n\n**Status**: Accepted\n\n## Context\n\nThis ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.\n\nThis allows downstream a step to run based on whether a previous step succeeded or failed.\n\nReminder, currently the steps contains `steps.<id>.outputs`.\n\n## Decision\n\nFor steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:\n\n- `success`\n- `failure`\n- `cancelled`\n- `skipped`\n\nWhen a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`.\n\n### Example\n\n```yaml\nsteps:\n\n  - id: experimental\n  continue-on-error: true\n  run: ./build.sh experimental\n\n  - if: ${{ steps.experimental.outcome == 'success' }}\n  run: ./publish.sh experimental\n```\n\n### Terminology\n\nThe runs API uses the term `conclusion`.\n\nTherefore we use a different term `outcome` for the value prior to continue-on-error.\n\nThe following is a snippet from the runs API response payload:\n\n```json\n  \"steps\": [\n  {\n  \"name\": \"Set up job\",\n  \"status\": \"completed\",\n  \"conclusion\": \"success\",\n  \"number\": 1,\n  \"started_at\": \"2020-01-09T11:06:16.000-05:00\",\n  \"completed_at\": \"2020-01-09T11:06:18.000-05:00\"\n  },\n```\n\n## Consequences\n\n- Update runner\n- Update [docs](https://help.github.com/en/actions/automating-your-workflow-with-github-actions/contexts-and-expression-syntax-for-github-actions#steps-context)"}
{"repositoryUrl": "https://github.com/architecture-topography/topo.git", "path": "doc/adr/0003-graphql-as-api-query-language-via-appolo-server.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-06-06T04:16:22Z", "lastCommit": "2020-06-06T11:00:32Z", "numberOfCommits": 2, "title": "3. GraphQL as API Query Language via Apollo Server", "wordCount": 100, "authors": {"name1": 2}, "content": "# 3. GraphQL as API Query Language via Apollo Server\n\nDate: 2020-06-06\n\n## Status\n\nAccepted\n\n## Context\n\nWe need a way to populate our Neo4J database via an API layer.\n\n## Decision\n\nUse GraphQL libraries for the server API.  \nGraphQL has been increasing in popularity and maturity lately. It provides a more flexible API query layer compared to REST based interactions. Specifically being able to perform arbitrarily structured queries, with optional sub-elements and with query parameters.\n\n[Apollo Server](https://www.apollographql.com/) will be used as it's a popular well documented implementation.\n\n## Consequences\n\nWe will need to write the API server in Javascript or similar. (We're using TypeScript)\n"}
{"repositoryUrl": "https://github.com/guardian/manage-frontend.git", "path": "docs/04-node.md", "template": "unknown", "status": null, "firstCommit": "2018-05-31T11:35:27Z", "lastCommit": "2019-01-25T12:23:41Z", "numberOfCommits": 3, "title": "To use Node and Express for the server", "wordCount": 99, "authors": {"name1": 1, "name2": 2}, "content": "# To use Node and Express for the server\n\n## Context\n\nIt's not intended for this app to have a significant server-side component, instead it will directly send calls from the front end to a service layer. This service layer will be discussed in its own project, but presently comprises solely of [members data API](https://github.com/guardian/members-data-api), the same back end that currently provides account management functionality.\n\n## Decision\n\nThe back end for this app will have two responsibilities. Firstly, it will act as a proxy for calls to the service layer. Secondly, it will provide server side rendering capabilities to improve user experience.\n\n## Status\n\nAccepted\n"}
{"repositoryUrl": "https://github.com/SparksNetwork/backend-services.git", "path": "adr/adr-003-sending-notifications.md", "template": "Nygard", "status": "**proposal** | ~accepted~ | ~depreciated~ | ~superceded~", "firstCommit": "2016-10-19T06:54:06Z", "lastCommit": "2016-10-19T06:54:06Z", "numberOfCommits": 1, "title": "ADR 3: Sending notifications", "wordCount": 661, "authors": {"name1": 1}, "content": "# ADR 3: Sending notifications\n\n* Jeremy Wells\n\n## Status\n\n**proposal** | ~accepted~ | ~depreciated~ | ~superceded~\n\n## Context\n\n### Definitions\n\n* **Command message**: This is a message on a stream that has been generated by \na user action.\n* **Command service**: This is a service that listens to command messages.\n* **Data message**: This is a message on a stream that has been generated by \nanother service asking to persist data.\n\nThe application needs to send notifications to end users about certain events. \nFor example, send the volunteer an email to say that their application has \nbeen accepted.\n\nHowever, there are rules around the sending of notifications. With the above\n example, the volunteer should receive the accepted application when the \n following is true:\n\n* Engagement isAccepted = true\n* Opp confirmationsOn = true\n\nFurther, there are notifications with time based restrictions. For example, it\n may be desirable with the above email to wait for an amount of time before \n sending the notification so that an event coordinator can move an engagement \n around without causing a flood of emails.\n\nThe above rules mean that if an engagement is accepted, but the confirmations \nare not turned on, and then the confirmations get turned on, then the \nnotifications for existing accepted engagements get sent. Any engagements \nthat are accepted from that point in time are also sent.\n\n## Options\n\n1. Create the emails in the command services. For the above example, this would require code in the engagements service and the opps service. The engagements \ncode would check isAccepted && confirmationsOn. The opps service would \ncheck confirmationsOn and then loop over all engagements.\n\n  In order to implement the timing rules the service would add a sendAt to the email record. The email service itself would need to do something with this.\n  \n@startuml\nactor User\nparticipant engagements\ncontrol emails\ncontrol data\n\nUser -> engagements: isAccepted = true\nengagements -> engagements: confirmationsOn?\nalt true\n  engagements -> emails: Send accepted\nend\nengagements -> data: Update\n@enduml\n\n@startuml\nactor User\nparticipant opps\ncontrol emails\ncontrol data\n\nUser -> opps: confirmationsOn = true\nloop isAccepted engagements\n  opps -> emails: Send accepted\nend\nopps -> data: Update\n@enduml\n  \n**Pros**\n* Natural way to write it\n\n**Cons**\n* Services have too many responsibilities\n* Duplication\n* Complexity - loop over opp engagements\n* Hard to prevent duplicate emails\n\n2. Create a separate service that listens on data topics (engagements, opps) \nand when it sees an update to isAccepted or confirmationsOn, performs the above\n rules and sends to the emails topic.\n \n(for brevity user omitted): \n \n@startuml\nparticipant engagements\ncontrol data\nparticipant \"engagements accepted\"\ncontrol emails\n\nengagements -> data: Update isAccepted = true\ndata -> \"engagements accepted\"\n\"engagements accepted\" -> \"engagements accepted\": confirmationsOn?\n\"engagements accepted\" -> emails: Send accepted\n@enduml\n\n@startuml\nparticipant opps\ncontrol data\nparticipant \"engagements accepted\"\ncontrol emails\n\nopps -> data: Update confirmationsOn = true\ndata -> \"engagements accepted\"\nloop accepted engagements\n  \"engagements accepted\" -> emails: Send accepted\nend\n@enduml\n\n**Pros**\n* Services have single responsibility\n* Testing is easier\n\n**Cons**\n* Complexity - loop over opp engagements\n* Hard to prevent duplicate emails\n\n3. Introduce a notification model to the services in option 2. Thus instead of\nemitting email specific messages, the services emit normal data messages. In \nthe given example, when an engagement is accepted an accepted notification is \ncreated.\n\n  The accepted notification model / service will deal with the logic. As \n  engagements are accepted the notifications will be collected. When the opp\n  confirmationsOn flag is set the existing notifications will be sent.\n  \n  The implementation of a notifications service would need to run on a \n  schedule or loop.\n  \n@startuml\nparticipant engagements\nparticipant \"engagements accepted\" as accepted\ncontrol data\ndatabase database\n\nengagements -> data: Update isAccepted = true\ndata -> database: Update engagement\ndata -> accepted\naccepted -> data: create notification\ndata -> database: create notification\n@enduml\n\n@startuml\nparticipant notifications\ndatabase database\ncontrol emails\n\n[--> notifications: scheduler\nnotifications -> database: load\ndatabase -> notifications\nloop notifications\n  notifications -> notifications: condition check\n  alt true\n  notifications -> emails: Send email\n  notifications -> database: Remove notification\n  end\nend\n@enduml\n  \n**Pros**\n* No duplication\n* Services have single responsibility\n* Notification logic is separated\n* Notifications are tracked\n* Notifications are generic - will support more than email\n\n**Cons**\n* More code\n* Notification logic is separated\n* Notification model or service requires logic\n\n## Decision\n\nOption 3 is my preference\n\n## Consequences\n\n"}
{"repositoryUrl": "https://github.com/adaptris/interlok.git", "path": "docs/adr/0006-workflow-callback.md", "template": "Madr", "status": "ACCEPTED", "firstCommit": "2019-11-05T10:36:16Z", "lastCommit": "2020-02-12T11:56:20Z", "numberOfCommits": 5, "title": "(sem título)", "wordCount": 473, "authors": {"name1": 1, "name2": 4}, "content": "---\nlayout: page\ntitle: 0006-workflow-callback\n---\n# Add a new method to AdaptrisMessageListener\n\n* Status: ACCEPTED\n* Deciders: Aaron McGrath, Lewin Chan, Gerco Dries, Matt Warman, Sebastien Belin, (Paul Higginson)\n* Date: 2019-11-06\n\n## Context and Problem Statement\n\nWhen you enable [Dead Letter Queues](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html) in SQS; the contract is that messages read from a source queue are failed over to the dead letter queue once the redrive policy is fired (i.e. after a max number of attempts to deliver the message). From testing, this is predicated on the fact that the message is not deleted. Of course, our sqs-polling-consumer deletes the message after submitting it to the workflow.\n\nSince we can't rely on the JMS semantics either (since onMessage() traditionally doesn't throw exceptions, and RuntimeExceptions aren't propagated by Interlok) we then have to think about some way of having an asynchronous callback.\n\n\n## Considered Options\n\n* Do Nothing\n* Modify AdaptrisMessageListener to have callbacks.\n\n## Decision Outcome\n\nChosen option: Modify AdaptrisMessageListener to have callbacks.\n\n## Pros and Cons of the Options\n\n### Do Nothing\n\n\n* Good, because no change to code.\n* Bad, because we can't use SQS DLC behaviours.\n* Bad, because we might not preserve all message attributes (depending on configuration).\n* Neutral, error handling is still fully within the purview of Interlok (which is predictable)\n\n### Modify AdaptrisMessageListener to have callbacks.\n\nIf we change AdaptrisMessageListener to be this :\n\n```\ndefault void onAdaptrisMessage(AdaptrisMessage msg) {\n  onAdaptrisMessage(msg, (s)-> {}, (f)->());\n}\n\nvoid onAdaptrisMessage(AdaptrisMessage msg, java.util.function.Consumer<AdaptrisMessage> success, Consumer<AdaptrisMessage> failure);\n```\n\nThen we can effectively make our SQS polling consumer this :\n```\nfor (Message message : messages) {\n  try {\n  AdaptrisMessage adpMsg = AdaptrisMessageFactory.defaultIfNull(getMessageFactory()).newMessage(message.getBody());\n  // stuff skipped for brevity.\n  final String handle = message.getReceiptHandle();\n  // on success we delete the message, on failure we leave it in situ.\n  // Might need to make that configurable, since we don't want poison messages\n  retrieveAdaptrisMessageListener().onAdaptrisMessage(adpMsg, (s) -> {\n  sqs.deleteMessage(new DeleteMessageRequest(queueUrl, handle))\n  }, (f) -> {});\n  if (!continueProcessingMessages(++count)) {\n  break messageCountLoop;\n  }\n  }\n  catch (Exception e){\n  log.error(\"Error processing message id: \" + message.getMessageId(), e);\n  }\n}\n\n```\n\n* Good, because this makes the behavour controllable from the consumers perspective (e.g. fs-consumer could wait until the workflow completed before deleting the file...)\n* Good, because this will have the side effect of enabling callbacks for all consumers if they want it; which means we can get rid of the object monitors and things that we do Jetty + pooling workflow...\n* Bad, because it's a callback, and it happens at some point in the future... is the session/queue whatever still valid.\n* Bad, because threadsafe is hard.\n* Neutral, all workflows have to change (and message listener stub implementations).\n\n### Note 2020-02-12\n\nThe API change is going to be `void onAdaptrisMessage(AdaptrisMessage msg, Consumer<AdaptrisMessage> success);` since the semantics of how the failure will be handled (or where it should be fired) isn't clear at the moment. As of the next major release (v4); we are proposing a different way of handling things like this, since asynchronous callbacks are becoming ever more popular."}
{"repositoryUrl": "https://github.com/alphagov/verify-stub-idp.git", "path": "docs/adr/0001-record-architecture-decisions.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-04-09T16:35:20Z", "lastCommit": "2018-04-09T16:35:20Z", "numberOfCommits": 1, "title": "1. Record architecture decisions", "wordCount": 58, "authors": {"name1": 1}, "content": "# 1. Record architecture decisions\n\nDate: 2018-02-15\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to record the architectural decisions made on this project.\n\n## Decision\n\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n\n## Consequences\n\nSee Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's _adr-tools_ at https://github.com/npryce/adr-tools.\n"}
{"repositoryUrl": "https://github.com/nats-io/nats-architecture-and-design.git", "path": "adr/ADR-10.md", "template": "unknown", "status": null, "firstCommit": "2021-07-01T15:02:24Z", "lastCommit": "2021-07-09T13:19:27Z", "numberOfCommits": 3, "title": "JetStream Extended Purge", "wordCount": 286, "authors": {"name1": 3}, "content": "# JetStream Extended Purge\n\n|Metadata|Value|\n|--|---|\n|Date  |2021-06-30|\n|Author  |@aricart|\n|Status  |Implemented|\n|Tags  |server, client, jetstream|\n\n## Context\n\nJetStream provides the ability to purge streams by sending a request message to:\n`$JS.API.STREAM.PURGE.<streamName>`. The request will return a new message with\nthe following JSON:\n\n```typescript\n{\n  type: \"io.nats.jetstream.api.v1.stream_purge_response\", \n  error?: ApiError,\n  success: boolean,\n  purged: number\n}\n```\n\nThe `error` field is an [ApiError](ADR-7.md). The `success` field will be set to `true` if the request\nsucceeded. The `purged` field will be set to the number of messages that were\npurged from the stream.\n\n## Options\n\nMore fine-grained control over the purge request can be achieved by specifying\nadditional options as JSON payload.\n\n```typescript\n{\n  seq?: number,\n  keep?: number,\n  filter?: string\n}\n```\n\n- `seq` is the optional upper-bound sequence for messages to be deleted\n  (non-inclusive)\n- `keep` is the maximum number of messages to be retained (might be less\n  depending on whether the specified count is available).\n- The options `seq` and `keep` are mutually exclusive.\n- `filter` is an optional subject (may include wildcards) to filter on. Only\n  messages matching the filter will be purged.\n- `filter` and `seq` purges all messages matching filter having a sequence\n  number lower than the value specified.\n- `filter` and `keep` purges all messages matching filter keeping at most the\n  specified number of messages.\n- If `seq` or `keep` is specified, but `filter` is not, the stream will\n  remove/keep the specified number of messages.\n- To `keep` _N_ number of messages for multiple subjects, invoke `purge` with\n  different `filter`s.\n- If no options are provided, all messages are purged.\n\n## Consequences\n\nTooling and services can use this endpoint to remove messages in creative ways.\nFor example, a stream may contain a number of samples, at periodic intervals a\nservice can sum them all and replace them with a single aggregate.\n"}
{"repositoryUrl": "https://github.com/arachne-framework/architecture.git", "path": "/adr-009-datomic-config-ontology.md", "template": "Nygard", "status": "PROPOSED", "firstCommit": "2016-07-19T16:25:16Z", "lastCommit": "2016-08-20T22:11:13Z", "numberOfCommits": 4, "title": "Architecture Decision Record: Configuration Ontology", "wordCount": 357, "authors": {"name1": 3, "name2": 1}, "content": "# Architecture Decision Record: Configuration Ontology\n\n## Context\n\nIn [ADR-003](adr-003-config-implementation.md) it was decided to use a Datomic-based configuration, the alternative being something more semantically or ontologically descriptive such as RDF+OWL. \n\nAlthough we elected to use Datomic, Datomic does not itself offer much ontological modeling capacity. It has no built-in notion of types/classes, and its attribute specifications are limited to what is necessary for efficient storage and indexing, rather than expressive or validative power.\n\nIdeally, we want modules to be able to communicate additional information about the structure and intent of their domain model, including:\n\n- Types of entities which can exist\n- Relationships between those types\n- Logical constraints on the values of attributes:\n  - more fine grained cardinality; optional/required attributes\n  - valid value ranges\n  - target entity type (for ref attributes)\n\nThis additional data could serve three purposes:\n\n- Documentation about the intended purpose and structure of the configuration defined by a module.\n- Deeper, more specific validation of user-supplied configuration values\n- Machine-readable integration point for tools which consume and produce Arachne configurations.\n\n## Decision\n\n- We will add meta-attributes to the schema of every configuration, expressing basic ontological relationships.\n- These attributes will be semantically compatible with OWL (such that we could conceivably in the future generate an OWL ontology from a config schema)\n- The initial set of these attributes will be minimal, and targeted towards the information necessary to generate rich schema diagrams\n  - classes and superclass\n  - attribute domain\n  - attribute range (for ref attributes)\n  - min and max cardinality\n- Arachne core will provide some (optional) utility functions for schema generation, to make writing module schemas less verbose.\n\n## Status\n\nPROPOSED\n\n## Consequences\n\n- Arachne schemas will reify the concept of entity type and the possible relationships between entities of various types.\n- We will have an approach for adding additional semantic attributes in the future, as it makes sense to do so.\n- We will not be obligated to define an entire ontology up front\n- Modules usage of the defined ontology is not technically enforced. Some, (such as entity type relationships) will be the strong convention and possibly required for tool support; others (such as min and max cardinality) will be optional.\n- We will preserve the possibility for interop with OWL in the future.\n"}
{"repositoryUrl": "https://github.com/ministryofjustice/cloud-platform.git", "path": "architecture-decision-record/004-use-kubernetes-for-container-management.md", "template": "Nygard", "status": "✅ Accepted", "firstCommit": "2018-04-05T09:27:00Z", "lastCommit": "2020-08-17T10:59:03Z", "numberOfCommits": 5, "title": "Use kubernetes for running containerised applications", "wordCount": 311, "authors": {"name1": 2, "name2": 2, "name3": 1}, "content": "# Use kubernetes for running containerised applications\n\nDate: 10/04/18\n\n## Status\n\n✅ Accepted\n\n## Context\n\nMOJ Digital's approach to infrastructure management and ownership has evolved over time, and has led to the following outcomes:\n\n- Unclear boundaries on ownership and responsibilities between service teams and the cloud platforms team\n- Significant variation in deployment, monitoring and lifecycle management across products\n- Inefficient use of AWS resources due to the use of virtual machine-centric architecture, despite our standardisation on Docker containers\n\nThe last few years has seen the advent of several products specifically focused on the problem of running and managing containers in production:\n\n- Kubernetes\n- Mesos / Mesosphere / DC/OS\n- Docker Swarm\n- AWS ECS\n- CloudFoundry\n\nGiven the technology landscape within MOJ, we require a container management platform that can support a wide range of applications, from \"modern\" cloud-native 12-factor applications through to \"legacy\" stateful monolithic applications, potentially encompassing both Linux- and Windows-based applications; this removes CloudFoundry from consideration, given its focus on modern 12-factor applications and reliance on buildpacks to support particular runtimes.\n\nFrom the remaining list of major container platforms, Kubernetes is the clear market leader:\n\n- Rapid industry adoption during 2017 establishing it as the emerging defacto industry standard\n- Managed Kubernetes services from all major cloud vendors\n- Broad ecosystem of supporting tools and technologies\n- Increasing support for Kubernetes as a deployment target for commercial and open-source software projects\n\nThere is also precedent for Kubernetes use within MOJ, as the Analytical Platform team has been building on top of Kubernetes for around 18 months.\n\n## Decision\n\nUse Kubernetes as the container management component and core technology for our new hosting platform.\n\n## Consequences\n\n1. Several technical spikes looking at Kubernetes itself and associated components - logging, monitoring, application deployment, etc.\n2. Requirement to use an identity broker for Kubernetes user authentication, given the decision to use Github as an identity provider and the lack of a common auth protocol between Kubernetes and Github\n"}
{"repositoryUrl": "https://github.com/Trendyol/ios-architecture-decision-logs.git", "path": "adr/0005-inject-ab-config-global-values-toPresenter.md", "template": "unknown", "status": null, "firstCommit": "2020-07-22T10:19:45Z", "lastCommit": "2020-07-22T10:19:45Z", "numberOfCommits": 1, "title": "Inject AB / Config / Global Values to Presenters For Testablity", "wordCount": 93, "authors": {"name1": 1}, "content": "# Inject AB / Config / Global Values to Presenters For Testablity\n\n* Status: accepted\n* Deciders: iOS Team\n* Date: 2020-07-21\n\n## Context\n\nWe faced a problem that missing test cases on some presenters because of not injectable variables like ab tests, config or global values on presenters. So we want to cover all of these missings.\n\n## Decision\n\nWe decided to inject this variables to related presenters from their constructors.\n\n## Consequences\n\nInjecting this variables to presenters from their constructors are giving us to more testable presenters. So we can tests all production code cases which is involving business logics.\n"}
{"repositoryUrl": "https://github.com/elifesciences/tech-team.git", "path": "adr/0002-use-containers-for-foreign-languages.md", "template": "Nygard", "status": "Proposed", "firstCommit": "2017-11-09T11:49:01Z", "lastCommit": "2017-11-09T11:49:01Z", "numberOfCommits": 1, "title": "1. Use containers to deploy tools from foreign languages source code", "wordCount": 210, "authors": {"name1": 1}, "content": "# 1. Use containers to deploy tools from foreign languages source code\n\nDate: 2017-10-25\n\n## Status\n\nProposed\n\n## Context\n\neLife has a small set of supported languages: PHP, Python, JavaScript; in-house developers are present for them. All other languages are defined as **foreign**.\n\nThere are tools that are peculiar to our infrastructure, such as [goaws](https://github.com/p4tin/goaws) for AWS simulation, but are written in languages no one is an expert in (Go).\n\nThere are also tools that were originally written in another language but are being adopted by us, like [INK](https://gitlab.coko.foundation/INK/ink-api) for document conversion, written in Ruby.\n\nThese tools are usually distributed as source code. The operational overhead of writing formulas for the environment to build them is a form of waste.\n\nSome tools written in Java instead have a very stable runtime platform (ElasticSearch, even Jenkins), as they are distributed as binaries.\n\n## Decision\n\nWe will use existing Docker containers to deploy tools that require building from source in a foreign language, in testing or production environments.\n\n## Consequences\n\nWe should not see too many runtimes being supported in builder-base-formula.\n\nWe should not allocate time to resolve versioning or building issues for languages such as Go or Ruby, but reuse existing container.\n\nWe should tag the container images we use and push them onto [https://hub.docker.com/u/elifealfreduser/] for reproducibility.\n"}
{"repositoryUrl": "https://github.com/ec-europa/europa-component-library.git", "path": "docs/decisions/005-themes.md", "template": "unknown", "status": null, "firstCommit": "2020-10-19T08:25:01Z", "lastCommit": "2020-10-26T08:10:18Z", "numberOfCommits": 2, "title": "Handle `themes` in ECL v3", "wordCount": 677, "authors": {"name1": 2}, "content": "# Handle `themes` in ECL v3\n\n| Status  | accepted  |\n| --- | --- |\n| **Proposed**  | 07/10/2020  |\n| **Accepted**  | 19/10/2020  |\n| **Driver**  | @kalinchernev   |\n| **Approver**  | [@team](https://github.com/orgs/ec-europa/teams/inno) |\n| **Consulted** | @emeryro, @planctus   |\n| **Informed**  | [@team](https://github.com/orgs/ec-europa/teams/inno) |\n\n## Decisions\n\n- Themes will be managed as separate packages (described in [this point](#option-4-themes-are-multiple-packages-multiple-presets-generate-multiple-themes-output))\n- Resources will be managed as separate packages (described in [this point](#option-1-separate-package-per-system-or-per-theme))\n- Decisions can be revised after actual iterations\n\n## Context\n\n- ECL v2 was made in order to generate two independent design systems, EC and EU.\n- ECL V3 starts from the assumption that there is only one source template and one design system with multiple `themes`.\n- The number of themes to accommodate is not set as of the time of the document writing, thus the magnitude could vary between 2 (previous systems) to 20+.\n\n## Prerequisite notes\n\nThe term \"code duplication\" used in this document follows the [conventional definition](https://en.wikipedia.org/wiki/Duplicate_code). Duplicate code example in ECL v2 can be illustrated with source code for EC and EU which is completely identical and exists in several occassions. However, splitting a module file of hundreds of lines of code into multiple files containing portions of the same code is not considered code duplication but code separation or code splitting.\n\n## Consequences\n\n- There are continous internal discussions for the definition of a \"theme\" and whether it's overlapping with the concept of presets of ECL v2.\n- The CSS source code needs an improved organization. It must scale with time and reduce complexity and code duplication.\n- Release packages will require changes. The question on publishing \"themes\" is unclear.\n- Storybook application(s) (ECL Playground website) will most probably also need changes accordingly. It is unclear whether a single instance of a Storybook application can accommodate the new themes architecture or there will be multiple instances.\n- There is an strong uncertainty on how to consistently manage all other assets such as JavaScript, markdown files, icon and logo resources, templates, etc. Environment variables have been considered as a global contextual information but above-mentioned ambiguous topics are unclear at this stage.\n\n## Scopes\n\nThe topics below are categorized with focus on separation of subjects rather than specific type of code.\n\n### Package organization\n\nAlthough the concept of \"presets\" was put under question during the analysis of v3, they are still used throughout this document in terms of code organization, not ECL release packages. This means that regardless of the usage of presets below, they may not be part of the resulting release of ECL v3.\n\n#### Option 1: ECL v2 is preserved\n\nThis option has been discarded from the very beginning because it cannot achieve the goal without increased complexity and code duplication.\n\n#### Option 2: \"themes\" is single package, single preset generates multiple themes output\n\nThis option has been discarded as too complex for the worth of the result. The complexity comes from SASS language limitations as dynamic imports are not supported. Every dynamic theme generation approach with SASS involves workarounds which were not acceptable for the team.\n\n#### Option 3: \"themes\" is single package, multiple presets generate multiple themes output\n\n- `@ecl/ecl-base` - common variables\n- `@ecl/ecl-theme` - set of variables related to the themes (global parameters), one set per theme\n- `@ecl/ecl-preset-{ec-core}` - base + theme ec core + components\n- `@ecl/ecl-preset-{eu-core}` - base + theme eu core + components\n- `@ecl/ecl-preset-{ec-standardised}` - base + theme ec standardised + components\n- `@ecl/ecl-preset-{eu-standardised}` - base + theme eu standardised + components\n- `@ecl/ecl-preset-{custom}` - template for theme generation\n\n#### Option 4: \"themes\" are multiple packages, multiple presets generate multiple themes output\n\nSame as option 3 with the following difference in theme-related packages which are multiple:\n\n- `@ecl/ecl-theme-{ec-core}` - set of variables\n- `@ecl/ecl-theme-{eu-core}` - set of variables\n- `@ecl/ecl-theme-{ec-standardised}` - set of variables\n- `@ecl/ecl-theme-{eu-standardised}` - set of variables\n\nIn this version, each target bundle is composed of a pair of a theme + preset.\n\n### Resource organization\n\nResources are: logo, favicons, icons and other similar types of assets which are not SCSS or JavaScript and are used by consumers of ECL.\n\n#### Option 1: separate package per system or per theme\n\nCurrent organization of v2.\n\n- `@ecl/resources-ec-logo`\n- `@ecl/eu-resources-logo`\n\n#### Option 2: single package\n\n- `@ecl/resources`\n\n#### Option 3: include resources in themes\n\nThis option does imply that each theme comes in a dedicates package ([see above](#option-4-themes-are-multiple-packages-multiple-presets-generate-multiple-themes-output)).\nResources useful to display a theme are stored directly in it.\nTo limit duplication, some global resources may still be put in dedicated packages.\n"}
{"repositoryUrl": "https://github.com/azavea/franklin.git", "path": "docs/0001-record-architecture-decisions.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-01-06T15:50:25Z", "lastCommit": "2020-01-06T15:50:25Z", "numberOfCommits": 1, "title": "(sem título)", "wordCount": 48, "authors": {"name1": 1}, "content": "---\nid: 0001-record-architecture-decisions\ntitle: 1 - Recording Architecture Decisions\n---\nDate: 2019-12-30\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to record the architectural decisions made on this project.\n\n## Decision\n\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n\n## Consequences\n\nSee Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's [adr-tools](https://github.com/npryce/adr-tools).\n"}
{"repositoryUrl": "https://github.com/ebi-uniprot/uniprot-rest-api.git", "path": "doc/architecture/decisions/0004-lombok.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-03-08T16:50:33Z", "lastCommit": "2019-03-08T16:50:33Z", "numberOfCommits": 1, "title": "4. Lombok", "wordCount": 95, "authors": {"name1": 1}, "content": "# 4. Lombok\n\nDate: 2018-08-02\n\n## Status\n\nAccepted\n\n## Context\n\nJava projects often contain a large amount of boilerplate code, e.g., defining data/value classes, builders, etc. All\nsuch code follows a certain pattern and needs testing -- and writing both of these can be error prone. A library\nthat enables cutting down boilerplate code, and which generates tested code would be beneficial to the project.\n\n## Decision\n\nWe will use the [Lombok](https://projectlombok.org/) library to reduce the amount of boilerplate code we need to write.\n\n## Consequences\n\nWith less code to maintain, we envisage a more succint codebase whose function is more readable.\n"}
{"repositoryUrl": "https://github.com/cerner/beadledom.git", "path": "adr/health/default_dependencies_primary.md", "template": "unknown", "status": null, "firstCommit": "2018-11-29T15:24:55Z", "lastCommit": "2018-11-29T15:24:55Z", "numberOfCommits": 1, "title": "**Title**: Dependencies Default to Primary", "wordCount": 109, "authors": {"name1": 1}, "content": "## **Title**: Dependencies Default to Primary\n\n## **Status**: accepted\n\n## **Context**:\n\nPrimary Health checks were not checking if dependencies were primary or not before checking their health.\nThis was not the intended behavior of the primary health check endpoint, as it should only check the primary dependencies of any given project.\n\n## **Decision**:\n\nIn order to do this passively, we changed the dependency class to default to primary unless otherwise specified.\nWe did this because we don't know if our consumers rely on the previous behavior of meta/health checking dependencies if they were of unspecified importance.\n \n## **Consequences**:\n\nAny system that wants a dependency to be secondary must explicitly state it as such.\n\n"}
{"repositoryUrl": "https://github.com/Voronenko/runbooks-mkdocs.git", "path": "docs/architecture/decisions/0003-use-plantuml-for-diagramming.md", "template": "Nygard", "status": "Accepted Amended by [5. Use PlantUML for diagramming with use of stdlib](0005-use-plantuml-for-diagramming-with-use-of-stdlib.md)", "firstCommit": "2020-11-03T21:04:48Z", "lastCommit": "2020-11-03T21:11:53Z", "numberOfCommits": 2, "title": "3. Use PlantUML for diagramming", "wordCount": 60, "authors": {"name1": 2}, "content": "# 3. Use PlantUML for diagramming\n\nDate: 2020-11-03\n\n## Status\n\nAccepted\n\nAmended by [5. Use PlantUML for diagramming with use of stdlib](0005-use-plantuml-for-diagramming-with-use-of-stdlib.md)\n\n## Context\n\nThe issue motivating this decision, and any context that influences or constrains the decision.\n\n## Decision\n\nThe change that we're proposing or have agreed to implement.\n\n## Consequences\n\nWhat becomes easier or more difficult to do and any risks introduced by the change that will need to be mitigated.\n"}
{"repositoryUrl": "https://github.com/AbhishekJoshi/ionic-dummy-repo.git", "path": "@docs/adr/ADR-001.md", "template": "Nygard", "status": ":bulb: Proposed on 2018-01-24 :white_check_mark: Accepted on 2017-06-23", "firstCommit": "2018-02-22T10:48:03Z", "lastCommit": "2018-02-22T10:48:03Z", "numberOfCommits": 1, "title": "ADR-001<br/> Should the seed template be opinionated on state management framework choice?", "wordCount": 270, "authors": {"name1": 1}, "content": "# ADR-001<br/> Should the seed template be opinionated on state management framework choice?\n\n\n## Context\nGiven the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard? \n\nDevelopers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?\n\n\n### Who Was Involved in This Decision\n- Alex Ward\n- Chris Weight\n\n\n### Relates To\n- N/A\n\n\n## Decision\nThe Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.\n\n\n## Status \n:bulb: Proposed on 2018-01-24\n:white_check_mark: Accepted on 2017-06-23\n\n\n## Consequences\n\n### - Positives\n\n+ Developers are free to work to their strengths and pre-existing knowledge when starting a project\n+ There is flexibility should a 'better solution' become prevalent in *the future*\n+ Reduces maintenance overhead in the starter seed\n+ Developers not familiar with a particular state-management approach, framework, library etc have the opportunity to study how it is used 'in the wild' and gain expertise\n\n### - Negatives\n\n- A Developer on-boarding onto an existing project may not be familiar with the design pattern expressed by a chosen state-management framework or library. This will increase on-boarding time\n- Consistency across projects is not guarenteed\n- An unknown chosen state-management approach may ultimately prove to be a bad choice (hard to reason, difficult to maintain, overly complex etc)\n\n"}
{"repositoryUrl": "https://github.com/HHS/TANF-app.git", "path": "docs/Technical-Documentation/Architecture-Decision-Record/008-deployment-flow.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2022-01-04T18:34:02Z", "lastCommit": "2022-01-04T18:34:02Z", "numberOfCommits": 1, "title": "8. Deployment Flow", "wordCount": 574, "authors": {"name1": 1}, "content": "# 8. Deployment Flow\n\nDate: 2021-01-27 (_Updated 2022-07-18_)\n\n## Status\n\nAccepted\n\n## Context\n\nOur Cloud.gov organization currently has three Spaces -- `tanf-dev`, `tanf-staging`, and `tanf-prod`. The vendor team currently has access to the tanf-dev space only.\n\nSince the recent changes to our [Git workflow](https://github.com/HHS/TANF-app/blob/main/docs/Technical-Documentation/Architecture-Decision-Record/009-git-workflow.md) we believe our current deploy strategy should be updated to more closely match the workflow. Previously, since we had approvals on two different repositories we decided that it made sense to maintain [two separate staging sites](https://github.com/HHS/TANF-app/blob/837574415af7c57e182684a75bbcf4d942d3b62a/docs/Architecture%20Decision%20Record/008-deployment-flow.md). We would deploy to one with approval in the raft-tech repository, and another with approval to HHS. Since we now have all approvals made in raft-tech, the deploy after approval serves the same purpose as deploying to the Government staging site would have.\n\nAdditionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to \"crowding\", where multiple in-progress features by different devs all want to be deployed to the same environment for testing. \n\nAs of Spring 2022, following [ADR 018](https://github.com/HHS/TANF-app/blob/main/docs/Technical-Documentation/Architecture-Decision-Record/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.   \n\n## Decision\n\nDeploy Environment | Cloud.gov Space | Cloud.gov Dev Access | Role   | Deploys when ...  |\n---|---|--|--|---|\nDev  | Tanf-Dev  | Vendor & Gov  | Deploy code submitted for gov review  | Relevant github label assigned as shown below   |\nDevelop  | Tanf-Staging  | Vendor & Gov  | Deploy code once gov-approved   | Code merged to `raft-tech/TANF-app:develop` |\nStaging  | Tanf-Staging  | Gov   | Deploy code once gov-approved   | Code merged to `HHS/TANF-app:main` |\nProduction   | Tanf-Prod   | Gov   | Deploy code tested in staging & ready for prod  | Code merged to `HHS/TANF-app:master`  |  \n\n### Gitflow and Deployments\nWe will be following the Gitflow process which is an industry standard. You can read more about it [in our ADR](./018-versioning-and-releases.md). I will just highlight the parts relevant for our deployment strategy. Release branches will be merged to `HHS/TANF-app:master` which will deploy to our production sites. Code merged to `raft-tech/TANF-app:develop` will be deployed to our staging sites.\n\n### Dev deployments\nWithin the dev space, there is no correlation for branch to environment as these feature or bugfix branches will constantly vary:\n\n| Dev Site | Frontend URL | Backend URL | Purpose  |\n| -- | -- | -- |--|\n| A11y | https://tdp-frontend-a11y.app.cloud.gov | https://tdp-backend-a11y.app.cloud.gov/admin/ | Space for accessibility testing  |\n| QASP | https://tdp-frontend-qasp.app.cloud.gov | https://tdp-backend-qasp.app.cloud.gov/admin/ | Space for QASP review  |\n| raft | https://tdp-frontend-raft.app.cloud.gov | https://tdp-backend-raft.app.cloud.gov/admin/ | Space for Raft review  |\n\n## Consequences\n\n**Pros**\n* 3-space strategy instead of 4 aligns better with our current gitflow. \n* Code deploys automatically upon gov approval, but does not deploy immediately to prod, leaving room for further gov testing. Any mistakes that make it past gov review will not deploy immediately to prod.\n* Vendor dev team \"crowding\" should be reduced through this solution.\n* Only need to maintain/monitor vendor dev access to a single Cloud.gov Space using this solution -- least privilege, simplified account management.\n* Frees up memory and disk space in the dev environment.\n* Dedicated environment for release-specific features.\n\n**Risks**\n* None that we can see at this time\n\n## Notes\n\n- As of June 2022, CircleCI supplies environment variable key-value pairs to multiple environments (e.g. vendor's CircleCI deploys applications to dev and staging environments). The values from CircleCI are expected to be unique per environment, so until [#1826](https://github.com/raft-tech/TANF-app/issues/1826) is researched and addressed, these values will need to be manually corrected in cloud.gov immediately following the execution of the execution of the [`<env>-deployment` CircleCI workflow](../../.circleci/config.yml) CircleCI workflow. This workaround applies to backend applications in the TDP staging environment.\n"}
{"repositoryUrl": "https://github.com/eclipse/winery.git", "path": "docs/adr/0019-version-in-the-name.md", "template": "unknown", "status": null, "firstCommit": "2018-04-18T08:30:39Z", "lastCommit": "2018-04-19T07:05:53Z", "numberOfCommits": 2, "title": "Versions of TOSCA elements in the name", "wordCount": 298, "authors": {"name1": 2}, "content": "# Versions of TOSCA elements in the name\n\nIn order to enable the versioning of TOSCA elements, the version corresponding to one element\nmust be saved in a TOSCA compliant way. \n\nForces:\n- TOSCA compliant\n- The version identifier must be detectable in the XML file\n\n\n## Considered Options\n\n* Version in the name\n* Version in the namespace\n* Save version externally\n\n## Decision Outcome\n\n* Chosen Option: version in the name/id because it is compliant to the TOSCA specification and shows the version \n  directly in the XML file.\n* Easiest and best fit regarding compliance\n\n## Pros and Cons of the Options\n\n### Version in the name\n\n* Good, because it is consistent to the TOSCA specification\n* Good, because even from outside of the winery, definitions can be detected in the specific version on first sight\n* Good and bad, because it requires a deep copy of all files and definitions on creating a new version*\n* Bad, because Introduces naming conventions to the naming of components: '_' are not allowed anymore*\n\n### Version in the namespace\n\n* Good, because it is easy and well established method in XML\n* Good, because the definition’s name/id stays intact\n* Bad, because it implies that all elements in the corresponding namespace have the same version\n* Bad, because it is usually used to specify the version of the XML’s vocabulary only\n\n### Save version externally\n\n* Good, because it requires less disk space than\n* Bad, because the version is not detectable in the XML\n\n\n## License\n\nCopyright (c) 2017 Contributors to the Eclipse Foundation\n\nSee the NOTICE file(s) distributed with this work for additional\ninformation regarding copyright ownership.\n\nThis program and the accompanying materials are made available under the\nterms of the Eclipse Public License 2.0 which is available at\nhttp://www.eclipse.org/legal/epl-2.0, or the Apache Software License 2.0\nwhich is available at https://www.apache.org/licenses/LICENSE-2.0.\n\nSPDX-License-Identifier: EPL-2.0 OR Apache-2.0\n"}
{"repositoryUrl": "https://github.com/anton-liauchuk/educational-platform.git", "path": "docs/architecture-decisions/0009-architecture-tests.md", "template": "unknown", "status": null, "firstCommit": "2020-06-27T11:09:40Z", "lastCommit": "2020-12-26T21:04:26Z", "numberOfCommits": 2, "title": "9. Architecture tests.", "wordCount": 33, "authors": {"name1": 2}, "content": "# 9. Architecture tests.\nDate: 2020-06-27\n\n## Status\nAccepted\n\n## Context\nWe need to have the mechanism for supporting and validating common architecture principles in all application.\n\n## Decision\nArchitecture tests with using Archunit should be implemented.\n"}
{"repositoryUrl": "https://github.com/mozmeao/infra.git", "path": "docs/architecture/decisions/0007-service-dns-patterns.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2021-01-06T17:33:19Z", "lastCommit": "2021-01-06T17:33:19Z", "numberOfCommits": 1, "title": "7. Service dns Patterns", "wordCount": 285, "authors": {"name1": 1}, "content": "# 7. Service dns Patterns\n\nDate: 2021-01-04\n\n## Status\n\nAccepted\n\n## Context\n\nOur current dns naming follows a couple of very similar patterns. Sometimes using 'gcp', 'frankfurt', 'oregon-b' as ways to separate different environments.  We should have one pattern and stick to it the best we can.\n\nThings the pattern needs to solve for:\n\n* Should be 'the same' for all of the meao services (for example: nucleus/bedrock/snippets).\n* should allow for multiple 'environments' of a service to be deployed in the same region ('prod'|'stg'|'dev')\n* should allow for multiple regions/deployments of the same service + environment ('or', 'fr', 'ia')\n* should also have a good 'user facing' pattern, that is not the same as the above pattern. (www.mozilla.org -> 'bedrock' 'prod' 'or' && 'bedrock' 'prod' 'fr' with some mechanism for choosing between the two deployments.)\n\n## Decision\n\nFor the backend deployments follow this pattern: 'service'.'environment'.'region'.'domain'. An incomplete list of each of examples of values for those variables:\n\n| Service  |\n|--|\n| bedrock  |\n| nucleus  |\n| snippets |\n| prom   |\n\n| Environments |\n|--|\n| dev  |\n| stg  |\n| prod   |\n| demo1  |\n\n| Region | Description   |\n|--|---|\n| or   | oregon eks cluster  |\n| fr   | frankfurt eks cluster |\n| ia   | iowa gcp cluster  |\n\n| Domain   |\n|--|\n| moz.works  |\n| mozmar.org |\n| ramzom.org |\n\nThis leads to a few examples:\n\n| Examples   |\n|--|\n| bedrock.dev.or.moz.works |\n| prom.prod.fr.mozmar.org  |\n| nucleus.stg.ia.moz.works |\n\n\nNote that these are for 'internal' use primarily.  The user facing domains will stay as they are.  A few examples, nucleus.mozilla.org (prod) and nucleus.allizom.org (stg), www.mozilla.org (bedrock prod) www.allizom.org (bedrock stg).  The connection between the new dns entries and the user facing will stay the same. (If we're using a r53 traffic policy now, we will continue to after this change, if we're just using cname/alias records we will again after this change,etc, including cloudflare vs cloudfront etc.)\n\n\n\n## Consequences\n\nAll services will live at new addresses.\nOld addresses will need to be deleted.\n"}
{"repositoryUrl": "https://github.com/holochain/holochain-rust.git", "path": "doc/architecture/decisions/0017-capabilities.md", "template": "Nygard", "status": "Draft", "firstCommit": "2019-02-19T20:47:32Z", "lastCommit": "2019-02-26T14:23:59Z", "numberOfCommits": 2, "title": "Capabilities & Security ADR", "wordCount": 1377, "authors": {"name1": 1, "name2": 1}, "content": "# Capabilities & Security ADR\n\nDate: 2019-02-26\n\n## Status\nDraft\n\n## Context\n\nA unified security model for Holochain applications:\n* Each zome must be able to represent and enforce its own security modeling because that is the appropriate place to do so. (Push the intelligence to the edges.)\n* Developers must be able to build in granualar specificity and revokability of access to functions and entries.\n* We must be able to distinguish between application security model, and architectural and code security model.  I.e. what is the security model application developers build into their apps, and the security model of Conductor/Core, etc.  E.g. we have to ensure that zome calls aren't subject to replay attacks in general, and also allow zome developers to declare and create security policies in specific.\n\n## Decision\n\nHolochain will use a variant of the [capabilities](https://en.wikipedia.org/wiki/Capability-based_security) security model. Holochain DNA instances will grant revokable, cryptographic capability tokens which are shared as access credentials. Appropriate access credentials must be used to access to functions and private data.\n\nThis enables us to use a single security pattern for:\n - connecting end-user UIs,\n - calls across zomes within a DNA,\n - bridging calls between different DNAs,\n - and providing selective users of a DNA the ability to query private entries on the local chain via send/receive.\n\nEach capability grant gets recorded as a private entry on the grantor's chain, and are validated against for every zome function call.\n\n## Elements:\n### CapabilityGrant\nThe CapabilityGrant is by default a private entry recorded on the chain because this does not need to be published to the DHT (though it may be in some cases). The Address (i.e. hash) of these entries serves as the capability token credential that is part of an access request.  The instance can look up the grant by that address and confirm if the request conforms to the grant type.   DNA developers can do this manually using `grant` and `verify_grant` api calls for custom access use-cases, but this is also done automatically to verify both zome function calls as well as bridge and cross zome calls.\n\n#### Grant attributes:\n1. **Asignees:** a list of agents to whom the grant applies.  If the list is not specified then, then the grant is assumed to be transferable, i.e. possesion of the token is sufficient and serves as a password.  Note that \"anonymous\" and assigned grants are mutually exclusive.\n2. **Pre-filled Parameters:** A zome-function call grant may also specify a template for parameter values of a the function being granted access.  This allows for \"currying\" type behavior on grants, where the grant itself forces a function parameter to specifc value.\n\n*Comments:* In the past we have talked about the \"public\" capability. All access must be signed.  \"public\" access comes from publishing an unassigned token publicly.\n\n#### Special Case -- Agent Grant:\nThere is one special case grant, the **Agent Grant**.  So far in Holochain, we have been saying that the second entry on the chain, after the DNA, is the AgentID entry, that identifies the agency by public-key that \"owns\" the chain.  We have also talked about (and implemented in proto along with the revoking and re-issue of the AgentID entries). In the capabilities security model we can unify this with capability grants where that second entry can be thought of as \"super-user/admin/root\" capability which should indeed be granted to the agent who \"owns\" the chain, but the token of that agency (the public key) may need to be revoked and replace, just like any other capability token.\n\n### CapabilityRequest\nA capability request is a structure for making a request referring to a particular capability grant.\n\nRequest attributes:\n1. **Token:** the address of the capability grant entry committed to the chain\n2. **Provenance:** the address of the requester and the signature of request contents.\n3. **Contents:** the exact data of the contents of the request that is signed.  In the case of zome function call requests this is the function name, the function parameters, and a nonce for preventing replay attacks.\n\n*Comments:* Core has to be able to do three things with a CapabilityRequst:\n1. Load the grant from the address (or detect that this is a special case grant)\n2. Confirm that the Contents matches the the Provenance.\n3. Extract the data from the contents that's needed for the purpose, i.e. actually get the function name and parameters of the zome-call.\n\n## Processes\n### Zome Function Calls\n\nCapabilities allow developers to specify control access to zome function calling, either from exterior calls or via bridging or even cross-zome calling.  This latter may seem odd, but it's important for enabling more secure development patterns for zome mix-ins.  See Consequences for details.  A broad overview of how zome function call from an outside source would flow under the capabilities model:\n\n1. Agent bundles function name, parameters and a timestamp into a call request block, and signs it with the agent's private key, and sends it to Conductor along with any other parameters necessary for routing to the correct instance over what ever interface is being used.\n2. Conductor creates a CapabilityRequest structure and passes it into holochain Core.\n3. Core loads CapabilityGrant from chain by it's address, and checks validity according to the grant's parameters, returning a CapabilityCheckFailed error, or calling the zome function if successfull.  It may also check the timestamp of the call to make-sure it's within a reasonble window to prevent some re-play attacks.  Additionally, if complete security from replay attacks is necessary we may implement an additional handshake where the agent makes a \"pre-call\" indicating the desire to make a zome call.  In that case the Conductor would have to pass this request into core where a nonce could be generated that the client has to include in the call request block. Note that this also requires implementing an ephemeral store in core, something that's on our development path.\n\n*Comments:*\n\n1. The \"agent\" above could be a web-UI that holds the private key (in some Holo cases) or could be an extended Conductor that is an electron app.  In the former case, the necessary token grants have to be passed to the UI.  If the agency in the UI is the \"owner\" of the chain, then it can effectively use the special case agent grant to make any zome call it wants.  Otherwise, it will have to have received the public token, or an assigned or transferable token.  See Consequences below.\n\n#### Calling Elements/Structs\nWe will use a simple JSON structure for what gets signed by which ever component of the system has the private key:\n\nTBD. Draft structures: https://hackmd.io/cvXMlcffThSpN-C5WrfGzg?view#\n\n### Genesis\n\n- We use the convention of using a reserved-trait name (\"hc_public\") to identify functions for which such a public grant can be created at genesis time, and be made available to the Conductor to send to UIs (or proxy on their behalf in the various use-cases i.e. as a web-proxy) for creating provenance for public access.\n\n*Comments:* this has been implemented in `capabilities-3`\n\n### Bridging\n\nTBD\n\n## Consequences\n\n### Exposing Tokens\n\n#### Public Token\nWe need to add a method/convention for agents to be able to access the public token generated at genesis time.  In `capabilities-3` the public token is returned as part of the results of initialization during genesis.  That initialization data strcture from genesis should be made available in the HDK through a new PUBLIC_TOKEN global, and to the conductor for additions to the conductor_api.\n\n#### Conventions & examples for generated tokens\nWe need to provide some examples of zome functions/patterns of how to request tokens, generate them and return them for use by calling agents, both at the level of UI zome function calling, and at the application level for use in node-to-node send & receive communications.\n\n### Zome mix-in security\nThe capability model is not only useful for extra-membrane security, but also intra-membrane security for Cross-zome calls.  Because our composibility model includes drop-in zomes, for which the developer may not be able to see the source code (i.e. they only get the WASM), it is important to create the ability to make calling functions in other zomes subject to a capability request on a specific grant.\n\nFor this to work, we may need to expand the expressivity of the `sign` API call.  i.e. we may need to limit under what agency that call can be made for certain zomes.\n"}
{"repositoryUrl": "https://github.com/simpledotorg/simple-server.git", "path": "doc/arch/014-region-level-sync.md", "template": "Nygard", "status": "Accepted. This feature was released to all users by Feb 2020.", "firstCommit": "2021-05-03T13:19:48Z", "lastCommit": "2021-05-03T13:19:48Z", "numberOfCommits": 1, "title": "Region level sync", "wordCount": 544, "authors": {"name1": 1}, "content": "# Region level sync\nSeptember 2020\n\n_This ADR has been added retroactively in Apr 2021 to capture our switch to block-level syncing._\n\n## Status\nAccepted. This feature was released to all users by Feb 2020.\n\n## Context\n[PRD](https://docs.google.com/document/d/1Cflct0Y-44IRUVw_5-NptcnNSX1UgAPBiXqoXHq22io/edit)\n\nUsers in large districts reported that the Simple app was running very slow, making the app near-unusable.\nThe slowdown was caused by the volume of patient data synced to the user’s phone. We realised that the amount of data\nbeing stored on the device had to be reduced for better long-term performance.\n\nCurrently we sync the entire district's records to a user's phone. Some of the large districts have upto 50,000 patients,\nwhich can amount to 400-500 MB of data. On lower-end phones we noticed the app started slowing down when the DB size grew beyond 250 MB.\n\nA district typically has between 1-20 blocks. From trends in IHCI, we found it's uncommon for patients to visit facilities across blocks.\nPatients that have a BP taken in more than 1 block is around 2%, with the exceptions of:\nSindhudurg (9.8%), Hoshiarpur (5.3%), Bathinda (3.1%).\nThis means that we can sync only a block's data to the user's phone and be reasonably confident about finding patients on the app.\n\n## Decision\n- The server will sync records from the user's block instead of the entire district.\n  Specifically the following patients will be synced:\n  - patients that registered at a facility in the same block,\n  - patients that are assigned to a facility in the same block, and\n  - patients that have an appointment scheduled at a facility in the same block.\n- All other sync resources will return records belonging to these patients only.\n- The sync mechanism should support the ability to adjust the sync radius to any sync region.\n  This is important in case we need to change the kind of records that are synced to the app in the future.\n  See the [wiki entry on Region level sync](../wiki/adjusting-sync-boundaries.md) for how it works.\n\n### On the app\n- Users can continue selecting any facility in their district when switching facilities.\n- Users can continue selecting any facility in their district when scheduling a patient’s next visit or preferred facility. \n- It is possible that a patient will visit a facility outside their block and their record will not be found on the user’s device. In this case the user should \n  - Scan the patient’s BP passport if they have one.\n  - Register the patient again, as if they were new. Make sure to attach their existing BP passport to the registration.\n  - The duplicate patient records will be merged by the Simple team later.\n\n## Consequences\n- The Simple app will not be able to find patients who moved from one block to another.\n- Block is currently a freeform text field on the `Facility` model.\n  It needs to be a first-class entity to make block-level syncing possible.\n  This is introduced through the `Region` model.- Block is currently a freeform text field on the `Facility` model.\n  It needs to be a first-class entity to make block-level syncing possible.\n  This is introduced through the `Region` model.\n- When a patient gets registered across blocks as a duplicate, we will need to identify them and merge their data. \n  We plan to implement online patient lookup for the case where a patient is not found locally.\n"}
{"repositoryUrl": "https://github.com/embvm/embvm-core.git", "path": "docs/architecture/decisions/0009-event-driven-framework-design.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-09-24T21:42:21Z", "lastCommit": "2020-09-24T21:42:21Z", "numberOfCommits": 1, "title": "9. Event Driven Framework Design", "wordCount": 150, "authors": {"name1": 1}, "content": "# 9. Event Driven Framework Design\n\nDate: 2018-07-06\n\n## Status\n\nAccepted\n\n## Context\n\n* Embedded systems are highly event-driven, as they are responding to external stimuli and reacting in a planned way\n* Event-driven APIs reduce coupling, as the various objects don't need to know anything about other objects that they work with\n* We can reduce the number of threads used by relying on event-driven behavior\n\n## Decision\n\nThe framework will be defined with interfaces and processing models that support event-driven development (callbacks, events, register/unregister for events).\n\nDispatch queues will be provided to assist with the event driven model.\n\nPlatform examples will default to dispatch-based processing models.\n\n## Consequences\n\n* All of our APIs should support an event driven interface\n* Consideration of callback functions & notification registering needs to be included at all stages\n* Threading will still be allowed and usable, maintaining flexibility\n* Dispatch queues will be used to handle generic callbacks without blocking high-priority operations\n\n## Further Reading\n\n* [Observer Pattern](../../patterns/observer.md)\n"}
{"repositoryUrl": "https://github.com/Flowminder/FlowKit.git", "path": "docs/source/developer/adr/0010-prefect-for-autoflow.md", "template": "Nygard", "status": "Pending", "firstCommit": "2019-11-22T16:45:56Z", "lastCommit": "2019-11-22T16:45:56Z", "numberOfCommits": 1, "title": "Prefect for workflow definition and execution in AutoFlow", "wordCount": 243, "authors": {"name1": 1}, "content": "# Prefect for workflow definition and execution in AutoFlow\n\nDate: 22 November 2019\n\n# Status\n\nPending\n\n## Context\n\nThe first prototype of AutoFlow used [Apache Airflow](https://airflow.apache.org/) (as used in FlowETL) to define and execute workflows. However, this proved to be problematic in some respects - Airflow has limited support for parametrising DAG runs and sharing data between tasks, and re-running a DAG for an execution date for which it has already run is complicated.\n\n[Prefect Core](https://docs.prefect.io/) is an alternative open-source workflow engine, which allows DAGs to be parametrised and run simultaneously for multiple sets of parameters, and allows data exchange between tasks. Prefect also allows the creation of dynamically-generated tasks mapped over the outputs from running another task, which makes it easier for AutoFlow to spawn multiple runs of a workflow when the sensor finds multiple days of data for which the workflow has not previously run.\n\n## Decision\n\nAutoFlow will use Prefect to define and run workflows.\n\n## Consequences\n\nThe process of parametrising and dynamically mapping workflow runs is simpler than it would be with Airflow.\n\nUnlike Airflow, Prefect Core is not a full workflow management system - it provides the functionality for defining workflows and running them individually, but the full workflow orchestration and monitoring system is left to the proprietary Prefect Cloud platform. As a result, AutoFlow cannot benefit from a UI such as Airflow provides, and if in the future we want AutoFlow to run multiple sensor workflows we will need to write our own code to do this concurrently.\n"}
{"repositoryUrl": "https://github.com/zendesk/link_platform.git", "path": "doc/architecture/decisions/0014-use-redux.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-08-01T22:52:28Z", "lastCommit": "2018-08-22T20:26:36Z", "numberOfCommits": 2, "title": "14. Use redux", "wordCount": 78, "authors": {"name1": 1, "name2": 1}, "content": "# 14. Use redux\n\nDate: 2018-08-01\n\n## Status\n\nAccepted\n\n## Context\n\nRedux is a state container for JavaScript applications. It helps to manage state across an application.\n\n[This article](https://hackernoon.com/the-react-state-museum-a278c726315) provides a nice non-comprehensive listing of various alternative state management libraries.\n\n## Decision\n\nWe will use Redux in `link_platform` for state management.\n\n## Consequences\n\nRedux introduces a layer of complexity when introducing components and handling user interactions. This complexity may be undesirable if our application is simple or does not deal with many changes in state.\n"}
{"repositoryUrl": "https://github.com/home-assistant/architecture.git", "path": "adr/0016-home-assistant-core.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-07-07T16:11:05Z", "lastCommit": "2020-07-07T16:11:05Z", "numberOfCommits": 1, "title": "0016. Installation method: Home Assistant Core", "wordCount": 488, "authors": {"name1": 1}, "content": "# 0016. Installation method: Home Assistant Core\n\nDate: 2020-07-01\n\n## Status\n\nAccepted\n\n## Context\n\nDefine a supported installation method as per [ADR-0012](https://github.com/home-assistant/architecture/blob/master/adr/0012-define-supported-installation-method.md).\n\n## Decision\n\nThis is for running just the Home Assistant Core application directly on Python. It does not provide the full Supervisor experience and thus does not provide the Supervisor panel and add-ons.\n\n### Supported Operating Systems and versions\n\n- All major Linux distributions, latest stable and major versions.\n- Windows; only using WSL.\n- macOS; Python via Homebrew.\n\n### Supported Python versions\n\nRunning Home Assistant Core is only supported when running the application using the official Python virtual environment. Running Home Assistant Core without a virtual environment, system/globally installed Python packages, is not supported.\n\nDetails on the supported Python versions are defined in [ADR-0020](./0020-minimum-supported-python-version.md).\n\n### Documentation\n\nSome operating systems will require extra libraries or packages to be installed prior to installing the Python requirements. In this case our documentation shall link to the installation instructions of the Python requirement that requires them.\n\nIn case that is not available or possible, we will name the libraries or packages that need to be installed. We do not aim not include installation instructions for every OS.\n\n### Required Expertise\n\n- **Installation**\n  Requires installing Python 3 with venv support (the default except on Debian based systems). Then create a virtual environment and install Home Assistant Core via pip.\n\n  For packages that require compilation, the user will need to install compilers and other development packages. If those development packages are not the same as provided by the operating system, you can break your system.\n\n* **Start when the system is started:** This is the responsibility of the user. It is based on their operating system.\n* **Run with full network access:** Works, is the only option.\n* **Access USB devices:** This works out of the box.\n\n* **Maintaining the Home Assistant installation**\n  Maintenance requires more time, effort, skills, and experience than the other methods.\n\n  - **Python upgrades:** Home Assistant upgrades Python every year. It can happen that your current operating system doesn’t support the new minimum required version out of the box. In that case, you need to find unofficial Python packages for your system or compile Python from source.\n  - **Installing Python dependencies:** Some Python packages need compilation. Users are responsible for having the right compilers and development packages installed.\n  - **Updating Home Assistant:** Updating happens via the pip command-line tool.\n\n- **Maintaining the Operating System**\n  Home Assistant Core runs in a Python virtual environment. Anything outside of that is the responsibility of the user.\n\n* **Security updates for OS:** Responsibility of the user.\n\n* **Maintaining the components required for the Supervisor:** No supervisor, so N/A\n\n**Conclusion:**\nThis is an expert installation method. Based on the integrations that you’re running, you will need a lot of extra packages installed.\n\n## Consequences\n\nUpdate documentation on how to install this method, the required experience and the expected maintenance.\n\nMove existing documentation that does not match supported installation methods to the community guides wiki.\n\nNotify user during onboarding of expected maintenance for their installation method.\n"}
{"repositoryUrl": "https://github.com/poorprogrammer/cfo.git", "path": "doc/adr/0001-record-architecture-decisions.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-05-07T14:59:51Z", "lastCommit": "2020-05-07T14:59:51Z", "numberOfCommits": 1, "title": "1. Record architecture decisions", "wordCount": 45, "authors": {"name1": 1}, "content": "# 1. Record architecture decisions\n\nDate: 2020-05-07\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to record the architectural decisions made on this project.\n\n## Decision\n\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n\n## Consequences\n\nSee Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's [adr-tools](https://github.com/npryce/adr-tools).\n"}
{"repositoryUrl": "https://github.com/alphagov/gsp.git", "path": "docs/architecture/adr/ADR011-build-artefacts.md", "template": "Nygard", "status": "Pending", "firstCommit": "2019-07-04T16:41:05Z", "lastCommit": "2019-07-05T16:23:08Z", "numberOfCommits": 2, "title": "ADR011: Build Artefacts", "wordCount": 189, "authors": {"name1": 1, "name2": 1}, "content": "# ADR011: Build Artefacts\n\n## Status\n\nPending\n\n## Context\n\nAs part of our pipelines we will be building artefacts that will be used to test\nand deploy our applications. We will be deploying applications to Kubernetes. We\nwill need to build a container image of some kind.\n\nThere are some competing container image formats, namely:\n\n* [OCI]\n* [ACI]\n\nThe OCI image format is [based on the Docker v2][oci-standard] image format.\n\nThe Kubernetes project appears to [prefer Docker/OCI][k8s-preferance] images\nover ACI.\n\n[rkt is moving to OCI][rkt-oci] and away from ACI. OCI will become the preferred\nimage format.\n\nDocker has wide industry adoption and appears to have wide understanding within\nGDS.\n\nDocker is the default container runtime for Kubernetes.\n\n## Decision\n\nWe will build and store OCI images built using Docker.\n\n## Consequences\n\n* It may be tricky to deploy apps outside of a container orchestrator of some\n  kind.\n* This means that the images will need to be pre-built and stored in accessible\n  way.\n* We will be unable to use container runtimes that do not support OCI images.\n\n[OCI]: https://github.com/opencontainers/image-spec\n[ACI]: https://github.com/appc/spec/blob/259c2eebc32df77c016974d5e8eed390d5a81500/spec/aci.md#app-container-image\n[oci-standard]: https://blog.docker.com/2017/07/oci-release-of-v1-0-runtime-and-image-format-specifications/\n[k8s-preferance]: https://kubernetes.io/blog/2015/05/docker-and-kubernetes-and-appc/\n[rkt-oci]: https://github.com/rkt/rkt/blob/03285a7db960311faf887452538b2b8ae4304488/ROADMAP.md#oci-native-support\n"}
{"repositoryUrl": "https://github.com/wikimedia/mediawiki-extensions-Popups.git", "path": "docs/adr/0006-factories.md", "template": "Nygard", "status": "Accepted. This ADR was accepted implicitly by the (current) primary maintainer of this repository, Sam Smith. The date of this ADR was changed to reflect this.", "firstCommit": "2017-02-22T10:46:45Z", "lastCommit": "2019-01-17T16:11:29Z", "numberOfCommits": 3, "title": "6. Factories", "wordCount": 481, "authors": {"name1": 1, "name2": 1, "name3": 1}, "content": "# 6. Factories\n\nDate: 2016-11-08\n\n## Status\n\nAccepted.\n\nThis ADR was accepted implicitly by the (current) primary maintainer of this\nrepository, Sam Smith. The date of this ADR was changed to reflect this.\n\n## Context\n\nGiven that the majority of the codebase is going to be rewritten, there's\na need for a consistent style for building the system anew.\n\nThe [Reading Web team](https://www.mediawiki.org/wiki/Reading/Web/Team),\nhistorically, has tended towards taking an object oriented approach to building\nsoftware. However, a typical result of this approach are classes that have many\nvaried concerns, share – not specialise – behaviour via inheritance rather than\nvia composition. These issues are evidenced by a lack of unit tests, i.e. the\nclasses become increasingly hard to test and even harder to test in isolation\nto the point where high-level integration tests are relied on for validation of\nthe design.\n\nUnless attention is paid, these classes have all of their members exposed by\ndefault due to a lack of support for visibility modifiers from either the\nJavaScript language or our (current) tooling. Like other teams, the [Reading\nWeb team](https://www.mediawiki.org/wiki/Reading/Web/Team) tends to follow the\nconvention of prefixing private member names with an underscore.\n\nMoreover, while planning the rewrite of the codebase, [the decision to use\nRedux to maintain state](./0002-contain-and-manage-state.md) was made very\nearly on. A significant part of the codebase will be written in the style that\nRedux requires: functions that return objects, or _factories_.\n\nWhat's needed, then, is a general rule that, when applied, leads the Reading\nWeb team to produce a codebase that's easier to maintain (verify and modify)\nand is familiar. This rule must also acknowledge that it must be broken now and\nagain.\n\n## Decision\n\n1. Favour factories over classes – however we wish to define them, e.g. with\n   [OOjs](https://www.mediawiki.org/wiki/OOjs) – by default.\n2. Favour classes when the performance benefits of prototypal inheritance far\n   outweigh the benefits of consistency and simplicity.\n\n## Consequences\n\nThe most obvious consequence of this decision is the easy portability of the\ncodebase: there's no requirement for a framework to help define classes and\nmanage inheritance, e.g. [OOjs](https://www.mediawiki.org/wiki/OOjs). Moreover,\nthe `new` operator in `new Foo()`, is simply replaced with the `createFoo`\nfactory function, and the `instanceof` operator is rendered useless.\n\nThe more subtle consequence is that behaviour must be shared via composition\nsince the use of inheritance is strongly discouraged. The most important\npositive consequence of this is is that the system will be more flexible as\nit'll be composed of implementations of small interfaces. The most important\nnegative consequence is that more effort will be required when sharing\nbehaviour between parts of the system, which is trivial using inheritance, as\nattention must be paid when designing these interfaces.\n\nDespite being the negative consequence, requiring more attention to be paid\nwhen defining behaviour should make it harder to write – and easier to spot\n– components \"that have many varied concerns\" and, hopefully, result in\ncomponents that are easier to test.\n"}
{"repositoryUrl": "https://github.com/UST-MICO/docs.git", "path": "adr/0013-source-to-image-workflow.md", "template": "Madr", "status": null, "firstCommit": "2018-11-25T15:14:50Z", "lastCommit": "2018-12-01T16:50:59Z", "numberOfCommits": 3, "title": "Source-to-Image Workflow", "wordCount": 549, "authors": {"name1": 3}, "content": "# Source-to-Image Workflow\n\nTechnical Story: [Evaluate Knative build](https://github.com/UST-MICO/mico/issues/49)\n\n## Context and Problem Statement\n\nWe want to have a Source-to-Image workflow to import services based on a GitHub repository. It should run inside our Kubernetes cluster, however currently Kubernetes doesn't have a resource build-in that is able to build container images. Therefore another technology is required.\n\n## Decision Drivers\n\n* MUST run on our Kubernetes cluster\n* MUST run completely in userspace (no root access required)\n* MUST be sufficient to provide a single URL to a GitHub repository (with included Dockerfile)\n* SHOULD be independent of any cloud service provider\n\n## Considered Options\n\n* Gitkube\n* Google Cloud Build\n* Azure Container Registry Tasks (ACR Tasks)\n* OpenShift Source-to-Image (S2I)\n* Knative Build\n\n## Decision Outcome\n\nChosen option: *Knative Build*, because it meets all of our criterion decision drivers. It allows us to implement a Source-to-Image workflow on our Kubernetes cluster independently to any cloud service provider.\n\n### Positive Consequences\n\n* By using *Knative Build* we have the choice to use different kinds of `Builders`, follow-up decision is required: [Building OCI images](./0015-building-oci-images.md)\n\n### Negative consequences\n\n* *Nothing known*\n\n## Pros and Cons of the Options\n\n### Gitkube\n\n[GitHub: Gitkube](https://github.com/hasura/gitkube)\n\n* Good, because it is easy to use (only git and kubectl are required)\n* Good, because it is designed to build and deploy images to Kubernetes\n* Bad, because it uses Docker-in-Docker approach (see [Building OCI Images](./0015-building-oci-images.md))\n* Bad, because it is build for a different purpose: Helping developers to trigger the build and deployment of an image by using `git push`\n\n### Google Cloud Build\n\n[Cloud Build documentation](https://cloud.google.com/cloud-build/docs/)\n\n* Good, because it is easy to use, nothing has to be installed (only a call to the `Cloud Build API` is required)\n* Good, because it is a managed service therefore it doesn't consume our own resources\n* Good, because it allows us to use different `Builder` technologies (see [Building OCI Images](./0015-building-oci-images.md))\n* Bad, because we depend on the continuity of a third-party service (Google Cloud Build)\n* Bad, because it runs only on Google Cloud, that forces vendor lock-in\n* Bad, because it leads to additional costs (first 120 builds-minutes per day are free, see [Pricing](https://cloud.google.com/cloud-build/pricing)\n\n### Azure Container Registry Tasks (ACR Tasks)\n\n[Tutorial: Automate container image builds with Azure Container Registry Tasks](https://docs.microsoft.com/en-us/azure/container-registry/container-registry-tutorial-build-task)\n\n* Good, because it is a managed service therefore it doesn't consume our own resources\n* Good, because we already run on the Azure Cloud\n* Bad, because it uses the Docker daemon internally, no other `Builders` can be used (see [Building OCI Images](./0015-building-oci-images.md))\n* Bad, because there is no public API available, only usable with the Azure CLI\n* Bad, because we depend on the continuity of a third-party service (ACR Tasks)\n* Bad, because it runs only on Azure, that forces vendor lock-in\n* Bad, because it leads to additional costs (100 minutes are included for free per month, see [Pricing calculator](https://azure.microsoft.com/en-us/pricing/calculator/#)\n\n### OpenShift Source-to-Image (S2I)\n\n[GitHub: Source-To-Image (S2I)](https://github.com/openshift/source-to-image)\n\n* Good, because it is the way to go for creating a Source-to-Image workflow on a OpenShift cluster\n* Bad, because it is designed for the OpenShift Container Platform (additional effort is required to use it on plain Kubernetes)\n* Bad, because it uses the Docker daemon internally (in future *Buildah*), no other `Builders` can be used (see [Building OCI Images](./0015-building-oci-images.md))\n\n### Knative Build\n\n[GitHub: Knative Build](https://github.com/knative/build)\n\n* Good, because it has the backing of industry giants (Google, Red Hat, IBM, SAP)\n* Good, because it is designed for Kubernetes\n* Good, because it provides a standard, portable, reusable, and performance optimized method for defining and running on-cluster container image builds\n* Good, because it allows us to use different `Builder` technologies (see [Building OCI Images](./0015-building-oci-images.md))\n* Good, because *Knative Build* consumes little resources (2 pods a ~11 MB).\n* Bad, because it is still work-in-progress\n"}
{"repositoryUrl": "https://github.com/nulib/meadow.git", "path": "doc/architecture/decisions/0008-api-documentation.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-07-11T15:29:30Z", "lastCommit": "2019-07-11T15:29:30Z", "numberOfCommits": 1, "title": "8. api-documentation", "wordCount": 38, "authors": {"name1": 1}, "content": "# 8. api-documentation\n\nDate: 2019-07-01\n\n## Status\n\nAccepted\n\n## Context\n\nWe want our [API](./0004-api.md) to be self-documenting and testable as we go.\n\n## Decision\n\nUse an [OpenAPI hex package](https://hexdocs.pm/open_api_spex) to automate OpenAPI tasks.\n\n## Consequences\n\nGenerating, validating, coding, and testing our APIs is much faster and easier.\n"}
{"repositoryUrl": "https://github.com/alphagov/verify-proxy-node.git", "path": "doc/adr/20190327-hashing-attributes.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-03-28T11:23:01Z", "lastCommit": "2019-03-28T11:23:01Z", "numberOfCommits": 1, "title": "Hashing the identity information", "wordCount": 313, "authors": {"name1": 1}, "content": "# Hashing the identity information\n\nDate: 2019-03-27\n\n## Status\n\nAccepted\n\n## Context\n\nThe SAML profile used by the eIDAS scheme requires that SAML messages are sent directly to the receiving proxy node via a user's browser. This introduces a host of security concerns, especially when we take the history of SAML exploits and XML security in general into account. Due to many of the attacks being centred around breaking XML parsing, the main concerns raised were of SAML being parsed by a public-facing service or the service responsible for communicating with the hardware security module (HSM). With all this considered, the role of the SAML parsing has been assigned to the Verify Service Provider (VSP). \n\nWe need to have confidence that the identity information we are sending back to the Member State is the same as the identity information which was originally issued by the identity provider (IDP). The Gateway service will receieve a SAML response back from the Hub which is sent to the Translator service, before being passed onto the VSP which will parse the response. Due to the exploits surrounding XML parsing, we need to be confident that the VSP hasn't been compromised, as it could result in manipulated attributes being sent to the Translator which would ultimately result in fraudulent identities being sent back to a Member State.\n\n## Decision\n\nTo ensure that we are confident that the identity information we are sending back to the Member State is the same as what was originally issued by the IDP, we will LOG to monitoring a secure one-way SHA-256 hash of the user attributes in the Verify Hub and within the Translator service in the eIDAS Proxy Node. Any mismatches between the two Hashes will trigger an alert. \n\n## Consequences\n\n- This would be more beneficial to be implemented as a technical control, to have a higher confidence that no malicious transactions are approved."}
{"repositoryUrl": "https://github.com/Alfresco/alfresco-anaxes-shipyard.git", "path": "docs/adrs/0009-using-one-db-per-service.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-01-23T06:04:46Z", "lastCommit": "2018-01-30T11:58:49Z", "numberOfCommits": 2, "title": "9. Using one database per service", "wordCount": 220, "authors": {"name1": 1, "name2": 1}, "content": "# 9. Using one database per service\n\nDate: 2018-01-10\n\n## Status\n\nAccepted\n\n## Context\n\nMany of the components the Digital Business Platform offers need a database for storing information.\nCurrently Content Services, Process Services and Sync Service do this but there might be more to come in the future. All three are using the same version of the database so the obvious question was raised: Can we just use one database for all our services?\n\n\nWe identified several advantages of using one database per service over one for all the services.\n\nChoosing one database per service would put us in line with the current best practices in the microservices world, as well as allowing us to change versions of the DB individually, and as an added benefit it kills off the possibility of one service being able to impact performance on other services.\n\nIf we would choose one database for all our components however we would have fewer components to configure, keep track of, monitor and backup.\n\n## Decision\n\nWe will proceed with the one database per service for existing and future components as there are a number of advantages over having all our services fight over the same database.\n\n## Consequences\n\nWith each new component that has the need for database storage, we would automatically have one more configuration to manage, monitor and backup.\n"}
{"repositoryUrl": "https://github.com/sul-dlss-labs/infrastructure-adrs.git", "path": "/0002-extract-only-useful-technical-metadata.md", "template": "unknown", "status": null, "firstCommit": "2020-01-28T21:24:57Z", "lastCommit": "2020-01-28T21:24:57Z", "numberOfCommits": 1, "title": "(sem título)", "wordCount": 552, "authors": {"name1": 1}, "content": "---\nlayout: default\ntitle: ADR-0002\nnav_order: 5\npermalink: records/0002/\n---\n# Extract Only Useful Technical Metadata\n\n* Status: drafted\n* Decider(s): <!-- required -->\n  * Andrew Berger\n  * Hannah Frost\n  * Vivian Wong\n  * Infrastructure Team\n  * Justin Coyne\n  * Mike Giarlo\n  * Peter Mangiafico\n  * Jeremy Nelson\n  * Justin Littman\n  * Naomi Dushay\n  * John Martin\n  * Aaron Collier\n* Date(s): <!-- required -->\n  * drafted: 2019-10-29\n\n## Context and Problem Statement <!-- required -->\n\nCurrently we are using JHOVE 1.x to generate voluminous technical metadata for every file of every object accessioned in SDR, and we do not use most of this metadata. This is problematic especially for large & many files: we cannot currently accessioning books with many pages because the technical metadata robot consumes all system memory which causes the virtual machine to kill the JHOVE process. We believe that only a small subset of the JHOVE output will ever be useful to SDR consumers.  Note: SMPL content ships with its own metadata typically from MediaInfo rather than JHOVE.\n\n## Decision Drivers <!-- optional -->\n\n* Cannot accession large files (objects > 1GB or so)\n* Cannot accession objects with many pages, such as books\n* Blocker for Google Books project\n* Causes extreme delays accessioning other content\n\n## Open Questions\n\n* What does SMPL do with technical metadata datastream. Should this be cross-walked or added as an attached binary?\n* How does this relate to the file identification as part of the contentMetadata? Should we persist the modeled data as a new type of datastream or as part of contentMetadatata?\n\n## Considered Options <!-- required -->\n\n1. Do nothing\n1. Identify, and only persist, *needed* technical metadata in a well-defined data model (*.e.g.*, the [one used in Princeton's figgy app](https://github.com/pulibrary/figgy/blob/main/app/resources/nested_resources/file_metadata.rb#L4-L35)), using a tool (or tools) other than JHOVE\n1. Stop extracting technical metadata, though this may conflict with being considered a trusted digital repository\n1. Add resources to worker machines\n1. Run extraction using cloud-based processing\n\n## Decision Outcome <!-- required -->\n\n**Preferred** (by Infrastructure Team) option: option 2, because:\n\n* Option 1 is preventing us from accessioning books and other large objects, which is unacceptable to SDR customers\n* Option 3 is an unsound preservation strategy and does not meet SDR user needs\n* Option 4 has already been pursued a number of times already, and there's only so much we can toss at the worker machines\n* Option 5 has been rejected as a general deployment strategy for now\n\nThus, option 2 is the only option that currently meets the department's and its customers' needs.\n\nAs part of this work, we will move forward with a two-prong strategy in order to resolve the tension between the need to come up with a sound, community-oriented preservation practice and the need to accession large-scale content now.\n\nIn the short-term, we will come up with a short list of technical metadata attributes that will be extracted from all files and from all files of certain types. We will convene a “technical metadata strike team” in short order that will review attributes being used in Samvera and make recommendations based thereupon. The aim is for this group to finalize their recommendations in advance of the January 2020 Google Books work cycle.\n\nIn parallel, we will pursue a longer-term effort for determining what an ideal, community-oriented strategy is for doing this work building on best practices (which are currently murky/non-emergent). Along with this longer-term work, we will look into how to support on-demand regeneration of technical metadata so that we can iterate on the short-term work in the prior bullet.\n"}
{"repositoryUrl": "https://github.com/budproj/architecture-decision-log.git", "path": "adl/0003-domain-driven-design.md", "template": "Nygard", "status": "Accepted.", "firstCommit": "2020-10-09T20:27:18Z", "lastCommit": "2021-07-23T14:24:50Z", "numberOfCommits": 3, "title": "ADR 0003: Domain-Driven Design", "wordCount": 325, "authors": {"name1": 2, "name2": 1}, "content": "# ADR 0003: Domain-Driven Design\n\n* [Table of contents](#)\n  * [Context](#context)\n  * [Decision](#decision)\n  * [Status](#status)\n  * [Consequences](#consequences)\n  * [More reading](#more-reading)\n\n## Context\n\nDuring software development, the most common mistake is not speaking the same language with stakeholders and the product team. It is common having different points of view of the product architecture, and more significant gaps mean higher blockers and mistakes in the future.\n\nSince we're creating a new product, we have the advantage of having the same language and architecture overview from day zero, meaning a more stable and robust infrastructure.\n\n## Decision\n\nTo prevent these issues, we decided to implement the Design-driven Development framework to architect our domain collaboratively with the product team and business owners.\n\nWith DDD, we can ensure proper refactors in the future, having a clear overview of our entities' relationships and domains. To learn more about DDD, [check this summary](https://medium.com/@ruxijitianu/summary-of-the-domain-driven-design-concepts-9dd1a6f90091).\n\nWe can't have a clear, detailed overview of our entire domain structure (that would be overwhelming), but we've organized our DDD in a way that we can have a high-level concept of the whole company and drill-down to see a more detailed view of each bounded context.\n\n**The high-level concept of our Domain-Driven Design** is located in this repository, at [ADR#0015](0015-model-overview.md). You can take a look at that record to learn more about our architecture overview.\n\n**Bounded context detailed domains** are located in the same folder (`records/domains`), but there is a single file for each bounded context. Those architectures focus only on the given bounded context and any entity that relates to it.\n\nEach microservice we have are related to a single bounded context. So, to create new microservices, you must add a new bounded context to our domain architecture.\n\n## Status\n\nAccepted.\n\n## Consequences\n\nDomain-Driven Design is not a common topic. We must teach our developers to use it. But, this effort may reward us well. A good, stable, and reliable domain architecture is the key to a successful company.\n\nWe must ensure our onboarding process covers this paradigm to avoid any unwanted unrelated domain entity.\n\n---\n\n## More reading\n\n* [Brief DDD summary](https://medium.com/@ruxijitianu/summary-of-the-domain-driven-design-concepts-9dd1a6f90091) \n* [Book](https://www.amazon.com.br/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215/ref=asc_df_0321125215/?tag=googleshopp00-20&linkCode=df0&hvadid=379735814613&hvpos=&hvnetw=g&hvrand=12360278098423015108&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1001751&hvtargid=pla-449269547899&psc=1)\n"}
{"repositoryUrl": "https://github.com/status-im/status-react.git", "path": "doc/decisions/0009-release-process-mobile.md", "template": "unknown", "status": null, "firstCommit": "2018-07-25T06:52:55Z", "lastCommit": "2018-07-25T06:52:55Z", "numberOfCommits": 1, "title": "0009. Mobile App Release Process", "wordCount": 1014, "authors": {"name1": 1}, "content": "# 0009. Mobile App Release Process\n\n| Date | Tags |\n|---|---|\n| Tue Jul 24  | process, release |\n\n\n## Status\n\naccepted\n\n\n## Context\n\nClassical release approach: we cut off the release branch, stabilize it, fix every critical issue and release. Some changes are cherry-picked to the release branch.\n\nIt has a couple of downsides:\n\n- Resources are scattered across two branches;\n\n- Unique fixes and unique combination of commits in the release branch;\n\n- Unpredictable release schedule (any day can be a release day!).\n\n## The Process\n\nWe do \"failable releases\" approach instead, when the release either happens on\na specific day, or doesn't happen.\nWe aim for a weekly cadence.\nBut even more, we aim to be frank about the release state, give ourselves\na permission to fail a release.\n\n\n### The Release Checklist\n_☝️  a release blocker is a GHI with “release” tag on it_\n\n1. Do a release testing on a nightly.\n1. If it is good enough (no release blockers), cut a release branch, like `releases/0.9.23`. If there is an existing branch from the previous unsuccessful release, rebase it to the current state of `develop`.\n1. 🔄 Test the release branch, fix release blockers.\n1. Cherry-pick only release blockers to the release branch.\n1. Check-up with other teams (@go, #core-infra).\n1. Mobile releases should not happen at the same time as cluster upgrades.\n1. *After* the release branch is cut \n  - update status-go on `develop` (NOT the release branch);\n  - bump the app version on `develop` (NOT the release branch).\n1. Update release notes, and app descriptions in GP and App Store (see [this section](#release-notes)).\n1. If [“go/no go” assessment](#go-no-go) is negative (“no go”), just abandon the release branch.\n\n\n### Failable Releases\nFailable release philosophy:\n- We track potential release blockers as early as possible (based on testing of nightlies);\n- We cut off the release branch when there are no big blockers (wednesday morning the latest);\n- We fix remaining release blockers on `develop` and cherry-pick fixes to the release branch;\n- If we aren’t able to fix all release blockers in time™, leaving enough time for QA to thoroughly test the release, we mark this release as failed and focus on releasing next week;\n- Next week we just rebase the release branch on `develop`.\n\n### What is this failed release anyway?\n1. Nothing is published to our users;\n1. We don’t keep the release branch around, next week we force-rebase it to the latest state of `develop`.\n\n### Is it bad to fail a release?\nNope. One of the nice side-effects of the failable release approach that it shows the real state of the develop branch. \nTrying too hard to release anyway might paint a picture that is better than the reality.\nIf there are too many failed releases, it is an indication that something is wrong with our `develop` or PR intakes, not with the release process. Don’t shoot the messenger :)\n\n### <a name=\"go-no-go\"></a> “Go/No-Go” decision\nThe “no-go” decision can be make:\n1. If there is a huge blocker on develop that we are not sure we will be able to fix in time;\n1. If there is no time left for QA to make thorough testing;\n1. If we don’t feel confident in some critical feature of the app, even if there is no critical issues found there;\n1. If one of the teams (status-go, cluster, etc) isn’t ready for this release (check with the `#core-infra` or `@go`).\n\n### Schedule\n- We aim to submit an iTC build every Friday to have time for Apple to review it.\n- We aim to publish a release every Monday.\n\nNote, that to aim is a key word there. If we fail to release in time, we just skip this week’s slot and try to release next week.\n\nSo, schedule might look like that:\n```\nMay, 11: Release 1\nMay, 18: failed release, nothing is published\nMay, 25: failed release, nothing is published\nJune, 1: Release 2\n...\n```\n\nSo, as you can see releases happen only on Mondays.\nThey might or might not happen, but the schedule stays consistent.\n\n### <a name=\"release-notes\"></a>Release Notes\nWe keep the file [`CHANGELOG.md`](../../CHANGELOG.md) in the repository.\nWe also have an ongoing document with them.\n\n#### iOS Test Flight Release\n**Upload to AppStore Connect**\nUse [this Jenkins job](https://jenkins.status.im/job/status-mobile/job/upload_release_ios/)\n\n**“What to test” field**\nWhen AppStore Connect asks you to fill in the field called “what to test”, just copy the release notes there.\n\n**Submitting to the review**\nWe submit it on Friday, fix the compliance and add the group called “External Testers”. Don’t make the group name scare you, the real testers are in the group called “testflight-boarding”. \n\nDon’t forget to update screenshots if necessary!\n\nThen we submit it to Apple review.\n\n**Releasing to our beta-testers**\nIf reviewed successfully, we can share it to our users by adding “testflight-boarding” group to our build. As soon as it is added, invitations to upgrade are sent to our beta-testers!\n\n### GP Release\nThe uploaded release is **immediately available**! \nDo it only if the iOS build is approved by Apple!\n\nUse [this Jenkins job](https://jenkins.status.im/job/status-mobile/job/upload_release_android/)\n\nDon’t forget to update the screenshots if necessary!\n\n**App Description**\nApp description needs to be updated. It is much shorter than the release notes, so it is important to trim them down for GP.\n\n### Flexing & Planning\nWith this approach we don’t plan features for release. We plan features for priorities.\nWhat is the difference?\nWhen we plan features for release, a feature A absolutely has to be included in release 0.2. That means, that if feature is not ready yet, the release 0.2 is not happening.\n\nIn features for priorities approach we plan which feature comes before or after which. Say, if we have features A, B and C and we know that A is more important than B and C then we will try to release it earlier. \n\nIn that case releases history might look like these:\n```\nWeek 1: 0.1 - failed\nWeek 2: 0.1 - feature A released\nWeek 3: 0.2 - polishing of feature A and bugfixes\nWeek 4: 0.3 - feature B\nWeek 5: 0.4 - feature C\n...\n```\n\nSo we keep releasing cadence even if there are no features to release.\n\n### Retrospective\nEach release ends with a short retrospective/planning session for the next week.\n\n\n\n"}
{"repositoryUrl": "https://github.com/cosmos/ibc-go.git", "path": "docs/architecture/adr-001-coin-source-tracing.md", "template": "unknown", "status": null, "firstCommit": "2021-04-26T13:42:28Z", "lastCommit": "2021-09-17T06:50:55Z", "numberOfCommits": 2, "title": "ADR 001: Coin Source Tracing", "wordCount": 2049, "authors": {"name1": 1, "name2": 1}, "content": "# ADR 001: Coin Source Tracing\n\n## Changelog\n\n- 2020-07-09: Initial Draft\n- 2020-08-11: Implementation changes\n\n## Status\n\nAccepted, Implemented\n\n## Context\n\nThe specification for IBC cross-chain fungible token transfers\n([ICS20](https://github.com/cosmos/ibc/tree/master/spec/app/ics-020-fungible-token-transfer)), needs to\nbe aware of the origin of any token denomination in order to relay a `Packet` which contains the sender\nand recipient addresses in the\n[`FungibleTokenPacketData`](https://github.com/cosmos/ibc/tree/master/spec/app/ics-020-fungible-token-transfer#data-structures).\n\nThe Packet relay sending works based in 2 cases (per\n[specification](https://github.com/cosmos/ibc/tree/master/spec/app/ics-020-fungible-token-transfer#packet-relay) and [Colin Axnér](https://github.com/colin-axner)'s description):\n\n1. Sender chain is acting as the source zone. The coins are transferred\nto an escrow address (i.e locked) on the sender chain and then transferred\nto the receiving chain through IBC TAO logic. It is expected that the\nreceiving chain will mint vouchers to the receiving address.\n\n2. Sender chain is acting as the sink zone. The coins (vouchers) are burned\non the sender chain and then transferred to the receiving chain through IBC\nTAO logic. It is expected that the receiving chain, which had previously\nsent the original denomination, will unescrow the fungible token and send\nit to the receiving address.\n\nAnother way of thinking of source and sink zones is through the token's\ntimeline. Each send to any chain other than the one it was previously\nreceived from is a movement forwards in the token's timeline. This causes\ntrace to be added to the token's history and the destination port and\ndestination channel to be prefixed to the denomination. In these instances\nthe sender chain is acting as the source zone. When the token is sent back\nto the chain it previously received from, the prefix is removed. This is\na backwards movement in the token's timeline and the sender chain\nis acting as the sink zone.\n\n### Example\n\nAssume the following channel connections exist and that all channels use the port ID `transfer`:\n\n- chain `A` has channels with chain `B` and chain `C` with the IDs `channelToB` and `channelToC`, respectively\n- chain `B` has channels with chain `A` and chain `C` with the IDs `channelToA` and `channelToC`, respectively\n- chain `C` has channels with chain `A` and chain `B` with the IDs `channelToA` and `channelToB`, respectively\n\nThese steps of transfer between chains occur in the following order: `A -> B -> C -> A -> C`. In particular:\n\n1. `A -> B`: sender chain is source zone. `A` sends packet with `denom` (escrowed on `A`), `B` receives `denom` and mints and sends voucher `transfer/channelToA/denom` to recipient.\n2. `B -> C`: sender chain is source zone. `B` sends packet with `transfer/channelToA/denom` (escrowed on `B`), `C` receives `transfer/channelToA/denom` and mints and sends voucher `transfer/channelToB/transfer/channelToA/denom` to recipient.\n3. `C -> A`: sender chain is source zone. `C` sends packet with `transfer/channelToB/transfer/channelToA/denom` (escrowed on `C`), `A` receives `transfer/channelToB/transfer/channelToA/denom` and mints and sends voucher `transfer/channelToC/transfer/channelToB/transfer/channelToA/denom` to recipient.\n4. `A -> C`: sender chain is sink zone. `A` sends packet with `transfer/channelToC/transfer/channelToB/transfer/channelToA/denom` (burned on `A`), `C` receives `transfer/channelToC/transfer/channelToB/transfer/channelToA/denom`, and unescrows and sends `transfer/channelToB/transfer/channelToA/denom` to recipient.\n\nThe token has a final denomination on chain `C` of `transfer/channelToB/transfer/channelToA/denom`, where `transfer/channelToB/transfer/channelToA` is the trace information.\n\nIn this context, upon a receive of a cross-chain fungible token transfer, if the sender chain is the source of the token, the protocol prefixes the denomination with the port and channel identifiers in the following format:\n\n```typescript\nprefix + denom = {destPortN}/{destChannelN}/.../{destPort0}/{destChannel0}/denom\n```\n\nExample: transferring `100 uatom` from port `HubPort` and channel `HubChannel` on the Hub to\nEthermint's port `EthermintPort` and channel `EthermintChannel` results in `100\nEthermintPort/EthermintChannel/uatom`, where `EthermintPort/EthermintChannel/uatom` is the new\ndenomination on the receiving chain.\n\nIn the case those tokens are transferred back to the Hub (i.e the **source** chain), the prefix is\ntrimmed and the token denomination updated to the original one.\n\n### Problem\n\nThe problem of adding additional information to the coin denomination is twofold:\n\n1. The ever increasing length if tokens are transferred to zones other than the source:\n\nIf a token is transferred `n` times via IBC to a sink chain, the token denom will contain `n` pairs\nof prefixes, as shown on the format example above. This poses a problem because, while port and\nchannel identifiers have a maximum length of 64 each, the SDK `Coin` type only accepts denoms up to\n64 characters. Thus, a single cross-chain token, which again, is composed by the port and channels\nidentifiers plus the base denomination, can exceed the length validation for the SDK `Coins`.\n\nThis can result in undesired behaviours such as tokens not being able to be transferred to multiple\nsink chains if the denomination exceeds the length or unexpected `panics` due to denomination\nvalidation failing on the receiving chain.\n\n2. The existence of special characters and uppercase letters on the denomination:\n\nIn the SDK every time a `Coin` is initialized through the constructor function `NewCoin`, a validation\nof a coin's denom is performed according to a\n[Regex](https://github.com/cosmos/cosmos-sdk/blob/a940214a4923a3bf9a9161cd14bd3072299cd0c9/types/coin.go#L583),\nwhere only lowercase alphanumeric characters are accepted. While this is desirable for native denominations\nto keep a clean UX, it presents a challenge for IBC as ports and channels might be randomly\ngenerated with special and uppercase characters as per the [ICS 024 - Host\nRequirements](https://github.com/cosmos/ibc/tree/master/spec/core/ics-024-host-requirements#paths-identifiers-separators)\nspecification.\n\n## Decision\n\nThe issues outlined above, are applicable only to SDK-based chains, and thus the proposed solution\nare do not require specification changes that would result in modification to other implementations\nof the ICS20 spec.\n\nInstead of adding the identifiers on the coin denomination directly, the proposed solution hashes\nthe denomination prefix in order to get a consistent length for all the cross-chain fungible tokens.\n\nThis will be used for internal storage only, and when transferred via IBC to a different chain, the\ndenomination specified on the packed data will be the full prefix path of the identifiers needed to\ntrace the token back to the originating chain, as specified on ICS20.\n\nThe new proposed format will be the following:\n\n```go\nibcDenom = \"ibc/\" + hash(trace path + \"/\" + base denom)\n```\n\nThe hash function will be a SHA256 hash of the fields of the `DenomTrace`:\n\n```protobuf\n// DenomTrace contains the base denomination for ICS20 fungible tokens and the source tracing\n// information\nmessage DenomTrace {\n  // chain of port/channel identifiers used for tracing the source of the fungible token\n  string path = 1;\n  // base denomination of the relayed fungible token\n  string base_denom = 2;\n}\n```\n\nThe `IBCDenom` function constructs the `Coin` denomination used when creating the ICS20 fungible token packet data:\n\n```go\n// Hash returns the hex bytes of the SHA256 hash of the DenomTrace fields using the following formula:\n//\n// hash = sha256(tracePath + \"/\" + baseDenom)\nfunc (dt DenomTrace) Hash() tmbytes.HexBytes {\n  return tmhash.Sum(dt.Path + \"/\" + dt.BaseDenom)\n}\n\n// IBCDenom a coin denomination for an ICS20 fungible token in the format 'ibc/{hash(tracePath + baseDenom)}'. \n// If the trace is empty, it will return the base denomination.\nfunc (dt DenomTrace) IBCDenom() string {\n  if dt.Path != \"\" {\n  return fmt.Sprintf(\"ibc/%s\", dt.Hash())\n  }\n  return dt.BaseDenom\n}\n```\n\n### `x/ibc-transfer` Changes\n\nIn order to retrieve the trace information from an IBC denomination, a lookup table needs to be\nadded to the `ibc-transfer` module. These values need to also be persisted between upgrades, meaning\nthat a new `[]DenomTrace` `GenesisState` field state needs to be added to the module:\n\n```go\n// GetDenomTrace retrieves the full identifiers trace and base denomination from the store.\nfunc (k Keeper) GetDenomTrace(ctx Context, denomTraceHash []byte) (DenomTrace, bool) {\n  store := ctx.KVStore(k.storeKey)\n  bz := store.Get(types.KeyDenomTrace(traceHash))\n  if bz == nil {\n  return &DenomTrace, false\n  }\n\n  var denomTrace DenomTrace\n  k.cdc.MustUnmarshalBinaryBare(bz, &denomTrace)\n  return denomTrace, true\n}\n\n// HasDenomTrace checks if a the key with the given trace hash exists on the store.\nfunc (k Keeper) HasDenomTrace(ctx Context, denomTraceHash []byte)  bool {\n  store := ctx.KVStore(k.storeKey)\n  return store.Has(types.KeyTrace(denomTraceHash))\n}\n\n// SetDenomTrace sets a new {trace hash -> trace} pair to the store.\nfunc (k Keeper) SetDenomTrace(ctx Context, denomTrace DenomTrace) {\n  store := ctx.KVStore(k.storeKey)\n  bz := k.cdc.MustMarshalBinaryBare(&denomTrace)\n  store.Set(types.KeyTrace(denomTrace.Hash()), bz)\n}\n```\n\nThe `MsgTransfer` will validate that the `Coin` denomination from the `Token` field contains a valid\nhash, if the trace info is provided, or that the base denominations matches:\n\n```go\nfunc (msg MsgTransfer) ValidateBasic() error {\n  // ...\n  return ValidateIBCDenom(msg.Token.Denom)\n}\n```\n\n```go\n// ValidateIBCDenom validates that the given denomination is either:\n//\n//  - A valid base denomination (eg: 'uatom')\n//  - A valid fungible token representation (i.e 'ibc/{hash}') per ADR 001 https://github.com/cosmos/ibc-go/blob/main/docs/architecture/adr-001-coin-source-tracing.md\nfunc ValidateIBCDenom(denom string) error {\n  denomSplit := strings.SplitN(denom, \"/\", 2)\n\n  switch {\n  case strings.TrimSpace(denom) == \"\",\n  len(denomSplit) == 1 && denomSplit[0] == \"ibc\",\n  len(denomSplit) == 2 && (denomSplit[0] != \"ibc\" || strings.TrimSpace(denomSplit[1]) == \"\"):\n  return sdkerrors.Wrapf(ErrInvalidDenomForTransfer, \"denomination should be prefixed with the format 'ibc/{hash(trace + \\\"/\\\" + %s)}'\", denom)\n\n  case denomSplit[0] == denom && strings.TrimSpace(denom) != \"\":\n  return sdk.ValidateDenom(denom)\n  }\n\n  if _, err := ParseHexHash(denomSplit[1]); err != nil {\n  return Wrapf(err, \"invalid denom trace hash %s\", denomSplit[1])\n  }\n\n  return nil\n}\n```\n\nThe denomination trace info only needs to be updated when token is received:\n\n- Receiver is **source** chain: The receiver created the token and must have the trace lookup already stored (if necessary *ie* native token case wouldn't need a lookup).\n- Receiver is **not source** chain: Store the received info. For example, during step 1, when chain `B` receives `transfer/channelToA/denom`.\n\n```go\n// SendTransfer\n// ...\n\n  fullDenomPath := token.Denom\n\n// deconstruct the token denomination into the denomination trace info\n// to determine if the sender is the source chain\nif strings.HasPrefix(token.Denom, \"ibc/\") {\n  fullDenomPath, err = k.DenomPathFromHash(ctx, token.Denom)\n  if err != nil {\n  return err\n  }\n}\n\nif types.SenderChainIsSource(sourcePort, sourceChannel, fullDenomPath) {\n//...\n```\n\n```go\n// DenomPathFromHash returns the full denomination path prefix from an ibc denom with a hash\n// component.\nfunc (k Keeper) DenomPathFromHash(ctx sdk.Context, denom string) (string, error) {\n  hexHash := denom[4:]\n  hash, err := ParseHexHash(hexHash)\n  if err != nil {\n  return \"\", Wrap(ErrInvalidDenomForTransfer, err.Error())\n  }\n\n  denomTrace, found := k.GetDenomTrace(ctx, hash)\n  if !found {\n  return \"\", Wrap(ErrTraceNotFound, hexHash)\n  }\n\n  fullDenomPath := denomTrace.GetFullDenomPath()\n  return fullDenomPath, nil\n}\n```\n\n```go\n// OnRecvPacket\n// ...\n\n// This is the prefix that would have been prefixed to the denomination\n// on sender chain IF and only if the token originally came from the\n// receiving chain.\n//\n// NOTE: We use SourcePort and SourceChannel here, because the counterparty\n// chain would have prefixed with DestPort and DestChannel when originally\n// receiving this coin as seen in the \"sender chain is the source\" condition.\nif ReceiverChainIsSource(packet.GetSourcePort(), packet.GetSourceChannel(), data.Denom) {\n  // sender chain is not the source, unescrow tokens\n\n  // remove prefix added by sender chain\n  voucherPrefix := types.GetDenomPrefix(packet.GetSourcePort(), packet.GetSourceChannel())\n  unprefixedDenom := data.Denom[len(voucherPrefix):]\n  token := sdk.NewCoin(unprefixedDenom, sdk.NewIntFromUint64(data.Amount))\n\n  // unescrow tokens\n  escrowAddress := types.GetEscrowAddress(packet.GetDestPort(), packet.GetDestChannel())\n  return k.bankKeeper.SendCoins(ctx, escrowAddress, receiver, sdk.NewCoins(token))\n}\n\n// sender chain is the source, mint vouchers\n\n// since SendPacket did not prefix the denomination, we must prefix denomination here\nsourcePrefix := types.GetDenomPrefix(packet.GetDestPort(), packet.GetDestChannel())\n// NOTE: sourcePrefix contains the trailing \"/\"\nprefixedDenom := sourcePrefix + data.Denom\n\n// construct the denomination trace from the full raw denomination\ndenomTrace := types.ParseDenomTrace(prefixedDenom)\n\n// set the value to the lookup table if not stored already\ntraceHash := denomTrace.Hash()\nif !k.HasDenomTrace(ctx, traceHash) {\n  k.SetDenomTrace(ctx, traceHash, denomTrace)\n}\n\nvoucherDenom := denomTrace.IBCDenom()\nvoucher := sdk.NewCoin(voucherDenom, sdk.NewIntFromUint64(data.Amount))\n\n// mint new tokens if the source of the transfer is the same chain\nif err := k.bankKeeper.MintCoins(\n  ctx, types.ModuleName, sdk.NewCoins(voucher),\n); err != nil {\n  return err\n}\n\n// send to receiver\nreturn k.bankKeeper.SendCoinsFromModuleToAccount(\n  ctx, types.ModuleName, receiver, sdk.NewCoins(voucher),\n)\n```\n\n```go\nfunc NewDenomTraceFromRawDenom(denom string) DenomTrace{\n  denomSplit := strings.Split(denom, \"/\")\n  trace := \"\"\n  if len(denomSplit) > 1 {\n  trace = strings.Join(denomSplit[:len(denomSplit)-1], \"/\")\n  }\n  return DenomTrace{\n  BaseDenom: denomSplit[len(denomSplit)-1],\n  Trace:   trace,\n  }\n}\n```\n\nOne final remark is that the `FungibleTokenPacketData` will remain the same, i.e with the prefixed full denomination, since the receiving chain may not be an SDK-based chain.\n\n### Coin Changes\n\nThe coin denomination validation will need to be updated to reflect these changes. In particular, the denomination validation\nfunction will now:\n\n- Accept slash separators (`\"/\"`) and uppercase characters (due to the `HexBytes` format)\n- Bump the maximum character length to 128, as the hex representation used by Tendermint's\n  `HexBytes` type contains 64 characters.\n\nAdditional validation logic, such as verifying the length of the hash, the  may be added to the bank module in the future if the [custom base denomination validation](https://github.com/cosmos/cosmos-sdk/pull/6755) is integrated into the SDK.\n\n### Positive\n\n- Clearer separation of the source tracing behaviour of the token (transfer prefix) from the original\n  `Coin` denomination\n- Consistent validation of `Coin` fields (i.e no special characters, fixed max length)\n- Cleaner `Coin` and standard denominations for IBC\n- No additional fields to SDK `Coin`\n\n### Negative\n\n- Store each set of tracing denomination identifiers on the `ibc-transfer` module store\n- Clients will have to fetch the base denomination every time they receive a new relayed fungible token over IBC. This can be mitigated using a map/cache for already seen hashes on the client side. Other forms of mitigation, would be opening a websocket connection subscribe to incoming events.\n\n### Neutral\n\n- Slight difference with the ICS20 spec\n- Additional validation logic for IBC coins on the `ibc-transfer` module\n- Additional genesis fields\n- Slightly increases the gas usage on cross-chain transfers due to access to the store. This should\n  be inter-block cached if transfers are frequent.\n\n## References\n\n- [ICS 20 - Fungible token transfer](https://github.com/cosmos/ibc/tree/master/spec/app/ics-020-fungible-token-transfer)\n- [Custom Coin Denomination validation](https://github.com/cosmos/cosmos-sdk/pull/6755)\n"}
{"repositoryUrl": "https://github.com/nhsuk/sexual-health-service-finder.git", "path": "doc/adr/0005-calculate-distance-between-origin-and-result-items-within-the-application.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-12-05T08:13:41Z", "lastCommit": "2018-12-05T08:13:41Z", "numberOfCommits": 1, "title": "5. Calculate distance between origin and result items within the application", "wordCount": 192, "authors": {"name1": 1}, "content": "# 5. Calculate distance between origin and result items within the application\n\nDate: 2018-12-05\n\n## Status\n\nAccepted\n\n## Context\n\nThe move to Azure search has introduced the need to calculate the distance\nbetween the search point and each result item. Previously, when using\nElasticsearch, the distance was returned within the query response. Azure\nsearch does not have this capability, it is currently a\n[feature request](https://feedback.azure.com/forums/263029-azure-search/suggestions/17760211-support-geo-distance-in-select-result).\n\n## Decision\n\nThe decision is to calculate the distance between the search point and each\nresult item within the consuming application i.e. the web app. The calculation\nfor\n[great-circle distance](https://en.wikipedia.org/wiki/Great-circle_distance)\nis well known and available in numerous languages.\n\n## Consequences\n\nOne of the consequences is the web app will contain additional code to perform\nthe calculation. It is unlikely the code will need to be changed therefore the\nadditional overhead will be minimal.\nAnother consequence is that the web app will take some additional time in order\nto calculate the distance and serve requests. No specific timings have been\nrecorded to compare the duration of the web app performing the calculation\nrather than the search API. The timing is small regardless of where it occurs\nand no appreciable difference to the overall request/response duration is\nexpected.\n"}
{"repositoryUrl": "https://github.com/dennisseidel/saas-plaform-tenant-identity-provider.git", "path": "adr/0001-record-architecture-decisions.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-06-18T06:50:50Z", "lastCommit": "2019-06-18T06:50:50Z", "numberOfCommits": 1, "title": "1. Record architecture decisions", "wordCount": 45, "authors": {"name1": 1}, "content": "# 1. Record architecture decisions\n\nDate: 2019-02-05\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to record the architectural decisions made on this project.\n\n## Decision\n\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n\n## Consequences\n\nSee Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's [adr-tools](https://github.com/npryce/adr-tools).\n"}
{"repositoryUrl": "https://github.com/DFE-Digital/buy-for-your-school.git", "path": "doc/architecture/decisions/0002-use-pull-request-templates.md", "template": "Nygard", "status": "![Accepted](https://img.shields.io/badge/adr-accepted-green)", "firstCommit": "2021-06-23T10:15:01Z", "lastCommit": "2021-07-12T07:15:13Z", "numberOfCommits": 3, "title": "2. Use Pull Request Templates", "wordCount": 327, "authors": {"name1": 1, "name2": 2}, "content": "# 2. Use Pull Request Templates\n\nDate: 2019-08-16\n\n## Status\n\n![Accepted](https://img.shields.io/badge/adr-accepted-green)\n\n## Context\n\nThe quality of information included in our pull requests varies greatly which can lead to code reviews which take longer and are harder for the person to understand the considerations, outcomes and consequences of a series of changes.\n\nA couple of recent projects have found a GitHub pull request template to have been a positive change. Prompting what pull request descriptions should include has lead to better documented changes that have been easier to review on the whole. \n\n## Decision\n\nInclude a basic pull request template for GitHub so that every pull request prompts every author to fill it out.\n\n## Consequences\n\n- a small overhead to every pull request which may prompt us to spend more time in creating a pull request. We accept the cost of this to gain the benefits of easier reviews, and better documented changes\n- writing good pull request descriptions is not to be done instead of commit messages, they are the primary source of storing rationale that will persist. Commit messages may be duplicated into the pull request description.\n- prompting authors to articulate their thought process and decision making can improve the quality of code before a reviewer becomes involved. Essentially you are following the process of [rubber ducking](https://en.wikipedia.org/wiki/Rubber_duck_debugging) which gives you the final opportunity to amend your proposal\n- this is not compatible when source code is hosted on other services eg. [GitLab requires a file of a different name](https://docs.gitlab.com/ee/user/project/description_templates.html#creating-merge-request-templates), as all of our Rails projects are on GitHub this should be a minor issue\n- asking for screenshots doesn't always make sense especially when production deployments are facilitated by pull requests from develop into master. We accept that in these cases the cost of editing the template is small enough to not be a problem. The first header that prompts to describe changes does still apply\n- not all pull requests result in a frontend change that requires screenshots, we believe that these prompts can be removed quickly with a minor cost to the author\n"}
{"repositoryUrl": "https://github.com/openregister/openregister-java.git", "path": "doc/arch/adr-011-root-path-handling.md", "template": "unknown", "status": null, "firstCommit": "2018-11-15T11:05:00Z", "lastCommit": "2018-11-16T11:05:54Z", "numberOfCommits": 3, "title": "Decision record: Root path handling", "wordCount": 138, "authors": {"name1": 3}, "content": "# Decision record: Root path handling\n\n## Context\n\nAs part of the work to release the next major version, there are three routes\nthat need special handling: `/`, `/v1` and `/v2`.\n\nThis ADR proposes a behaviour for each one of them.\n\n## Decision\n\n### Root (`/`)\n\nWhen HTML is requested, it should return the exact same HTML as per now.\n\nOtherwise it should redirect (301) to the Register resource\n(`/{version}/register`) for the requested version in JSON regardless of the\nformat specified.\n\n### Version root `/v1`\n\nWhen HTML is requested, it should redirect (301) to `/` (See above).\n\nOtherwise it should redirect (301) to the Register resource\n(`/{version}/register`) for the requested version in JSON regardless of the\nformat specified.\n\n### Version root `/v2`\n\nIt should redirect (301) to the Register resource\n(`/{version}/register`) for the requested version in JSON regardless of the\nformat specified.\n\n## Status\n\nAccepted.\n"}
{"repositoryUrl": "https://github.com/edx/event-routing-backends.git", "path": "docs/decisions/0009-persistence-and-retries-for-events.rst", "template": "Nygard", "status": "Approved", "firstCommit": "2021-07-14T07:02:20Z", "lastCommit": "2021-07-29T07:06:16Z", "numberOfCommits": 9, "title": "(sem título)", "wordCount": 339, "authors": {"name1": 9}, "content": "Persistence and retries for events\n==\n\nStatus\n--\n\nApproved\n\nContext\n---\n\n`event-routing-backends` transmits events to configured recipients (Learning Record Stores) via http protocol in near real-time. A strategy needs to be implemented to handle the case when an LRS's link is down.\n\nDecision\n--\n\n1. A celery task will be created for each transformation (xAPI or Caliper) of an event. Once the transformation is complete, nested celery tasks will be created, one for each recipient, to route the event.\n\n2. Retry attempts shall be made for each recipient, for all events types, and for a configured number of retries and delay between each retry.\n\n3. A limited type of events (namely *business critical events*) shall be persisted even after all retry attempts have been exhausted. Celery tasks, that failed to route the transformed event to their intended recipients, will be stored in a database. Each of these tasks (persisted via `celery-utils`) will include just enough information about the event that it gets resent appropriately after persistence. Events that consumers of LRS may use for record keeping such as course enrollment and completion events, shall be classified as *business critical events*.\n\n4. A scheduled process will retry transmitting all persisted events in the database to respective recipient(s) at a configured frequency (e.g. once a day). This process will also check if the number of persisted events is higher than a configured threshold. If so, it will generate an alert for the admin.\n\n5. An interface shall be provided for admin to view the list of recipient(s) whose events are persisting in the database. The admin may choose to contact the recipient(s) to try and resolve the communication issue.\n\nConsequences\n--\n\n1. All but *business critical events*, will be lost after the time and number of retry attempts in decision # 2 expire.\n\n2. Decision # 1 is necessary to enable decision # 3 but will also increase the number of celery workers in use.\n\n3. The admin will need to respond to alert discussed in decision # 4 to avoid unnecessary utilisation of storage space.\n"}
{"repositoryUrl": "https://github.com/alphagov/verify-service-provider.git", "path": "docs/adr/0025-we-will-only-release-one-configuration-file.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-11-06T15:23:15Z", "lastCommit": "2018-03-23T11:50:24Z", "numberOfCommits": 2, "title": "25. We will only release one configuration file", "wordCount": 370, "authors": {"name1": 1, "name2": 1}, "content": "# 25. We will only release one configuration file\n\nDate: 2017-11-06\n\n## Status\n\nAccepted\n\n## Context\n\nHistorically we have had \"two ways\" of configuring Verify Service Provider:\n\n- Using environment variables\n- Using a YAML file\n\nWhen using environment variables the application used the verify-service-provider-env.yml\nfile from the resources directory (so inside the jar). When using the YAML file you would\npass the path to a different file as a command line parameter - usually people\nwould use the example one that's contained in the repo.\n\nThere were a couple of reasons for the extra complexity of managing two files, both due to\nrestrictions with the java buildpack used by cloudfoundry:\n\n- It's not possible to specify command line arguments through the java buildpack,\n  so you can't specify a path to your config file\n- We weren't confident in the way cloudfoundry manages static files, so we didn't want\n  to rely on one.\n\nThere was also a philosophical point that 12 factor applications should be configured through\ntheir environment. This made the \"hide the configuration in the .jar and do everything through\nenv vars\" way appealing.\n\n## Decision\n\nWe will remove the verify-service-provider-env.yml file from src/main/resources\n\nThe application will default to the verify-service-provider.yml\nfile that's included in the .zip if no command line arguments are provided.\n\nIf the application is started without command line arguments specifying a yml file\nAND no environment variables have been set, startup should error gracefully and tell\nthe user that the configuration fields have not been specified for example:\n\n\"ERROR - no configuration fields found, either set environment variables or specify\na configuration file using command line arguments ```server <path/to/verify-service-provider.yml>```\"\n\nWe will establish the path to verify-service-provider.yml by asking java for the\npath to the .jar file containing the Application class and looking in the parent\nfolder.\n\n## Consequences\n\nWe will have to play a story to make the default configuration file work in a\nway that's compatible with the current environment variable based solution.\n\nGoing forward, we will only need to maintain one configuration file instead of two.\n\nUsers will not have to learn about the dichotomy between \"configure with env vars\"\nand \"configure with files\".\n\nThe application will still run on PaaS using the default java buildpack.\n\n"}
{"repositoryUrl": "https://github.com/huifenqi/arch.git", "path": "decisions/0049-moving-your-website-to-https.md", "template": "Nygard", "status": "Proposed", "firstCommit": "2018-07-03T06:07:59Z", "lastCommit": "2018-07-03T06:07:59Z", "numberOfCommits": 1, "title": "49. 全站 https", "wordCount": 214, "authors": {"name1": 1}, "content": "# 49. 全站 https\n\nDate: 2018-06-27\n\n## Status\n\nProposed\n\n## Context\n\n0. 客户网络环境不佳，存在流量劫持；使用的第三方服务要求 https；\n1. 10+ 主域名，50+ 二级域名；\n2. 190+ 项目；\n3. 10+ 数据库表；\n4. 大量自行托管及第三方资源文件。\n\n## Decision\n\n### why\n\n1. 防止流量劫持，插入广告；\n2. 防止账号密码等隐私数据被盗取；\n3. 支持 HTML5 API，如用户地理位置，音视频等隐私数据获取；\n4. 支持 HTTP/2；\n5. Apple，微信等有要求。\n\n### How\n\n1. 了解整个系统，统计使用到的域名及购买什么类型的域名证书；\n2. 相关资源支持 https，解决 Mixed Content 问题\n\t1. 针对此问题，浏览器会提示警告，Android 的 webview 直接无法打开；\n\t2. 前端页面的外链资源（CSS、JS、图片、音频、字体文件、异步接口、表单 action 地址等等）固定了协议引用(http://, https://)，需更新为(//)；\n\t3. 后端代码中协议是否为动态的；\n\t\t1. 返回接口协议应和请求协议保持一致；\n\t4. 数据库；\n\t\t1. 针对自有资源，应只保存路径信息，协议和域名建议动态补充；\n\t\t2. 针对第三方资源，默认保留原地址，跟进需要再做转换，实在不行需要做代理。\n\t5. 资源文件：确保支持 https。\n3. 支持 https；\n\t1. 移动端适配 https\n\t\t1. 针对运营商 DNS 劫持(降低 https 请求成功率)，需支持两种协议，并有动态降级方案；\n\t 2. nginx proxy + backend server\n\t\t1. 需关注 scheme 获取是否正确，注意 log 记录。\n\t3. 跳转\n\t\t1. 先 302 测试 https 全站正常，再 301 跳转；\n\t\t2. POST 请求会丢失 body 信息。\n\t4. 测试\n\t\t1. [https://www.ssllabs.com/ssltest/][1]\n4. 强制 https；\n5. 所有环境均要升级 https；\n\t1. 除生产外，需要将开发、测试、预发布均进行升级，保持环境的一致性，减少不可预估的问题发生\n6. 优化。\n\n## Consequences\n\n1. 性能(访问速度)有降低；\n2. 增加系统复杂性；\n3. 证书及资源(CDN 等)成本；\n\nRefs:\n\n1. [为什么我们应该尽快升级到 HTTPS？][2]\n2. [浅谈推进有赞全站 HTTPS 项目-工程篇][3]\n3. [启用全站HTTPS后不仅更安全而且更快 看淘宝是如何做到的][4]\n4. [更快更安全的HTTPS 优化总结][5]\n\n[1]:\thttps://www.ssllabs.com/ssltest/\n[2]:\thttps://imququ.com/post/moving-to-https-asap.html\n[3]:\thttps://juejin.im/post/5aa22db7518825558804fbac\n[4]:\thttps://mp.weixin.qq.com/s?__biz=MzAxNDEwNjk5OQ==&mid=402402866&idx=1&sn=f3fde8ece13d51397c12f1a08713ebeb\n[5]:\thttps://zhuanlan.zhihu.com/p/35233649"}
{"repositoryUrl": "https://github.com/yldio/asap-hub.git", "path": "docs/decision/03-data-storage.md", "template": "unknown", "status": null, "firstCommit": "2020-05-27T17:21:26Z", "lastCommit": "2020-05-27T17:21:26Z", "numberOfCommits": 3, "title": "Data Storage", "wordCount": 676, "authors": {"name1": 1, "name2": 2}, "content": "# Data Storage\n\nStatus: Draft\n\nDate: 2020-05-22\n\nAuthor: Filipe Pinheiro <filipe@yld.io>\n\nReviewed-by: Tim Seckinger <tim.seckinger@yld.io>\n\n## Context\n\nThe ASAP Hub has to store data about users and usage of the application. To decide how to implement our data storage, we need to take into consideration the data model and data access patterns the application needs.\n\n## Data Model\n\n### Entity Relational Diagram\n\n```\nerDiagram\n  User ||..|| Invite : has\n  User ||..o{ Auth : has\n```\n\nhttps://mermaid-js.github.io/mermaid-live-editor/\n[![](https://mermaid.ink/img/eyJjb2RlIjoiZXJEaWFncmFtXG4gIFVzZXIgfHwuLnx8IEludml0ZSA6IGhhc1xuICBVc2VyIHx8Li5veyBBdXRoIDogaGFzIiwibWVybWFpZCI6eyJ0aGVtZSI6ImRlZmF1bHQifSwidXBkYXRlRWRpdG9yIjpmYWxzZX0)](https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZXJEaWFncmFtXG4gIFVzZXIgfHwuLnx8IEludml0ZSA6IGhhc1xuICBVc2VyIHx8Li5veyBBdXRoIDogaGFzIiwibWVybWFpZCI6eyJ0aGVtZSI6ImRlZmF1bHQifSwidXBkYXRlRWRpdG9yIjpmYWxzZX0)\n\n### Potential data access patterns\n\n**Invite (one time token)**\n\n- Create a one time token related to a particular user\n- Fetch a one time token and the related user information\n\n**Auth**\n\nWe need to correlate the authentication provided by the external service to the profiles of the user.\n\n- Create a new identity and the connection to the user account\n- Fetch the user account associated with a particular identity\n\n**User**\n\nThe user entity contains the information about a user of the platform:\n\n- Create a user account and an invite used to join the platform\n- Fetch the account information based on the code on an invite\n\n### Data Storage Options\n\nA decision on what storage solution to choose should consider the entities of the system, how they relate to each other, and the data access patterns to retrieve them.\nAs you can see above, the ERD doesn't have many of the entities and relationships required later in the project, but we need to pick a first storage solution now.\n\nAn important decision is about managed or unmanaged services. A managed service is better in our context due to the size of the team and to enable focus on different areas.\n\nOur options are:\n\n- A SQL database. Our data is strongly relational. A relational database would be an excellent fit.\n- MongoDB. MongoDB is a document database with high adoption due to the flexibility of schemaless documents.\n- DynamoDB. We are using serverless in AWS for the backend, which is an attractive pairing with DynamoDB. DynamoDB is a key/value and document database.\n\nA **relational database** allows us to map the ERD without too much consideration due to the possibility to join data on different tables.\nThe tooling for the relational database is wide-spread, and the database gives us substantial flexibility in our data model.\n\n**MongoDB** is the go-to solution when considering a document database due to the simple API.\nModelling data in MongoDB isn't as straightforward as in a relational database, but you still have flexibility when querying your data and changing access patterns.\n\n**DynamoDB** is a document database hosted by AWS. DynamoDB offers a simple API tailored for data sets with known access patterns.\nTo ensure the scaling capabilities, you need to think about your data model in a more intentional way than for a MongoDB. As you can read in the Peter Parker principle \"with great power comes great responsibility\".\n\n**Considerations**\n\n- Due to the serverless nature of our application, databases that use connection pools to manage database connection aren't a good fit. A serverless architecture can exhaust the connection limit, creating zombie connections that may impact database performance.\n\n## Options\n\n### SQL\n\n**Amazon Aurora Serverless**\n\nAmazon Aurora Serverless is an on-demand database. It starts, stops, and scales according to the needs of the application.\nAmazon Aurora Serverless supports the Data API, removing the need to have a persistent connection to the cluster.\n\nhttps://aws.amazon.com/rds/aurora/pricing/\n\nAmazon Aurora is a MySQL and PostgreSQL-compatible relational database. It is priced per hour starting at \\$0.082/h (~\\$60/mo).\n\nhttps://aws.amazon.com/rds/aurora/\n\n### MongoDB\n\n**MongoDB Atlas**\n\nMongoDB Atlas is a managed service, and we can deploy the cluster on AWS, GCP, or Azure. In our case, AWS is the most suitable option so that we can leverage VPC peering to added security. It it priced per hour starting at \\$0.08/h (~\\$58/mo).\n\nhttps://www.mongodb.com/cloud/atlas/pricing/\n\n### DynamoDB\n\nDynamoDB is a pay-per-request or pay-per-provisioned-capacity. Since we don't know the capacity needed for our application, the choice would need to be its on-demand mode.\nDynamoDB is a key-value and document database performant at scale. It's fully managed, and the HTTP API fits nicely with the serverless model.\n\nhttps://aws.amazon.com/dynamodb/pricing/on-demand/\n\n## Decision\n"}
{"repositoryUrl": "https://github.com/buildit/midashboard-infrastructure.git", "path": "docs/architecture/decisions/0002-use-aws-bare-metal-rig-approach.md", "template": "Nygard", "status": "Accepted Amended by [3. Use AWS CodePipeline and CodeBuild instead of Travis](0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md)", "firstCommit": "2017-10-24T19:58:34Z", "lastCommit": "2017-10-24T19:58:34Z", "numberOfCommits": 1, "title": "2. Use AWS Bare Metal Rig approach", "wordCount": 111, "authors": {"name1": 1}, "content": "# 2. Use AWS Bare Metal Rig approach\n\nDate: 2017-09-27\n\n## Status\n\nAccepted\n\nAmended by [3. Use AWS CodePipeline and CodeBuild instead of Travis](0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md)\n\n## Context\n\nWe need to create a riglet for our new bookit project so that we practice what we preach.\n\n## Decision\n\nWe will use the AWS Bare Metal Riglet from bookit-riglet as a starting point for our riglet.  We will keep the previous bookit-riglet and create a new bookit-infrastructure project/repo.\nTechnologies:\n\n* AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB\n* Deployment Mechanism: Docker images\n* Build: Travis\n\n## Consequences\n\n* This will tie us to the AWS platform.\n* The bookit-riglet is not \"complete.\"  There a number of improvements that can be made along the way that we will have to balance with feature work.\n"}
{"repositoryUrl": "https://github.com/MadDonkeySoftware/mdsCloudDocs.git", "path": "developer/ADRs/0002 - ORID.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2021-02-03T01:47:11Z", "lastCommit": "2021-02-03T01:47:11Z", "numberOfCommits": 1, "title": "0002. ORID", "wordCount": 178, "authors": {"name1": 1}, "content": "# 0002. ORID\n\nDate: 2020-JUL-31\n\n## Status\n\nAccepted\n\n## Decision\n\nAll services shall emit and consume, where applicable, a ORID. ORID shall conform to the [specification version 1](https://github.com/MadDonkeySoftware/OridNode/blob/master/docs/v1.md) where the custom fields are as follows:\n\n| field  | mapping   | sample value |\n|--|---|--|\n| custom-1 | N/A   |  |\n| custom-2 | N/A   |  |\n| custom-3 | accountId | 123  |\n\n## Context\n\nThe issue motivating this decision, and any context that influences or constrains the decision.\n\nAs the mdsCloud ecosystem expands it can no longer be assumed that consumers of the service know internal APIs to issue commands. For example the current state machine implementation has a Task definition where the attribute \"Resource\" is a URL to invoke the function. The internal IP addresses and ports should not be known to consumers of the service as it needlessly tightly couples the system. This is compounded by operational concerns where an operator may need to \"shuffle\" or reconfigure systems during maintenance operations.\n\n## Consequences\n\nWhat becomes easier or more difficult to do and any risks introduced by the change that will need to be mitigated.\n\nMoving to an ORID supported system will allow for many of these concerns to be mitigated."}
{"repositoryUrl": "https://github.com/kgrzybek/modular-monolith-with-ddd.git", "path": "docs/architecture-decision-log/0006-create-facade-between-api-and-business-module.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-11-09T21:17:02Z", "lastCommit": "2019-11-09T21:17:02Z", "numberOfCommits": 1, "title": "6. Create façade between API and business module", "wordCount": 171, "authors": {"name1": 1}, "content": "# 6. Create façade between API and business module\n\nDate: 2019-07-01\n\nLog date: 2019-11-04\n\n## Status\n\nAccepted\n\n## Context\n\nOur API layer should communicate with business modules to fulfill client requests. To support the maximum level of autonomy, each module should expose a minimal set of operations (the module API/contract/interface).\n\n## Decision\n\nEach module will provide implementation for one interface with 3 methods:</br>\n\n```csharp\nTask<TResult> ExecuteCommandAsync<TResult>(ICommand<TResult> command);\n\nTask ExecuteCommandAsync(ICommand command);\n\nTask<TResult> ExecuteQueryAsync<TResult>(IQuery<TResult> query);\n```\n\nThis interface will act as a façade (Façade pattern) between API and module. Only Commands, Queries and returned objects (which are part of this interface) should be visible to the API. Everything else should be hidden behind the façade (module encapsulation).\n\n## Consequences\n- API can communicate with the module only by façade (the interface).\n- Implementation of API is simpler\n- We can change module implementation and API does not require change if the interface is not changed\n- We need to focus on module encapsulation, sometimes it involves additional work (like instantiation using internal constructors)"}
{"repositoryUrl": "https://github.com/alphagov/verify-onboarding-prototypes.git", "path": "prototypes/prototype-0/docs/adr/0004-users-will-be-able-to-provide-relay-state.md", "template": "Nygard", "status": "Pending (may want to change if the added complexity is high)", "firstCommit": "2017-06-01T11:21:36Z", "lastCommit": "2017-06-02T15:39:24Z", "numberOfCommits": 2, "title": "4. Users will be able to provide relay state", "wordCount": 136, "authors": {"name1": 1, "name2": 1}, "content": "# 4. Users will be able to provide relay state\n\nDate: 01/06/2017\n\n## Status\n\nPending (may want to change if the added complexity is high)\n\n## Context\n\nIn SAML RPs can provide some extra data along with the request. This is\ncalled RelayState. Some existing RPs use this, but we're not sure what\nthey use it for.\n\nWe're not aware of any need for the service-provider to use relay state itself.\n\n## Decision\n\nUsers will be able to specify whatever relay state they want to and it will be\nprovided in the response.\n\n## Consequences\n\n* The service provider won't be able to use relay state itself\n* The user will need to be able to customize one of the form inputs\n* The client-side (node JS) code will need to provide some way of getting\n  relay state from the response.\n"}
{"repositoryUrl": "https://github.com/hugolhafner/analytics-server.git", "path": "docs/adr/004-minimally-transform-source-code-during-build.md", "template": "unknown", "status": null, "firstCommit": "2020-03-12T13:22:39Z", "lastCommit": "2020-03-12T13:22:39Z", "numberOfCommits": 1, "title": "ADR-004: Minimally Transform Source Code During Build", "wordCount": 103, "authors": {"name1": 1}, "content": "# ADR-004: Minimally Transform Source Code During Build\n\n- Status: Accepted\n- People involved: @sublimesneaks\n\n## Issue\n\nThere are many ways to transform the source code during build, including:\n\n- Bundle into a single file\n- Minify\n\nWhat transformations should the build use?\n\n## Assumptions\n\nThe code is not consumed on client-side.\n\n## Constraints\n\nAs the code is written in TypeScript, there must be _some_ transformation (e.g. ESM to CJS).\n\n## Decision\n\nMinimally transform the source code during build.\n\nHas the following advantages:\n\n- Easier to debug and trace problems to a specific file (even with source maps)\n- Allows the consumer to `require` only the file(s) they need within the directory tree\n"}
{"repositoryUrl": "https://github.com/pulibrary/dpul.git", "path": "architecture-decisions/0001-document-architecture-decisions.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-01-29T20:44:02Z", "lastCommit": "2020-01-29T20:44:02Z", "numberOfCommits": 1, "title": "1. Record architecture decisions", "wordCount": 47, "authors": {"name1": 1}, "content": "# 1. Record architecture decisions\n\nDate: 2020-01-29\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to record the architectural decisions made on this project.\n\n## Decision\n\nWe will use Architecture Decision Records, as described at https://adr.github.io/\n\n## Consequences\n\n* Pull requests that significantly change Pomegranate's architecture should be accompanied by an ADR.\n"}
{"repositoryUrl": "https://github.com/apache/james-project.git", "path": "src/adr/0046-generalize-event-bus.md", "template": "Nygard", "status": "Implemented, used for JMAP notifications, however not yet used on top of the user entity as described in this document.", "firstCommit": "2021-01-22T07:35:31Z", "lastCommit": "2021-10-21T07:28:22Z", "numberOfCommits": 2, "title": "46. Generalize EventBus", "wordCount": 377, "authors": {"name1": 1, "name2": 1}, "content": "#46. Generalize EventBus\n\nDate: 2020-06-11\n\n## Status\n\nImplemented, used for JMAP notifications, however not yet used on top of the user entity as \ndescribed in this document.\n\n## Context\n\nUser email storage usage is limited both in size and count via quotas (IMAP RFC-2087). In order to ease administrating large user bases, the quota search extension allows administrator\nto retrieve all users whose email usages are exceeding a given occupation ratio.\n\nWhen searching for users by quota ratio if we set the value of the parameters to 0, for example: `/quotas/users?minOccupationRatio=0&maxOccupationRatio=0`, the search feature is supposed to return newly created users\nwho have not received any email yet at that point. However, this is not the case because the quotas are currently being initialized only after\na user has received the first email.\n\nWe need to initialize user quotas upon user creation time. The problem is: there is currently no event at user creation \nand since the quota-search feature is a plugin of James, it cannot be hardwired into the domain logic of user management to initialize the quota for a just created user.\n\n## Decision\n\nFor quota-search to be initialized/removed for a given user while keeping this feature as a plugin, we decided to adopt the Event Driven pattern we already use in Mailbox-api. \nWe can create new events related to user management (UserCreated, UserRemoved and so on).\n\nTo achieve that, we will extract the EventBus out of mailbox-api in order to make it a utility component (eventbus-api), then we will make both mailbox-api and data-api depend on that new module. \n\n## Consequences\n\nMailbox-api would leverage the EventBus to keep exposing the mailbox-listener-api without changes on top of the generified EventBus. We need to define a common Event interface in eventbus-api, \nthen each EventBus usage will define its own sealed event hierarchy implementing Event.\n\nDeadLetter storage needs to be reworked in order to store events of various EventBus separately (which is needed for knowing which EventBus the event should be reprocessed on \nand knowing which sealed hierarchy an event belongs to.)\n\nAs a consequence, we will need a Cassandra data migration to add the EventBus name as part of the EventDeadLetter primary key. \n\nWe could rely on the EventBus reliability for building any feature in James."}
{"repositoryUrl": "https://github.com/mofeixiaobao/gatemint-sdk.git", "path": "docs/architecture/adr-008-dCERT-group.md", "template": "Nygard", "status": "> Proposed", "firstCommit": "2020-09-08T03:36:28Z", "lastCommit": "2020-09-08T03:36:28Z", "numberOfCommits": 1, "title": "ADR 008: Decentralized Computer Emergency Response Team (dCERT) Group", "wordCount": 1264, "authors": {"name1": 1}, "content": "# ADR 008: Decentralized Computer Emergency Response Team (dCERT) Group\n\n## Changelog\n\n- 2019 Jul 31: Initial Draft\n\n## Context\n\nIn order to reduce the number of parties involved with handling sensitive\ninformation in an emergency scenario, we propose the creation of a\nspecialization group named The Decentralized Computer Emergency Response Team\n(dCERT).  Initially this group's role is intended to serve as coordinators\nbetween various actors within a blockchain community such as validators,\nbug-hunters, and developers.  During a time of crisis, the dCERT group would\naggregate and relay input from a variety of stakeholders to the developers who\nare actively devising a patch to the software, this way sensitive information\ndoes not need to be publicly disclosed while some input from the community can\nstill be gained. \n\nAdditionally, a special privilege is proposed for the dCERT group: the capacity\nto \"circuit-break\" (aka. temporarily disable)  a particular message path. Note\nthat this privilege should be enabled/disabled globally with a governance\nparameter such that this privilege could start disabled and later be enabled\nthrough a parameter change proposal, once a dCERT group has been established. \n\nIn the future it is foreseeable that the community may wish to expand the roles\nof dCERT with further responsibilities such as the capacity to \"pre-approve\" a\nsecurity update on behalf of the community prior to a full community\nwide vote whereby the sensitive information would be revealed prior to a\nvulnerability being patched on the live network.  \n\n## Decision\n\nThe dCERT group is proposed to include an implementation of a `SpecializationGroup`\nas defined in [ADR 007](./adr-007-specialization-groups.md). This will include the \nimplementation of: \n - continuous voting\n - slashing due to breach of soft contract\n - revoking a member due to breach of soft contract\n - emergency disband of the entire dCERT group (ex. for colluding maliciously) \n - compensation stipend from the community pool or other means decided by\n   governance\n\nThis system necessitates the following new parameters: \n - blockly stipend allowance per dCERT member \n - maximum number of dCERT members \n - required staked slashable tokens for each dCERT member \n - quorum for suspending a particular member \n - proposal wager for disbanding the dCERT group \n - stabilization period for dCERT member transition\n - circuit break dCERT privileges enabled \n\nThese parameters are expected to be implemented through the param keeper such \nthat governance may change them at any given point. \n\n### Continuous Voting Electionator\n\nAn `Electionator` object is to be implemented as continuous voting and with the\nfollowing specifications:\n - All delegation addresses may submit votes at any point which updates their \n   preferred representation on the dCERT group. \n - Preferred representation may be arbitrarily split between addresses (ex. 50%\n   to John, 25% to Sally, 25% to Carol) \n - In order for a new member to be added to the dCERT group they must \n   send a transaction accepting their admission at which point the validity of\n   their admission is to be confirmed. \n   - A sequence number is assigned when a member is added to dCERT group. \n   If a member leaves the dCERT group and then enters back, a new sequence number\n   is assigned.  \n - Addresses which control the greatest amount of preferred-representation are\n   eligible to join the dCERT group (up the _maximum number of dCERT members_). \n   If the dCERT group is already full and new member is admitted, the existing\n   dCERT member with the lowest amount of votes is kicked from the dCERT group.\n   - In the split situation where the dCERT group is full but a vying candidate \n   has the same amount of vote as an existing dCERT member, the existing \n   member should maintain its position. \n   - In the split situation where somebody must be kicked out but the two\n   addresses with the smallest number of votes have the same number of votes,\n   the address with the smallest sequence number maintains its position.  \n - A stabilization period can be optionally included to reduce the\n   \"flip-flopping\" of the dCERT membership tail members. If a stabilization\n   period is provided which is greater than 0, when members are kicked due to\n   insufficient support, a queue entry is created which documents which member is\n   to replace which other member. While this entry is in the queue, no new entries\n   to kick that same dCERT member can be made. When the entry matures at the\n   duration of the  stabilization period, the new member is instantiated, and old\n   member kicked.\n\n### Staking/Slashing\n\nAll members of the dCERT group must stake tokens _specifically_ to maintain\neligibility as a dCERT member. These tokens can be staked directly by the vying\ndCERT member or out of the good will of a 3rd party (who shall gain no on-chain\nbenefits for doing so). This staking mechanism should use the existing global\nunbonding time of tokens staked for network validator security. A dCERT member\ncan _only be_ a member if it has the required tokens staked under this\nmechanism. If those tokens are unbonded then the dCERT member must be\nautomatically kicked from the group.  \n\nSlashing of a particular dCERT member due to soft-contract breach should be\nperformed by governance on a per member basis based on the magnitude of the\nbreach.  The process flow is anticipated to be that a dCERT member is suspended\nby the dCERT group prior to being slashed by governance.  \n\nMembership suspension by the dCERT group takes place through a voting procedure\nby the dCERT group members. After this suspension has taken place, a governance\nproposal to slash the dCERT member must be submitted, if the proposal is not\napproved by the time the rescinding member has completed unbonding their\ntokens, then the tokens are no longer staked and unable to be slashed. \n\nAdditionally in the case of an emergency situation of a colluding and malicious\ndCERT group, the community needs the capability to disband the entire dCERT\ngroup and likely fully slash them. This could be achieved though a special new\nproposal type (implemented as a general governance proposal) which would halt\nthe functionality of the dCERT group until the proposal was concluded. This\nspecial proposal type would likely need to also have a fairly large wager which\ncould be slashed if the proposal creator was malicious. The reason a large\nwager should be required is because as soon as the proposal is made, the\ncapability of the dCERT group to halt message routes is put on temporarily\nsuspended, meaning that a malicious actor who created such a proposal could\nthen potentially exploit a bug during this period of time, with no dCERT group\ncapable of shutting down the exploitable message routes. \n\n### dCERT membership transactions\n\nActive dCERT members \n - change of the description of the dCERT group\n - circuit break a message route\n - vote to suspend a dCERT member. \n\nHere circuit-breaking refers to the capability to disable a groups of messages,\nThis could for instance mean: \"disable all staking-delegation messages\", or\n\"disable all distribution messages\". This could be accomplished by verifying\nthat the message route has not been \"circuit-broken\" at CheckTx time (in\n`baseapp/baseapp.go`). \n\n\"unbreaking\" a circuit is anticipated only to occur during a hard fork upgrade\nmeaning that no capability to unbreak a message route on a live chain is\nrequired. \n\nNote also, that if there was a problem with governance voting (for instance a\ncapability to vote many times) then governance would be broken and should be\nhalted with this mechanism, it would be then up to the validator set to\ncoordinate and hard-fork upgrade to a patched version of the software where\ngovernance is re-enabled (and fixed). If the dCERT group abuses this privilege\nthey should all be severely slashed.\n\n## Status\n\n> Proposed\n\n## Consequences\n\n### Positive\n\n - Potential to reduces the number of parties to coordinate with during an emergency \n - Reduction in possibility of disclosing sensitive information to malicious parties\n\n### Negative\n\n - Centralization risks\n\n### Neutral\n\n## References\n \n  (Specialization Groups ADR)[./adr-007-specialization-groups.md]\n"}
{"repositoryUrl": "https://github.com/operate-first/blueprint.git", "path": "adr/0007-alerting-setup.md", "template": "Madr", "status": "accepted", "firstCommit": "2021-01-19T16:01:01Z", "lastCommit": "2021-11-04T16:42:06Z", "numberOfCommits": 3, "title": "Alerting Setup for Operate First Monitoring", "wordCount": 595, "authors": {"name1": 1, "name2": 1, "name3": 1}, "content": "# Alerting Setup for Operate First Monitoring\n\n* Status: accepted\n* Deciders: hemajv, 4n4nd, anishasthana, HumairAK, tumido, mhild\n\nTechnical Story: [issue-1](https://github.com/operate-first/SRE/issues/14), [issue-2](https://github.com/operate-first/SRE/issues/19), [issue-3](https://github.com/operate-first/blueprint/issues/16)\n\n## Context and Problem Statement\n\nAs we have multiple services/applications deployed and monitored in the Operate First environment (ex. Jupyterhub, Argo, Superset, Observatorium, Project Thoth, AICoE CI pipelines etc), we need to implement an incident reporting setup for handling outages/incidents related to these services.\n\nAll the services are being monitored by [Prometheus](https://prometheus.io/). Prometheus scrapes and stores time series data identified by metric key/value pairs for each of the available services. These metrics can be used for measuring the service performance and alerting on any possible service degradation such as basic availability, latency, durability and any other applicable SLI/SLOs. These SLI/SLOs for the various services are defined and documented in the [SRE repository](https://github.com/operate-first/SRE/tree/master/sli-slo).\n\nAlerting with Prometheus is separated into two parts. Alerting rules in Prometheus servers send alerts to an [Alertmanager](https://prometheus.io/docs/alerting/latest/alertmanager/). The Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email, on-call notification systems, and chat platforms.\n\nWhether its a major bug, capacity issues, or an outage, users depending on the services expect an immediate response. Having an efficient incident management process is critical in ensuring that the incidents are always communicated to the users (via on-call notification systems) and handled by the team immediately. An on-call notification system is a system/software that provides an automated means of contacting users and communicating pertinent information during an incident. It also has additional on-call scheduling features that can be used to ensure that the right people on the team are available to address a problem during an incident. There are multiple on-call notification systems such as [PagerDuty](https://www.pagerduty.com/), [JIRA](https://www.atlassian.com/software/jira) etc that can be used for incident reporting, but which of these tools are best suitable for reporting the outages/incidents to our users?\n\n## Decision Drivers <!-- optional -->\n\n* Visibility of incident reporting\n  * How can the incidents be tracked and reported in an open and transparent manner for our users?\n* Compatibility with Prometheus\n  * Is the incident reporting tool compatible with Prometheus i.e. can it handle/receive Prometheus alerts?\n* Complexity and cost of incident reporting tool\n  * How easy/hard is it to manage and operate the incident reporting tool?\n  * Is it a free or paid tool?\n  * Is it open source?\n\n## Considered Options\n\n* Option 1:\n  * Use [PagerDuty](https://www.pagerduty.com/) which is a popular paid on-call management and incident response platform\n* Option 2:\n  * Use open source tools like:\n  * [Cabot](https://github.com/arachnys/cabot) - Python/Djano based monitoring platform\n  * [OpenDuty](https://github.com/openduty/openduty) - Incident escalation tool similar to PagerDuty\n  * [Dispatch](https://github.com/Netflix/dispatch) - Incident management tool by Netflix\n  * [Response](https://github.com/monzo/response) - Django based incident management tool\n\n  which are free, self-hosted infrastructure that provides some of the best features of PagerDuty, Pingdom etc without their cost and complexity\n* Option 3:\n  * Use [GitHub Alertmanager receiver](https://github.com/m-lab/alertmanager-github-receiver) which is a Prometheus Alertmanager webhook receiver that creates GitHub issues from alerts\n\n## Decision Outcome\nChosen option: **Option 3**, because:\n  * The [GitHub alertmanager receiver](https://github.com/m-lab/alertmanager-github-receiver) can easily be configured and operated to function with Prometheus alerts. It automatically creates issues in GitHub repositories for any active alerts being fired, making it visible for any user to track\n  * All communication/updates/concerns related to the incident can be easily handled by adding comments in the issues created by the GitHub receiver\n  * Unlike Option 1, there is no additional cost involved\n  * There is no requirement for using JIRA/Slack for incident tracking, which are the only supported options in some of the tools listed in Option 2 (such as [Dispatch](https://github.com/Netflix/dispatch) and [Response](https://github.com/monzo/response)) In any case that such a requirement surfaces, we can use GitHub bots for different platforms such as [GitHub for Slack](https://slack.github.com/) and [Google Chat](https://support.google.com/chat/answer/9632291?co=GENIE.Platform%3DAndroid&hl=en) to notify us of the issues immediately\n  * It is actively being maintained and supported compared to some of the tools in Option 1 (such as [Cabot](https://github.com/arachnys/cabot) and [OpenDuty](https://github.com/openduty/openduty)) which lack community support\n"}
{"repositoryUrl": "https://github.com/CorneliuPopescu/insight.git", "path": "_docs/adr/0002-tweepy.md", "template": "Nygard", "status": "Proposed", "firstCommit": "2018-12-13T00:59:56Z", "lastCommit": "2018-12-13T00:59:56Z", "numberOfCommits": 1, "title": "2. Tweepy", "wordCount": 69, "authors": {"name1": 1}, "content": "# 2. Tweepy\n\nDate: 2018-12-12\n\n## Status\n\nProposed\n\n## Subject Matter Expert (SME)\n\nCorneliu Popescu\n\n## Context\n\nTo programmatically access twitter we need to use a Python library able to support APIs we need. The chosen library should be well reviewed and have wide community support.\n\n## Decision\n\nWe choose [Tweepy](https://github.com/tweepy/tweepy) as our Twitter API Pyhon library.\n\n## Consequences\n\nWe expect to have available a large number of examples and use cases.\n\n## Dependencies\n\n- NA\n\n## References\n\n- NA\n"}
{"repositoryUrl": "https://github.com/jmoratilla/devops-challenge.git", "path": "doc/adr/0008-feat-ideas-for-resilience.md", "template": "Nygard", "status": "Draft", "firstCommit": "2020-02-25T10:05:06Z", "lastCommit": "2020-02-25T10:05:06Z", "numberOfCommits": 1, "title": "8. feat_ideas_for_resilience", "wordCount": 734, "authors": {"name1": 1}, "content": "# 8. feat_ideas_for_resilience\n\nDate: 2020-02-25\n\n## Status\n\nDraft\n\n## Context\n\nHere I'm adding some ideas to add as a cul-de-sac about resilience and \n previous experiences in order to keep a production environment under\n control.\n\n### Mechanisms for resilience and fault tolerance\n\n#### What can go wrong?\n\nTo identify possible points of failure, I will go upstreams from production to development.\n\n#### Production\n\nAttending to Murphy's Law: Anything that can possibly go wrong, does, I should meter anything.  So first point of failure here is monitoring.\n\n##### Monitoring\n\nIf is not monitorized, it is not in production\n\nAnything that goes in production is a result of an effort to earn money, so we should be consciuous of the benefits of the feature as soon as it is deployed.\n\nFor me, any new feature request should have a way to meter the success or failure rate of the solution, and that check must be defined before the feature runs in production, just to show it is not in production yet.\n\nTo handle this, I would add a requirement in the specs for any feature request that should establish what is a success and what is a failure for the product owner that requested it.\n\nA product owner dashboard should exist so any product owner should see a way to measure the success rates of her decisions.\n\nThis monitoring comes from Business, but as technicians we need to measure how the platform is performing, and the capacity of the current resources in order to answer the following question:\n\nWill the next feature get into the current platform?\n\nYou can use [RED method](https://thenewstack.io/monitoring-microservices-red-method/) to monitoring features.\n\n##### Name Resolution\n\nWhat if you have a website nobody can reach because your nameservers are \n down?\n\nYou should have a primary and secondary nameservers in different \n providers.\n\n##### Name System Attacks\n\nWhat if your nameservers have been poisoned to deliver different \n addresses?\n\nYour nameservers must verify and check their data consistency.\n\n##### Name Changes Response Time\n\nYour servers must be quite fast to respond queries, and fast enough to \n propagate changes when endpoints are affected.\n\n##### Network and Security\n\nYour services should have enough bandwidth to avoid queueing requests and deliver quick responses.  Your network is vulnerable to several attacks, from DDoS to specific website malformed urls or known product vulnerabilities.\n\nMonitor your network capacity and get a provider that can handle all your needs.  In cloud environments is easy if you have the money.\n\nMonitor your traffic using Firewalls or Web Application Firewalls, that inspect the requests looking for known vulnerabilities.\n\nIf you are using web services, perform the following recomendations:\n\n- Use HTTP2\n- Use CDN solutions\n- Use WAF solutions that can automatically ban attackers IP addresses\n\n\n##### SSL certificates\n\nYour web applications and sites won't work if your SSL certificates are not valid.\n\nValid certificates means they are in working conditions and they are recognized by the users as trusted certificates.\n\nMonitor your SSL certificates expiration dates and set a prodedure to renew them and publish them automatically.\n\nUse global solutions like [Cloudflare](https://cloudflare.com) that provide solutions for DNS, CDN and Certificate management.  But keep an eye on other providers to have a failover.\n\n##### Know your servers, avoid stupid downtimes\n\nSome servers can create unexpected conditions when you don't know their limits.  For example: nginx is known for be a good load balancer and reverse proxy, but it's also true that it caches the IP addresses of the upstream servers.  If the IP address of an upstream server changes (because reprovisioning a node), then nginx can behaves unexpectedly and it can return error codes.\n\nOther issues can be related to timeouts.  If a web request last more than 40 seconds, nginx automatically rejects the request with a 504 (Gateway Timeout) error code.\n\nTo solve this, use your preproduction / staging environments to reproduce possible scenarios, and get knowledge about the limits of the servers, services and platforms used.\n\n##### Invest in your logs\n\nGet all the possible info from the servers and any service involved in the production environment.\n\nProcess all your logs in order to aggregate data and set a baseline to detect anomalies.\n\nUse solutions like Splunk, Logentries, Devo, Airbrake\n\n#### Preproduction\n\n##### Schedule backups from production to preproduction / staging\n\nTBD\n\n##### Pseudoanonymize backups from production to upstream environments\n\nTBD\n\n##### Perform load tests\n\nTBD\n\n##### Perform releases in staging and take times\n\nTBD\n\n#### QA\n\n##### Match specs with tests\n\nTBD\n\n#### Development\n\nTBD\n\n## Decision\n\nHere are lots of ideas to explore for next discussions.\n\n## Consequences\n\nTo discuss."}
{"repositoryUrl": "https://github.com/CMSgov/easi-app.git", "path": "docs/adr/0003-use-golang-for-server.md", "template": "unknown", "status": null, "firstCommit": "2019-10-17T22:43:24Z", "lastCommit": "2019-10-17T22:43:24Z", "numberOfCommits": 1, "title": "Use Go As Server Language", "wordCount": 228, "authors": {"name1": 1}, "content": "# Use Go As Server Language\n\nThe EASi developers need a language\nto support the application server side.\n\n## Considered Alternatives\n\n* Go\n* Language with widespread web framework\n  (ex. Ruby or Python)\n* JavaScript/Node\n\n## Decision Outcome\n\n* Chosen Alternative: *Go*\n\nThe top reason for selecting Go are\nthat the team has in house expertise.\nThe application team is immediately ready\nto produce client value in Go\ndue to their familiarity with the language and ecosystem.\n\nIt is also a modern strong typed language,\nwhich provides compile time safety,\nas opposed to common web app alternatives,\nsuch as Ruby, Python, or JavaScript.\n\nThe benefits of using languages\nwith more mature web frameworks\n(such as Rails or Django),\nare outweighed by the flexibility\nof using a frontend framework driven UI\nwith a JSON API for data retrieval.\nFor the latter,\nGo provides excellent tooling\nin its standard libraries and ecosystem,\nfor building mature, stable, maintainable APIs.\n\n## Pros and Cons of the Alternatives\n\n### Go\n\n* `+` Familiarity with team.\n* `+` Common tools across Truss projects.\n* `+` Static typed.\n* `-` Relatively immature compared to Ruby/Python\n  in web app ecosystem.\n\n### Language with Widespread Web Framework (Ruby or Python)\n\n* `+` Easy to bootstrap common application features.\n* `+` Strong, mature ecosystem and community.\n* `-` Dynamically typed\n* `-` Not in application team's toolset.\n\n### JavaScript with Node\n\n* `+` Frontend/Server in same language.\n* `+` Single package system (NPM or yarn)\n* `-` Dynamically typed.\n* `-` Not in application team's toolset,\n  especially for server side applications.\n"}
{"repositoryUrl": "https://github.com/alphagov/gsp.git", "path": "docs/architecture/adr/ADR039-cloudhsm-namespace-network-policy.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-09-09T10:11:11Z", "lastCommit": "2019-09-09T10:11:11Z", "numberOfCommits": 1, "title": "ADR039: Restricting CloudHSM network access to particular namespaces", "wordCount": 323, "authors": {"name1": 1}, "content": "# ADR039: Restricting CloudHSM network access to particular namespaces\n\n## Status\n\nAccepted\n\n## Context\n\n[ADR036](ADR036-hsm-isolation-in-detail.md) described the network and\ncredential isolation we use to ensure that unauthorised users cannot\naccess the CloudHSM.\n\nRecently in 3ea9de2ff, we introduced a GlobalNetworkPolicy object,\nwhich is a Calico feature that allows a cluster-wide network policy to\nbe imposed.  This allows us to control network access to and from\nparticular namespaces in a way which cannot be overridden by tenants.\n\nIn particular, currently access to the HSM is only allowed from pods\nannotated with a `talksToHsm=true` label.\n\nWhen working in their own namespace, a developer has full control over\nwhat labels they put on their pods, so they can still choose to put\nthe `talksToHsm=true` label on their pods.  But they do not have\ncontrol over what labels the namespace itself has; to change this\nwould require a change to the `gsp` or appropriate `cluster-config`\nrepository, which would make such a change visible to many more\npeople.\n\nTherefore, if we extend the GlobalNetworkPolicy to require a\n`talksToHsm=true` label on *both* the pod *and* the namespace, we will\nprevent tenants from unilaterally opening up network access to the HSM\nfrom their namespaces.\n\n## Decision\n\nWe will augment the GlobalNetworkPolicy (previously described in ADR036) by:\n\n - setting a `GlobalNetworkPolicy` that denies access to the\n   CloudHSM's IP address unless the pod carries a label\n   (`talksToHsm=true`) and the namespace also carries a label\n   (`talksToHsm=true`) and allows all other egress traffic\n\n## Consequences\n\nControl of which namespaces get the `talksToHsm=true` label will be\nvia the appropriate `-cluster-config` repo.  If a developer wants to\nallow a new namespace to talk to the HSM, they will need to issue a PR\nagainst that repo.\n\nIf we are confident in the GlobalNetworkPolicy's control of HSM\naccess, we could consider reducing the technical controls required on\nnon-HSM namespaces.  For example, we could consider allowing\ndevelopers to run plain `kubectl apply` in unprivileged namespaces,\nfor fast-feedback learning.\n"}
{"repositoryUrl": "https://github.com/guttih/island.is-glosur.git", "path": "docs/adr/0011-open-source-license.md", "template": "Madr", "status": "proposed", "firstCommit": "2020-09-04T09:55:11Z", "lastCommit": "2020-09-04T09:55:11Z", "numberOfCommits": 1, "title": "Open Source License", "wordCount": 445, "authors": {"name1": 1}, "content": "# Open Source License\n\n* Status: proposed\n* Deciders: dev, devops, managers\n* Date: 2020-06-07\n\n## Context and Problem Statement\n\nIt is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.\n\nThere are two main types of open source licences:  more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).\n\nDevelopment for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.\n\n## Decision Drivers\n\n* The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software.\n* It is important to build on the experience of similar government led inititives in other countries.\n* Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen.\n* It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.\n\n## Considered Options\n\nThe different licenses\n\n* Apache\n* BSD\n* GNU GPL\n* MIT\n\n## Decision Outcome\n\nThe MIT license was chosen, for the following reasons:\n\n* It is the least restrictive of the licenses.\n* It is very consise, simple and easy to understand and therefore should be clear to users and developers.\n* Digital Iceland does not require protection of patents or existing intelletual property.\n* Well known government lead initiatives like uk.gov and X-Road use the MIT license.\n* The MIT license is the best known and most widely used free and open-source license in the world.\n\n## Pros and Cons of the Options\n\n### Apache\n\n* Good, because is well known and very permissive like the MIT license.\n* Bad, it is has restrictions around redistribution that do not apply for Digital Iceland.\n* Bad, is way very long and wordy and therefore requires more effort to understand.\n\n### BSD\n\n* Good, because is well known and very permissive like the MIT license.\n* Bad, because it has restrictions about using the names of the copyright holder which is not a concern for digital Iceland.\n\n### GNU GPL\n\n* Good, because it is very well known.\n* Bad, that it is not permissive and requires derived software to adopt the license as well. \n\n## Links\n\n* [Stjórnarráðið - Umfjöllun um opinn hugbúnað](https://www.stjornarradid.is/verkefni/upplysingasamfelagid/stafraent-frelsi/opinn-hugbunadur)\n* [Ríkisendurskoðun - Frjáls og opinn hugbúnaðr](https://rikisendurskodun.is/wp-content/uploads/2016/01/Frjals_og_opinn_hugbunadur_01.pdf)\n\n[Apache]: <https://www.apache.org/licenses/LICENSE-2.0>\n[BSD]: <https://opensource.org/licenses/bsd-license.php>\n[GNU GPL]: <https://www.gnu.org/licenses/gpl-3.0.html>\n[MIT]: <https://opensource.org/licenses/mit-license.php>\n"}
{"repositoryUrl": "https://github.com/learnitmyway/my-notes.git", "path": "adr/cypress.md", "template": "unknown", "status": null, "firstCommit": "2019-04-04T09:18:44Z", "lastCommit": "2019-04-09T10:41:21Z", "numberOfCommits": 2, "title": "Sentry", "wordCount": 35, "authors": {"name1": 2}, "content": "## Sentry\n\n### Context\n- e2e/UI testing framework\n- Was not able to get it to run on Netlify\n\n### Decision\nI won't be able to start using it until I can get it working with a CI tool."}
{"repositoryUrl": "https://github.com/geodocker/geodocker.git", "path": "docs/arch/adr-0001-deployment.md", "template": "unknown", "status": null, "firstCommit": "2016-07-14T14:45:09Z", "lastCommit": "2016-07-14T14:45:09Z", "numberOfCommits": 1, "title": "(sem título)", "wordCount": 778, "authors": {"name1": 1}, "content": "0001 - GeoDocker deployment\n===\n\nContext\n---\nCurrently, it is possible to start a distributed geodocker cluster manually.\nIt is not yet clear how to bring them up automatically at scale.\nThe ideal solution should be available not only for services with a rich API (like AWS or DigitalOcean)\nbut also for hosting providers that lack this sophisticated API and tooling.\nCurrently, support for providers lacking such an API is not a priority.\n\nDecision\n--\nWas decided to look at popular docker orchestration tools:\n  * [ECS](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html)\n  * [DCOS](http://dcos.io)\n  * [Rancher](http://rancher.com)\n\n##### ECS\n\nECS is a highly scalable, fast, container management service, and it is a part of AWS ecosystem.\nIt's incredibly easy to launch containers via ECS,\nhowever it is not clear from the docs or from the community (on forums) how to launch distributed applications using ECS.\nThe problem is that it provides no internal docker network between EC2 nodes,\nand it is not possible to communicate with docker containers on different nodes by some internal address.\nIn addition, there is no API (or such API was not found / mentioned in docs / on forums) to launch containers on different nodes.\nA common use case for ECS is to launch linked containers,\nto scale them (all linked containers would be just launched on separate node(s)),\nand to balance requests between these nodes using load balancer.\nIf we want containers to communicate with each other, it is possible to forward all necessary ports to the host network manually\n(a fact which spark deployment impossible) and to talk with containers as with nodes.\n\n##### DCOS\n\nDC/OS is a distributed operating system based on the Apache Mesos distributed systems kernel.\nIt has a community version, though DC/OS is mostly (it's supposed) oriented on enterprise users.\nThis explains the instability of its latest community version.\nIt has quickstart templates for  [AWS](https://mesosphere.com/amazon/) and for [DigitalOcean](https://docs.mesosphere.com/1.7/administration/installing/cloud/digitalocean/), but is not simple to install it on [some other hosting provider](https://dcos.io/docs/1.7/administration/installing/custom/) (it is not a one-liner).\nFor research purposes, an AWS template was used ([modified](https://gist.github.com/pomadchin/c898fb767ce4d8bb943c2794c565fa8c) to use spot instances).\nDC/OS operates with mesos DNS, and it enables docker container communication on separate nodes.\n[Marathon](https://mesosphere.github.io/marathon/) is used to manage docker containers.\nAll docker containers start with their own internal IP addresses (available via mesos DNS),\nbut they are not accessable by potentially exposed ports (marathon specification requires explicit ports forwarding at least to the mesos internal network).\nThe upshot of this is that you can't start two containers on the same mesos node with the same exposed port\nand there is no in-built port forwarding (on some internal docker network).\nBecause of this, it is not possible to start our own dockerized Spark using Marathon.\nAs a fast and simple solution for deployment, it is possible to start DC/OS built-in Hadoop and Spark packages,\nand to start Accumulo using [this](https://gist.github.com/pomadchin/2193ed3a10808e9368d326a0cebe393f) Marathon job specification.\nTo solve Marathon DNS restrictions (as a consequence of port auto forwarding),\nit is possible to use [Calico](https://www.projectcalico.org/) (though not in the current DC/OS AWS template due to old docker version),\nand [Weave](https://www.weave.works/) (still has no [Weave.Net](https://www.weave.works/products/weave-net/) package for DC/OS).\nSolutions are possible but require further investigation.\n\n##### Rancher\n\nRancher is a completely open source docker management system.\nIt has an easy [insallation](http://docs.rancher.com/rancher/latest/en/installing-rancher/installing-server/) (one-liner, per machine) and may work with AWS and DigitalOcean using their APIs.\nIt is possible to provide slave nodes for Rancher manually (just by running another one liner on potential slaves)\nand it takes control over all docker containers on them.\nRancher includes support for multiple orchestration frameworks: Cattle (native orchestrator),\nDocker Swarm, Kubernetes, Mesos (beta support).\nIt also provides its own DNS on top of docker bridges.\nCattle supports a sort of modified docker-compose.yml (v1) file. \nLaunching a cluster using Cattle is possible via [rancher-compose](http://docs.rancher.com/rancher/v1.0/zh/rancher-compose/).\nHowever Rancher provides DNS on top of docker bridges that causes a following problem with Spark:\nSpark master listens to `localhost` / `container_name` and this name in terms of a master container is an internal _docker_ IP address (17x.xxx.xxx),\nand in terms of some other container master address is an internal _rancher_ ip address (10.xxx.xxx),\nwhich makes master not available for other containers.\nA similar thing happens with Accumulo: it writes the wrong ip addresses / DNS records into Zookeeper and Accumulo master\njust is not available for tablets / other Accumulo processes.\nA solution is possible but requires further investigation.\n\nConsequences\n--\nA clear deployment solution is still not obvious.\nECS is an Amazon service and we can just await necessary functionality.\nDC/OS Community version is _very_ unstable, and has some non-trival dns problems requiring (at the current moment) third-party libraries (Calico, Weave) and a non-trival installation proccess (generally).\nRancher looks more stable, and has a more user-friendly (simpler to understand) ui / tools.\nBut Rancher has it's own specific features to be explored and probably requires more time to research.\n"}
{"repositoryUrl": "https://github.com/lorenzo-deepcode/buildit-all.git", "path": "bookit-api/docs/architecture/decisions/0005-use-id-token-from-microsoft-as-bearer-token.md", "template": "Nygard", "status": "Accepted Amends [4. Security](0004-security.md) Alternative considered [6. Use Okta as Identity provider](0006-use-okta-as-identity-provider.md) Alternative considered [7. Use Pac4J to validate tokens](0007-use-pac4j-to-validate-tokens.md)", "firstCommit": "2018-12-05T13:33:09Z", "lastCommit": "2018-12-05T13:33:09Z", "numberOfCommits": 1, "title": "5. Use id_token from Microsoft as Bearer token", "wordCount": 113, "authors": {"name1": 1}, "content": "# 5. Use id_token from Microsoft as Bearer token\n\nDate: 2017-12-01\n\n## Status\n\nAccepted\n\nAmends [4. Security](0004-security.md)\n\nAlternative considered [6. Use Okta as Identity provider](0006-use-okta-as-identity-provider.md)\n\nAlternative considered [7. Use Pac4J to validate tokens](0007-use-pac4j-to-validate-tokens.md)\n\n## Context\n\nIn the interest of time and getting something to work, we are going to break up the steps further\n\n## Decision\n\n* Instead of exchanging id_token for opaque access_token, the client will always send the id_token as the Bearer token\n* Proper validation of the id_token will still occur\n\n## Consequences\n\n* The security implications are uncertain.  This is definitely not what the id_token is inteded for but it's unclear after much googling what the security holes are. \n* If/when we need actual delegated authorization, especially against the MS Graph API, we will need to revisit this and acquire access_tokens\n"}
{"repositoryUrl": "https://github.com/HFAnalyticsLab/HES_pipeline.git", "path": "doc/adr/storing_dates_in_database.md", "template": "Nygard", "status": "Implemented, and previous code for date conversion has been rolled back.", "firstCommit": "2020-02-20T14:48:48Z", "lastCommit": "2020-04-20T16:16:19Z", "numberOfCommits": 2, "title": "Storing dates in SQLite database", "wordCount": 118, "authors": {"name1": 1, "name2": 1}, "content": "# Storing dates in SQLite database\r\n\r\n## Context\r\n\r\nSQLite does not feature a date format data type. As such writing a date format\r\ndata object from R, results in conversion to an integer with no relevance to the\r\noriginal date.\r\n\r\n## Decision\r\n\r\nIncoming raw data will not be converted to date format in R, and instead \r\nmaintained as a string for full dates (Y-m-d) or part dates (Y-m) and as an \r\ninteger for years.\r\n\r\n## Status\r\n\r\nImplemented, and previous code for date conversion has been rolled back.\r\n\r\n## Consequences\r\n\r\nFor date analyses, when using the HES database date columns must be converted to\r\ndate format after performing a query. A handy function has been created to do \r\nthis in a standardised way (see [here](src/clean.R#L28))"}
{"repositoryUrl": "https://github.com/KonduitAI/dl4j-dev-tools.git", "path": "codegen/adr/0006-op_specific_enums.md", "template": "Nygard", "status": "ACCEPTED Discussed by: Alex Black, Robert Altena and Paul Dubs on 26. November 2019", "firstCommit": "2019-11-26T10:09:44Z", "lastCommit": "2019-11-26T10:45:59Z", "numberOfCommits": 3, "title": "Op specific enums", "wordCount": 195, "authors": {"name1": 3}, "content": "# Op specific enums\n\n## Status\n\nACCEPTED\n\nDiscussed by: Alex Black, Robert Altena and Paul Dubs on 26. November 2019\n\n## Context\nSome ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name\nmakes usage and documentation easier. \n\n\n## Decision\nWe allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values\nfor this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from \n`0`. \n\nA runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default\nvalues match one of the possible values (if applicable).\n\nOn code generation, an appropriate representation of this enum will be generated in the target language. The name of \nthe generated enum will be derived from the name of the arg.\n\n### Example\n```kotlin\nArg(ENUM, \"padMode\"){ \n  possibleValues = listOf(\"CONSTANT\", \"REFLECT\", \"SYMMETRIC\")\n  description = \"padding mode\"  \n}\n```\n\n## Consequences\n\n### Advantages\n* We get easily understandable names for otherwise in-transparent ordinal mode modifiers\n\n### Disadvantages\n* The defined enum can only be used for a single op\n* The defined enum is only usable with a single arg\n"}
{"repositoryUrl": "https://github.com/UST-MICO/docs.git", "path": "adr/0026-implementation-of-complex-eai-patterns-with-faas.md", "template": "unknown", "status": null, "firstCommit": "2019-05-16T13:39:05Z", "lastCommit": "2019-05-28T21:44:04Z", "numberOfCommits": 9, "title": "Implementation of complex EAI-Patterns with FaaS", "wordCount": 844, "authors": {"name1": 7, "name2": 1, "name3": 1}, "content": "# Implementation of complex EAI-Patterns with FaaS\n\n\n## Context and Problem Statement\n\nSome [Enterprise Integration Patterns](https://www.enterpriseintegrationpatterns.com) have a complex structure where parts of the behaviour can be implemented generically while some parts need to be modifiable by the end user (in our case the system admin using MICO).\nWe have already [decided to use a FaaS platform](0023-faas.md) to provide this modifiability in form of code as configuration.\nWhile this works well for most patterns, for some of the more complex patterns it is not easy to allow modifiability via FaaS.\nThis is especially the case if the user want to write as little code as possible meaning the [generic part](0025-generic-component.md) of the component has to be implemented by the MICO team.\n\n\n## Decision Drivers\n\n* Modifiability of the patterns must be provided via a FaaS function\n* The function should only have to contain as little code as possible\n* Generic code for the pattern should be provided by the MICO platform either as a library to import into the FaaS function or in the component that calls said function\n\n\n## Challenges\n\n### Where / how to implement generic logic\n\n* Implement all logic in the FaaS function\n* Implement custom logic + some of the generic logic in the FaaS function\n* Implement only custom logic in the FaaS function\n\n### State of configuration channel\n\nHow can the FaaS function get the current state of the configuration (e.g. dynamic router).\n\n* Let the FaaS function subscribe to Kafka\n* Have a separate DB\n* Send the current configuration together with the message\n\n### Sending to multiple destinations / unknown destinations\n\nA router does not have a specific endpoint to send all messages to and may even send messages to many endpoints.\n\n* Implement generic routing with `recipientList` / `routingSlip` only and let FaaS function write the route into the message header\n* Let the function send the messages directly to Kafka\n\n### Stateful components\n\nSome patterns are inherently stateful.\n\n* Store state in a DB\n* Store state in generic component and send to FaaS function with message\n\n### Performing intermediate request steps\n\nA content enricher needs to perform requests to get the content to inject into the message.\n\n* Offload implementation completely to the user\n* Split into two parts, one determining which requests are needed and one injecting the requested content into the message.\n\n\n## Affected patterns\n\n```eval_rst\n=== == == ==\n Pattern   dynamic config   destinations   stateful\n=== == == ==\nRouter  no   yes  maybe\nDynamic Router  yes  yes  maybe\nContent based Router  maybe  yes  maybe\nAggregator  no   no   yes\nResequencer   no   no   yes\nProcess Manager   maybe  yes  maybe\nMessage Normalizer  maybe  no   no\n=== == == ==\n```\n\n## Decision Outcome\n\n**Where to implement logic**: To be decided\n\n**State of configuration channels**: To be decided\n\n**Stateful functions**: To be decided\n\n**Routing**: We will support custom routing decisions in the FaaS function by always interpreting a routing slip if it is present.\nThe routing slip has to support multiple destinations for one routing step.\nThis will also allow us to make more patterns possible (everything that is not stateful) with a single generic kafka to FaaS connector.\n\n**Intermediate requests**: To be decided\n\n\n## Evaluation of Options\n\n### Where to implement generic logic\n\n#### Implement generic logic in FaaS-Function\n\n* Good because we can use Kafka as trigger for the function\n* Good because it allows easy extension and modification of the generic logic where needed\n* Bad because we need to implement the generic logic in all supported languages as a library/framework\n* Bad because the alternative would mean complex FaaS networks/dependencies if generic logic is capsuled in FaaS functions\n\n#### Implementing generic logic in Kafka to FaaS Adapter\n\n* Good because we only need one implementation in one language\n* Bad because generic logic needs to support all patterns => possibly more complex implementation\n\n### Stateful patterns\n\n#### Store state in compacted Kafka topic\n\n* Good because it uses no extra component\n* Bad because only useful for storing configuration channel state/state that does not change too often\n* Bad because it needs many assumptions\n* Bad because whole channel needs to be read to load state\n\n#### Store state in DB\n\n* Good because DBs are really good for storing and querying state\n* Bad because management of DBs (isolation of DBs for different FaaS functions) is difficult\n\n#### Send state with the message to the FaaS function\n\n* Good because FaaS function can be trivially stateless\n* Bad because state could be large\n* Bad because generic component needs to know what state to send (up priori assumption)\n\n### Sending to multiple or unknown destinations\n\n#### Allow FaaS function to send messages directly to kafka\n\n* Good because it allows complex routing patterns\n* Bad because FaaS function needs to understand and react to various header fields in the cloud event (routingSlip, route history)\n\n#### Implement generic routing via header fields (routingSlip/recipientList)\n\n* Good because we can ensure semantics of some header fields in our implementation\n* Bad because header fields need to be sufficiently complex to support complex routing decisions\n\n### Intermediate requests\n\n#### Offload complete implementation to user\n\n* Good because all requests can be controlled by user code\n* Bad because user may need to (re-)implement generic code\n* Bad because FaaS function needs to know about kafka topics\n\n#### Split into request part and merge part\n\n* Good because split implementation is easyer to understand\n* Good because split implementation allows for generic request handling implemented by MICO\n* Bad because split pattern implementation is  more complex\n"}
{"repositoryUrl": "https://github.com/StuPro-TOSCAna/TOSCAna.git", "path": "docs/dev/adr/0005-springfox.md", "template": "unknown", "status": null, "firstCommit": "2017-12-01T10:24:41Z", "lastCommit": "2018-03-25T10:57:21Z", "numberOfCommits": 2, "title": "*Use Springfox to automatically generate API Documentation*", "wordCount": 260, "authors": {"name1": 1, "name2": 1}, "content": "# *Use Springfox to automatically generate API Documentation*\n\n**User Story:** *As a developer i want to know how to use the provided REST API*\n\nThe documentation of the API is very important, we therefore have to decide on a way to document our REST API \n\n## Considered Alternatives\n\n* [Springfox](https://springfox.github.io/springfox/)\n* Manually created [Swagger](https://swagger.io/) Documentation\n* Markdown based API documentation\n\n\n## Decision Outcome\n\n* Chosen Alternative: *Springfox*\n* Springfox allows us to keep the documentation of the API within the java code, we do not need to create a seperate document for it. The Autogeneration is also nice because we don't have to do anything manually once the implementaion is done.\n\n## Pros and Cons of the Alternatives \n\n### *Springfox*\n\n* `+` The generation of the API documentation can be fully automated `->` API docs can be always up to date\n* `+` Commenting of the API docs is done with annotations\n* `+` Allows access to the API using `swagger-ui` this results in a quite crude but usable \"web ui\"\n* `-` Natively documenting everything (without the use of workarounds is not possible)\n* `-` Annotations have to be learned\n* `-` Code will have dead classes (used to model parts of the API)\n* `-` Generates a JSON based swaggerfile (not really human readable)\n\n### *Manual Swagger*\n\n* `+` Decent readability\n* `+` The resulting documentation can be complete (no workarounds needed)\n* `-` Manuall updates of the file needed\n* `-` A quite complex DSL (domain specific language) has to be learned\n* `-` A major API change will result in a lot of work\n\n### *Markdown*\n\n* `+` Good readability\n* `+` The resulting documentation can be complete (no workarounds needed)\n* `-` Manuall updates of the file needed\n* `-` A major API change will result in a lot of work\n"}
{"repositoryUrl": "https://github.com/department-of-veterans-affairs/va.gov-team.git", "path": "docs/adr/0005-use-github-actions-for-vsp-platform-ci.md", "template": "unknown", "status": null, "firstCommit": "2021-02-16T20:17:12Z", "lastCommit": "2021-02-16T20:17:12Z", "numberOfCommits": 1, "title": "Context", "wordCount": 1001, "authors": {"name1": 1}, "content": "# Context\n\nhttps://dsva.slack.com/archives/C01CJV0L9PS/p1612651359029600?thread_ts=1612651359.029600&cid=C01CJV0L9PS\n\nTL;DR: CircleCI as implemented with VA restrictions does not allow easy management of permissions on ‘Contexts’ which contain credentials which are available to the entire ‘department-of-veterans-affairs’ Github Org. Although they are not displayed in plaintext in the CircleCI UI, they are obtainable in plaintext by running a build in CircleCI and using SSH to connect to your in-progress build, and outputting ENV variables. If the entire Org has access to the context, there is no way to stop anyone in the Org from doing so.\n\nIn addition, CircleCI as implemented with VA restrictions does not allow use of ‘third-party’ Orbs (reusable code) which means that we cannot publish our own Orbs either.\n\nThese two items are not preventing the use of CircleCI completely, as it is still possible to manage per-project secrets (only available to those with Github permissions on that repo) and you are free to cut and paste code from one workflow to another to re-use code, however, it makes managing multiple repos that have similar functions very tedious, and error prone.\n\nIn addition, managing the secrets per-repo means that if the Operations team wanted to rotate the credentials for that repo, they would have to use the CircleCI UI to change all of those credentials, one project at a time.\n\nI have been evaluating multiple options for CI at this point and I think the best use-case for Operations is Github Actions, however, I have come to the conclusion that there would be no reason an application team could not use CircleCI, and manage the secrets in their own repo level secrets. There is also no reason that an application team could not use their own CI server that they manage, like CMS Tugboat does, or if they love Jenkins and want to use it, they can run their own Jenkins server somewhere.\n\n \n\nI think our platform will strive to be CI-agnostic, but we will 100% provide examples of how to interact with ArgoCD, which WILL be required in the workflow\n\n \n\nthis has the benefit of removing Kubernetes credentials and interaction from the application teams completely. the Operations team will document what ArgoCD wants to receive as input, and how the team can make that input, and then Argo is the only place outside of members of the Operations team that has credentials to make changes to EKS clusters.\n\n \n\nthe application teams will also be able to log into ArgoCD and see their applications in Argo, along with logs and events that will help them troubleshoot their applications and deployments.\n\n\nJust to be clear, we're talking about CI for:\n\n  VSP\n\n  VFS teams building on VSP\n\n  Others: CMS, VA Notify, etc.\n\nFor 3 I can see being agnostic. For 1 and 2, I think we should be consistent so we can support the VFS teams.\nfor 2, we will provide examples and best practices\n\nand we will support VFS teams in building those\nbut I don’t want to REQUIRE them to use a particular process\n\nI will strongly recommend to them that they use the process we build for them and my team will help them build it, absolutely.\nwhat I’m saying is that if they love Circle and want to use it, we can support that\n\nas long as they manage their own project-level credentials\n\nthe problems with CircleCI for the Operations team is that we have many repos and managing those credentials at the repo level across all of those, as well as managing them all as separate pieces of code (when in reality they are 99% similar) is tedious, error prone, and makes Circle a bad fit (as long as the VA maintains those limitations)\nGithub Actions is, I think, the best option.\n\n \n\n# Vision\n\nEach application has it’s own repository, it’s own ECR repo, and it’s own AWS user, with limited permission scope (able to push and pull from the application’s ECR repo, able to read parameters from only their path in Parameter Store). That AWS user has a token which is in Github Encrypted Secrets, managed by the application team. Any further secrets needed and variables for their application, are pulled from Parameter Store.\n\nthe Operations team manages these AWS users, ECR repos, and Github repos (using the Github terraform provider) with Terraform, as IaC. (edited) \n\nthat eases the lift on the Operations team because we can manage all of this using the tool we already use, we manage the permissions on the github repos by the github teams that already exist, and we scope the credentials we put on the repo to limit any blast radius.\n\n \n\n# Other tools that were considered\n\n \n\nDrone.io is really cool, but in the end too simple for our need.\n\nConcourse-ci is also a great tool but almost as complex as Jenkins, with a steep learning curve\n\nI had a demo of GoCD from some folks at Thoughtworks that Karl Brown got me in touch with and it is an amazing CI tool, miles ahead of Jenkins, but also very complex, and overkill for our needs\n\nJenkins is not viable for a number of reasons, not the least of which is the maintenance of the tool\n\n\nAWS CodePipeline/Codebuild was also considered, but in order to use that, the Operations team would have to write all the projects, etc, in Terraform, which would increase the management load for my team\n\n \n\n# Why Github Actions?\n\n \n\nGithub Actions is something we already have, fills all the needs, along with allowing us to maintain permissions over credentials\nand in addition, we can run on-premise runners if we need to, for things like the drupal pull which need VAEC network access1\nalso on-prem runners dont count toward your minutes usage in Github Actions so we can mitigate any cost issues that way\n\nwe can also write our own github actions and share them, as you could with Orbs in CircleCI\n\nwhich we can’t do in Circle because of VA restrictions\n"}
{"repositoryUrl": "https://github.com/eclipse/winery.git", "path": "docs/adr/0031-reuse-refinement-code-for-pattern-detection.md", "template": "Madr", "status": null, "firstCommit": "2021-05-27T18:39:07Z", "lastCommit": "2021-05-27T18:39:07Z", "numberOfCommits": 1, "title": "(sem título)", "wordCount": 480, "authors": {"name1": 1}, "content": "<!---~\n  ~ Copyright (c) 2021 Contributors to the Eclipse Foundation\n  ~\n  ~ See the NOTICE file(s) distributed with this work for additional\n  ~ information regarding copyright ownership.\n  ~\n  ~ This program and the accompanying materials are made available under the\n  ~ terms of the Eclipse Public License 2.0 which is available at\n  ~ http://www.eclipse.org/legal/epl-2.0, or the Apache Software License 2.0\n  ~ which is available at https://www.apache.org/licenses/LICENSE-2.0.\n  ~\n  ~ SPDX-License-Identifier: EPL-2.0 OR Apache-2.0\n  ~-->\n\n\n# Reuse the pattern refinement implementation for pattern detection\n\n## Context and Problem Statement\n\nTo create an executable deployment model, the pattern refinement process replaces a matching subgraph with the Refinement Structure of a PRM.\nTo create a PbDCM, the pattern detection process replaces a matching subgraph with the Detector of a PRM.\nThe replacement procedure is identical for both processes, only the structures used for the replacement differ.\nTherefore, the implementation of the pattern refinement process should be reused to implement the pattern detection process.\n\n## Decision Drivers\n\n* Avoid duplicate code\n* Avoid introducing errors and inconsistencies during reimplementation\n\n## Considered Options\n\n* Swap the Detector of all PRMs with their Refinement Structures\n* Reimplementation\n* Use common interface\n\n## Decision Outcome\n\nChosen option: \"Swap the Detector of all PRMs with their Refinement Structures\", because reimplementation introduces too much duplicate code and a common interface requires a lot of boilerplate code while also decreasing readability.\n\n### Positive Consequences <!-- optional -->\n\n* Complete pattern refinement implementation can be reused\n\n### Negative consequences <!-- optional -->\n\n* Readability and understandability decreases\n\n## Pros and Cons of the Options\n\n### Swap the Detector of all PRMs with their Refinement Structures\n\nIn the backend, the elements of the PRMs retrieved from the repository are swapped, .i.e, the Detector of each PRM is set to its Refinement Structure, its Refinement Structure is set to its Detector, and all mappings are adapted accordingly.\n\n* Good, because complete refinement code can be reused\n* Bad, because decreases readability and understandability\n\n### Reimplementation\n\nThe complete pattern refinement code is reimplemented for pattern detection, i.e., the reimplemented code considers the Detector during the replacement, redirection of Relations using the Relation Mappings, and retaining elements using the Stay Mappings.\n\n* Good, because better readability\n* Bad, because results in a lot of duplicate code\n* Bad, because the reimplemented code can contain errors and inconsistencies already fixed in the refinement implementation\n\n### Use common interface\n\nImplement an interface which returns the Refinement Structure of a PRM for the replacement procedure of the pattern refinement process and returns the Detector of a PRM during the pattern detection process.\n\n* Good, because refinement code can be reused\n* Bad, because requires a lot of boilerplate code\n* Bad, because it decreases readability\n\n## License\n\nCopyright (c) 2021 Contributors to the Eclipse Foundation\n\nSee the NOTICE file(s) distributed with this work for additional\ninformation regarding copyright ownership.\n\nThis program and the accompanying materials are made available under the\nterms of the Eclipse Public License 2.0 which is available at\nhttp://www.eclipse.org/legal/epl-2.0, or the Apache Software License 2.0\nwhich is available at https://www.apache.org/licenses/LICENSE-2.0.\n\nSPDX-License-Identifier: EPL-2.0 OR Apache-2.0\n"}
{"repositoryUrl": "https://github.com/kbase/sample_service.git", "path": "design/implementation_notes.md", "template": "unknown", "status": null, "firstCommit": "2020-05-16T00:40:51Z", "lastCommit": "2020-05-22T23:21:53Z", "numberOfCommits": 4, "title": "Relevant ArangoDB documentation", "wordCount": 1107, "authors": {"name1": 4}, "content": "# Relevant ArangoDB documentation\n\n* Transactions: https://www.arangodb.com/docs/stable/transactions.html\n* Write conflicts: https://github.com/arangodb/arangodb/issues/9430\n* Transaction failures: https://github.com/arangodb/arangodb/issues/11424\n* Sorting graph traversal results: https://github.com/arangodb/arangodb/issues/11260\n\n# Data links\n\n## Deleted objects / workspaces\n\n* Currently links to deleted objects can be returned from `get_links_from_sample`.\n  * Should this be changed? It means an extra workspace call.\n  * It also has reproducibility issues - calls to the method with the same `effective_time` may\n  not produce the same results.\n  * That being said, changing permissions to workspaces can also change what links are returned\n  over time.\n  * If we don't return links to deleted objects, should the links be autoexpired if they aren't\n  already?\n  * This assumes `get_object_info3` with `ignoreErrors: 1` will only return `null` for deleted\n  objects when called via `administer` - verify\n  * What about expired links to deleted objects with an `effective_time` in the past? Return them?\n\n* Links to deleted objects can be expired as long as the user has write access to the workspace.\n  However, links to objects in deleted workspaces **cannot** be expired by anyone, including\n  admins, given the current implementation.\n  * This naively seems ok since the links aren't accessible by anyone other than admins.\n\n## Creating / updating links\n\n### 10k link limit\n\n* The Sample Service allows no more than 10k non-expired links from any one version of a sample or\n  from any one version of an object. That means that a single object version can have no more\n  than 10k data IDs (e.g. column names for matrix data).\n* This requirement was originally agreed upon because there appears to be no way to\n  [efficiently sort and page results with graph queries in ArangoDB](https://github.com/arangodb/arangodb/issues/11260).\n  Allowing the collection to grow without limit means that eventually sorted graph queries will\n  OOM / hog CPU on the database (or be consistently killed if the DB has that capability). 10K\n  is small enough that the results can be sorted in a small amount of memory in the DB,\n  application, or UI.\n* Link lookups are currently implemented without graph traverals and so in theory the\n  10K limit could be lifted.\n  * Would need sort / paging / indexes. As usual, paging needs to be based on some aspect of\n  the link that is unique, which is tricky here\n* HOWEVER - if any of the query parameters or expectations change the limit may have to be\n  reinstated, e.g.\n  * Changes to how the search regards ACLs.\n  * Searching on additional properties, e.g.\n  * Workspace object properties\n  * Sample properties\n\n### Implementation notes\n\n* Since multiple clients may be creating, updating, or expiring links on the same sample\n  or data object simultaneously, the code needs to account for possible race conditions on\n  those operations.\n* Links are immutable once created, *except* that they can be expired.\n* The unique ID of a link in the database is a fn of the data unit ID (e.g.\n  workspace UPA + data id) for an unexpired link, and the DUID + created time for expired links.\n  This ensures there's only one non-expired link per DUID in the DB.\n\n### Implementation\n\n* Parameters: `new_link`, `update` boolean indicating `current_link` should be replaced if it\n  exists\n* Start a transaction with a collection exclusive lock.\n  * This is required since we're counting multiple documents. Allowing other writes while the\n  transaction is in progress will make those counts inaccurate.\n* Fetch the `current_link` for the data unit ID, if any\n* If `current_link`:\n  * If not `update`: fail\n  * If `current_link` == `new_link`: abort transaction and return (no-op)\n  * If `new_link` is to a different sample and count_links(`new_link.Sample`) > 10K: fail\n  * Link look up is based on the data, so we know it's to the same UPA, and since we're\n  expiring and replacing a link the link count for the UPA doesn't change.\n  * Expire and save `current_link`\n  * Creates a new ArangoDB document with a new `_key`\n* Else:\n  * If count_links(`new_link.UPA`) > 10K: fail\n  * If count_links(`new_link.Sample`) > 10K: fail\n* Save `new_link`\n* Complete the transaction.\n\n### Extant failure modes\n\n* ArangoDB transactions can fail on one node and succeed on another. This will commit the\n  changes that succeeded but cause the transaction as a whole to fail client-side. This means\n  in theory the client could determine the current state of the DB and try to repair any\n  inconsistencies, but in practice this is very complicated.\n  * A link could be expired without the current link being updated, effectively leaving an\n  expired and current version of the same link. If this link were to be expired again an\n  error would occur and the DB would have to be manually corrected.\n  * A link could be updated without the prior link being expired, effectively causing the record\n  of the prior link to disappear.\n\n## Expiring links\n\n### Implementation notes\n\n* See the implementation notes for creating / updating links above.\n* Arango does not support atomically changing a document's `_key`. Since expiring a link\n  means changing the `_key`, we use a transaction to reduce the possibility of inconsistent\n  database state.\n  * But does a transaction really help?\n\n### Implementation\n\n* Parameters: `duid` - the data unit ID\n* Fetch `current_link` from the DB via the `duid`.\n* If not `current_link`: fail\n* Start a transaction.\n* Expire `current_link` and save.\n  * Creates a new ArangoDB document with a new `_key`\n  * Fail if a duplicate key error occurs, meaning the link was expired after fetching the document.\n* Delete the old `current_link` document.\n* Complete the transaction.\n\n### Extant failure modes\n\n* ArangoDB transactions can fail on one node and succeed on another. This will commit the\n  changes that succeeded but cause the transaction as a whole to fail client-side. This means\n  in theory the client could determine the current state of the DB and try to repair any\n  inconsistencies, but in practice this is very complicated.\n  * A link could be expired without the current link being deleted, effectively leaving an\n  expired and current version of the same link. If this link were to be expired again an\n  error would occur and the DB would have to be manually corrected.\n  * A link could be deleted without the expired document being written, destroying the link's\n  history.\n* Since expiration does not use an exclusive lock, it is possible for other writes to collide\n  with the expired link document and cause the write to fail.\n  * It is not clear if this will\n  [cause the transaction as a whole to fail](https://github.com/arangodb/arangodb/issues/11424).\n* If the delete fails, an error will be raised and the transaction will be aborted.\n  * Funnily enough, there is an `ignore_missing` parameter, but from the `python-arango`\n  documentation:\n  ```\n  :param ignore_missing: Do not raise an exception on missing document.\n  This parameter has no effect in transactions where an exception is  \n  always raised on failures.\n  ```\n  * This could be caused by:\n  * Another thread expiring and deleting the link, in which case no further action is necessary.\n  * Another thread expiring and updating the link, in which case the current operation should\n  not expire the new link.\n  * This error takes split second timing and is highly unlikely to occur, and should not\n  leave the database in an inconsistent state.\n"}
{"repositoryUrl": "https://github.com/elastic/cloud-on-k8s.git", "path": "docs/design/0008-volume-management.md", "template": "unknown", "status": null, "firstCommit": "2019-03-13T07:03:43Z", "lastCommit": "2019-07-30T12:47:44Z", "numberOfCommits": 3, "title": "8. Volume Management in case of disruption", "wordCount": 1405, "authors": {"name1": 1, "name2": 1, "name3": 1}, "content": "# 8. Volume Management in case of disruption\n\n**Update (2019-07-30)**:\nWe decided to rely on StatefulSets to manage PersistentVolumes. While this ADR remains valid, our range of action is now limited to what is supported by the StatefulSet controller, responsible for creating and reusing PVCs.\n\n* Status: proposed\n* Deciders: cloud-on-k8s team\n* Date: 2019-03-08\n\n## Context and Problem Statement\n\nThe aim of this document is to capture some scenarios where a pvc gets orphaned and define how the “reuse pvc” mechanism must behave.\nThis document does not deal with the reuse of a PVC after the spec of the cluster has been updated _(that is, \"inline\" VS \"grow-and-shrink\" updates)_.\nIt is a complex scenario which deserves its own ADR.\n\n\nAs a preamble before we dive into the different use-cases and scenarios here are some considerations about what can lead\nto a disruption and a reminder about some constraints raised by storage classes.\n\n### Disruptions\nA Pod does not disappear until a person or the controller deletes it or there is an unavoidable hardware or system software error.\nThe reasons can be classified into 2 main categories:\n\n* There is an **external involuntary** disruption:\n  * Hardware failure\n  * VM instance is deleted\n  * Kernel panic\n  * Any runtime panic (for example `containerd` crash)\n  * Eviction, but it is not supposed to happen as long as we use a [QoS class of Guaranteed](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed)\n* There is an **external voluntary** disruption\n  * The node hosting the Pod is drained because, some _(non exhaustive)_ examples are:\n  * The K8S node is about to be upgraded or repaired\n  * The K8S cluster is scaling down\n  * The pod is manually deleted by someone (not only as in error but also because sometimes a reboot can fix a problem)\n\n### Storage class constraints\nStorage classes do not all provide the same capabilities when it comes to reusing a volume, for instance:\n\n* Google persistent disks can be attached from a single availability zone\n* Regional persistent disks replicate the data between 2 zones in the same region\n* A volume backed by our elastic-local storage class can only be reused on the same node\n\nAt this stage it is worth mentioning that even if the K8S scheduler uses some predicates to reschedule a pod on a node\nwhere the volume can be reused or attached, it does not preserve the capacity needed to reschedule the pod.\nFor instance if a pod was using a local volume and if the node runs out of capacity while the pod is being recreated\nthen it becomes impossible to reuse the volume until some capacity is freed.\n\nWhen a disruption occurs either a volume is considered to be **recoverable** or it is considered **unrecoverable**.\n\n#### Unrecoverable volume\n\n`Unrecoverable` is a state that can be reached in two situations:\n\n* The administrator knows that the volume can't be recovered and it must be abandoned.\n* The Elastic operator creates a new pod in an attempt to reuse the volume but the pod is still not scheduled after a given amount of time.\n\n#### Recovering strategies\n\nThere are 2 possible strategies when it is time to try to recover the data from a PVC:\n\n##### Recoverable required\n\nThe Elastic operator **must not delete** a PVC that may hold the only copy of some data.\n`Recoverable required` is a state in which the volume **must** be recovered to get the missing data back online.\n\n##### Recoverable optional\n\n`Recoverable optional` is a state where the missing data is available on some others nodes. For instance if a K8S node with a local volume is down\nand if data can be replicated from other nodes then it is not mandatory for the Elastic operator to wait forever.\n\nIt is a best effort scenario, we have to choose between:\n\n* Wait for the node to be back online\n\nVS\n\n* Paying the cost of a replication from other nodes\n\nIt means that in such scenario we have to find a way to determine the time the operator will wait before it is decided that a new pod must be created.\nIt may be hard to find this exact timeout, it must be user configurable but a sane default value should be set, based on some criteria like, for example:\n* the shard size\n* the number of replicas still available\n\nTODO: check if the controller has access to PVC but not to PV or nodes, we can't watch nodes or PV, we can't only watch the claims\n\n## Decision Drivers\nThe solution must be able to handle the following use cases:\n\n### UC1: The K8S cluster is suffering a external involuntary disruption and the volumes cannot be recovered\n\nIn this scenario we must consider the data as permanently lost _(for example vm with local storage has been destroyed)_.\n\nWe need to give a way to the user to instruct the Elastic operator that:\n* It should immediately move the volume into a `Recovering` strategy.\n* If the volume is in the `Recoverable required` state the user should be able to forcibly not reuse the PVC, even if there is no other replica available.\n\n### UC2: The K8S cluster is suffering a external involuntary or voluntary disruption but the volumes can be eventually recovered\n\nThe Elastic operator will create a new pod and according to the PV affinity the scheduler will hopefully find a new node where the data is available.\nIf it takes to much time to schedule the pod then the volume is moved into one of the two `Recoverable` states.\n\n### UC3: As an admin I want to plan a voluntary disruption and the volumes cannot be recovered\n\nIn this scenario the administrator want to definitively evacuate a node and the data will not be available\nanymore (for example, a server with a local storage is definitively removed from the cluster)\n\nIt is usually done in two steps:\n\n1. Cordon the node\n1. Evict or delete the pods\n\n## Considered options\n\n### Option 1: Add a finalizer to the PVC\n\nA PVC that is used by a pod will not be deleted immediately because of a finalizer set by the scheduler.\nWe can add our own finalizer to:\n1. Create a new pod\n1. Migrate the data and delete the pod.\nOnce the pod has been deleted the PVC can be deleted by K8S.\n\n### Option 2: handle PVC deletion with an annotation\n\nA tombstone is set on the PVC as an annotation.\nThe annotation `elasticsearch.k8s.elastic.co/delete` can be set on a PVC with the following values:\n\n* `graceful`:  migrate the data, delete the node and the PVC.\n* `force`: discard the data, the operator does not try to reuse the PVC, the PVC is deleted by the Elastic operator.\n\n\n### Option 3: Add a kubectl plugin to add some domain specific commands\n\n`kubectl` can be extended with new sub-commands: https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/\n\nfor example:\n```bash\n$ kubectl elastic migrate elasticsearch-sample-es-qlvprlqnnk -n default # will migrate the data then delete the pod and the pvc\n$ kubectl elastic delete elasticsearch-sample-es-qlvprlqnnk -n default # will delete the pod **and** the pvc\n```\n\n### Option 4\n\nTry to handle pod eviction and PVC deletion with a webhook.\n\n## Pros and Cons of the Options\n\n### Option 1\n\nPros:\n* Looks like a simple approach, just do a `kubectl delete pvc/XXXXX` to migrate the data and delete the pod.\n\nCons:\n* If the volume can't be recovered the user will have to remove the `Finalizer`\n* Administrator has to `uncordon` the node and delete manually _(also known as error prone)_ the `PVC` if he wants to drain it.\n\n### Option 2\nPros:\n* The Elastic operator can figure out easily if it must try to migrate the data or abandon the volume because\nit can't be recovered.\n\nCons:\n* Admin has to `uncordon` the node and annotate the `PVC` manually _(still error prone)_ if he wants to drain it.\n* Admin must remember the annotations\n\n### Option 3\nPros:\n* Provides a meaningful interface\n\nCons:\n* Stable?: Even if plugins were introduced as an alpha feature in the v1.8.0 release it has been reworked in v1.12.0\n* End user must install the plugin\n* Admins still have to evict nodes manually when the node is drained\n\n### Option 4\nPros:\n* Integrate smoothly in the `cordon` + `drain` scenario.\n\nCons:\n* It doesn't seem possible to handle a node eviction _(needs to be confirmed)_.\n* Setting a webhook requires some privileges at the cluster level.\n* Is it even possible to use a webhooks to safely migrate some data when an eviction occurs?\n\n## Links\n\n* Strimzi: [Deleting Kafka nodes manually](https://strimzi.io/docs/master/#proc-manual-delete-pod-pvc-kafka-deployment-configuration-kafka)\n* Kubernetes [isBeingUsed](https://github.com/kubernetes/kubernetes/blob/a3ccea9d8743f2ff82e41b6c2af6dc2c41dc7b10/pkg/controller/volume/pvcprotection/pvc_protection_controller.go#L210)\nfunction: A PVC can be deleted *only* if it is not used by a scheduled pod _(including the `Unknown` state)_\n"}
{"repositoryUrl": "https://github.com/vwt-digital/operational-data-hub.git", "path": "architecture/adr/0034-2fa-on-all-user-identities.md", "template": "Nygard", "status": "Accepted Implements [6. Implement Security by Design](0006-implement-security-by-design.md)", "firstCommit": "2020-09-21T14:31:15Z", "lastCommit": "2020-10-21T07:23:44Z", "numberOfCommits": 2, "title": "34. 2FA on all user identities", "wordCount": 210, "authors": {"name1": 1, "name2": 1}, "content": "# 34. 2FA on all user identities\n\nDate: 2020-09-21\n\n## Status\n\nAccepted\n\nImplements [6. Implement Security by Design](0006-implement-security-by-design.md)\n\n## Context\n\nTwo-Factor Authentication (2FA) is sometimes called multiple factor authentication. In simple terms, it adds an extra layer of security to every online platform you access. The first layer is generally a combination of a username and password. Adding one more step of authenticating your identity makes it harder for an attacker to access your data. This drastically reduces the chances of fraud, data loss, or identity theft.\n\nPasswords have been the mainstream form of authentication since the start of the digital revolution. But, this security measure is far from infallible. Here are some worrying facts about this traditional security measure:\n* 90% of passwords can be cracked in less than six hours.\n* Two-thirds of people use the same password everywhere.\n* Sophisticated cyber attackers have the power to test billions of passwords every second.\n\nThe vulnerability of passwords is the main reason for requiring and using 2FA.\n\n## Decision\n\nWe will use 2FA on all user identities.\n\n## Consequences\n\nDespite the additional overhead of the second factor to authenticate, the additional protection of the user identity is worth the effort. Most identity providers facilitate 2FA using a mobile app, which limits the additional effort required.\n\n## References\n\n* https://secureswissdata.com/two-factor-authentication-importance/, retrieved 21 October 2020\n"}
{"repositoryUrl": "https://github.com/actions/runner.git", "path": "docs/adrs/0297-base64-masking-trailing-characters.md", "template": "unknown", "status": null, "firstCommit": "2020-01-28T02:38:01Z", "lastCommit": "2021-02-05T18:29:43Z", "numberOfCommits": 2, "title": "ADR 0297: Base64 Masking Trailing Characters", "wordCount": 424, "authors": {"name1": 1, "name2": 1}, "content": "# ADR 0297: Base64 Masking Trailing Characters\n\n**Date** 2020-01-21\n\n**Status** Proposed\n\n## Context\n\nThe Runner registers a number of Value Encoders, which mask various encodings of a provided secret. Currently, we register a 3 base64 Encoders:\n- The base64 encoded secret\n- The secret with the first character removed then base64 encoded\n- The secret with the first two characters removed then base64 encoded\n\nThis gives us good coverage across the board for secrets and secrets with a prefix (i.e. `base64($user:$pass)`).\n\nHowever, we don't have great coverage for cases where the secret has a string appended to it before it is base64 encoded (i.e.: `base64($pass\\n))`). \n\nMost notably we've seen this as a result of user error where a user accidentally appends a newline or space character before encoding their secret in base64.\n\n## Decision\n\n### Trim end characters\n\nWe are going to modify all existing base64 encoders to trim information before registering as a secret.\nWe will trim:\n- `=` from the end of all base64 strings. This is a padding character that contains no information. \n  - Based on the number of `=`'s at the end of a base64 string, a malicious user could predict the length of the original secret modulo 3. \n  - If a user saw `***==`, they would know the secret could be 1,4,7,10... characters.\n- If a string contains `=` we will also trim the last non-padding character from the base64 secret.\n  - This character can change if a string is appended to the secret before the encoding.\n\n\n### Register a fourth encoder\n\nWe will also add back in the original base64 encoded secret encoder for four total encoders:\n- The base64 encoded secret\n- The base64 encoded secret trimmed\n- The secret with the first character removed then base64 encoded and trimmed\n- The secret with the first two characters removed then base64 encoded and trimmed\n\nThis allows us to fully cover the most common scenario where a user base64 encodes their secret and expects the entire thing to be masked.\nThis will result in us only revealing length or bit information when a prefix or suffix is added to a secret before encoding. \n\n## Consequences\n\n- In the case where a secret has a prefix or suffix added before base64 encoding, we may now reveal up to 20 bits of information and the length of the original string modulo 3, rather then the original 16 bits and no length information\n- Secrets with a suffix appended before encoding will now be masked across the board. Previously it was only masked if it was a multiple of 3 characters\n- Performance will suffer in a negligible way\n"}
{"repositoryUrl": "https://github.com/Branchout/branchout.git", "path": "doc/adr/0002-language.md", "template": "Nygard", "status": "Accepted Testing [3. testing](0003-testing.md)", "firstCommit": "2018-12-01T08:27:56Z", "lastCommit": "2018-12-01T08:27:56Z", "numberOfCommits": 1, "title": "2. language", "wordCount": 55, "authors": {"name1": 1}, "content": "# 2. language\n\nDate: 2018-10-31\n\n## Status\n\nAccepted\n\nTesting [3. testing](0003-testing.md)\n\n## Context\n\nA language should be universal, simple and easily testable\n\nOptions\n* Shell\n* Go\n* Java\n* JavaScript\n\nThere should be very few dependencies\n\n## Decision\n\nShell\n\n* No dependencies\n* Installed pretty much everywhere developers are\n\n## Consequences\n\nTesting will be a learning curve - bats\nEnsuring portability - shellcheck\nAsync is a little awkward - xargs\n"}
{"repositoryUrl": "https://github.com/OpenTOSCA/container.git", "path": "docs/adr/0002-use-spring-dependency-incjection.md", "template": "Madr", "status": "accepted", "firstCommit": "2020-06-18T14:49:08Z", "lastCommit": "2020-06-18T14:49:08Z", "numberOfCommits": 1, "title": "Use Spring for Dependency Injection and discovery of plugins", "wordCount": 212, "authors": {"name1": 1}, "content": "# Use Spring for Dependency Injection and discovery of plugins\n\n* Status: accepted\n\n## Context and Problem Statement\n\nTo facilitate testing, a clean, object oriented architecture as well as the plugin systems for various components a configurable Inversion of Control (IOC) container is required.\nThis container is responsible for plugin discovery, as well as injecting services required by the API to serve it's \"external\" customers.\n\n## Decision Drivers \n\n* Support for a plugin system that can discover additional components not originally compiled into the deployed WAR\n* Support for minimal configuration, allowing easy modification and discovery by convention\n\n## Considered Options\n\n* Spring\n* Guice\n\n## Decision Outcome\n\nThe chosen IoC container is Spring, because it supports plugin discovery at minimal configuration and has easy support for servlet-based injection with `spring-mvc` and `spring-web`.\n\n### Negative consequences\n\nThe Plugins loaded cannot be adjusted at runtime.\nAt time of writing, no such capability is required or planned.\n\n## Pros and Cons\n\n### Spring\n\n* Well-Maintained and diverse IoC container supporting various configuration mechanisms\n* No support for changes in registration during runtime\n* Support for `javax.Inject` annotations as well as injection-site adaption through custom annotations\n* No direct support for servlet-based injection, but available as `spring-web`\n\n### Guice\n\n* No support for XML-Based configuration\n* No direct support for servlet-based injection\n* At time of writing seems to be discontinued by original maintainer Google\n"}
{"repositoryUrl": "https://github.com/marcusholmgren/crispy-dragon.git", "path": "docs/OPM-1-Decision-Tracking.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2021-02-03T19:42:04Z", "lastCommit": "2021-02-03T19:42:04Z", "numberOfCommits": 1, "title": "OPM1: Use ADRS for decision tracking", "wordCount": 159, "authors": {"name1": 1}, "content": "# OPM1: Use ADRS for decision tracking\nDate: 2021-01-28\n\n## Status\nAccepted\n\n## Context\nA microservices architecture is complex and we'll need to make many decisions.\nWe'll need a way to keep track of the important decision we make, so that we can revisit and re-evalute them in the future.\nWe'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.\n\n## Decision\nWe've decided to use [Michael Nygard's lightweight architectural decision record (LADR)](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) format.\nLADR is text based and is lightweight enough to meet our needs.\nWe'll keep each LADR record in its own text file and manage the files like code.\n\nWe also considered the following alternative solutions:\n* Project management tooling (not selected, because we didn't want to install tools)\n* Informal or \"word of mouth\" record keeping (not reliable)\n\n## Consequences\n* We'll need to write decision records for key decisions\n* We'll need a source code management solution to manage decision record files\n"}
{"repositoryUrl": "https://github.com/arachne-framework/architecture.git", "path": "/adr-014-project-templates.md", "template": "Nygard", "status": "PROPOSED", "firstCommit": "2016-10-21T13:26:53Z", "lastCommit": "2016-10-21T15:04:55Z", "numberOfCommits": 3, "title": "Architecture Decision Record: Project Templates", "wordCount": 909, "authors": {"name1": 3}, "content": "# Architecture Decision Record: Project Templates\n\n## Context\n\nWhen starting a new project, it isn't practical to start completely from scratch, every time. We would like to have a varity of \"starting point\" projects, for different purposes.\n\n### Lein templates\nIn the Clojure space, Leiningen Templates fill this purpose. These are sets of special string-interpolated files that are \"rendered\" into a working project using special tooling.\n\nHowever, they have two major drawbacks:\n\n- They only work when using Leiningen as a build tool.\n- The template files are are not actually valid source files, which makes them difficult to maintain. Changes need to be manually copied over to the templates.\n\n### Rails templates\n\nRails also provides a complete project templating solution. In Rails, the project template is a `template.rb` file which contains DSL forms that specify operations to perform on a fresh project. These operations include creating files, modifying a projects dependencies, adding Rake tasks, and running specific _generators_. \n\nGenerators are particularly interesting, because the idea is that they can generate or modify stubs for files pertaining to a specific part of the application (e.g, a new model or a new controller), and they can be invoked _at any point_, not just initial project creation. \n\n## Decision\n\nTo start with, Arachne templates will be standard git repositories containing an Arachne project. They will use no special syntax, and will be valid, runnable projects out of the box.\n\nIn order to allow users to create their own projects, these template projects will include a `rename` script. The `rename` script will recursively rename an entire project directory to something that the user chooses, and will delete `.git` and re-run `git init`, \n\nTherefore, the process to start a new Arachne project will be:\n\n1. Choose an appropriate project template.\n2. Clone its git repository from Github\n3. Run the `rename` script to rename the project to whatever you wish\n4. Start a repl, and begin editing.\n\n### Maven Distribution\n\nThere are certain development environments where there is not full access to the open internet (particularly in certain governmental applications.) Therefore, accessing GitHub can prove difficult. However, in order to support developers, these organizations often run their own Maven mirrors.\n\nAs a convenience to users in these situations, when it is necessary, we can build a wrapper that can compress and install a project directory as a Maven artifact. Then, using standard Maven command line tooling, it will be possible to download and decompress the artifact into a local filesystem directory, and proceed as normal.\n\n## Status\n\nPROPOSED\n\n## Consequences\n\n- It will take only a few moments for users to create new Arachne projects.\n- It will be straightforward to build, curate, test and maintain multiple different types of template projects.\n- The only code we will need to write to support templates is the \"rename\" script.\n- The rename script will need to be capable of renaming all the code and files in the template, with awareness of the naming requirements and conventions for Clojure namespaces and code.\n- Template projects themselves can be built continuously using CI\n\n### Contrast with Rails\n\nOne way that this approach is inferior to Rails templates is that this approach is \"atomic\"; templating happens once, and it happens for the whole project. Rails templates can be composed of many different generators, and generators can be invoked at any point over a project's lifecycle to quickly stub out new functionality.\n\nThis also has implications for maintenance; because Rails generators are updated along with each Rails release, the template itself is more stable, wheras Arachne templates would need to be updated every single time Arachne itself changes. This imposes a maintenance burden on templates maintained by the core team, and risks poor user experience for users who find and try to use an out-of-date third-party template.\n\nHowever, there is is mitigating difference between Arachne and Rails, which relates directly to the philosophy and approach of the two projects.\n\nIn Rails, the project *is* the source files, and the project directory layout. If you ask \"where is a controller?\", you can answer by pointing to the relevant `*.rb` file in the `app/controllers` directory. So in Rails, the task \"create a new controller\" _is equivalent to_ creating some number of new files in the appropriate places, containing the appropriate code. Hence the importance of generators.\n\nIn Arachne, by contrast, the project is not ultimately defined by its source files and directory structure; it is defined by the config. Of course there *are* source files and a directory structure, and there will be some conventions about how to organize them, but they are not the very definition of a project. Instead, a project's _Configuration_ is the canonical definition of what a project is and what it does. If you ask \"where is a controller?\" in Arachne, the only meaningful answer is to point to data in the configuration. And the task \"create a controller\" means inserting the appropriate data into the config (usually via the config DSL.) \n\nAs a consequence, Arachne can focus less on code generation, and more on generating *config* data. Instead of providing a _code_ generator which writes source files to the project structure, Arachne can provide _config_ generators which users can invoke (with comparable effort) in their config scripts.\n\nAs such, Arachne templates will typically be very small. In Arachne, code generation is an antipattern. Instead of making it easy to generate code, Arachne focuses on building abstractions that let users specify their intent directly, in a terse manner."}
{"repositoryUrl": "https://github.com/vwt-digital/operational-data-hub.git", "path": "architecture/adr/0060-lock-pip-requirements.md", "template": "Nygard", "status": "Accepted Implements [4. Create software defined everything](0004-create-software-defined-everything.md)", "firstCommit": "2020-09-21T14:31:15Z", "lastCommit": "2021-06-08T14:26:23Z", "numberOfCommits": 3, "title": "60. Lock pip requirements", "wordCount": 82, "authors": {"name1": 1, "name2": 2}, "content": "# 60. Lock pip requirements\n\nDate: 2021-06-08\n\n## Status\n\nAccepted\n\nImplements [4. Create software defined everything](0004-create-software-defined-everything.md)\n\n## Context\n\nCode Injection is a specific type of injection attack where an executable program statement is constructed involving user input at an attack surface that becomes vulnerable when it can be manipulated in an unanticipated way to invoke functionality that can be used to cause harm.\n\n## Decision\n\nTo prevent dependency injection attacks we decided to have both a requirements.in file and a [pip-tools/pip-compile](https://github.com/jazzband/pip-tools) generated requirements.txt\n\n## Consequences\n\nPlease check the 'Way of Working' document on [Confluence](https://recognize.atlassian.net/wiki/spaces/DAT/pages)\n"}
{"repositoryUrl": "https://github.com/psu-libraries/scholarsphere.git", "path": "doc/architecture/decisions/0004-blacklight-for-search-only.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-01-15T15:22:10Z", "lastCommit": "2020-01-15T15:22:10Z", "numberOfCommits": 1, "title": "4. blacklight-for-search-only", "wordCount": 144, "authors": {"name1": 1}, "content": "# 4. blacklight-for-search-only\n\nDate: 2020-01-15\n\n## Status\n\nAccepted\n\n## Context\n\nThere are two ways we can display works and work versions in Scholarsphere: 1) using the record that is in the \nPostgres database; or, 2) using the record that is in Solr.\n\n## Decision\n\nWe going to use the Postgres record for displaying individual records, leaving Blacklight's Solr record for displaying\nsearch results only. The Solr record, or SolrDocument, will not be used when displaying the detailed record for a\nwork or work version. It will only be used within the context of a list of search results.\n\n## Consequences\n\nBlacklight dependency footprint is reduced, i.e. we're relying on it to do less. However, we also loose some baked-in\nfeatures such as json API displays and other methods of reformating existing records, such as xml views, etc. That\ndoesn't mean we can't re-use them later.\n"}
{"repositoryUrl": "https://github.com/etienneleba/TDD-hexagonal-project.git", "path": "Docs/ADRS/2020_12_02_9_20_PROJECT.md", "template": "unknown", "status": null, "firstCommit": "2020-12-02T08:30:47Z", "lastCommit": "2020-12-02T08:30:47Z", "numberOfCommits": 1, "title": "Context", "wordCount": 52, "authors": {"name1": 1}, "content": "### Context\n\nThis is the beginning of the project \n\n### Decision\n\nI decide to create a _Docs_ folder to store all the documentation. \nIn this _Docs_ folder there will also have a _ADRS_ folder to store all the decision on this project. \n\n### Consequences \n\nThere will be a single point to store all the documentation "}
{"repositoryUrl": "https://github.com/Ensembl/ols-client.git", "path": "doc/adr/0003-looping-over-list.md", "template": "Nygard", "status": "Done", "firstCommit": "2018-07-26T13:24:31Z", "lastCommit": "2018-07-26T13:24:31Z", "numberOfCommits": 1, "title": "3. looping over list", "wordCount": 141, "authors": {"name1": 1}, "content": "# 3. looping over list\n\nDate: 2018-07-26\n\n## Status\n\nDone\n\n## Context\n\nWe want to be able to loop simply over Ontologies / Terms results, without bothering if a\nnew call is made to change page.  \n\nOLS API results are paginated, the page size is a parameter in Query.\nThere is no simple way to loop over all elements, and returning all results is not a solution, considering amount of data\nThe actual calls to API are hidden from final users.\n\n```python\n\nfrom ebi.ols.api.client import OlsClient\n\nclient = OlsClient()\nontology = client.ontology('fpo')\n\nterms = ontology.terms()\nindividuals = ontology.individuals()\nproperties = ontology.properties()\n\n# work with all 'list' item types\nfor term in terms:\n  # do whatever\n  print(term)\n\n# Direct List'like access on all list types\nterm = terms[1254]\nindividual = individuals[123]\n# ...\n\n```  \n\n## Decision\n\nTo Implement\n\n## Consequences\n\n- [~] The list must keep track of current loaded, therefore if initial request  \n- [~] ListMixin object are state-full. \n"}
{"repositoryUrl": "https://github.com/raster-foundry/raster-foundry.git", "path": "docs/architecture/adr-0027-annotate-project-STAC-export.md", "template": "unknown", "status": null, "firstCommit": "2019-08-22T18:36:31Z", "lastCommit": "2019-08-22T18:36:31Z", "numberOfCommits": 1, "title": "0027 Annotate Project STAC Export", "wordCount": 846, "authors": {"name1": 1}, "content": "# 0027 Annotate Project STAC Export\n\n## Context\n\nThis is an ADR documenting decisions made on MVP features of exporting Raster Foundry/Annotate label data and scenes in STAC-compliant catalogs. This aims to make us one step closer towards integrating STAC to the path of interoperability among Azavea’s Machine Learning workflow.\n\nIn Raster Foundry, we support creating geospatial labels based on images and scenes added to project layers. This feature is further extended in Annotate App, which is an application backed by Raster Foundry APIs, so that teams can work on an image labeling projects simultaneously within status-tracked tasks. To enable downstream Machine Learning work better in interoperating these ground truth labels, we have decided to implement a feature for exporting images and labels in STAC catalogs in an asynchronous manner. The following sections will go into details about the export process, the structure of the exported catalog, and where they are stored.\n\n## Decision\n\n### CRUD endpoints\n\nSimilar to image and scene exports in Raster Foundry, we have created CRUD endpoints for scene and label exports in the form of STAC catalogs by `api/stac`. More details about the API are in the [spec](https://github.com/raster-foundry/raster-foundry/blob/1.27.0/docs/swagger/spec.yml#L4802).\n\n1. Create\n\n`POST` JSON in the following shape creates an export record and kicks off the catalog building batch process.\n\n```json\n{\n   \"name\": \"Annotate project export test\",\n   \"layerDefinitions\": [\n  {\n   \"projectId\": \"<UUID of a project>\",\n   \"layerId\": \"<UUID of a project layer>\"\n  }\n   ],\n   \"taskStatuses\": [“<task status>”]\n}\n```\n\nOne or multiple objects for `layerDefinitions` may be supplied so that the exported catalog contains images and labels from one or many project layers. The creation will succeed only if the operating user has `VIEW` access to the project, and the layer exists within the project.\n\n`taskStatuses` should be one or multiple of `UNLABELED`, `LABELING_IN_PROGRESS`, `LABELED`, `VALIDATION_IN_PROGRESS`, and `VALIDATED`. These are enums marking statuses of tasks in the database according to Annotate frontend operations. Only labels spatially fall in tasks of these statuses are included in the exported catalog. The geometry of the exported STAC Label Collection and Item will exclude the unexported task areas.\n\n2. Update\n\nUpdate is only permitted for super users or export owners. Only `name`, `export_location`, and `export_status` are open for updates -- the latter two fields will be updated after the catalog is exported and stored on S3.\n\n3. List, get, and delete\n\nThere is nothing too special about these three, except that only super users or export owners are able to have valid results from these actions.\n\n### STAC catalog builder\n\nWe have created a STAC catalog export builder that will build the catalog in the following structure:\n\n```\nExported Catalog:\n   |-> Layer collection\n   |   |-> Scene Collection\n   |   |   |-> Scene Item\n   |   |   |-> (One or more scene items)\n   |   |-> Label Collection\n   |   |   |-> Label Item (Only one)\n   |   |   |-> Label Data in GeoJSON Feature Collection\n   |-> (One or more Layer Collections)\n```\n\nOne may think of an export as a snapshot of scenes and labels contained in the specified layers that fall into tasks marked with certain statuses at a certain time. So an exported Catalog may contain multiple Layer Collections with each layer being a project layer in Raster Foundry. A Layer Collection contains one Scene Collection and one Label Collection. A Scene collection has one or multiple Scene Items with assets pointing to `COG` resources on S3, and these resources are from the `ingest_location` field of scenes in Raster Foundry database. A Label collection contains one Label Item representing ground truth labels with an asset pointing to a GeoJSON Feature Collection of the label data and with links pointing to Scene items representing the labelled imagery.\n\nThe `id` of the export record is the `id` of the Catalog. A Layer Collection’s `id` maps to a layer in Raster Foundry database. Scene Items’ `id`s are also scene `id`s in Raster Foundry database. `id` field in each `Feature` of the `FeatureCollection` of the ground truth data are annotation IDs in Raster Foundry database. Other IDs are generated on the fly.\n\n### Async batch job\n\nWe have created jobs in AWS batch to build these exports in an asynchronous manner. A STAC export job is kicked off when a user successfully performs a `POST` with the above mentioned JSON to the `api/stac` endpoint from Raster Foundry.\n\n### Static catalogs on S3\n\nThe exported STAC catalogs live on environment specific Raster Foundry S3 buckets. The S3 locations are determined by the self links in all STAC resources from the export builder. Resources are linked to each other by absolute links currently, except that `root` links use relative links. In future work, we will update the links so that only `self` links are absolute, and the rest will use relative links.\n\n## Consequences\n\nThe STAC endpoints, export builder, and the export batch job transform scenes and labels from layers in Raster Foundry database to STAC resources stored on S3.\n\n## Future Work\n\nIn terms of areas of enhancement for future work and better interoperability, some of these may be considered: update absolute links to relative links wherever makes sense, reuse the email notification component and notify export creator about the resource on S3 when the export is ready, support providing masks on exports to create training, validation, test sets, etc.\n"}
{"repositoryUrl": "https://github.com/mlibrary/nebula.git", "path": "doc/adr/0001-dont-export-moku-init-execs.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-02-27T15:50:33Z", "lastCommit": "2019-02-27T15:50:33Z", "numberOfCommits": 2, "title": "1. Don't export `moku init` execs", "wordCount": 507, "authors": {"name1": 2}, "content": "# 1. Don't export `moku init` execs\n\nDate: 2019-02-15\n\nStatus\n--\n\nAccepted\n\nContext\n---\n\nLet's say we have ten applications (`app_1`, `app_2`, ..., `app_10`)\ndefined in hiera such that the deploy host can read them. Furthermore,\nlet's say we have named instances of some of them on four different\nproduction hosts, like this:\n\n```\n   +---+\n   | Deploy Host |\n   +---+\n\n+---+   +---+   +---+   +---+\n| App Host A: |   | App Host B: |   | App Host C: |   | App Host D: |\n| - app_1   |   | - app_1   |   | - app_1   |   | - app_1   |\n| - app_2   |   | - app_2   |   | - app_3   |   | - app_4   |\n+---+   +---+   +---+   +---+\n```\n\nSo only apps 1–4 are actually instantiated, and a couple of them are on\nmore than one host. In this setup, we need the deploy host to run `moku\ninit` once (and only once) for each of those four apps (and not at all\nfor the other apps which haven't yet been instantiated).\n\nIf the deploy host runs the command for each app it knows how to set up,\nthen it'll only run `init` once per app, but it'll run it for apps 5–10,\nwhich we do not want it to do.\n\nIf the app hosts export `exec` resources, then, in this example, the\ndeploy host would find 4 of the same command for `app_1`, 2 for `app_2`,\nand 1 each for `app_3` and `app_4`. It wouldn't find any for apps 5–10,\nbut it'd find too many for apps 1 and 2.\n\nIf the deploy host runs a puppetdb query to find all named instances to\nget a list of unique instance names, then it will get a list of apps\n1–4, which it could use to run `moku init` for each of them. This is the\ndesired behavior, but our experience with puppetdb queries is negative:\nthey are very hard to test, and they are very ugly and hard to read.\n\nIn addition to all this, the `moku init` command requires that a json\nfile exist on the deploy host for each application (based on its\nhieradata). Among other things, this json file contains a hash, keyed on\ndatacenter, of lists of application hosts.\n\nIt is unacceptable for the deploy host to run `moku init` with an\nincomplete list of application hosts.\n\nDecision\n--\n\nWe will run `moku init` by hand but have puppet manage the configuration\njson. Also, the init command will accept application hosts to be keyed\non hostname instead of datacenter, so instead of:\n\n```yaml\ndeploy:\n  sites:\n  hatcher:\n  - node_1\n  - node_2\n  macc:\n  - node_3\n  - node_4\n```\n\nWe'll have:\n\n```yaml\ndeploy:\n  sites:\n  nodes:\n  node_1: hatcher\n  node_2: hatcher\n  node_3: macc\n  node_4: macc\n```\n\nThat way, we don't need to supply separate order numbers for each\ndatacenter or try and come up with a way to do anything clever with the\nway concat fragments handle json. Each named instance can simply export\na line with its hostname and datacenter and be done.\n\nConsequences\n--\n\nWe'll have to run moku init by hand, but subsequent configuration has to\nbe done by hand anyway, and it's still helpful for puppet to set up the\njson with truthful information about which hosts are ready to serve an\napplication.\n"}
{"repositoryUrl": "https://github.com/alphagov/verify-service-provider.git", "path": "docs/adr/0016-we-will-have-a-healthcheck-endpoint.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-07-28T14:34:27Z", "lastCommit": "2017-10-09T14:04:26Z", "numberOfCommits": 2, "title": "16. We will have a healthcheck endpoint", "wordCount": 129, "authors": {"name1": 1, "name2": 1}, "content": "# 16. We will have a healthcheck endpoint\n\nDate: 2017-07-25\n\n## Status\n\nAccepted\n\n## Context\n\nIn various user research sessions we've observed users start the MSA or the verify service provider\nand then want to check whether it's working correctly. There's also a need for users to be able to\nmonitor the health of the system once it's deployed to their environment.\n\nDropwizard allows you to configure an HTTP endpoint as a healthcheck. This can perform some arbitrary\nactions that check the health of the system.\n\n## Decision\n\nWe will have a healthcheck endpoint that will check the verify-service-provider can read metadata from\nthe hub and the MSA.\n\n## Consequences\n\nUsers will be able to check and monitor the health of the verify-service-provider.\n\nWe will have to add some healthchecks.\n\n"}
{"repositoryUrl": "https://github.com/vwt-digital/operational-data-hub.git", "path": "architecture/adr/0028-a-solution-is-implemented-by-one-or-more-gcp-projects.md", "template": "Nygard", "status": "Accepted Implements [17. DDD defines ubiquitous language](0017-ddd-defines-ubiquitous-language.md) Implements [26. Solution facilitates a coherent set of business functions](0026-solution-facilitates-a-coherent-set-of-business-functions.md) Related to [27. A GCP project belongs to a single domain](0027-a-gcp-project-belongs-to-a-single-domain.md)", "firstCommit": "2020-09-21T14:31:15Z", "lastCommit": "2020-10-16T06:52:42Z", "numberOfCommits": 2, "title": "28. A solution is implemented by one or more GCP projects", "wordCount": 115, "authors": {"name1": 1, "name2": 1}, "content": "# 28. A solution is implemented by one or more GCP projects\n\nDate: 2020-09-21\n\n## Status\n\nAccepted\n\nImplements [17. DDD defines ubiquitous language](0017-ddd-defines-ubiquitous-language.md)\n\nImplements [26. Solution facilitates a coherent set of business functions](0026-solution-facilitates-a-coherent-set-of-business-functions.md)\n\nRelated to [27. A GCP project belongs to a single domain](0027-a-gcp-project-belongs-to-a-single-domain.md)\n\n## Context\n\nA [solution facilitates a coherent set of business functions](0026-solution-facilitates-a-coherent-set-of-business-functions.md). Those functions can originate from multiple domains. A [project always belongs to a single domain](0027-a-gcp-project-belongs-to-a-single-domain.md). Therefore, a solution can be implemented by multiple projects, either due to the fact that it requires functions from multiple domains, or because projects allow better modularization of the solution, or both.\n\n![Structure of projects, domains and solutions](solution_project_domain.png \"Projects in different domains implementing a solution\")\n\n## Decision\n\nWe implement a solution by one or more projects.\n\n## Consequences\n\n### Advantages\n\n* The platform's project structure is used to facilitate separation of concerns and modularization.\n* Responsibility for functionality is easily defined along the project boundaries.\n\n### Disadvantages\n\n* Additional overhead when multiple small projects need to be created as functionality from multiple domains is required.\n"}
{"repositoryUrl": "https://github.com/MITLibraries/thing.git", "path": "docs/architecture_decisions/0003-deploy-via-heroku-pipelines.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-12-04T17:25:39Z", "lastCommit": "2017-12-04T17:25:39Z", "numberOfCommits": 1, "title": "3. Deploy via Heroku Pipelines", "wordCount": 77, "authors": {"name1": 1}, "content": "# 3. Deploy via Heroku Pipelines\n\nDate: 2017-12-01\n\n## Status\n\nAccepted\n\n## Context\n\nInitially this project was not appropriate for deploy on Heroku because it\nneeded MIT Touchstone authentication. However, based on\n[ADR-0002 Authentication via Touchstone SAML](0002-authentication-via-touchstone-saml.md)\nwe are now able to remove the `mod_shib` requirement that initially prevented\nus from using Heroku.\n\n## Decision\n\nWe will use Heroku Pipelines for Staging / Production and PR builds.\n\n## Consequences\n\nWe will have CI / CD for this application in an environment we have proven in\nproduction on previous applications.\n"}
{"repositoryUrl": "https://github.com/alphagov/monitoring-doc.git", "path": "documentation/architecture/decisions/0010-packaging-node-exporter-as-deb-for-verify.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-08-15T11:04:12Z", "lastCommit": "2018-11-06T16:16:27Z", "numberOfCommits": 3, "title": "10. Packaging Node Exporter as .deb for Verify", "wordCount": 107, "authors": {"name1": 1, "name2": 2}, "content": "# 10. Packaging Node Exporter as .deb for Verify\n\nDate: 2018-08-15\n\n## Status\n\nAccepted\n\n## Context\n\nNode Exporter needs to be installed on Verify's infrastructure so that machine metrics can be gathered.\nVerify runs Ubuntu Trusty which does not have an existing node exporter package.\nVerify has an existing workflow for packaging binaries which can be leveraged to package node exporter. \n## Decision\n\nNode exporter will be packaged as a deb using FPM following Verify's exiting packaging workflow.\n\n## Consequences\n\nThe use of Verify's infrastructure ties the Node exporter package to Verify, the node exporter would need to be repackaged for other programs to be able to be used.\n"}
{"repositoryUrl": "https://github.com/nicfix/py-ddd-cqrs-microservice-boilerplate.git", "path": "docs/adr/tooling/004-license_checker.md", "template": "Nygard", "status": "proposed", "firstCommit": "2020-05-23T11:07:51Z", "lastCommit": "2020-05-23T11:07:51Z", "numberOfCommits": 1, "title": "Use an automated license checker", "wordCount": 169, "authors": {"name1": 1}, "content": "# Use an automated license checker\n\n## Status\n\nproposed\n\n## Context\n\nWorking with OpenSource software for commercial purposes requires\nevery developer to check the licenses of the packages that he uses.\n\nSome licenses might be not-compatible with the project needs and\ncause some legal issue if not properly handled.\n\nUsing open-source libraries speeds up development in a \nsignificant way but has a drawback, nested dependencies.\n\nEach open-source library could depend on further open-source libraries\nthat could have different licenses for usage and re-distribution.\n\n## Decision\n\nGiven the previous concerns I suggest to use an automated\nlicenses checker on the installed packages.\n\nIn python you can use `pip-licenses` (which is distributed with MIT license)\n\n\n## Consequences\n\nAnytime a new library is added to the project a developer\ncan verify if a new/conflicting license has been added with that\nlibrary or with a nested dependency.\n\nThis allows the developer to take counter measures that can vary\nfrom mentioning the library in the list of open source technologies \nor deciding to not use that library at all."}
{"repositoryUrl": "https://github.com/raster-foundry/raster-foundry.git", "path": "docs/architecture/adr-0002-frontend-framework.md", "template": "unknown", "status": null, "firstCommit": "2016-07-19T18:10:27Z", "lastCommit": "2016-07-19T18:10:27Z", "numberOfCommits": 1, "title": "(sem título)", "wordCount": 1672, "authors": {"name1": 1}, "content": "Frontend Frameworks\n===\nContext\n---\n### Background\nBased on the advice of the Phase I team, we will be starting largely from scratch for the frontend code for this project. Many design elements are expected to be carried over from Phase I, but they will largely be rewritten to be more maintainable and extensible going forward. Therefore, we need to make a decision about which technologies to use for the frontend code.\n\nThe overwhelming majority of our team's frontend experience is with Angular 1.x. However, there are strong indications that we will need to migrate away from Angular 1.x eventually. Angular 2 is currently in RC, and represents essentially a complete rewrite of the framework. Once Angular 2 is released, we expect the Angular 1.x ecosystem to gradually decline in size and quality as libraries and developers shift to Angular 2. If we wrote this project's frontend in Angular 1.5 to start, we would almost certainly need to migrate away from Angular 1.5 at some point in the lifetime of the project, although whether that point would be in one year or five years is currently unclear. We expect that migration to be a complex and time-consuming task for anything other than a trivial application. This adds significant expected maintenance costs to an Angular-1-based approach, which has been our default for the past two years or so. In addition, our experience with Angular has revealed pain points that have led many team members to express a desire to try something new.\n\nGiven that migrating to a new framework seems inevitable within the lifespan of this project, it makes sense to consider alternatives to Angular 1.x now. There seem to be four possible paths which we could take:\n- Write Angular 1.5, upgrade later (the default if we change nothing now)\n- Switch to Angular 2 now for new development\n- Switch to React\n- Switch to Ember\n\n*Implicit in this is a rejection of more \"exotic\" approaches such as Elm, ScalaJS, and Aurelia, which are immature, have small ecosystems, and/or add significant complexity to our development pipelines at a time when we already expect to be learning some kind of new framework.*\n\n### Convergence\nDespite the multiplicity of frontend web frameworks, the story over the last year or so among the big three web frameworks (Angular, React, and Ember) has been one of convergence. All three are moving toward use of a \"next-generation\" syntax plus transpilation, with Angular defaulting to TypeScript, Ember selecting ES2015, and a variety of approaches, including Clojurescript, used with React.\nSimilarly, Angular 2 will introduce a new Angular-CLI which mimics many of the scaffolding features available from Ember's Ember-CLI. Angular 2 will bring significant improvements in page update speed over Angular 1 by adopting an approach similar to React's diffing algorithm, and Ember recently rolled out a new update engine, called Glimmer, which takes a similar approach. Ember has also adopted a strong component-based convention and one-way data flow by default, mimicking React, which Angular 2 will also adopt.\n\nTherefore, the differences between these frameworks are rapidly becoming less about the availability or lack of certain built-in features, and more about ecosystem quality and maturity, project goals and governance, and focus on a pleasant development experience.\n\n### Assessment\n#### 1. Angular 1.5 -> 2\nThis approach would have the lowest startup cost because we would be able to get started right away. In order to make the eventual transition to Angular 2 as easy as possible, we would have to adhere to the recommended Angular style guide during the course of Angular 1 development, although this largely matches our preferred Angular style already. The recommended way to migrate is then to introduce a 1-to-2 interoperability module into the project which upgrades or downgrades Angular 1 and 2 components so that they can talk to each other, and then to rewrite components piece by piece while using Angular 1 and 2 side-by-side.\n\nThe continuing support situation for Angular 1.5 after the release of Angular 2 is currently unclear. Google has provided assurances that they will not cease development for Angular 1.x until the \"vast majority\" of users have switched to Angular 2, but it's unclear when this will happen or what their definition of the \"vast\" majority is. We do not want to be left as members of the Angular 1.x-using minority when Google stops development for it.\n\n#### 2. Angular 2 now\nA way to avoid the problems with migrating to Angular 2 later in the life of the project is to begin using Angular 2 immediately, even in its RC state. This would gain us all the benefits of Angular 2 right from the start, and would allow us to learn the framework while the project is still small. It would also give us first-class usage of TypeScript, something which the other frameworks in this list lack.\n\nThe primary downside to Angular 2 is that it is still a young project; it can be expected to have bugs, usability and documentation issues, and a lack of libraries and community literature for the near future. There's also an unlikely but real possibility that the framework suffers from unknown design flaws that won't be exposed until it has received more real-world usage. Given Angular's popularity and backing by Google, we can assume that these issues will be ironed out eventually, but they will place a cost on development for at least the next year or two.\n\n#### 3. React\nThe React ecosystem is mature, and other teams at Azavea use it. One key difference between React and Angular / Ember is that React itself is not a full-featured framework: it is a library for generating components that communicate. Other modules, such as Flux or Redux, are required (or at least almost universally used) to build up a full frontend architecture.\n\nThis modular architecture runs somewhat counter to this team's framework preferences. Historically, we have preferred full-featured, opinionated frameworks over assembling collections of components, because our experience is that reducing the number of components in our application stack usually improves reliability and eliminates development headaches.\n\nThat said, opinionated frameworks can sometimes get in the way when we try to do something unusual, but our experience has been that as long as this is a rare occurrence, it is a price worth paying.\n\nStructural issues aside, React has a healthy ecosystem with numerous libraries and has been used by other teams at Azavea before, so we would be able to leverage some of that knowledge if we run into issues.\n\n#### 4. Ember\nEmber is a full-featured, opinionated MVC web framework that is quite mature in terms of features. Indeed, as mentioned above, Angular 2's Angular-CLI is heavily inspired by Ember's Ember-CLI. Similar to Angular 2, Ember has been working to incorporate more React-inspired features, including one-way dataflows by default and a faster differential page update engine. However, in contrast with Angular, Ember has already completed and rolled out these updates in Ember 2, which was released about a year ago. In addition, Ember did this while largely maintaining backwards compatibility, rather than rewriting the entire framework. The current release is 2.6.\n\nThe Ember-CLI offers impressive scaffolding capabilities -- it provides scaffolding for all common application components, including routes and tests, and the default project comes with a testing environment already set up and working. Ember-CLI also includes a package manager for adding community modules to one's project.\n\nEmber uses ES2015, which is a big step up from ES5, but which lacks the type safety possible with TypeScript in Angular 2. There is a plugin for using TypeScript with Ember but this would likely be imperfect.\n\nEmber is less well known than React and Angular, which results in a smaller ecosystem of literature and libraries. However, it is not a niche framework by any means. StackOverflow has about 19,000 questions tagged with \"ember.js\", as compared to 18,800 for \"reactjs\". Angular, however, has 185,600 (!) questions tagged  for \"angularjs\".\n\nDecision\n--\nThis project carries with it a large number of complex technical issues and new technologies related to raster processing. At the initial stages of the project, we will be more productive if we do not have to learn a new frontend framework alongside development of a complex backend architecture. This weighs in favor of sticking with what we know to start out. However, the long-term support situation for Angular 1 is currently unclear; it seems to be at least partially tied to community usage of Angular 1, which is tough to predict. Additionally, Angular 2 and Ember (and potentially more exotic frameworks such as Elm) appear to offer significant benefits over Angular 1.x such as improved page rendering speed and better developer ergonomics.\n\nRaster Foundry will initially use Angular 1.5, following the style recommended by the [upgrade guide](https://angular.io/docs/ts/latest/guide/upgrade.html). Additionally, we will invest in our long-term productivity and hedge against decay in the Angular 1.x ecosystem by devoting resources toward an incremental upgrade process starting around the beginning of the second quarter of development work (roughly, mid-November, 2016). This process is expected to initially consume no more than 10% of per-sprint development points. Although the exact first steps in an incremental upgrade process will need to be decided, some possibilities might include:\n- Introduce a TypeScript or ES2015 transpilation layer to the existing project and begin rewriting existing code in one of these two languages.\n- Identify existing functionality that is well compartmentalized, and rewrite it in another framework or frameworks to assess ergonomics and interoperability.\n- Identify a planned component that would benefit from the strengths of a different framework and write it in that framework.\n\nConsequences\n--\nAll developers will need to thoroughly read and familiarize themselves with the [Angular 2 Upgrade Guide](https://angular.io/docs/ts/latest/guide/upgrade.html). We will need to track development of the Angular 2 and 1 ecosystems. We will need to work to strike a balance between investment in new features and migrating away from Angular 1 to avoid accruing technical debt. We will likely need to develop temporary systems for doing development on Angular 1 alongside Angular 2 or another framework within the same frontend application.\n"}
{"repositoryUrl": "https://github.com/michelezamuner/smjs.git", "path": "docs/system/decisions/2019020401-handling-generic-input-output.md", "template": "Nygard", "status": "Proposed", "firstCommit": "2019-02-08T19:51:06Z", "lastCommit": "2019-04-06T16:43:34Z", "numberOfCommits": 6, "title": "Handling generic Input/Output", "wordCount": 5868, "authors": {"name1": 6}, "content": "# Handling generic Input/Output\n\n\n## Context\n\nCurrently we use a single View from the Presenter, to render the normal output of the use case, that is bound to the View interface in the general provider. However, we're not handling different views that might be needed, nor error messages, which are directly written to the STDERR of the application.\n\nRegarding the former, we're not allowing the user to choose the output representation she prefers to get. Regarding the latter, if we're decoupling the presenter from the controller to support segregation between input and output devices, we cannot then just assume that the input device will also be able to support output, like we're doing now.\n\n\n## Decision\n\n### Segregation of input and output devices\n\nWe assume that a generic application can be connected to several different input and output devices: this means that input may come from different input devices (for example keyboard and mouse), and the output may go to different output devices (for example `STDOUT`, a GUI, and the filesystem). Of course this requires us to drop the assumption that all usual Web applications make, that the input device is the same as the output device (a Web browser, or maybe a Web/application server to be more precise).\n\nOne of the objectives of modern architectures is exactly allowing multiple different output devices to be used by the same application, and also from the same input device. To allow this, it's necessary that the controller is oblivious of the specific output device that will be used: for example, the controller shouldn't know if the output is going to a GUI, a CLI, a log, an audio speaker or a combination of these.\n\nIt's also important to mention that the strict separation between input and output is an application-level concern, which expects input to come from an interface (input boundary), and to send output to a different interface (output boundary). This is important because the application doesn't need to care if the device providing the input is the same as the one receiving the output or not. However, in most cases the input device will also support some kind of output, at least to display errors of input validation, and it would be very weird if the input came from a device, and the output were sent to a different one, because the user using the first device would expect to see the output there as well. However, this kind of architecture easily supports also cases where the input device has no output capability, and the output is to be sent somewhere else (like a game controller and a monitor).\n\n### Controllers and presenters\n\nWhile a use case interactor performs its application logic, it may not need to perform a single presentation, for two reasons:\n- there are many different kinds of presentations, that will likely need to be handled differently, like main successful output, and error output\n- there are some presentations that will be produced immediately, like input validation results, and some that may be produced asynchronously, like command execution results\n\nFor this reason, the interactor will in general have to define several different presenter interfaces, instead of just one, and juggle the various presenters according to the application logic that is unfolding.\n\nNow, we're describing this on the grounds that the presenters will be injected into the interactor. Now, in theory it's possible to achieve the same by returning a single response from the interactor, instead of letting the interactor call the presenters. This would actually work, and keep the dependency inversion principle respected, since the interactor still knows nothing of the details of presentation, however:\n- we need to stuff all possible response cases into a single response object, instead of cleanly separate the various cases\n- the controller will need to parse the response, understand what case it's about, and call different presenters according to it, adding much more responsibility to the controller than just having to translate the input data from the adapter format to the application format; additionally, the controller would really be just unpacking the same information that was packaged by the application, i.e. that a certain presenter needs to be called in a certain output situation, and the controller cannot deviate from this logic either, otherwise it would be taking presentation responsibility\n- the single response object could be returning asynchronous output, in addition to synchronous one, so the controller will also need to check which part of the response should be treated as asynchronous (like attaching a callback to a promise), and which as synchronous\n- even if the interactor was a coroutine (avoiding the problem of creating a single response for all cases), the controller would still need to check the kind of output at each return, and do the job of the interactor of associating a presenter to that kind of output, while the interactor already knows this information in the first place, in addition to still having to handle asynchronous output\n\nFor these reason, although returning the response from the interactor is technically feasible (and in certain scenarios it's certainly the best solution), it cannot be chosen as the general approach to use: rather it should be regarded as a special case, that works best only in specific situations.\n\n### Presenters and views\n\nEach specific presenter that can be used to implement the output port represents a specific output model, which is characterized by the set of view fields and data type conversions it supports, or in other words, the kind of view model it will produce. For example, a \"screen\" view model might convert the application response to a certain set of fields of certain type, with the intent of having them being displayed to a screen, while a \"print\" view model might produce from the same application response a different set of fields, of different types, specialized for being sent to a printer.\n\nStill, the same view model, representing a certain output model, can be rendered on the selected output device in different ways: a \"screen\" output model, containing fields belonging to a screen representation, might still be rendered as an HTML page, or as TXT file, or again as a graph. All these alternatives are represented by different views. This means that a specific presenter component, for example the \"screen\" presenter component, will define a \"screen\" view interface, that will then be implemented by multiple actual \"screen\" view instances, like HTML screen view, graph screen view, etc., one of which will then be selected and injected in the presenter, to be used for rendering.\n\n### Input definition\n\nWe borrow a definition of \"input\" from the REST standard. According to this definition, an input will be made of:\n- a resource identifier (for example `/path/to/my/resource`, URI in the Web context)\n- some input parameters (for example `?some[]=value&some[]=other`)\n- a resource representation (for example `.html`, `.pdf`, etc.)\n\nthis means that:\n- the identifier is used to pick a couple of controller and action, that needs to be called to perform the requested use case\n- the parameters are just passed to the action when it's called\n- the representation is related to the specific view that will be used to render the output generated by the use case's presenter; additionally, once a view is selected, it's presenter is also chosen, because a presenter might have multiple views, but a view only belongs to one presenter\n\nFor example, let's assume we have an `entitiesList` control widget, which displays a specific representation of a list of entities. When we trigger the `update` event on the `entitiesList` control widget, a very specific input can be created: the identifier would reference the controller and the action, like `entitiesController.update`; the input parameters could perhaps be the name of a subset of entities that this specific widget is configured to represent; finally, the representation is related to the specific widget that is sending this input, meaning that the same resource needs different widgets to be represented with different views.\n\n### Using a router\n\nThe interesting problem that arises is that the view to be chosen is known at input time, because it's related to the representation: this may lead to think that it should be known by the controller. However, the controller should be independent from any choice regarding the output, including which representation is used.\n\nA solution can be to introduce a *router* component, that takes an input, and chooses a combination of *controller*, *action* and *view* according to the given input. This is much like what happens with usual MVC Web framework, but generalized for any kind of adapter. This choice made by the router can be statically configured, since it does not depend on the user input.\n\nThe input part is quite easy, in that the router needs just to create the right controller object, and call the right action method on it, passing the right parameters. The output part is trickier though: since the controller is decoupled from the presenter, it doesn't return any output object that can be then handled by the router, rather it forwards the execution to the presenter object, in a way that isn't in control of the router. What the router can do, though, is to select the correct view to be used, and use the container to bind it to the generic view interface. This way, when the presenter will be built, the right view object will automatically be injected into it.\n\n### Main output and side output\n\nA typical application can have many different kinds of output. One of these is used to talk back to the user that provided the input: we can call it *main output*, and it would be produced by the selected use case. The *side output* would be any other kind of output produced by the application, like logging, monitoring, etc., which is not directly related to the input, and usually is not expected to be received by the user who produced the input.\n\nWhile the main output is handled with a presenter, side output can be handled by using a message bus to send it to a non-specified recipient from the application layer: inside the adapter, then, one or more configured handlers will receive the messages, and output them to the configured devices.\n\nAs a case study, we can consider the verbose output of a console application. At first, this may seem a case where some side output (info logs) need to be redirected to the main output (console STDOUT). What's really happening, though, is that the side output is being produced in a way that is completely unrelated to the presenter and views used by the main output: messages are sent containing the side output, and then in the adapter a new handler is configured to listen to these messages, and print them to the same output device used by the main output. This way we can print to the same output device, but keep the two kinds of output completely independent.\n\n### Handling errors\n\nWe can identify three broad categories of errors: application errors, input errors and system errors. Errors generated by domain or application services can be caught inside the use case, and passed back to the presenter in the response. Input errors are related to wrong input, or input that doesn't pass validation, and they are again checked inside the application interactors: they shouldn't be handled by controllers, because controllers have no way to select a specific output device to forward errors to, and additionally it's better if controllers stick to their single responsibility of translating data using adapter-specific format, into request data using application-specific format; on the other hand, we can say that the use case interactor is taking the role that so-called \"controllers\" have in traditional Web applications, meaning validating input, using proper services to produce an output, and send the output to the proper presenter/view. Finally, system errors are programming or configuration errors that may happen in the adapter-specific code, and that are generally caught during development, but which may still happen in production, and make the application crash: since these errors might happen outside of the application logic, they cannot be handled by the application service, like application and input errors, but still they should be caught and displayed on an output device to notify the user.\n\nSystem errors, not being generated inside the use case, don't concern any use case, and as such they don't need to be handled by any presenter. The way to handle them is to let any piece of code that is catching them sending a message to a widget that is responsible for displaying error messages, without needing to go through any application logic. Errors generated by domain or application services can be caught inside the use case, and passed back to a specific error presenter, which in turn will be configured with a specific view to be used in that situation.\n\n### User interface widgets\n\nAny user interface, whether graphical or not, is composed of widgets, representing specific contexts for user interaction and presentation of data.\n\nUser interfaces are meant to be frequently changed, so they need to be as flexible as possible. To support this, widgets have to be designed so that they are as independent, reusable and replaceable as possible. A key tool to achieve this is using an events mechanism: instead of knowing of the existence of a specific widget, and calling a specific method on it, we send an event, and widgets that are interested in that event will respond to it with some action. This way we can easily replace widgets, or add new ones, without disrupting existing functionality.\n\nEvents can be generated by the domain, the application, or the widgets themselves. Thus, widgets must be allowed to know about all existing events. Of course who generates an event depends on where the event belongs to: if an event represents the completion of a domain task, then the domain will need to send it, whereas if an event represents the change of state of a graphical widget (not related to any application or domain concern), then the widget should send it.\n\nSometimes, however, it's not possible, or not convenient, to interact with widgets using events. For example it can happen that we need to be sure that a specific widget is actually called, but the event system doesn't ensure that any widget will respond to the event: for example if we want to display an error message, it doesn't make sense to send an event, hoping that a message box widget will catch it, rather we want to make sure that the message is actually displayed the way we need. To support these cases, widgets' actions must also be callable directly.\n\nWidgets can then be just regular objects, exposing a set of methods: these can both be registered as event handlers, to be automatically called when certain events are caught, and be called directly on the object itself. This way we can take into account any possible combination of input and output methods, without creating a taxonomy of widgets that can have input and output, or only input and not output, etc., which is not really relevant on the user interface layer.\n\n### Widgets and use cases\n\nThe model of the communication with the application layer (i.e. with a port) distinguishes controllers from presenters, where the first are responsible for handling input, and the second are responsible for handling output. The reason why the two must be separated is that the presenters need to be injected into the use case interactor, since it may need to call different kinds of presenters in different situations, and also in an asynchronous fashion, and these are concerns of the application layer, not of the adapter.\n\nOf course the use case interactor knows nothing of which widgets are used in the adapter, nor of the kind of user interface there is, so it cannot decide to update widgets: rather, the interactor defines some presenter interfaces representing the kinds of output it needs to perform, and then it is injected with the presenters of the actual widgets that are selected to be used.\n\nHowever, from the point of view of the adapter layer (user interface), we don't necessarily need to separate controller objects from presenter objects. For example, an \"entity list\" widget might have a \"load\" controller action, that can be called directly to load entity data from the application, or as response to an event: this action will delegate to the \"list entities\" use case interactor, which will go through the query model to fetch the entities, and then call the given presenter to display them. In this case, the presenter will be the \"entity list\" widget itself, having a \"fill\" method for example, to fill the list with data. From the point of view of the interactor, it's enough that the entity list (or an adapter of it) implements the interface required by the interactor.\n\nIn the previous example the use case interactor will likely need to be injected also with an error presenter to display possible errors: this presenter can perhaps be implemented by a message box widget. Now, since the list widget will be injected with the interactor, it won't be required to know of the existence of the message box widget, nor of the fact that the list widget is used as a presenter itself, because these things are decided in the application configuration (the provider).\n\nAdditionally, this is one of those situations where a method should only be called while going through the application layer: we cannot call the \"fill\" method with random data, because the list widget need to contain domain entities that have gone through the application logic. However, since this method is public (whether it is meant to be just an event handler or not), nothing prevents some other object, or widget, from directly calling this method passing in somer random data, so we are left to the discipline of the implementer to remember to never call this method directly, but only as a handler of the proper application event. This risk is however mitigated by the fact that we're anyway following the discipline of letting the controller be known only by those few objects that really need to call it directly, by virtue of dependency injection: for example, the main application widget will likely need to use the entity list widget to call \"load\" or \"show\" on it at startup, but this is likely the only object that will have a reference to it.\n\nRegarding widgets that display data, each widget is a specific view, or representation, of the data: this means that we can have different widgets (even at the same time) representing the same data, and thus using the same presenter. This means that presenters and controllers are not really part of the widgets themselves, but are actually distinct objects, that are used by one or more widgets. For example, we may have multiple different widgets dealing with weather data: one would display a graph of temperatures over time, another will display a geographical map with different colors for different temperatures, etc. All these widgets will need to use the same presenter, which converts the entities data coming from the \"show temperatures\" use case, to simple values: then, each different view will use the same values differently. So, when the graph widget receives the signal that temperatures have been updated, it uses the shared \"show temperatures\" controller to trigger the use case, which will have the presenter injected: now this presenter must be able to communicate with multiple different views at the same time, and the actual views may change during the execution, because we may hide some widget for example. So, the view interface used by the presenter won't be an actual object interface, but just a messaging protocol, and the interactor will be injected with a message bus, and calling the interface would mean sending a \"present\" message, to which any number of actual views can respond.\n\nA similar situation happens when the same controller is called by multiple event handlers, or methods, of the same widget, like \"load\" and \"update\" on the entities list: both will have to go through the \"list entities\" controller action, delegating to the same use case interactor.\n\nThe application has two ways to communicate with the widgets. The most direct one is just calling the presenters that have been injected. However, the application can also send messages, to which widgets may respond. However, since calling a presenter may also be done by sending messages (in case we need to support concurrent presenters), there's not much difference between the two from a technical perspective. The real difference lays in the fact that presentation methods are meant to communicate to the adapter that the main output has been produced, while any other message is meant to communicate side events that happened, and to which someone may want to respond. If we're injecting a presenter object, it's important that the main output is communicated through that object, and not through sending messages, to keep the intent of the communication clear. If we're using a message bus to communicate with presenters too, it's important that messages are properly named, to make it clear which are sending back the main output.\n\n### Examples of presentation situations\n\nA common situation is the one where we create a new entity, from a view that doesn't need to be updated to display the updated entity data:\n\nUse case, \"create order\":\n- given I'm in the checkout view\n- and the cart has valid information\n- when I request to create a new order\n- then a new order is created with the cart information\n- and I am notified of the successful order creation\n\nUse case \"error creating order\":\n- given I'm in the checkout view\n- and the cart has invalid information\n- when I request to create a new order\n- then no new order is created\n- and I am notified of the error with the cart information\n\nLet's say we have a GUI adapter, with a checkout window (view) and an orders window (view). The checkout window will have a \"create order\" control with a method that takes the data from the \"cart\" widget, crafts a use case request from it, and uses it to call the use case interactor, which will have been already injected with a \"message box\" widget as both error presenter and result presenter. When the interactor is called, it first does validation of the request data, immediately calling the error presenter if the validation failed, or using the command model to request the creation of a new order otherwise. The command model will be executed asynchronously, to avoid blocking the user interface: this will create the order, update the storage, and send a message when the operation is completed. This message is caught by the interactor again, which uses the result presenter (asynchronously here) to notify that the order has been succcessfully created. At this point, the user can use the \"list orders\" use case to open the orders window, whose interactor will just read the data from the denormalized storage and present it.\n\nLet's see now a case where we create a new entity in the same view where all entities are listed:\n\nUse case, \"create todo item\":\n- given I'm in the todo list view\n- and the item form has valid data\n- when I request to create a new todo item\n- then a new todo item is created with the given information\n- and the todo list is updated to show the new todo item as well\n\nUse case, \"error creating todo item\":\n- given I'm in the todo list view\n- and the item form has invalid data\n- when I request to create a new todo item\n- then no view item is created\n- and I am notified of the error with the item information\n\nHere we have a single window, which is the todo list view, with the following widgets: an \"item\" form, a \"create new item\" button, a todo list, an error box, and a loader widget. When the user loads the window, the \"list items\" use case is performed, meaning that an input is sent to the \"load\" action of the todo list, which will create a request for the \"list items\" interactor, which will be injected with the list presenter, and will use the query model to get all the currently stored items from the denormalized database, which will be used to create a response, which will be sent to the todo list presenter to be displayed in the widget. Then, when the user clicks the \"create new item\" button, its controller will read the data from the form, craft a use case request with it, and send it to the \"create item\" interactor, which will be pre-injected with the error presenter, and the result presenter: if the request data is invalid, the error presenter will be sent a response containing the validation errors, and the interactor will return, otherwise the interactor will asynchronously call the command model to create a new todo item, then call the result presenter to signal that the request has been accepted (this could perhaps trigger a loading animation to start), and finally call the result presenter again to signal that the request has been completed, from inside the event handler of the command model completion event. In the meanwhile, the todo list controller was already register to respond with the \"update\" action to the message that is sent by the command model after the creation of an item is completed, so as soon as the command model signals the termination of the job, the todo list controller is called, triggering the \"list items\" use case again to update the todo items in the list with the current values, containing also the new one. The same form widget can have a controller registered to respond to the creation started message: this way if the request fails because of validation issues, the information present in the form fields is maintained to allow the user to fix it without re-typing everything, but if instead the validation passes, the form can clean its fields to allow a new item to be added next. All of these are purely GUI concerns, and that's why they aren't handled by the interactor, which must be concerned only with what is explicitly mentioned in the use case scenarios.\n\nA less common case is the one where input data comes from a source that doesn't support any output, or is not interested in getting any. In actuality, this should be described by the application layer, because we can't use an adapter that doesn't support an output, with a port that provides one, because the role of the adapter is to support the use cases defined by the port. Thus, we can think of a use case that doesn't produce any outout.\n\nUse case, \"signal activates actuator\":\n- when the sensor sends a new signal\n- then the actuator is activated\n\nHere the interactor receives an input, in the form of a signal, but then produces no output, because activating the actuator is a secondary concern, adn the sensor doesn't support any output, since it can't even receive data back from its adapter. \n\n### Sloth machine views\n\nBeing a command-line application, the Sloth machine uses the process as the input device, and the process itself, in addition perhaps to the filesystem, as output devices. In particular, we want to use the exit status and the STDOUT and STDERR as main output devices. It's important, though, to highlight that these same devices can be used also to render output coming from the program that is being executed by the virtual machine, in addition to the virtual machine itself, and they can also be used to display side output, for example information messages in a verbose configuration. For this reason, we should constantly keep in mind where some output is coming from, and not only on which device it's displayed.\n\nThe first thing to do is clearly understand what's the output that the use case is supposed to provide: in our case the output comprises two elements:\n- the exit status of the program execution\n- the error message that might possibly have happened\n\nIt's important to underline that we want errors to be part of the output, because they're still significant for the application. Including errors in the response means that they can be handled by a presenter, and the views attached to it. Alternatively, we could have chosen to send the errors to some kind of side output, for example events, but in that case the decision of whether to display these errors or not would have been taken by the adapter, and not by the application. By sending errors with the response, we are clearly stating that we want errors to be displayed, as an application rule.\n\nThus, our application will need to handle two output boundaries: `ExitStatusPresenter`, receiving an `ExitStatusResponse`, and `ErrorPresenter`, receiving an `ErrorResponse`. For both of them, concrete instances will need to be injected into the interactor. From the adapter point of view, we should decide which output models we want to support. For instance, we can think of a \"console output\", which is meant to be used when the application should be used as a standard console application, properly using the exit status, STDOUT and STDERR; additionally, we can think of a \"textual output\", which is meant to be used when all data produced by the application should be available as human-friendly textual information. These output models define two distinct sets of presenters: for the console output, we'll have a `ConsoleExitStatusPresenter` and a `ConsoleErrorPresenter`, while for the textual output we'll have a `TextualExitStatusPresenter` and a `TextualConsoleErrorPresenter`:\n- the `ConsoleExitStatusPresenter` converts the `ExitStatus` object of the response into an integer included between `0` and `255` (using a default value if needed), because it needs to be sent to a console application exit status, abiding by POSIX standard, and produces a `ConsoleExitStatusViewModel`;\n- the `TextualExitStatusPresenter` converts the `ExitStatus` object of the response into a string, avoiding changing its semantic at all, and produces a `TextualExitStatusViewModel`;\n- both the `ConsoleErrorPresenter` and the `TextualErrorPresenter` will just convert the `Error` object of the response into a string, producing a `ConsoleErrorViewModel` or a `TextualErrorViewModel`;\n\nEach output model can then be rendered into different widgets (views): these will be organized into output configurations, that can be selected by the user, for example through command-line arguments.\n\nThe *integrated configuration* is used to run programs on the virtual machine, as if they were normal executables; this means that the program's exit status is returned as the application's own exit status, and the error message is printed to STDERR: \n- the `ExitStatusWidget` is used to return a number as the exit status of the console application;\n- the `StderrWidget` is used to print a message to the STDERR of the console;\n- the `IntegratedExitStatusView` is used to render the `ConsoleExitStatusViewModel` to the `ExitStatusWidget`;\n- the `IntegratedErrorView` is used to render the `ConsoleErrorViewModel` to the `StderrWidget`;\n\nThe *clean configuration* is used to run programs on the virtual machine, but hiding the error message, so to keep the console clean; the exit status is still returned as the application's own exit status, but now the error message is written to a file; a possible use case of this is when the actual program is writing stuff to STDERR, and we don't want it to mix with errors written by the application (instead of the program):\n- the `ExitStatusWidget` is used to return a number as the exit status of the console application;\n- the `OutputFileWidget` is used to write messages to an output file, instead of to STDOUT or STDERR;\n- the `CleanExitStatusView` is used to render the `ConsoleExitStatusViewModel` to the `ExitStatusWidget` (here we could share a single `ConsoleExitStatusView` with the previous case instead perhaps);\n- the `CleanErrorView` is used to render the `ConsoleErrorViewModel` to the `OutputFileWidget`;\n\nThe *verbose configuration* is used to gather all information on a centralized place, immediately visible; both the exit status and the error message are printed to STDOUT:\n- the `StdoutWidget` is used to print messages to the STDOUT of the console;\n- the `VerboseExitStatusView` is used to render the `TextualExitStatusViewModel` to the `StdoutWidget`;\n- the `VerboseErrorView` is used to render the `TextualErrorViewModel` to the `StdoutWidget`;\n\nThe *archived configuration* is used to gather all information on a centralized place, but without clogging the console (notice that STDOUT and STDERR may still contain what the actual program is writing to them); both exit status and error message are printed to a file:\n- the `OutputFileWidget` is used to write messages to an output file, instead of to STDOUT or STDERR;\n- the `ArchivedExitStatusView` is used to render the `TextualExitStatusViewModel` to the `OutputFileWidget`;\n- the `ArchivedErrorView` is used to render the `TextualErrorViewModel` to the `OutputFileWidget`;\n\nHere's what some pseudo-code would look like:\n```\nConsoleExitStatusPresenter\n  ConsoleExitStatusPresenter(ConsoleExitStatusView view)\n  present(ExitStatusResponse response)\n  exitStatus = normalizeExitStatus(response.getExitStatus())\n  viewModel = new ConsoleExitStatusViewModel(exitStatus)\n  view.render(viewModel)\n\nConsoleErrorPresenter\n  ConsoleErrorPresenter(ConsoleErrorView view)\n  present(ErrorResponse response)\n  error = response.getError().getMessage()\n  viewModel = new ConsoleErrorViewModel(error)\n  view.render(viewModel)\n\nIntegratedExitStatusView: ConsoleExitStatusView\n  IntegratedExitStatusView(ExitStatusWidget widget)\n  render(ConsoleExitStatusViewModel viewModel)\n  widget.setExitStatus(viewModel.getExitStatus())\n\nIntegratedErrorView: ConsoleErrorView\n  IntegratedErrorView(StderrWidget widget)\n  render(ConsoleErrorViewModel viewModel)\n  widget.setError(viewModel.getError())\n\nConsoleUi\n  ConsoleUi(Container container)\n  this.console = container.make(Console)\n  this.exitStatus = new ExitStatusWidget()\n  this.stderr = new StderrWidget()\n  container.bind(ExitStatusWidget, exitStatus)\n  container.bind(StderrWidget, stderr)\n  getExitStatus(): ExitStatusWidget\n  return exitStatus\n  getStderr(): StderrWidget\n  return stderr\n  render()\n  console.writeError(stderr.getError())\n  console.exit(exitStatus.getExitStatus())\n\nConsoleUi ui = new ConsoleUi(container);\n// send input, render views...\nui.render()\n```\n\nHere we encapsulate all UI related code into a single class, which binds specific instances of widgets to their classes, so when views are built by the container, they get the right widgets. This specific case is particularly interesting because we have to deal with the problem that when we call `console.exit()`, the application is terminated, so any other UI code that might need to run after that won't be able to. To avoid this, instead of directly call console methods inside views, we let the views populate the widgets, and then render the UI as the last thing. Had we allowed views to directly call the console, the order with which the various presenters were called inside the interactor would have been important for user interface concerns, because if the interactor called the exit status presenter before the error one (or a generic message one, since if there are errors, no exit status is produced in this specific case), then the application would have been terminated before the interactor even finished its execution, and this is of course unacceptable. Having had a graphical interactive UI, instead, we could've displayed the exit status in a widget as soon as it was produced, because it wouldn't have caused the application to terminate. \n\n\n## Status\n\nProposed\n\n\n## Consequences\n\nEverything regarding input and output for adapters is following a generic and reusable standard.\n\nDifferent output representations and output devices are supported.\n\nInput is actually completely decoupled from output.\n"}
{"repositoryUrl": "https://github.com/CSCfi/rems.git", "path": "docs/architecture/013-event-design.md", "template": "unknown", "status": null, "firstCommit": "2020-10-27T13:40:04Z", "lastCommit": "2020-10-28T11:43:10Z", "numberOfCommits": 4, "title": "013: Command & event design", "wordCount": 647, "authors": {"name1": 4}, "content": "# 013: Command & event design\n\nAuthors: @opqdonut @Macroz\n\nThis ADR outlines some discussions about event design that were had while implementing\n[#2040 Reviewer/decider invitation](https://github.com/CSCfi/rems/issues/2040).\n\nThe point of the feature is being able to invite reviewers and\ndeciders by email. This is similar to the existing feature of inviting\napplication members by email.\n\n## Existing commands & events\n\n- Command: invite-member\n  - Produces event: member-invited\n- Command: accept-invitation\n  - Produces: member-joined\n- Command: request-review\n  - Produces: review-requested\n- Command: request-decision\n  - Produces: decision-requested\n\n### Design 1\n\nFirst implementation. Trying to share decider and reviewer invitation\nlogic using a generic invite-actor command. Reuses the existing\naccept-invitation command to reuse some frontend & backend routes.\nCommit 2049a3651 in [PR #2415][2415].\n\n- Command: invite-actor with `:role :decider` or `:role :reviewer`\n  - Produces: actor-invited\n- Command: accept-invitation\n  - Produces: actor-joined with `:role :decider` or `:role :reviewer`\n  - (Or member-joined event if the invitation was for a member)\n\nProblems:\n\n- Naming is hard: actor is an already overloaded term (means the author of an event)\n- Actor-joined event needed to duplicate behaviour of existing review-requested event\n\n[2415]: https://github.com/CSCfi/rems/pull/2415\n\n### Design 2\n\nTrying to reuse even more code by reusing the review-requested event. Commit 33c3c79 in [PR #2415][2415].\n\n- Command: invite-actor with `:role :decider` or `:role :reviewer`\n  - Produces: actor-invited\n- Command: accept-invitation\n  - Produces: actor-joined with `:role :decider` or `:role :reviewer`\n  - Produces: review-requested OR decision-requested\n\nBenefits:\n\n- Nice split of responsibilities between events\n  - actor-joined marks invitation as used\n  - existing review-requested event used for granting reviewer rights\n\nProblems:\n\n- After accepting the invitation, the review-requested event triggers a new, potentially misleading notification email to the reviewer\n- Event log is also a bit confusing:\n  1. \"Alice invited Bob to review\"\n  2. \"Bob accepted the invitation to review\"\n  3. \"Alice requested a review from Bob\"\n\nPossible solutions:\n\n- Reword log message to be more passive, \"Bob is now a reviewer\"\n- Add some sort of `:notify false` option to the review-requested event to prevent emails (and possibly hide from event list)\n- Go back to design 1\n\n### Design 3\n\nAfter discussions ended up with less code sharing but more explicit API & event structure. Commit 253b66d2e in [PR #2415][2415].\n\n- Command: invite-reviewer\n  - Produces: reviewer-invited\n- Command: invite-decider\n  - Produces: decider-invited\n- Command: accept-invitation\n   - Produces: reviewer-joined OR decider-joined\n\nBenefits:\n\n- More consistent API: request-review, invite-reviewer; request-decision, invite-decider\n- Not reusing {request,decision}-requested events avoids pitfalls in event log & emails\n- Reusing the accept-invitation command makes sense because we don't want multiple /accept-invitation URLs\n\nProblems:\n\n- Some code duplication\n- Bots & external consumers interested in review requests now need to listen to two events instead of one\n\n## Discussion\n\nSince REMS commands are part of our public API, it makes sense to keep\ncommands large (that is, one command does a lot of things). This way\nthe user's intent can usually be represented with one command, keeping\nthe frontend simple. Also, since one command means one database\ntransaction (see [ADR 010: Database\ntransactions](010-transactions.md)), issuing a single command is safer\nthan issuing multiple commands. API usage is also nicer when you can\noften just post a single command.\n\nHowever, since commands and events are decoupled, we could have these\ncommands produce multiple small events. So far REMS has favoured most\ncommands producing just one nongeneric and large event (that is, an\nevent that has lots of effects). Also commands haven't reused events\n(for example review-requested and decision-requested are separate\nevents). This way the events are more explicit and mirror the user's\nintent just like our commands.\n\nDesign 1 was an attempt at using smaller, more decoupled events.\nHowever that immediately ran into problems with consuming events\ninternally: both the event log & email generation would have needed\nwork.\n\nPerhaps it is best for now to stick to large events so that we can\neasily react to the users intent in other code, instead of trying to\nrecombine smaller events to reproduce intent (e.g. not sending\nredundant email reminders)\n\nHowever, the decision to show the internal event log to users as-is\nmight make our life harder in the future. We might need to re-evaluate\nthis later.\n"}
{"repositoryUrl": "https://github.com/alphagov/content-data-api.git", "path": "docs/arch/adr-005-split-audit-tool-cpm.md", "template": "unknown", "status": null, "firstCommit": "2021-03-25T09:10:12Z", "lastCommit": "2021-03-25T09:16:46Z", "numberOfCommits": 2, "title": "ADR 005: Split Audit Tool and Content Performance Manager", "wordCount": 128, "authors": {"name1": 1, "name2": 1}, "content": "# ADR 005: Split Audit Tool and Content Performance Manager\n\n18-01-2018\n\n## Context\n\nContent Performance Manager and Content Audit Tool have lived on the same repo for 10 months as we didn't have a very clear picture of the process that was driving both tools, and the underlying user needs.\n\nWe decided to keep them together because:\n1. As per user research, both tools were addressing similar needs the content publishing workflow\n2. It is easier to split both apps once you know that they are different, than to join them later on.\n\nIn the last quarter, we noticed that both tools are addressing very different user needs, hence we need to split them.\n\n## Decision\n\n[Split Content Audit Tool and Content Performance Manager][1]\n\n## Status\n\nAccepted.\n\n\n[1]: https://trello.com/c/l7fn1C1P/20-2-split-term-generation-tool\n\n\n\n\n"}
{"repositoryUrl": "https://github.com/nhsuk/mongodb-updater.git", "path": "doc/adr/0003-host-two-mongodb-updaters-in-stack.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-07-05T17:32:01Z", "lastCommit": "2017-07-11T08:24:31Z", "numberOfCommits": 2, "title": "3. Host two mongodb-updaters in a Single Stack", "wordCount": 87, "authors": {"name1": 1, "name2": 1}, "content": "# 3. Host two mongodb-updaters in a Single Stack\n\nDate: 2017-06-28\n\n## Status\n\nAccepted\n\n## Context\n\nTwo different MongoDBs need to be updated on a daily basis.\n\n## Decision\n\nGiven the small number of databases to be updated both services will be hosted in the same stack, rather than manually create a stack for each database updater.\n\n## Consequences\n\nThe repository will hold a docker-compose file to define a stack containing the Pharmacy and the GP database updaters.\n\nBoth mongodb-updaters will be automatically deployed using the existing infrastructure as a single unit.\n"}
{"repositoryUrl": "https://github.com/cloudfoundry/cf-k8s-networking.git", "path": "doc/architecture-decisions/0015-app-access-logs-from-ingressgateway.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-07-06T18:20:55Z", "lastCommit": "2020-10-01T17:35:24Z", "numberOfCommits": 4, "title": "15. App Access Logs from IngressGateway", "wordCount": 488, "authors": {"name1": 2, "name2": 1, "name3": 1}, "content": "# 15. App Access Logs from IngressGateway\n\nDate: 2020-07-06\n\n## Status\n\nAccepted\n\n## Context\n\n`cf logs` for an app shows logs both emitted by that app, and access logs from\nthe Gorouter. The access logs from Gorouter look like this in the logstream:\n\n```\n2020-06-25T23:42:19.00+0000 [<source_type>/<instance_id>] OUT <log>\n```\n\nIn cf-for-k8s, an app developer should similarly be able to see access logs as\nrequests for the app travel through the routing tier. One discussion we had was\nwhether these access logs should come from the ingressgateway envoy and/or from the\napp sidecar envoys.\n\n\nOne important piece of functionality in cf-for-BOSH is that when an app process has been killed, healthcheck requests still\nshow up in the access log stream. This is because those requests still make it\nto the Gorouter, even though they do not make it to the app itself.\n\nThis is an example of an access log of a healthcheck request to a killed app.\nThe 503 is being returned directly from the Gorouter.\n```\n2020-07-06T10:45:55.83-0700 [RTR/0] OUT dora.maximumpurple.cf-app.com - [2020-07-06T17:45:55.828757970Z] \"GET /health HTTP/1.1\" 503 0 24 \"-\" \"curl/7.54.0\" \"35.191.2.88:63168\" \"10.0.1.11:61002\" x_forwarded_for:\"76.126.189.35, 34.102.206.8, 35.191.2.88\" x_forwarded_proto:\"http\" vcap_request_id:\"0cd79f32-3cde-4eea-5853-9a2ca401be40\" response_time:0.004478 gorouter_time:0.000433 app_id:\"1e196708-3b2d-4edc-b5b8-bf6b1119d802\" app_index:\"0\" x_b3_traceid:\"7f470cc2fcf44cc6\" x_b3_spanid:\"7f470cc2fcf44cc6\" x_b3_parentspanid:\"-\" b3:\"7f470cc2fcf44cc6-7f470cc2fcf44cc6\"\n```\n\nWe've done some previous work exploring access logs on the Istio Envoy\nIngressGateway (see related stories section below) and have [documented some of\nthe fields\nhere](https://github.com/cloudfoundry/cf-k8s-networking/blob/37dabf7907ffa7b284980cfcb6813ebcd449736c/doc/access-logs.md).\n\n## Decision\n\nWe decided to have the access logs come from the ingressgateway to begin with,\nas we think those provide the most valuable information.\n\nImagine a scenario where an app has crashed and the Pod is being rescheduled.\nThe Envoy on the ingressgateway will still log this failed request. The sidecar,\non the other hand, would be unreachable so it would not be able to log anything.\nHaving the failed request in the access logs in this scenario could be valuable\ninformation for a developer attempting to debug their app with `cf logs`.\n\nWe also decided that the access log format would be JSON with the [following\nfields](https://docs.google.com/spreadsheets/d/1CuvoUEkiizVKvSZ2IaLya40sgMbm5at78CqxB8uUe80/edit#gid=0)\n\nThe work to enable this was completed in [#173568724](https://www.pivotaltracker.com/story/show/173568724).\n\n## Consequences\n\n- In order to enable this, we added fluent-bit sidecars to our ingressgateways.\n  Information on why we decided to add our own fluent-bit images can be found in\n  this [draft PR](https://github.com/cloudfoundry/cf-k8s-networking/pull/57).\n  The final iteration of this was merged in from [this\n  PR](https://github.com/cloudfoundry/cf-k8s-networking/pull/63)\n- Will need to do [some extra work](https://www.pivotaltracker.com/story/show/172732552)\nto get logs from the ingressgateway pods into the log stream corresponding to the destination app.\nSee https://github.com/cloudfoundry/cf-k8s-logging/tree/main/examples/forwarder-fluent-bit for more information.\n- It is unclear how difficult it would be to add custom formatting to sidecar Envoy logs,\nwe [know how to do it for the ingressgateway logs](https://www.pivotaltracker.com/story/show/169739120)\n- We may need to revisit the sidecar logs later if we want access logs for container-to-container (c2c)\nnetworking (this doesn't exist for c2c in CF for BOSH today)\n\n---\n\n## Related Stories\nFor additional context, here are some stories our team has worked on in the\npast:\n\n- [Emit JSON ingress gateway access logs](https://www.pivotaltracker.com/story/show/169739120)\n- [Adding fields into access logs for gorouter parity](https://www.pivotaltracker.com/story/show/169737156)\n- [Explore emitting ingress gateway access logs with Fluentd](https://www.pivotaltracker.com/story/show/170119094)\n"}
{"repositoryUrl": "https://github.com/johanhaleby/occurrent.git", "path": "doc/architecture/decisions/0004-mongodb-datetime-representation.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-07-28T07:08:05Z", "lastCommit": "2020-09-10T09:19:50Z", "numberOfCommits": 2, "title": "4. MongoDB datetime representation", "wordCount": 544, "authors": {"name1": 2}, "content": "# 4. MongoDB datetime representation\n\nDate: 2020-07-28\n\n## Status\n\nAccepted\n\n## Context\n\nRepresenting RFC 3339 in MongoDB when using the CloudEvent Jackson SDK _and_ allow for queries is problematic. \nThis is because MongoDB internally represents a Date with only millisecond resolution (see [here](https://docs.mongodb.com/manual/reference/method/Date/#behavior)):\n\n> Internally, Date objects are stored as a signed 64-bit integer representing the number of milliseconds since the Unix epoch (Jan 1, 1970).\n\nSince the Java CloudEvent SDK uses `OffsetDateTime` we'll lose nanosecond precision and the timezone if converting the `OffsetDateTime` \nto `Date`. This is really bad since it goes against the \"understandability\" and \"transparent\" goals of Occurrent. I.e. if you create\na CloudEvent with a OffsetDateTime containing nanoseconds in timezone \"Europe/Stockholm\" you would expect to get the same value back on read \nwhich is not possible if we convert it to a date. \n\n## Decision\n\nWe've thus decided to just store the `ZoneDateTime` as an RFC 3339 string in MongoDB. This means that range queries on \"time\" will be \nhorrendously slow and probably not work as expected (string comparision instead of date comparision). The `EventStoreQueries` API\ncurrently supports sorting events by natural order ascending/descending which will be much faster (since it's using the timestamp of the \ngenerated mongodb object id).  \n\nNote that sort options `TIME_ASC` and `TIME_DESC` are still retained in the API. The reason for this that we may allow customizations\nto the serialization mechanism _if_ nanosecond resolution and is not required and timezone is always `UTC` in the future.  \n\n## Alternatives\n\nAn alternative worth considering would be to add an additional field in the serialization process. For example retain the \"time\" as RFC 3339 string\nbut add an additional field in MongoDB that stores the `Date` so it can be used for fast \"time\" queries. \n\nI've decided not to do this though for the following reasons:\n\n1. Code simplicity. We would have needed to handle \"time\" queries specially. For example if using \"eq\" we probably would like to compare \"time\" field \n   and if not using \"eq\" we want to compare if the `Date` field. Combinations such as \"gte\" becomes even more problematic.\n1. For time queries to be fast an index would be needed. It would introduce additional complexity for users of the MongoDB EventStore that this index would need\n   to be created for fast queries. Creating the index automatically would not be a good idea since it might not be required for every user.\n1. User may never which to query for \"time\"! In these cases storing an extra field is simply unnecessary.  \n\nFor these reasons we've decided that it's better for the user to simply add a custom extension field him-/herself and create custom queries for this field.\nThe `EventStoreQueries` api event supports querying custom fields right now \n(though we could expand the API to allow custom `SortBy` fields instead of hardcoding \"time\" and \"natural\"). \n\n## Consequences\n\nThis decision is quite sad since it's still very common represent time in Java application as a `Date`. In these cases it would be perfectly \nfine to use the native `ISODate` in MongoDB. If converting a `Date` to a `OffsetDateTime` it would be possible to get the best of both worlds and just\nstore the `Date` in MongoDB and one of the nice benefits of using CloudEvents are lost. "}
{"repositoryUrl": "https://github.com/aml-org/amf.git", "path": "adrs/0003-new-annotation-removal-stage-present-in-all-webapi-pipelines.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-10-21T18:55:31Z", "lastCommit": "2020-10-21T18:55:31Z", "numberOfCommits": 1, "title": "3. New annotation removal stage present in all webapi pipelines", "wordCount": 188, "authors": {"name1": 1}, "content": "# 3. New annotation removal stage present in all webapi pipelines\n\nDate: 2020-10-21\n\n## Status\n\nAccepted\n\n## Context\n\nWhen referencing external files there are certain cases where AMF inlines the parsed content without creating a links (this is because the targeted elements are not present in the references of the base unit). \nFor these cases, when a emitting an unresolved model these references are being emitted inlined. \n\n## Decision\n\nIn order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference. \nWhen saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model. \nThis leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.\n\n## Consequences\n\nReferences can be emitted when receiving an unresolved model with inlined content. \nThis stage iterates over the whole model removing specific annotations which can lead to more processing, however performance tests have been made showing that this processing is insignificant. \n"}
{"repositoryUrl": "https://github.com/alphagov/govuk-infrastructure.git", "path": "docs/architecture/decisions/0004-use-aws-load-balancer-controller-for-edge-traffic-services.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2021-08-12T15:41:42Z", "lastCommit": "2021-08-13T09:23:01Z", "numberOfCommits": 4, "title": "4. Use AWS Load Balancer Controller for edge traffic services", "wordCount": 364, "authors": {"name1": 1, "name2": 3}, "content": "# 4. Use AWS Load Balancer Controller for edge traffic services\n\nDate: 2021-08-12\n\n## Status\n\nAccepted\n\n## Context\n\nWe require a method of managing and directing external internet traffic into the cluster. Kubernetes provides [several options for handling inbound traffic](https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0).\n\nWe do not want to expose the cluster directly to the internet, and so require an AWS load balancer in front of the cluster. The load balancer must support TLS termination and integration with our DNS provider (AWS Route 53).\n\nHistorically Kubernetes has supported provisioning of ALBs and NLBs for `Service` resources of `type=LoadBalancer` via the in-tree (built-in) [AWS cloud provider](https://github.com/kubernetes/cloud-provider-aws), with out-of-tree controllers required for `Ingress` resources. Built-in cloud providers are now [considered deprecated overall, in favour of out-of-tree providers](https://kubernetes.io/blog/2019/04/17/the-future-of-cloud-providers-in-kubernetes/), so an [Ingress Controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/) with support for `Service` resources is required.\n\nThe primary and recommended ingress controller for AWS/EKS is the [AWS Load Balancer Controller](https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html), which can provision and manage [ALBs for `Ingress` resources and NLBs for `Service` resources](https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/#aws-load-balancer-controller).\n\nWe must also consider how Kubernetes edge services and AWS load balancers will interact with the existing [GOV.UK Router service](https://github.com/alphagov/router), as there is significant overlap in their functionality and responsibilities. This will require further investigation and likely experimentation, and so that end we should ensure that we're able to use both `Ingress` and `Service` Kubernetes resources so that we have the flexibility to support a wide range of use cases in the immediate term - L4 & L7 traffic, name-based routing, HTTP->HTTPS redirection, etc.\n\n## Decision\n\nUse the [AWS Load Balancer Controller](https://github.com/kubernetes-sigs/aws-load-balancer-controller).\n\n## Consequences\n\nThe AWS Load Balancer Controller supports TLS certificates via AWS Certificate Manager only, so certificates must be managed there (to be covered in a future ADR).\n\nThe load balancer controller does not handle DNS for declared ingress hostnames - a solution to this will be covered in a future ADR.\n\nAn appropriate ALB/NLB topography (how many LBs routing to where) will need to be established - by default the controller will provision one ALB per `Ingress` resource, which may not be what we want. Ingresses [can be grouped however](https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/ingress/annotations/#ingressgroup).\n\nThe load balancer controller supports [AWS WAF and Shield](https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/ingress/annotations/#addons), both of which are currently in use on GOV.UK.\n\nAccess control for Ingress rules must be investigated, likely in conjunction with Kubernetes `namespace` usage - if all of GOV.UK is deployed into a single namespace, and multiple users or service accounts have the same level of access to `Ingress` objects, then user or process for component A could modify or destroy ingress rules for component B.\n"}
{"repositoryUrl": "https://github.com/LBHackney-IT/remultiform.git", "path": "docs/adr/0003-use-rollup-to-build-distributables.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-10-29T12:46:32Z", "lastCommit": "2019-10-29T12:46:32Z", "numberOfCommits": 1, "title": "3. Use Rollup to build distributables", "wordCount": 83, "authors": {"name1": 1}, "content": "# 3. Use Rollup to build distributables\n\nDate: 2019-10-29\n\n## Status\n\nAccepted\n\n## Context\n\nWe want to be able to distribute this library to me ingested by TypeScript or\nplain JavaScript (both commonJS and module) applications.\n\n[Rollup](https://rollupjs.org/guide/en/) is a popular JavaScript bundler with\nsupport for TypeScript and simple configuration.\n\n## Decision\n\nWe will build distributables using Rollup.js.\n\n## Consequences\n\nWith Rollup.js we can build type declarations, commonJS, and module code using a\nsingle simple command, and some simple config. This makes it easier for us to\ndistribute our code.\n"}
{"repositoryUrl": "https://github.com/huifenqi/arch.git", "path": "decisions/0035-disaster-recovery.md", "template": "Nygard", "status": "Proposed", "firstCommit": "2017-06-05T06:02:01Z", "lastCommit": "2017-08-27T06:51:24Z", "numberOfCommits": 3, "title": "35. 关于灾难恢复", "wordCount": 212, "authors": {"name1": 3}, "content": "# 35. 关于灾难恢复\n\nDate: 2017-06-05\n\n## Status\n\nProposed\n\n## Context\n\n当前我们使用的是华北2可用区A机房（即北京昌平区的一个机房）部署了所有服务，存在以下几个问题，出现概率逐级减少：\n1. 服务本身部署在单台机器，单机的故障会导致服务的不可用，这个我们的业务服务频频出现；\n2. 所有服务部署于一个机房，机房电力，网络出现故障，将导致服务完全不可用，这个 2016 年中旬我们使用的 Aliyun 机房网络设备出现过一次问题，导致服务停服 1 小时左右（官方），实际对我们的影响在 12 个小时左右；\n3. 北京发生各种灾害，殃及所有机房，导致服务不可用。\n\n### 基础概念\n\n灾难恢复（Disaster recovery，也称灾备），指自然或人为灾害后，重新启用信息系统的数据、硬件及软体设备，恢复正常商业运作的过程。灾难恢复规划是涵盖面更广的业务连续规划的一部分，其核心即对企业或机构的灾难性风险做出评估、防范，特别是对关键性业务数据、流程予以及时记录、备份、保护。\n\n地域，即城市，不同的地域可以做到自然灾害级别的灾备，之间延迟较高\n\n可用区，即机房，不同的可用区可以做到电力和网络设备互相独立，之间有少量延迟\n\n两地三中心，业界目前最可靠的解决方案，即在两个城市共三个机房中部署服务\n\n### 灾备的两项指标\n\nRTO - Recovery Time Objective，它是指灾难发生后，从 IT 系统宕机导致业务停顿之时开始，到 IT 系统恢复至可以支持各部门运作、恢复运营之时，此两点之间的时间段称为 RTO\n\nRPO - Recovery Point Objective,是指从系统和应用数据而言，要实现能够恢复至可以支持各部门业务运作，系统及生产数据应恢复到怎样的更新程度，这种更新程度可以是上一周的备份数据，也可以是上一次交易的实时数据\n\n### 两地三中心\n\n![][image-1]\n\n* 主系统\n\t* 即当前对外提供服务的机房\n\t* 首先要做到自身的高可用，所有服务不因单机故障导致服务不可用\n\t* 无资源浪费，多一些部署和维护成本\n* 同城灾备\n\t* 单个城市多个机房，解决单机房电力或网络故障导致的服务不可用\n\t* 主备的方式会有一半的资源浪费，双活的方式对服务有一定的延迟\n\t* 有一定的部署和维护成本\n* 异地灾备\n\t* 即跨城市部署服务，解决自然灾害等引起的服务不可用\n\t* 由于延迟的原因，这部分资源属于备用服务，仅在发生灾害是激活\n\t* 平时资源都是浪费，并且有较高的部署和维护成本\n\n## Decision\n\n1. 同机房的服务高可用（进行中），这个是目前最高优先级；\n2. 同城双活（提议中），可以解决大部分我们遇到的机房问题；\n3. 异地灾备（暂不考虑），针对支付业务，当涉及合规性时，我们得考虑下；\n4. 明确我们各个服务的重要程度，分服务针对性的做高可用及灾备策略。\n\n## Consequences\n\nRefs:\n\n* 灾难恢复 [https://zh.wikipedia.org/wiki/%E7%81%BE%E9%9A%BE%E6%81%A2%E5%A4%8D][1]\n* Aliyun 地域和可用区 [https://help.aliyun.com/knowledge\\_detail/40654.html][2]\n* 阿里云华北2区网络故障导致业务中断1小时 [http://www.sohu.com/a/101817812\\_401503][3]\n* 因电缆井被烧，京津宁骨干网中断 [http://www.sohu.com/a/131579749\\_465914][4]\n* 经历不可抗力是一种什么体验 [https://zhuanlan.zhihu.com/p/26855422][5]\n* 金融云特性 [https://help.aliyun.com/document\\_detail/29851.html][6]\n* 云上场景：众安保险，两地三中心容灾部署实践 [https://yq.aliyun.com/articles/6633][7]\n\n[1]:\thttps://zh.wikipedia.org/wiki/%E7%81%BE%E9%9A%BE%E6%81%A2%E5%A4%8D\n[2]:\thttps://help.aliyun.com/knowledge_detail/40654.html\n[3]:\thttp://www.sohu.com/a/101817812_401503\n[4]:\thttp://www.sohu.com/a/131579749_465914\n[5]:\thttps://zhuanlan.zhihu.com/p/26855422\n[6]:\thttps://help.aliyun.com/document_detail/29851.html\n[7]:\thttps://yq.aliyun.com/articles/6633\n\n[image-1]:\tfiles/dr.png"}
{"repositoryUrl": "https://github.com/LBHackney-IT/remultiform.git", "path": "docs/adr/0007-use-dependabot-to-keep-dependencies-up-to-date.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-10-30T16:01:58Z", "lastCommit": "2019-10-30T16:01:58Z", "numberOfCommits": 1, "title": "7. Use Dependabot to keep dependencies up to date", "wordCount": 66, "authors": {"name1": 1}, "content": "# 7. Use Dependabot to keep dependencies up to date\n\nDate: 2019-10-30\n\n## Status\n\nAccepted\n\n## Context\n\nWe want to ensure all dependencies stay up to date. Dependabot offers a service\nwhere a bot opens PRs on GitHub when new versions of dependencies are released.\n\n## Decision\n\nWe will use Dependabot to monitor dependency updates.\n\n## Consequences\n\nUsing Dependabot means we don't need to manually monitor dependencies for new\nversions.\n"}
{"repositoryUrl": "https://github.com/linagora/linshare-mobile-flutter-app.git", "path": "doc/adr/0009-enhancement-retry-authen-interceptor.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2021-09-15T10:14:13Z", "lastCommit": "2021-09-15T10:14:13Z", "numberOfCommits": 1, "title": "9. enhancement-retry-authen-interceptor", "wordCount": 155, "authors": {"name1": 1}, "content": "# 9. enhancement-retry-authen-interceptor\n\nDate: 2021-05-10\n\n## Status\n\nAccepted\n\n## Context\n\n- The current `retry` logic is limited in the number of `retry` for failed requests. For example, at a screen where more than 3 different requests fail, the chance of each request being retried is very little, since they share a variable that counts the number of retry times.\n- Using a (in memory) counter variable for `retry` is risky and difficult to control\n\n## Decision\n\n1. Send the `retry` counter into request header for each request\n2. Retrieve the `retry` count from request data\n3. Validate with the maximum allowed. If `retry` in the limit, increment it then re-send a request network with updated `retry` counter\n\n```\n{\n  ...\n  SET retriesCount from request extra map with key RETRY_KEY\n  IF _isAuthenticationError(dioError, retriesCount) THEN\n  INCREMENT retriesCount\n  ADD retriesCount to header with key RETRY_KEY\n  CALL request again\n  ENDIF\n  ...\n}\n```\n\n## Consequences\n\n- The chance of each request being retried is increased\n- `Retry` counter is controlled inside the request"}
{"repositoryUrl": "https://github.com/ammarnajjar/reading-todo.git", "path": "adr/001-document-architecture-decisions-in-adrs.md", "template": "unknown", "status": null, "firstCommit": "2019-08-15T20:54:32Z", "lastCommit": "2019-09-04T14:48:51Z", "numberOfCommits": 3, "title": "Document Architecture Decisions in ADRs", "wordCount": 81, "authors": {"name1": 3}, "content": "# Document Architecture Decisions in ADRs\n\nDate: Thu Aug 15 22:48:16 CEST 2019\n\n## Milestone\n\n0.1.0\n\n## Context\n\nFor the project it is useful to keep all the architectural decisions documented in one place.\n\n## Decision\n\nAll development related decisions must be documented in ADRs.\n\n## Consequences\n\n### Pros:\n\n- Easy and fast to look up all architectural decisions.\n- Learn to stick to a decision until other proves being better.\n\n### Cons:\n\n- Write for every time a new decision is made a new markdown file :-D\n\n## References:\n\n- [wikipedia](https://en.wikipedia.org/wiki/Architectural_decision<Paste>)\n- [ADR](https://github.com/joelparkerhenderson/architecture_decision_record)\n"}
{"repositoryUrl": "https://github.com/perforce/helix-authentication-service.git", "path": "docs/architecture/decisions/0004-use-memory-store.md", "template": "unknown", "status": null, "firstCommit": "2020-08-21T21:28:23Z", "lastCommit": "2020-08-21T21:28:23Z", "numberOfCommits": 1, "title": "Use (only) Memory Store", "wordCount": 208, "authors": {"name1": 1}, "content": "# Use (only) Memory Store\n\n* Status: accepted\n* Deciders: Nathan Fiedler\n* Date: 2020-08-20\n\n## Context\n\nThis application handles several important pieces of information. First is mapping requests for a login to a particular login action, and second is the login results to the initial request. This allows for the client application to signal that a login will begin, get the login URL that should be used, and then query the results of that login action.\n\nFor the sake of reliability, this information could be written to a database on a storage device. However, that adds complexity to the deployment of the application, and most of this data is ephemeral at best, lasting only a few seconds and up to a few minutes. Alternatively, the application could simply use an in-memory data store, which avoids leaking data in the event of a system breach, at the expense of losing data if the application is restarted.\n\n## Decision\n\nThis application will use an **in-memory** store rather than files or a database. The particular Node.js module selected is named `memorystore` and supports time-to-live expiration of cached data, which limits the time and exposure of any sensitive user data.\n\n## Consequence\n\nThe application has been using an in-memory store since the beginning, with no issues.\n\n## Links\n\n* memorystore [GitHub](https://github.com/roccomuso/memorystore)\n"}
{"repositoryUrl": "https://github.com/kbremner/read-more-api.git", "path": "ReadMoreAPI/doc/adr/0002-use-asp-net-core.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-09-26T11:05:25Z", "lastCommit": "2017-09-26T11:05:25Z", "numberOfCommits": 1, "title": "2. Use ASP.NET Core", "wordCount": 133, "authors": {"name1": 1}, "content": "# 2. Use ASP.NET Core\n\nDate: 2017-09-24\n\n## Status\n\nAccepted\n\n## Context\n\nWith the introduction of .NET Core, we need to decide whether to use ASP.NET with .NET v4.x or ASP.NET Core.\n\n## Decision\n\nWe will use ASP.NET Core.\n\n## Consequences\n\nThe application can on a machine running Linux, MacOS or Windows, rather than being restricted to Windows.\n\nThe .NET Core implementation and tooling is still evolving, which may result in certain .NET Core releases requiring changes to the project structure.\n\nCommonly used libraries, such as Entity Framework, have specific versions for use with .NET Core. In some cases the APIs differ from the .NET v4.x versions.\n\nSome libraries have not yet carried out the work necessary to be compatible with .NET Core. It will not be possible to use these libraries until the required work is completed.\n"}
{"repositoryUrl": "https://github.com/apache/james-project.git", "path": "src/adr/0029-Cassandra-mailbox-deletion-cleanup.md", "template": "Nygard", "status": "Accepted (lazy consensus) & implemented", "firstCommit": "2020-05-07T02:11:46Z", "lastCommit": "2020-12-14T06:49:38Z", "numberOfCommits": 2, "title": "29. Cassandra mailbox deletion cleanup", "wordCount": 249, "authors": {"name1": 2}, "content": "# 29. Cassandra mailbox deletion cleanup\n\nDate: 2020-04-12\n\n## Status\n\nAccepted (lazy consensus) & implemented\n\n## Context\n\nCassandra is used within distributed James product to hold messages and mailboxes metadata.\n\nCassandra holds the following tables:\n - mailboxPathV2 + mailbox allowing to retrieve mailboxes informations\n - acl + UserMailboxACL hold denormalized information\n - messageIdTable & imapUidTable allow to retrieve mailbox context information\n - messageV2 table holds message metadata\n - attachmentV2 holds attachments for messages\n - References to these attachments are contained within the attachmentOwner and attachmentMessageId tables\n \nCurrently, the deletion only deletes the first level of metadata. Lower level metadata stay unreachable. The data looks \ndeleted but references are actually still present.\n\nConcretely:\n - Upon mailbox deletion, only mailboxPathV2 & mailbox content is deleted. messageIdTable, imapUidTable, messageV2, \n attachmentV2 & attachmentMessageId metadata are left undeleted.\n - Upon mailbox deletion, acl + UserMailboxACL are not deleted.\n - Upon message deletion, only messageIdTable & imapUidTable content are deleted. messageV2, attachmentV2 & \n attachmentMessageId metadata are left undeleted.\n\nThis jeopardize efforts to regain disk space and privacy, for example through blobStore garbage collection.\n\n## Decision\n\nWe need to cleanup Cassandra metadata. They can be retrieved from dandling metadata after the delete operation had been \nconducted out. We need to delete the lower levels first so that upon failures undeleted metadata can still be reached.\n\nThis cleanup is not needed for strict correctness from a MailboxManager point of view thus it could be carried out \nasynchronously, via mailbox listeners so that it can be retried.\n\n## Consequences\n\nMailbox listener failures lead to eventBus retrying their execution, we need to ensure the result of the deletion to be \nidempotent. \n\n## References\n\n - [JIRA](https://issues.apache.org/jira/browse/JAMES-3148)"}
{"repositoryUrl": "https://github.com/Crown-Commercial-Service/ReportMI-service-manual.git", "path": "docs/adr/0013-use-s3-for-storing-files.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-07-19T10:59:30Z", "lastCommit": "2018-12-04T15:36:30Z", "numberOfCommits": 3, "title": "13. Use AWS S3 for storing files", "wordCount": 223, "authors": {"name1": 2, "name2": 1}, "content": "# 13. Use AWS S3 for storing files\n\nDate: 2018-07-18\n\n## Status\n\nAccepted\n\n## Context\n\nThe Data Submission Service will need to store various files both during and\nafter the submission process.\n\n1. **Submission files** - the service will be receiving uploaded files from\nsuppliers each month. These files will need to be stored somewhere prior to\nprocessing, and will need to be retained for audit purposes for a period of time\nafterwards.\n1. **Finance export files** - the service will be producing a daily set of files\nto be transferred to the CCS finance system (Coda) to allow it to generate\ninvoices\n1. **Data Warehouse export files** - the service will be producing a daily set\nof files to be transferred to the CCS Data Warehouse to allow the MI team to\nperform analysis of the data.\n\nIn [ADR-0008][adr-0008] we decided to use Amazon Web Services (AWS) for hosting\nthe service. AWS offers object storage for this use-case: AWS [Simple Storage\nService][service-s3] (S3).\n\n\n## Decision\n\nWe will store submission and export files in AWS S3.\n\n## Consequences\n\nWe will configure the S3 buckets using Terraform as outlined in\n[ADR-0006][adr-0006].\n\nWe will need to be careful with the configuration of the S3 buckets to avoid\naccidental leakage of data. AWS provides useful tools to check that buckets\nare not publicly accessible.\n\n[adr-0006]: 0006-use-terraform-to-create-and-document-infrastructure.md\n[adr-0008]: 0008-use-aws-for-hosting.md\n[service-s3]: https://aws.amazon.com/s3\n"}
{"repositoryUrl": "https://github.com/libero/community.git", "path": "doc/adr/0008-libero-infrastructure.md", "template": "Nygard", "status": "Proposed", "firstCommit": "2020-02-05T10:22:10Z", "lastCommit": "2020-02-05T10:22:10Z", "numberOfCommits": 1, "title": "8. Limit scope of Libero infrastructure", "wordCount": 157, "authors": {"name1": 1}, "content": "# 8. Limit scope of Libero infrastructure\n\nDate: 2020-01-10\n\n## Status\n\nProposed\n\n## Context\n\nLibero infrastructure (servers, Kubernetes, buckets) supports Libero development by providing demo environments.\n\nService providers need documentation to learn to run Libero products.\n\nService providers cater for the disparate, very specific needs of publishers.\n\nService providers may consolidate their infrastructure with the rest of the platforms.\n\n## Decision\n\nLibero infrastructure should serve two purposes:\n\n- provide *demo* environments to showcase Libero products in certain configurations\n- provide realistic *reference* environments that do not serve real users but can be forked and adapted by service providers to kick start their Libero offering\n\n## Consequences\n\nDemo environments for Libero products are run on Libero infrastructure.\n\nOperational aspects that can be added to an environment without changing its architecture (e.g. backups, log aggregation) are considered out of scope for Libero infrastructure.\n\nLibero Infrastructure As Code is not directly runnable by a third party.\n\nLibero Helm charts are not directly installable by a third party.\n"}
{"repositoryUrl": "https://github.com/guttih/island.is-glosur.git", "path": "docs/adr/0008-use-oauth-and-openid-connect.md", "template": "Madr", "status": "accepted", "firstCommit": "2020-09-04T09:55:11Z", "lastCommit": "2020-09-04T09:55:11Z", "numberOfCommits": 1, "title": "Use OAuth 2.0 and OpenID Connect as protocols for Authentication and Authorization", "wordCount": 258, "authors": {"name1": 1}, "content": "# Use OAuth 2.0 and OpenID Connect as protocols for Authentication and Authorization\n\n- Status: accepted\n- Date: 2020-06-02\n\n## Context and Problem Statement\n\nWhat protocol(s) shall we use as the new standard for authentication and authorization.\nIt would be supported by our new centralized authority server and should be implemented in all new clients and\nresource systems needing authentication or authorization. A requirement might be made that the authority service need\nto support other protocols for legacy systems but all new systems should be encourage to use the same protocol.\n\n## Decision Drivers\n\n- Secure\n- Well defined and well reviewed standard\n- Easy to implement by client and resource systems\n- Support for non web client systems i.e. mobile devices\n\n## Considered Options\n\n- OAuth 2.0 + OpenID Connect\n- SAML 2.0\n\n## Decision Outcome\n\nChosen option: \"OAuth 2.0 + OpenID Connect\", because it is secure and well\nexamined and and has support libraries for our tech stack.\n\n## Pros and Cons of the Options\n\n### OAuth 2.0 + OpenID Connect\n\n- Good, because the authentication protocal is designed specifically to work with the authorization protocol.\n- Good, because it supports non web clients i.e. native apps.\n- Good, because it has certified, open source libraries for relying parties for OpenID authentication that match our\n  tech stack (javascript with typescript defenitions).\n- Bad, because it could require large tokens for authorization for multiple services, or split up tokens complicating\n  the process.\n\n### SAML 2.0\n\n- Good, because it is the currently used standard for legacy systems.\n- Bad, because it doesn't have good support for non web clients.\n- Bad, because main focus is on enterprise SSO, not centralized authorization.\n"}
{"repositoryUrl": "https://github.com/climatescape/climatescape.org.git", "path": "backend/doc/decisions/1-use-heroku.md", "template": "unknown", "status": null, "firstCommit": "2020-02-23T14:03:27Z", "lastCommit": "2020-02-24T16:09:23Z", "numberOfCommits": 3, "title": "(sem título)", "wordCount": 47, "authors": {"name1": 2, "name2": 1}, "content": "Decided to implement automatic data scraping backend as a customly built system deployed on Heroku, rather than using\na specialized platform for scraping such as Apify, because we think that the backend system will eventually outgrow mere\ndata scraping.\n\nSee [this message](https://github.com/climatescape/climatescape.org/issues/40#issuecomment-583264142) and the preceding\nmessages in the thread."}
{"repositoryUrl": "https://github.com/DFE-Digital/teaching-vacancies.git", "path": "documentation/adr/0001_get_postcode_from_coordinates.md", "template": "Nygard", "status": "approved", "firstCommit": "2019-10-15T13:51:48Z", "lastCommit": "2019-10-15T13:51:48Z", "numberOfCommits": 1, "title": "Use postcodes.io to get postcode from coordinates", "wordCount": 85, "authors": {"name1": 1}, "content": "# Use postcodes.io to get postcode from coordinates\nDate: 10/10/2019\n\n## Status\napproved\n\n## Context\nWe need to get a postcode from the coordinates we get from the browser.\n\n## Decision\nTo use postcodes.io instead of geocoder gem and just make a simple AJAX call from the browser.\n\n# Consequences\nWe avoid creating an endpoint on the server, therefore reducing the load we have to manage. On the other side we rely on a service that is less trusted than Google Maps, but open source and based on Open Data.\n"}
{"repositoryUrl": "https://github.com/adamAllaround/CardsApp.git", "path": "src/main/java/com/allaroundjava/architecture/decisionrecord/cardops-module-architecture.md", "template": "unknown", "status": null, "firstCommit": "2020-07-08T15:07:14Z", "lastCommit": "2020-07-11T06:45:45Z", "numberOfCommits": 2, "title": "Choice of application level architecture for cardops module", "wordCount": 154, "authors": {"name1": 2}, "content": "#Choice of application level architecture for cardops module\n\n##Context\nNeed to choose an applicable architectural approach to cardops module so that it can be consistently followed and the module can be developed and extended easily.\n\n##Drivers \n* The module contains business rules\n* It should be easy to prototype the solution\n* Time estimated for completion is 2 months\n* The application should be a prototype - not an enterprise class solution\n* The database should be quick to set up, best if it's known already\n* This domain is treated as a core domain\n* We're pootentially looking into changing the business rules around withdrawing, maybe introducing \nnew card types\n\n##Decision\nThis module is best realized with Hexagonal Architecture, aka ports and adapters. Since its the core of \nour domain. This architecture should enable easy prototyping and easy testing.\n  \n##Consequences\n* may be slightly more difficult to understand for newcomers\n\n##Other options\n* Standard three layer - harder to test the solution"}
{"repositoryUrl": "https://github.com/Azure-ukgov-paas/paas-team-manual.git", "path": "docs/architecture_decision_records/ADR016-end-to-end-encryption.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-02-23T15:02:33Z", "lastCommit": "2017-02-23T15:02:33Z", "numberOfCommits": 1, "title": "(sem título)", "wordCount": 188, "authors": {"name1": 1}, "content": "Context\n===\n\nIn order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.\n\nThere are 3 main network sections between the user and the application:\n\n* User to ELB\n* ELB to router\n* Router to cells\n\nDecision\n==\n\n* The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted.\n* The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS.\n* The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.\n\nStatus\n==\n\nAccepted\n\nConsequences\n==\n\nThe traffic is encrypted end-to-end between the user and the applications.\n"}
{"repositoryUrl": "https://github.com/NERC-CEH/datalab.git", "path": "architecture/decisions/0026-auth0-for-authentication.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-11-05T13:12:50Z", "lastCommit": "2017-11-05T13:12:50Z", "numberOfCommits": 1, "title": "26. Auth0 for authentication", "wordCount": 173, "authors": {"name1": 1}, "content": "# 26. Auth0 for authentication\n\nDate: 2017-11-04\n\n## Status\n\nAccepted\n\n## Context\n\nUser Authentication is a complex problem, can be time consuming to implement and errors\nin implementation can lead to security vulnerabilities. We feel that authentication,\nwhile critical, is not a differentiating factor and want to offload the work to a\nmanaged service.\n\n## Decision\n\nWe have opted to use [Auth0](https://auth0.com/) as our Identify provider. This gives us\na quick way to integrate authentication into our application with minimal effort and as\nan open source project we are able to use the service free of charge.\n\n## Consequences\n\nWe have a high quality identity provider with a rich management interface to get us\nstarted with user management workflows and identity.\n\nAuth0 also provides an extensive management API which will allow us to build our own\ninterfaces in the longer term if we choose.\n\nAuth0 provides the capability to federate identity with other identity providers. We may\nwant to allow user to authenticate using their CEDA accounts or Crowd accounts (in the\ncase of CEH) in the longer term.\n"}
{"repositoryUrl": "https://github.com/ASethi93/james.git", "path": "src/adr/0010-enable-elasticsearch-routing.md", "template": "Nygard", "status": "Accepted (lazy consensus) Additional performance testing is required for adoption.", "firstCommit": "2020-12-10T20:05:48Z", "lastCommit": "2020-12-10T20:05:48Z", "numberOfCommits": 1, "title": "10 Enable ElasticSearch routing", "wordCount": 204, "authors": {"name1": 1}, "content": "# 10 Enable ElasticSearch routing\n\nDate: 2019-10-17\n\n## Status\n\nAccepted (lazy consensus)\n\nAdditional performance testing is required for adoption.\n\n## Context\n\nOur queries are mostly bounded to a mailbox or a user. We can easily\nlimit the number of ElasticSearch nodes involved in a given query by\ngrouping the underlying documents on the same node using a routing key.\n\nWithout a routing key, each shard needs to execute the query. The coordinator\nneeds also to be waiting for the slowest shard.\n\nUsing the routing key unlocks significant throughput enhancement (proportional\nto the number of shards) and also a possible high percentile latency enhancement.\n\nAs most requests are restricted to a single coordination, most search requests will\nhit a single shard, as opposed to non routed searches which would have hit each shards \n(each shard would return the number of searched documents, to be ordered and limited \nagain in the coordination node). This allows to be more linearly scalable.\n\n## Decision\n\nEnable ElasticSearch routing.\n\nMessages should be indexed by mailbox.\n\nQuota Ratio should be indexed by user.\n\n## Consequences\n\nA data reindex is needed.\n\nOn a single ElasticSearch node with 5 shards, we noticed latency reduction for mailbox search (2x mean time and 3x 99 \npercentile reduction)\n\n## References\n\n - https://www.elastic.co/guide/en/elasticsearch/reference/6.3/mapping-routing-field.html\n - [JIRA](https://issues.apache.org/jira/browse/JAMES-2917)\n"}
{"repositoryUrl": "https://github.com/patshone-manifesto/hee-web-blueprint.git", "path": "pages/blueprint/adrs/0012-use-external-search-service.md", "template": "Nygard", "status": "Pending", "firstCommit": "2020-06-08T12:21:04Z", "lastCommit": "2020-06-29T13:59:04Z", "numberOfCommits": 2, "title": "(sem título)", "wordCount": 651, "authors": {"name1": 2}, "content": "---\nlayout: layouts/page.njk\ntitle: ADR-0012 Use external search service\npageTitle: ADR-0012 Use external search service\npageDescription: \npath: /blueprint\npermalink: /blueprint/adrs/ADR-0012-use-external-search-service.html\neleventyNavigation:\n  parent: Architecture decisions\n  key: ADR-0012 Use external search service\n  order: 12\n---\n\n# 12. Use external search service\n\nDate: 2020-06-08\n\n## Status\n\nPending\n\n## Context\n\nWe sought to determine whether the native out of the box solution provided by the platform nativly would be fit for purpose, or if an external service would be required. \n\nFor the purpose of the decision, the following were considered as requirements:\n\n* Flexible facet management\n* Synonym configuration \n* Query suggestions\n* Configurable misspelling tolerance\n* Natual language query support\n* Ability to integrate with external sources\n\n### Options\n\n#### Native Bloomreach Lucene Search\n\nThe OOTB search provided by the platform nativly is provided by Apache Lucene. This native search option provides standard search options, including free text search and faceted filters. Common query types are supported, such as wildcard searches. \n\nWhilst the native search functionality is fine for simple content websites, it lacks some of the enhanced functionality and configurability that the final solution will be asked to provide, including synonym configuration. \n\n#### Bloomreach Search & Merchandising (brSM) \n\nBloomreach offer an enhanced managed search service, called 'Search & Merchandising'\n\nhttps://www.bloomreach.com/en/products/search-merchandising\n\nThis tool provides several additional pieces of functionality including semantic understanding and personalisation. The product however is geared in the first instance to product and merchansing and many of the features are directed towards that usecase.  \n\nNHS digital are undertaking some work to prove out the use of brSM for this scenario. This is one of the first POCs using this search tool outside of a comemerce functionality. \n\n#### Algolia\n\nAlgolia is a decided search-as-a-service product that provides much of the functionality needed out of the box, including synonym support and filters and facets. It is also highly customisable through the UI, allowing for non-developers to configure, adjust and maintain the search offering. \n\nAlgolia also offers prebuilt configurable front end components, which make implementing the search experience quick and easily.\n\n#### Azure Cognative Search\n\nMicrosoft offer a cloud search service called Azure Cognative Search. This is a scalable search-as-a-service product, with a focus on machine learning powered capabilities such as optical character regognition \n\n#### Amazon Kendra/ Elasticsearch \n\nAmazon has long provided a well regarded open source search solution called Elasticsearch, which can be run on premises or on an EC2 or managed search instance. Amazon also offered Kendra, which is a machine learning based search service. \n\n## Decision\n\nWe believe that an external search service will be required to provide all of the capabailities that will ultimatly be needed to meet the complex user experience needs. Further to that, using an external search service will better suit the service based architectural model, where the search service will likely need to ultimatly take data from a variety of services such as the LKS document and colberation platform, and in future potentially other services such as Oriel and TIS. \n\nUsing a managed service such as Algolia provides a good balance between powerful and user friendly functionality and implementation complexity - Algolia was chosen as the basis for the POC in part owing to its comprehensive service offering combined with its prebuilt react component library offering fast and efficient implementation. \n\nBloomreach Search and Merchandising is an interesting option that provides advantages being tightly integrated into the core CMS project, however using it outside of commerce is somewhat unproven in the market. \n\nSearch as a service options such as Elastic or Azure Cognative search would also be viable candidates (assuming the organisational goal of aligning more functionality to MS's offerings, Azure would likely be recommended ahead of Elasticsearch), and the cost models of these offer likely cost savings, the trade off is more complexity to implement and maintain.  \n\n## Consequences\n\nAn integration with an external search provider brings additional complexity to the architectural stack, and requires additional considerations during development, as well as an additional ongoing cost for procuring the service (if a managed service is selected). \n"}
{"repositoryUrl": "https://github.com/ministryofjustice/opg-modernising-lpa-docs.git", "path": "src/adr/articles/0001-record-architecture-decisions.md", "template": "unknown", "status": null, "firstCommit": "2021-06-29T10:59:45Z", "lastCommit": "2021-07-13T07:12:11Z", "numberOfCommits": 7, "title": "(sem título)", "wordCount": 310, "authors": {"name1": 7}, "content": "---\nlayout: layouts/adr.njk\ntitle: \"Record architecture decisions\"\nweight: 1\ndate: 2021-06-27\nreview_in: 12 months\ntags: \n  - adr\n  - open_source\n  - open_data\nareas_of_coverage: [\"Digital Service\"]\nstatus: \"accepted\"\ncontributors: [\"John Nolan\"]\nadr_number: \"0001\"\n---\n\n## Context\n\nWe need to record the architectural decisions made on this project.\n\n## Technical\n\n### Interoperability - How does this enable the exchange of information\n\nAllows anyone to be able to follow and contribute to ongoing decisions made on the service.\n\n### Developer Knowledge - How well known is this in our current skill sets\n\n**Overall**: 8/10\nDevelopers are aware of ADRs, but their experience of doing them brings this score down a little.\n\n### Support/Open Source - Is it well supported\n\nFully Open Source.\n\n### Scalability\n\nUses Markdown and git to achieve scalability.\n\n## Ethics\n\n### Mitigate against being tech deterministic\n\nGiving visibility to our decisions and allowing contributions ensures Citizens and colleagues are able to have a voice and be aware of changes over time.\n\n### Ensure you conduct inclusive research\n\nThe design of the pages ensures the site is accessible to the highest standard. This is achieved by using the GDS pattern library.\n\nIn the future we should look at including a Welsh version to be more inclusive.\n\nLanguage within the ADRs will contain technical language so may not be accessible to persons not familiar with the terminology.\n\n### Think big and imagine what the impact of your work can be\n\nDecisions can be read and commented on inside and outside of government. Encouraging conversation with others exploring these technologies will enable better communities and feed back into our own decisions.\n\nThis will allow us to get a wider range of opinions on our decisions which we could not have got before.\n\n### Interrogate your data decisions\n\nN/A\n\n### Decision\n\nWe will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).\n\n### Consequences\n\nSee Michael Nygard's article, linked above.\n\nADRs will be public for visibility and collaboration.\n\nPull requests and Github issues can be used to drive conversations.\n"}
{"repositoryUrl": "https://github.com/LBHackney-IT/Data-Platform-Playbook.git", "path": "docs/architecture-decisions/records/002-ingest-google-sheets-data.md", "template": "unknown", "status": null, "firstCommit": "2021-04-30T13:50:54Z", "lastCommit": "2021-11-10T18:06:13Z", "numberOfCommits": 7, "title": "(sem título)", "wordCount": 121, "authors": {"name1": 1, "name2": 3, "name3": 2, "name4": 1}, "content": "---\nid: ingest-google-sheets-data\ntitle: \"Ingest Google Sheets Data - ADR 002\"\ndescription: \"\"\ntags: [adr]\nnumber: \"2\"\ndate-issued: \"2021-03-23\"\nstatus: \"Accepted\"\n---\n\n## Context\n\nHackney currently have datasets distributed over a number of different Google Sheets Documents due their data recovery\nefforts. We need to get this information pulled into the data platform for processing.\n\n## Decision\n\nWe will use a python based AWS Glue Job in conjunction with the gspread python library to pull the data onto platform\n\n## Consequences\n\nHaving the code sit with AWS Glue makes the data import easier to keep consistent with the rest of the platform but\nleaves us dependent on being able to import the gspread library into the AWS Glue job and requires that we access\ncredentials from Google Workspace\n"}
{"repositoryUrl": "https://github.com/chef/automate.git", "path": "dev-docs/adr/adr-2018-10-04.md", "template": "Nygard", "status": "* ACCEPTED (2018-10-24) * PROPOSED (2018-10-04)", "firstCommit": "2019-04-10T11:56:29Z", "lastCommit": "2019-04-10T11:56:29Z", "numberOfCommits": 1, "title": "ADR 2018-10-04: Names Not UUIDs for IAM API", "wordCount": 788, "authors": {"name1": 1}, "content": "# ADR 2018-10-04: Names Not UUIDs for IAM API\n\n## Status\n\n* ACCEPTED (2018-10-24)\n* PROPOSED (2018-10-04)\n\n## Context\n\nIn our current IAM implementation, some services identify entities using human-readable names and other services identify entities using UUIDs (which we'll refer to as IDs in the following discussion). \n\nUsing IDs makes ensuring entity uniqueness easier. With UUIDs we can allow independent and concurrent creation of identifiers across multiple instances of a given service. IDs have performance advantages for use in data stores since comparisons are faster and indices are smaller (in general) than those that use a natural key.\n\nAPIs that can be accessed using human-readable names tend to provide a more self-documenting protocol whereas those that deal only in IDs result in message payloads that require additional tooling to map into a human readable form. A named-based approach requires verifying uniqueness within a data store to maintain data consistency.\n\nWhen users are the one to select a name, they often want the ability to change their mind and change the name. Rename functionality improves usability by letting users reflect their updated understanding into the system. Renames also introduce implementation challenges as one needs to decide when and how to cascade renames throughout the system.\n\nWhen we consider building aggregate functionality combinging the capabilities of multiple services, we want to be able to share an entity identifier across the bounded context in which the entity resides. Once this occurs, changing the identifier (rename) becomes more complicated and, definitionally, will be outside of a transaction. Strategies to handle this distributed cascade problem include event driven architecture where such updates are published to the event bus and providing long-lived aliases such that previous identifiers continue to work (e.g. github repo rename behavior).\n\nLooking across the A2 API, today most messages use an `id` field as the name for the entity's unique identifier. This does not align with Google's API design guidance (https://cloud.google.com/apis/design/resource_names) the requires the field to be called `name`. The proposed decision is to make use of `name` following Google's API design guide for new APIs and to commit to evolving existing APIs to support `name` as well. The alternative is to agree that our identifiers are called `id` regardless of their form. Either way, a goal of this decision is to define a standard that will result in increased consistency across the A2 API.\n\n## Decision\n\nFor IAMv2 we will use human-readable resource identifiers for the user visible protocol. This means that policies, roles, scopes, teams, and users will all be identified using unique \"names\" in the public GRPC API and resulting HTTP/JSON API that are intended to be human readable \"friendly\" names.\n\nWe will disallow renames to an entity's identifying name so that the names can be shared outside of the IAMv2 system.\n\nWe will support a \"display name\" field for entities in IAMv2 that users can use to describe the item and that we can use for UI display. The display name can be renamed. The UX is undecided, but we can consider flows where resource name is derrived from display name or vice versa to reduce the number of inputs a user needs to provide when creating things. The GCP projects API serves as a good example. In general, uniqueness should be enforced for `display_name` since the intention is to provide a human readable and changable label for distinct items.\n\nIn the IAMv2 data store (postgresql), we will use UUIDs and the core IAM services will handle UUID/name mapping and uniqueness constraint enforcement.\n\nWe will use the `id` field to store the unique, unchanging identifier in our GRPC protocol message definitions. We will use `display_name` for the can-be-edited name. For now, we are deciding not to adopt `name` as the field name for an entity's unique identifier as suggested in Google's [design guidance](https://cloud.google.com/apis/design/resource_names). While we'd like to follow those guidelines, the change would cause more disruption than benefit at this time. We can reconsider in future.\n\nWhen using names instead of UUIDs, special consideraton needs to be given to handling delete operations for entities that may be referenced outside of their originating bounded context. The system cannot rely on cascading delete behavior when a given identifier is stored across different bounded contexts in the system. In these situations, we have to consider what happens when an item is deleted and then a new item is created with the same id. If the system allows this re-use of id after delete, then it must ensure that all references are removed or invalidated. Alternatively, the system can implement a delete marker (aka \"tombstone\") approach and disallow deleted identifiers from ever being reused.\n\n## Consequences\n\nOnce this decision is implemented, we will assess the resulting context by 2019-03-10.\n\n"}
{"repositoryUrl": "https://github.com/navikt/laundromat.git", "path": "docs/adr/0003-choice-of-entities.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-07-09T13:06:11Z", "lastCommit": "2020-09-01T23:03:13Z", "numberOfCommits": 2, "title": "3. Choice of entities for the Named Entity Recognizer", "wordCount": 220, "authors": {"name1": 2}, "content": "# 3. Choice of entities for the Named Entity Recognizer\n\nDate: 08-07-2020\n\n## Status\n\nAccepted\n\n## Context\nThe choice of entities, i.e. what categories of information we want to anonymise and the specificity of these categories, will greatly impact the performance of our model. Additionally, time constraints mean that there is an upper limit to how many entities can be included. As such, we concluded that those entities which fulfill at least two of the following three categories should be included:\n * It is either directly identifying or a close proxy (e.g. names, phone numbers, etc.)\n * It is a so-called “special categories” of information (e.g. medical information)\n * It is present in the data in non-trivial quantities \n\n## Decision\nWe have chosen the following NER entities:\n * ORG (Organisation)\n * LOC (Location)\n * PER (Person)\n * FNR (Personal number)\n * MONEY\n * DATE_TIME (Dates, time of day, name of day, and name of month)\n * MEDICAL_CONDITIONS\n \n\nEntities that will be left purely to RegEx are:\n * NAV_YTELSE and NAV_OFFICE\n * AGE\n * TLF (Telephone number)\n * BACC (Bank account number)\n\nWe believe this list strikes the right balance between performance (fewer entities are better) and coverage.\n\n## Consequences\n\nSince data will be labeled with these entities, changing this list will require substantial resources and possible relabeling of data. Increasing granularity especially will be difficult to achieve if desired. These will most likely be the entities used going forward.\n\n"}
{"repositoryUrl": "https://github.com/button-inc/service-development-toolkit.git", "path": "docs/decisions/7_directory-structure.md", "template": "unknown", "status": null, "firstCommit": "2020-11-21T00:43:53Z", "lastCommit": "2020-11-22T06:46:44Z", "numberOfCommits": 3, "title": "Design Decision 7 - Directory Structure", "wordCount": 183, "authors": {"name1": 1, "name2": 2}, "content": "# Design Decision 7 - Directory Structure\n\n## Background\n\nDecision for how to organize compiled npm package.\n\n## Findings\n\nWe considered module-support (See decision 6), bundler tools, and general structure. 3 major react libraries were evaluated for comparison:\n\n- [react-bootstrap](https://www.npmjs.com/package/react-bootstrap)\n- [semantic-ui react](https://github.com/Semantic-Org/Semantic-UI-React)\n- [material-ui](https://github.com/mui-org/material-ui)\n\n| React Bootstrap   | Semantic UI  | Material UI   |\n| --- | -- | --- |\n| Supports cjs, ems, and umd. Uses webpack to bundle. Matches our current structure. Builds components in typescript. | Supports cjs, ems, and umd. Structures components into individual folders. | Supports cjs, ems, and umd. Uses rollup as bundler. Organizes components into folders. Builds components in js, and exports type support for typescript |\n\n## Decision\n\nAfter evaluating existing structures and options, decided to use a structure similar to react bootstrap, with directories organized as:\n\n### Component-library\n\n```\n└───src\n│   └───Component1\n│   │   index.ts\n│   │   component1.ts\n│   │   ...\n│   └───Component2\n│   │   index.ts\n│   │   component2.ts\n│   │   ...\n```\n\nAnd structure the compiled npm module as:\n\n### Output package\n\n```\n└───Component-library\n|   └───lib\n│   └───Component1\n│   │   index.ts\n│   │   component.ts\n│   │   package.json\n│   │   ...\n│   └───Component2\n│   │   index.ts\n│   │   component.ts\n│   │   package.json\n│   │   ...\n|   ...\n│   └───esm\n|   |   Component1.js\n|   |   Component1.d.ts\n|   |   Component2.js\n|   |   Component2.d.ts\n|   |   ...\n│   └───cjs\n|   |   Component1.js\n|   |   Component1.d.ts\n|   |   Component2.js\n|   |   Component2.d.ts\n|   |   ...\n│   └───dist\n```\n\nExample component level package.json\n\n```\n{\n  \"name\": \"component-library/Button\",\n  \"private\": true,\n  \"main\": \"../cjs/Button.js\",\n  \"module\": \"../esm/Button.js\",\n  \"types\": \"../esm/Button.d.ts\"\n}\n```\n\nThis will meet the requirement for decision 6 to support esm, umd, and cjs support, and also support importing individual components.\n"}
{"repositoryUrl": "https://github.com/ammarnajjar/reading-todo.git", "path": "adr/003-postgres-as-database.md", "template": "unknown", "status": null, "firstCommit": "2019-09-19T19:42:31Z", "lastCommit": "2019-09-29T05:06:21Z", "numberOfCommits": 3, "title": "PostgreSQL as the RDBMS", "wordCount": 114, "authors": {"name1": 3}, "content": "# PostgreSQL as the RDBMS\n\nDate: Thu Sep 19 21:42:04 CEST 2019\n\n## Milestone\n\n0.1.0\n\n## Context\n\nFor the project a RDBMS need to be chosen and adopted.\n\n## Decision\n\n- [PostrgreSQL](https://www.postgresql.org/) will be the relational database management system of choice.\n\n## Consequences\n\nWhy:\n\n- Open source.\n- Free.\n- Available for many operating systems.\n- Its SQL implementation closely follows ANSI standards.\n- Widely used and well documented, so finding help is no issue.\n- Supported by many platforms for deployment (Amazon, Azure).\n- Official docker support.\n- For my use case, a very simple database, this looks so appealing, fast and easy to use.\n\nWhy not:\n\n- Not the bleeding edge technology.\n- Not the fastest DBMS\n- Not so much adopted in the commercial world.\n\n## References (optional)\n\n- [Official Website](https://www.postgresql.org/)\n- [SQLite vs MySQL vs PostgreSQL](https://www.digitalocean.com/community/tutorials/sqlite-vs-mysql-vs-postgresql-a-comparison-of-relational-database-management-systems)\n- [Wikipedia](https://en.wikipedia.org/wiki/PostgreSQL)\n"}
{"repositoryUrl": "https://github.com/joejag/wikiindex.git", "path": "doc/arch/adr-006-config.md", "template": "unknown", "status": null, "firstCommit": "2015-03-01T22:47:27Z", "lastCommit": "2015-03-01T22:47:27Z", "numberOfCommits": 1, "title": "Config", "wordCount": 47, "authors": {"name1": 1}, "content": "# Config\n\n## Context\n\n* We need to set the port and other config values for our application\n* We do not know the deployment environment yet\n\n## Decisions\n\n* We will use environment variables to configure the app, rather than a file\n\n## Alternatives Considered\n\n* A YAML file would allow for structured config"}
{"repositoryUrl": "https://github.com/edinella/micro.git", "path": "docs/adr/0002-have-a-single-repository-for-all-microservices.md", "template": "Madr", "status": null, "firstCommit": "2020-10-12T19:16:44Z", "lastCommit": "2020-10-12T19:16:44Z", "numberOfCommits": 1, "title": "Have a single repository for all microservices", "wordCount": 122, "authors": {"name1": 1}, "content": "# Have a single repository for all microservices\n\n## Context and Problem Statement\n\nA repository for each microservice or only one for all of them?\n\n## Decision Drivers\n\n* Development agility\n* Microservices decoupling\n* Big-picture scenario visibility\n\n## Considered Options\n\n* A repository for each microservice, and another for service definitions\n* A single repository with decoupled microservices including service definitions\n\n## Decision Outcome\n\nChosen option: A single repository for all microservices and service definitions this time, because this way its easy to get a big picture of the approach, for the purposes of this example.\n\nIn production, I'll consider separating microservices and service definitions on repos, in order to have separate triggers on CI for them.\n\nI think [this approach](https://medium.com/namely-labs/how-we-build-grpc-services-at-namely-52a3ae9e7c35) will be a good fit for address this concern in production.\n"}
{"repositoryUrl": "https://github.com/zooniverse/front-end-monorepo.git", "path": "docs/arch/adr-15.md", "template": "Nygard", "status": "Superseded by [ADR-22](adr-22.md).", "firstCommit": "2019-07-09T15:56:53Z", "lastCommit": "2019-12-19T15:55:26Z", "numberOfCommits": 3, "title": "ADR 15: Drawing tools API", "wordCount": 677, "authors": {"name1": 2, "name2": 1}, "content": "# ADR 15: Drawing tools API\n\nCreated: June 19, 2019\nUpdated: October 31, 2019\n\n## Context\n\nThe way the drawing tools currently function on Panoptes-Front-End (PFE) have numerous issues including:\n\n- Updating the classification annotation on each touch or pointer event which causes unnecessary re-rendering of the DOM\n- The separation concerns are not clear between components and stores. Multiple components can update the annotation making it hard to debug or add new features to. \n  - Example: The `MarkingsRenderer` and the `FrameAnnotator` both call change handlers that update the classification annotation? Can the drawing annotation be updated by both or is one solely responsible? It is unclear by reading the code. Why does something named `MarkingsRenderer` update the annotation?\n- Drawing tools have a complex API that involves exposing static methods to be called by their parent component\n- Annotation / classification payloads have no consistent standards for describing data: some tools mark rotation in differing directions, for example.\n\n## Decision\n\nWhat we do not want to do:\n- Re-render on every pointer or touch event.\n- update annotation state while drawing is in progress.\n- support more than one drawing task in a step.\n- Use D3.js since it has its own internal data store and it would be complicated to integrate that with a observable stream.\n\nWhat we do want to do:\n- Have a component, the interactive layer, that manages the interaction with the marks and pointer and touch events.\n  - The interactive layer should not allow events to bubble so the events are encapsulated to just the interaction with the subject. This is to help prevent browser scroll during drawing. An attempted fix on PFE for reference: zooniverse/Panoptes-Front-End#5411\n  - Events will be observed and be streamed via an observable. We will use rx.js to create an observer/observable event stream.\n  - The last state of the event stream will be used to add an annotation to the classification when next or done is clicked.\n  - The interactive layer will have local component state to keep track of which mark is selected for editing as well as handle the edit and delete events. \n- Have a component, the markings renderer, that manages the rendering of previous annotation marks as read only. It will hide them if hide previous marks is toggled.\n- These two components will initially use separate SVG layers that connect to the same stores. Later any duplicated code will be refactored into a higher order component.\n- Have a multi-image subject viewer. We will have to support projects that want each frame to have independent drawing annotations as well as projects that want each frame to have the same drawing annotations. Each frame should have the same pan/zoom function. We have two proposed options in implementation:\n  -  Create an interactive layer and markings renderer for each frame. Each interactive layer will initialize its own event stream.\n  - Create a single interactive layer and markings renderer and filter what is rendered by the frame index.\n  - Projects have requested each frame to have the same pan/zoom function, but we were unable to implement in PFE: zooniverse/Panoptes-Front-End#3465\n  - Are there any cases where projects want separate pan/zoom function for each frame?\n- Have a schema, or set of schemas, describing annotations.\n\n## Status\n\nSuperseded by [ADR-22](adr-22.md).\n\n## Consequences\n\n- This is the first time rx.js will be used in our code and there will be a learning curve.\n- The current feedback code may need refactoring as it is written only for the D3.js interactive plot subject viewers.\n\n### rx.js use\n\nWe had an early prototype at the start of this proposal using rx.js. This library is an implementation of the proposed observable specification for javascript and has an API for use with browser DOM events. After several months of experimentation, we have decided that we will proceed with implementing the drawing tools just with the browser pointer events API and potentially integrate rx.js at a later date as an enhancement.\n\nIn retrospect, trying to incorporate rx.js increased the complexity for implementation and for learning and contributed to delays. For future experiments, we should be sure to structure how we'll go about the experiment including evaluation milestones for its use from the start. \n"}
{"repositoryUrl": "https://github.com/Ensembl/gti-genesearch.git", "path": "doc/adr/adr-016.md", "template": "unknown", "status": null, "firstCommit": "2017-11-23T15:55:20Z", "lastCommit": "2018-04-30T18:33:05Z", "numberOfCommits": 2, "title": "ADR 015: Single type indices", "wordCount": 78, "authors": {"name1": 2}, "content": "# ADR 015: Single type indices\n\n## TL;DR\nSwitch from one index with multiple types to multiple indices.\n\n## Context\nElastic now recommend that there should only be one document type per index, and this feature will be dropped in future:\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/removal-of-types.html\n\nTo this end, we will make this change now to avoid future pain. This is particularly important as we bring in variants and other types into Elastic.\n\n## Status\nUnder development\n\n## Consequences\nmore config needed\nmore flexibility\n\n## Tags\n"}
{"repositoryUrl": "https://github.com/dxw/support-rota.git", "path": "doc/architecture/decisions/0002-use-a-changelog-for-tracking-changes-in-a-release.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-08-19T15:12:23Z", "lastCommit": "2020-08-19T15:12:23Z", "numberOfCommits": 1, "title": "2. Use a changelog for tracking changes in a release", "wordCount": 121, "authors": {"name1": 1}, "content": "# 2. Use a changelog for tracking changes in a release\n\nDate: 2019-09-13\n\n## Status\n\nAccepted\n\n## Context\n\nDocumenting changes for a release can be challenging. It often involves reading\nback through commit messages and PRs, looking for and classifying changes, which\nis a time consuming and error prone process.\n\n## Decision\n\nWe will use a changelog (`CHANGELOG.md`) in the\n[Keep a Changelog 1.0.0](https://keepachangelog.com/en/1.0.0/) format to be\nupdated when code changes happen, rather than at release time.\n\n## Consequences\n\nThis will make compiling releases much simpler, as the process for determining\nchanges in a release is simply a matter of looking at the changelog.\n\nThis does add some overhead to making code changes, and requires that releases\nupdate the changelog as part of their process, but those overheads are small.\n"}
{"repositoryUrl": "https://github.com/zooniverse/galaxy-zoo-touch-table.git", "path": "docs/arch/adr-6.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-07-26T19:49:15Z", "lastCommit": "2019-07-26T19:49:15Z", "numberOfCommits": 1, "title": "ADR 6: Choosing a Test Framework", "wordCount": 347, "authors": {"name1": 1}, "content": "## ADR 6: Choosing a Test Framework\nJuly 26, 2019\n\n### Context\nThe app needs tests. This is a must. Although test driven development is best, the app added tests after development began to test logic and encourage future changes do not break functionality. It would be best to test all aspects of the MVVM architecture, but view models are where the majority of the app logic lives.\n\n### Decision\nSeveral testing frameworks exist for .NET apps, with the main three being xUnit, NUnit and MSTest. The app uses xUnit to test view models, but any of the other two frameworks would've also been suitable. I chose xUnit because the testing syntax looked cleaner, the framework was a bit newer, and I found some nice tutorials that recommended xUnit above the other two. Perhaps most importantly, I wanted to get started with testing instead of spending too much time debating the merits of which one to chose.\n\nPerhaps just as important as which testing framework to use is which mocking package to use when faking API or other function calls in my tests. For that, Moq seems to be the clear winner. It's easy to find documentation online about using Moq with each testing framework.\n\n### Status\nAccepted\n\n### Consequences\nGetting started with testing is an undertaking, and it is very easy to let tests fall by the wayside when the touch table has a deadline and new features need to be added quickly. However, many xUnit tutorials seem incomplete and only test the view models. I'm curious if this is a sufficient amount of test coverage.\n\n_In Retrospect:_ Unfortunately, I added tests later in the development process, and that caused some unnecessary delay while I was restructuring the architecture. This could have been avoided if I was a bit more familiar with how [Moq](https://www.nuget.org/packages/moq/) worked from the outset of building the app. I also had to explore using [Unity](https://www.nuget.org/packages/Unity/) to make sure dependency injection was in place for my unit tests. For more information in using Unity, refer to the `ContainerHelper` class in the `Lib` folder of the app.\n"}
{"repositoryUrl": "https://github.com/opentransportmanager/otm-docs.git", "path": "docs/adr/simulator_language.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-05-14T14:15:22Z", "lastCommit": "2020-05-14T14:15:22Z", "numberOfCommits": 1, "title": "Simulator Language", "wordCount": 32, "authors": {"name1": 1}, "content": "# Simulator Language\n\n## Status\n\nAccepted\n\n## Context\n\nSimulator should do multiple calculations in a short time. Choice should be dictated by the speed of code execution.\n\n## Decision\n\nGolang\n\n## Consequences\n\n- Multithreading\n- Dynamic typed and compiled"}
{"repositoryUrl": "https://github.com/CleverCloud/clever-components.git", "path": "docs/adr/adr-0005-how-should-we-implement-copy-to-clipboard.md", "template": "unknown", "status": null, "firstCommit": "2020-12-14T13:36:32Z", "lastCommit": "2021-10-18T12:35:25Z", "numberOfCommits": 3, "title": "(sem título)", "wordCount": 414, "authors": {"name1": 2, "name2": 1}, "content": "---\nkind: '📌 Architecture Decision Records'\n---\n\n# ADR 0005: How should we implement copy to clipboard?\n\n🗓️ 2019-10-05 · ✍️ Hubert Sablonnière\n\nThis ADR tries to explain the choices we made to add a copy to clipboard feature on our `<cc-input-text>` component.\n\n## Technical choices?\n\n### Is there a standard way to achieve this?\n\nWe use to use [document.execCommand()](https://developer.mozilla.org/en-US/docs/Web/API/document/execCommand) to do so but there is now a standard way to do it which is based on promises.\nIt's called [clipboard.writeText()](https://developer.mozilla.org/en-US/docs/Web/API/Clipboard/writeText).\nThe problem is that it does not work on Safari 12+.\n\n### Are there any polyfills?\n\nYes, there is [clipboard-polyfill](https://github.com/lgarron/clipboard-polyfill).\nIt has most of the `Clipboard` object API but it also means lots of things we don't need.\nIt's 2.3 KB (min+gzip).\n\n### Alternative?\n\nThis polyfill is small and working but we found a smaller lib that only does that: [clipboard-copy](https://github.com/feross/clipboard-copy).\nIt's basically a \"use `clipboard.writeText()` and fallback to `document.execCommand()`\" in 433 B (min+gzip).\nIt's made by [feross](https://github.com/feross) ;-).\n\n## UI/UX choices?\n\nWe looked at how others were doing \"copy to clipboard\" like the copy URL to clone in GitHub and GitLab.\nWe also looked at 1password and a few others.\nMost of them have an icon button on the right which is grouped with the text input.\nThe button has the same behaviour and states as other buttons in the site.\nSometimes the icon is a clipboard, sometimes it's the classic copy icon (2 files on top of each other).\n\nWe tried to adapt our existing `<cc-button>` to support icons and then embed it in the `<cc-input-text>` but it was easier just to use a `<button>` directly.\nWe also tried to apply the same behaviour and states (focus, active, hover...) we had on our `<cc-buttons>` but it was not a good idea:\n\n* The box shadow thing we have on hover that disappears when active was not working with the rest of the design.\n* The whole concept of a button as we have with `<cc-button>` grouped visually with `<cc-input-text>` would not work in `multi` mode with lots of lines since the height can be quite big.\n\nThen we got inspiration from Slack's buttons/actions that are in the main input text of the chat.\nThose are just icons at first but they have hover, focus and active states that make sense.\n\nIn the end, we decided to:\n\n* Have an icon button like Slack does\n* Align it to top right, whatever the height\n* Make it display a green tick for a second after the click (just like GitHub does in Pull Request copy branch)\n"}
{"repositoryUrl": "https://github.com/zooniverse/tove.git", "path": "docs/arch/adr-02.md", "template": "unknown", "status": null, "firstCommit": "2019-10-21T22:25:10Z", "lastCommit": "2019-10-24T22:24:12Z", "numberOfCommits": 2, "title": "ADR 02: Credential Storage", "wordCount": 537, "authors": {"name1": 2}, "content": "# ADR 02: Credential Storage\n\n* Status: accepted\n* Interested parties: @zwolf, @camallen, @adammcmaster\n* Date: October 21, 2019\n\n## Context\n\nRails apps need access to environment-based credentials (API keys, db URLs and passwords, etc). We do this a few different ways across all of our Rails apps. This ADR is a chance to take everyone's temperature on using a neat but new bit of Rails 6 and inform similar decisions later.\n\n## Considered Options\n\n* Kubernetes secret storing encoded environment variables\n* Rails internal credential storage solution\n* Kubernetes mounted dotenv volume\n\n### Option 1: Kubernetes encoded env variables\n\nA list of environment variables are base64-encoded and piped into a k8s secret. Loaded by being added individually to the templates.\n\nPros:\n* Our current standard (Caesar, PRNAPI)--or as close to one as we have.\n* Each var exists in template, so the contents are clearly defined.\n\nCons:\n* Whole base64 encoding thing makes reading/editing credentials a chore\n* Credentials are stored entirely seperate from the app, tying their values to deployment/k8s instead of to the app.\n\n### Option 2: Rails internal credentials\n\nAs of Rails 5.1, Rails supports storing its own credentials. Rails 6 includes support for this feature across multiple environments. A `config/[environment].yml.enc` file is encrpyted with a `config/[environment].key`. The latter is stored as a k8s secret and mounted in `/config`. The encrypted yml file can then theoretically be included in version control, but could also be stored in the same volume mount if that makes people nervous. Development key+creds can be kept in git and are used by default (via `RAILS_ENV`).\n\nSyntax for the Rails helpers is as follows:\n`rails edit:credentials --environment staging`\n\nThe `--environment` arg looks for `config/[environment].key` to decode `config/[environment].yml.enc`.\n\nPros:\n* Simpler templates, since every var doesn't have to be included to still be accessible\n* Follows new conventions, built into Rails.\n* Keeps the app's requirements within the context of the app. A record is kept (potentially versioned, even) and redeployment (say, to Azure) has less steps.\n\nCons:\n* Rails 6 only (for multiple envs). Our other apps will need upgrade all the way to use the same functionality.\n* Different. This already an issue with our various other Rails apps, so it would be yet another strategy, but a fairly self-documenting one.\n* Rails 6 is released and stable, but this feature is kind of new. 5.1 was a while ago, though.\n\n\n### Option 3: k8s Mounted secrets volumes\nUsed by old Rails apps deployed to k8s (eduapi, for instance). Roughly the same as Option 1, since it's a list of envvars that is loaded by k8s into the environment, only with a mounted volume that completely obfuscates the contents everywhere. So it's like the first one, only worse. Including for reference, but not the direction we want to go.\n\n## Decision Outcome\n\nDecided to go with Option 2, Rails credentials. It's the most forward-looking option and isn't terribly different from existing setups. There's even a precedent in the graphql stats API. Also, as it's already being done in the aforementioned API, we're going to store encoded credentials in the repo.\n\n### Links\n* rails docs: https://edgeguides.rubyonrails.org/security.html#custom-credentials\n* PR that added environment specificity: https://github.com/rails/rails/issues/31349\n* quick blog post on use: https://blog.saeloun.com/2019/10/10/rails-6-adds-support-for-multi-environment-credentials.html\n"}
{"repositoryUrl": "https://github.com/kbremner/read-more-api.git", "path": "ReadMoreAPI/doc/adr/0007-deploy-to-heroku.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-09-26T11:05:25Z", "lastCommit": "2017-09-26T11:05:25Z", "numberOfCommits": 1, "title": "7. Deploy to Heroku", "wordCount": 227, "authors": {"name1": 1}, "content": "# 7. Deploy to Heroku\n\nDate: 2017-09-24\n\n## Status\n\nAccepted\n\n## Context\n\nThe application needs to be deployed somewhere.\n\nThere are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.\n\nHeroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https://devcenter.heroku.com/articles/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.\n\nHeroku has several pricing tiers for machines that the application will run on, including a free tier.\n\nHeroku provides a free hosted PostgreSQL option. It will handle setting a \"DATABASE_URL\" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.\n\nWe want the setup process to be as simple as possible.\n\n## Decision\n\nWe will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.\n\n## Consequences\n\nWe will have no costs associated with hosting the application.\n\nThe suitability of the free tier will need to be investigated if performance becomes an issue.\n\nA database will not need to be setup seperately.\n\nThe suitability of the free database tier will need to be investigate if performance or the quantity of data becomes an issue."}
{"repositoryUrl": "https://github.com/cljdoc/cljdoc.git", "path": "doc/adr/0019-use-custom-search.md", "template": "Nygard", "status": "Proposed", "firstCommit": "2019-06-05T07:46:06Z", "lastCommit": "2019-06-05T07:46:06Z", "numberOfCommits": 1, "title": "Implement our own indexing and artifacts search", "wordCount": 224, "authors": {"name1": 1}, "content": "# Implement our own indexing and artifacts search\n\n## Status\n\nProposed\n\n## Context\n\nWe need to be able to search (documented) projects, whether \nthey come from Clojars or Maven Central.\n\nSee https://github.com/cljdoc/cljdoc/issues/85\n\n## Decision\n\nImplement our own search, using direct integration with Lucene. Download and \nindex artifact list from Clojars and \"org.clojure\" artifacts from Maven Central.\n\nWe will use Lucene as that is the absolutely prevailing solution for search in\nJava. Direct Java interop is quite idiomatic in Clojure; it isn't too much work as we\nonly need to implement the parts relevant for us and not a generic Lucene wrapper.\nWe avoid the risk of depending on incomplete and potentially abandoned library\n(as happend to Clojars with clucy). And to be able to use Lucene efficiently we\nneed to understand it sufficiently anyway.\n\n## Consequences\n\n* Our search results will be different from those you get from Clojars, we won't\n  benefit from any improvements on Clojars' side\n* We will be able to search also for non-Clojars projects (and thus also make \n  their docset available in Dash)\n* Cljdoc will become slightly more complex and possibly expensive to operate \n  (due to downloading the ~10MB index from Clojars regularly)\n* New projects will not appear immediately in the search until the next scheduled\n  indexing (unless we mitigate that somehow)\n* We can fine-tune the search to prioritize the results we want"}
{"repositoryUrl": "https://github.com/Azure-ukgov-paas/paas-team-manual.git", "path": "docs/architecture_decision_records/ADR013-building-bosh-releases.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-02-23T15:02:33Z", "lastCommit": "2017-02-23T15:02:33Z", "numberOfCommits": 1, "title": "(sem título)", "wordCount": 163, "authors": {"name1": 1}, "content": "Context\n===\nWe use [Bosh](https://bosh.io/) to create and manage our cloudfoundry deployment on AWS.\nTo deploy software, Bosh needs certain binary dependencies available.\nThese are known as bosh [releases](https://bosh.io/docs/release.html).\n\nBefore this decision, we usually built and uploaded releases to Bosh as part of our [concourse](https://concourse.ci/) pipeline.\nOccasionally, we would manually build a release, store it on GitHub, and point Bosh to it there. \n\n### Building Bosh Releases\n\nWe investigated different approaches to creating bosh releases, in particular\n\n* Multiple pipelines created dynamically using [branch manager](https://github.com/alphagov/paas-concourse-branch-manager)\n* A single pipeline using [pullrequest-resource](https://github.com/jtarchie/pullrequest-resource)\n\nThe work on these spikes was recorded in\nhttps://www.pivotaltracker.com/n/projects/1275640/stories/115142265\nhttps://www.pivotaltracker.com/n/projects/1275640/stories/128937731\n\nDecision\n==\nWe will use the [pullrequest-resource](https://github.com/jtarchie/pullrequest-resource) approach to build all our Bosh releases in a consistent way.\n\nStatus\n==\nAccepted\n\nConsequences\n==\nWe must gradually migrate all our Bosh release builds to their own build pipelines.\nWe will need separate jobs to build from master - this already has a proof of concept in the spike.\nWe may have to add additional config in projects we fork to allow us to create final builds.\n"}
{"repositoryUrl": "https://github.com/alphagov/paas-team-manual.git", "path": "source/architecture_decision_records/ADR002-concourse-pool-resource.html.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-04-26T16:22:14Z", "lastCommit": "2021-03-18T22:01:24Z", "numberOfCommits": 5, "title": "(sem título)", "wordCount": 149, "authors": {"name1": 2, "name2": 1, "name3": 1, "name4": 1}, "content": "---\ntitle: ADR002 - Concourse pool resource\n---\n\n# ADR002: Concourse pool resource\n\n## Context\n\nWhen building pipelines using concourse, we investigated using the [pool\nresource](https://github.com/concourse/pool-resource) in order to control flow\nthrough jobs. This was an alternative to the use of the\n[semver resource](https://github.com/concourse/semver-resource).\n\nThese 2 resources are both workarounds to solve the problem of triggering jobs\nwhen we haven't made changes to a resource.\n\nThe problem is that the pool resource relies on write access to a github repo,\nwhich means we must pass public keys that allow this access into the pipeline\nand deployed concourse instance - we want to minimise the number of credentials\nwe pass, and the semver resource relies on AWS credentials that are already\npassed.\n\n## Decision\n\nWe will not use the pool resource for flow between jobs - instead we will use\nthe semver resource\n\n## Status\n\nAccepted\n\n## Consequences\n\nThis was an investigation into a different approach, so no consequences\n"}
{"repositoryUrl": "https://github.com/opinionated-digital-center/python-library-project-generator.git", "path": "docs/adr/0002-use-gnu-make-to-centralise-all-tasks.md", "template": "Madr", "status": "accepted", "firstCommit": "2020-05-07T16:40:06Z", "lastCommit": "2020-05-15T22:06:49Z", "numberOfCommits": 2, "title": "Use GNU Make to centralise all tasks", "wordCount": 426, "authors": {"name1": 2}, "content": "# Use GNU Make to centralise all tasks\n\n* Status: accepted\n* Date: 2020-05-07\n\n## Context and Problem Statement\n\nWe want to be able to centralise in a single tool all tasks to be called:\n* During the development cycle.\n* During the build cycle.\n* In the CI/CD pipelines.\n\n## Decision Drivers\n\n* Must have a significant user base and community.\n* Must not require significant installation.\n* Must be sufficiently simple to learn and use.\n\n## Considered Options\n\n* [`GNU Make`](https://www.gnu.org/software/make/).\n* [`Invoke`](http://www.pyinvoke.org/).\n* [`Rake`](https://github.com/ruby/rake).\n* [`SCons`](https://scons.org/).\n\n## Decision Outcome\n\nChosen option: `GNU Make`, because compared to the other evaluated tools (see\n[Pros and Cons](#pros-and-cons-of-the-options)), it fits the bill where as:\n* `Invoke` is not well maintained, nor well documented, nor a de facto standard, nor has\n  a sufficient community.\n* `Rake` required to install and learn Ruby.\n* `SCons` is more a build tool and seems difficult to apprehend/get to grips with.\n\n## Pros and Cons of the Options\n\n### [`GNU Make`](https://www.gnu.org/software/make/)\n\n`GNU Make` [manual](https://www.gnu.org/software/make/manual/).\n\n* Good, because:\n  * It is a well known and widely used standard.\n  * It has a huge existing community.\n  * It is pre-installed by default on most systems, and can easily be installed on\n  most existing systems.\n  * It does not require to install a specific language (python, ruby, etc.)\n  * It does enough for the purpose of writing and using tasks.\n  * It has plenty of documentation.\n  * It has plenty of existing information sources.\n* Bad, because:\n  * Some concepts can be difficult to grasp for our use.\n  * Some default behaviors can be unclear and cause scripts not to work.\n  * It can be difficult to debug when using complex features.\n\n### [`Invoke`](http://www.pyinvoke.org/)\n\n* Good, because:\n  * It is Python based.\n  * It promises:\n  * Like Ruby’s Rake, a clean, high level API.\n  * Like GNU Make, an emphasis on minimal boilerplate for common patterns\n  and the ability to run multiple tasks in a single invocation.\n* Bad, because:\n  * It is not a de facto standard for Python.\n  * It has a large number of\n  [unresolved and untriaged issues](https://github.com/pyinvoke/invoke/issues)\n  (202 at time of writing) and a large number of\n  [opened Pull Requests](https://github.com/pyinvoke/invoke/pulls), some dating back\n  2014, which suggests a slack in maintenance.\n  * It does not seem to have a large enough community.\n  * Previous points seem to disqualify this solution.\n\n### [`Rake`](https://github.com/ruby/rake)\n\n* Good, because:\n  * It is a proven tool.\n  * It is a de facto standard in the Ruby world.\n  * It has an extensive community.\n  * It has extensive documentation.\n* Bad, because:\n  * It requires to know/learn another language than Python (Ruby).\n  * It requires to install a Ruby stack.\n\n### [`SCons`](https://scons.org/)\n\n* Good, because:\n  * Written in Python.\n  * Seems fairly active.\n* Bad, because:\n  * A build tool more than a task tool.\n  * Getting to grip the tool through the documentation is cumbersome.\n  * It has a large number of\n  [unresolved and untriaged issues](https://github.com/pyinvoke/invoke/issues)\n  (604 at time of writing), which suggests a slack in maintenance.\n  * Limited number of stars on github.\n"}
{"repositoryUrl": "https://github.com/corgibytes/cukeness.git", "path": "doc/architecture/decisions/0003-api-server-responsible-for-interaction-with-external-services.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-08-28T01:47:42Z", "lastCommit": "2018-08-28T01:47:42Z", "numberOfCommits": 1, "title": "3. API Server Responsible for Interaction with External Services", "wordCount": 207, "authors": {"name1": 1}, "content": "# 3. API Server Responsible for Interaction with External Services\n\nDate: 2018-08-27\n\n## Status\n\nAccepted\n\n## Context\n\nThere are a couple external services that need to be communicated with. These are expected to be a source control system and a server running the [Cucumber Wire Protocol](1987e2349b14ca0fe93e879d762df09f1a9b3934). There are other functions that are needed such as authentication and authorization, and an abstraction around the storage and organization of Gherkin-based executable specifications\n\n## Decision\n\nAn API server will be built to handle the following:\n\n* Integration with source control systems (initially just `git` but the addition of others needs to be possible)\n* Communication with Cucumber Wire Protocol service\n* Required abstractions for creating, modifying, organizing, and executing Gherkin-based executable specifications\n* Abstractions for authentication and authorization (initially fulfilled by a simple database authentication mechanism, but eventually allowing other authentication sources such as OAuth)\n\nThese functions will be independent of any user interface that's presented to facilitate carrying out these actions.\n\n## Consequences\n\nThis will drastically simplify the user interface, because it will only need to concern itself with communication with one entity to function. However, this also makes the API server rather complex, in that it has to juggle many different responsibilities. These responsibilities can be decomposed into ancillary services and libraries as needed to mitigate this extra complexity.\n"}
{"repositoryUrl": "https://github.com/mozilla/sre-adrs.git", "path": "decisions/websre/0002-web-sre-service-documentation.md", "template": "Nygard", "status": "Approved", "firstCommit": "2021-06-01T14:22:58Z", "lastCommit": "2021-06-08T18:58:24Z", "numberOfCommits": 7, "title": "2. Web SRE Service Documentation", "wordCount": 754, "authors": {"name1": 6, "name2": 1}, "content": "# 2. Web SRE Service Documentation\n\nDate: 2021-06-01\nScope: Web SRE Team\n\n## Status\n\nApproved\n\n## Context\n\nThe Web SRE Team manages a number of services that vary in users, technologies used, setup, our commitment, infrastructure, process, etc. \n\nPre-existing documentation for these services, if it exists, also varies - in location, level of detail, scope, structure, presumed audience, etc.\n\nTo help share knowledge between the Web SRE Team about these services' unique situations, a consistent baseline for our services documentation is a required first step.\n\n## Decision\n\nA Web SRE service here is defined as any codebase where we have some ownership over an aspect of this codebase being functional - e.g. the deployment process, automation, infrastructure running the service, observability of some aspect(s), or authoring the code ourselves. Web SRE services could be externally or internally-facing, including infrastructure services only Web SRE are aware of and leverage. Web SRE services also include non-prod environments for testing infrastructure code (like staging Kubernetes clusters to test building new clusters) and supporting services (log aggregation, alerting, etc) as well as non-prod application code. \n\nFor each Web SRE Service, we will author a Service Documentation page in Mana as a child of https://mana.mozilla.org/wiki/display/SRE/Service+Documentation.\n\nService Documentation pages:\n* use the Web SRE Services Docs confluence page template for its structure;\n* have a primary audience of the Web SRE Team itself, with a secondary audience of other SRE teams;\n* are open to Mozilla internally, however not all teams of Mozilla are the intended audience of these pages (SRE-external teams will focus on the escalation path section primarily);\n* have Runbook pages (what to do when a specific symptom of an issue is recognized) as child pages to the relevant Service Documentation page;\n* have How-to pages (how to perform specific tasks for a service) as child pages to the relevant Service Documentation page;\n* are maintained to reflect the current state - to the best of our knowledge - of a service's context, including a service being decommissioned (the Service Documentation page should note that but be left up as a tombstone marker of the decision);\n* cover Web SRE Services as defined above.\n* replace SRE_INFO.md files in Web SRE team-managed source code GitHub repositories. When an SRE_INFO.md file is encountered, it should be reviewed for any information that can be added to a Service Documentation page, removed, and an link to the Service Documentation page added to the codebase's REAMDE.md.\n\nService Documentation pages do not:\n* include all possible services Web SRE might work with - e.g. we don't document AWS Services generically or services owned and maintained by other teams; \n* include all possible details for all audiences beyond SRE - when such documentation requests come up, they can be added as a How To as a child to a service page, or a generic How To if not limited to one service;\n* live anywhere other than Mana. Web SRE Service documentation managed elsewhere, e.g. GitHub, Google Docs, should be migrated & then deprecated via links pointing readers to the Mana page;\n* replace other forms of documentation living elsewhere - e.g. codebase-specific documentation within a git repository, decision records in this repository, collaboration / draft developer notes in Google documents, Mana pages in other places and formats walking through shared infrastructure or processes, etc..\n\nService Documentation pages optionally have Runbook & How-To child pages. For these, there are the following expectatiosn:\n\nRunbook pages should:\n* include ways to validate what state a system is in (e.g. how to reproduce the problem);\n* be as self-contained as is feasible;\n* explain what problem a given state indicates;\n* give commands to resolve the problem as clearly as possible;\n* outline a fallback plan (who to call, what to do next).\n\nHow-to pages should:\n* repeat existing documentation as little as possible. Linking to external docs is encouraged, perhaps augmented with our specific contexts;\n* assume a high level of competence from the audience (don't explain how to download a csv file, though giving an example command to establish context is great);\n* explain the decision points in a process, and how to make them;\n* outline what is needed to perform the work as early as possible in the document (what access, what tools);\n* outline who needs to approve the work, or how to decide if it's safe to do the work as early as possible.\n\n## Consequences\n\nThis will require a fair amount of work in getting the Web SRE Services portfolio adequately covered by the guidelines above.\n\nThis will also require maintenance work ensuring the drift between service state and documentation state is as minimal as is feasible.\n\n## Resources\n\n* [Service Documentation Folder](https://mana.mozilla.org/wiki/display/SRE/Service+Documentation)\n* [Web SRE Service Documentation Template (Web SRE Team & Jira Space admins only can view)](https://mana.mozilla.org/wiki/pages/templates2/viewpagetemplate.action?entityId=131596432&key=SRE)\n"}
{"repositoryUrl": "https://github.com/LBHackney-IT/lbh-adrs.git", "path": "Platform/Accepted/Frontend-Tech-Stack.md", "template": "unknown", "status": null, "firstCommit": "2021-05-12T11:38:02Z", "lastCommit": "2021-05-13T21:07:21Z", "numberOfCommits": 4, "title": "Frontend Tech Stack", "wordCount": 194, "authors": {"name1": 4}, "content": "# Frontend Tech Stack\n\n### **Date:** 30th March 2021\n\n### **Status:** ACCEPTED\n\n## **Context**\n\nHackney has several frontends applications using different programming languages and frameworks:\n\n- Angualr/JS\n- Ruby\n- React/TS\n\nHackney has got a  microservices architecture exposing APIs to be used by different consumers and so each frontend can be developed with its own programming language and framework. Anyway, unless an application requires a particular programming language, it’s still good to agree on a common language/framework for mtfh-T&L workstream. This will make it easier to shift developers from different teams. \n\n## **Decision**\n\n**React with TS (Type Script)**\n\nReact with TypeScript is the most common framework/language used for the majority of Hackney frontend application, it’s the programming language that Hackney frontend developers are more familiar with, plus it has got the following well known advantages:\n\n- It’s open source\n- Easy to learn because of its simple design and less time worrying about the framework specific code\n- Better user experience and very fast performance compared with other FE frameworks\n\n## **Consequences**\n\nNo consequences as all frontends of each team are more familiar with React, plus, before the Cyber attack, Hackney started to migrate in React the old Angular and Ruby apps."}
{"repositoryUrl": "https://github.com/ministryofjustice/offender-management-architecture-decisions.git", "path": "decisions/0007-use-ruby-for-new-applications-for-manage-offenders-in-custody.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-10-26T11:55:50Z", "lastCommit": "2018-11-16T16:39:22Z", "numberOfCommits": 2, "title": "7. Use Ruby for new applications for Manage Offenders in Custody", "wordCount": 1195, "authors": {"name1": 2}, "content": "# 7. Use Ruby for new applications for Manage Offenders in Custody\n\nDate: 2018-10-19\n\n## Status\n\nAccepted\n\n## Context\n\n### Language use across HMPPS\n\nHMPPS has active development in four languages, including services with\nsignificant prison-staff-facing components in all four: Ruby, Python,\nJavaScript and Java.\n\nHMPPS has live services (which have passed service assessments) built in Ruby\nand Python. Ensuring that we support our existing users should be our top\npriority, so it is essential to maintain our skills in the languages used in\nour live services.\n\nOne of the advantages of a microservices approach is that teams can work on\nseparate services in different languages, using HTTP APIs to share data and\nfunctionality. There is no need for all services to be built in the same\nlanguage. We are already using this approach across HMPPS.\n\nThere is no clear vision or strategy at the moment for changing the number of\nlanguages in use across HMPPS. We are not in a position to decide that for all\nof HMPPS.\n\n### Team skills\n\nAll four languages in active use across HMPPS are represented to varying\ndegrees in the skill sets of the current members of the team, but only Ruby is\ncommon to all of them. The team have worked together on a live service built in\nRuby for all of their time at MOJ/HMPPS. We still own that service and\ncontinuously improve it alongside our work on Manage Offenders in Custody,\nalthough we are spending the majority of our time on the latter.\n\nThe primary language skills of HMPPS's civil servant developers and technical\narchitect (a significant proportion of whom are on this team) are in Ruby and\nPython. It is unrealistic to expect people to be equally proficient in many\nlanguages at the same time.\n\nThe team have already committed to learning about Kubernetes for the new Cloud\nPlatform (see [ADR 0002](0002-use-cloud-platform-for-hosting.md)) and to\nlearning Java so that we can collaborate on the APIs which are being built in\nSheffield (see [ADR 0006](0006-use-the-custody-api-to-access-nomis-data.md)).\nThis is already a significant proportion of unfamiliar technologies for the\nteam to learn and use.\n\nDeveloping applications involves much more than using the standard library of\na language. The ecosystem of libraries tools around that language often takes\nmuch more work and time to become familiar with than the basics of the language\nitself. Although all of the team know some JavaScript, we do not all have\nexperience of using it for building server-side applications. We would therefore\nhave a lot more to learn if we were to choose to use JavaScript, as some\nrelated but separate services do.\n\n### Time constraints\n\nThe Offender Management in Custody programme has fixed timelines for its\nnational rollout in the next year. Although we are not committing to delivering\nparticular services at set dates months in advance, we will reduce our\nopportunity for learning from a smaller set of real users before the national\nrollout if we are not ready to take advantage of the Wales pilot which begins\nin January.\n\nWe know that allocations is only the first of several areas of the programme\nwhich are likely to need support from us, so timescales are tight for us.\n\nWe anticipate that the complexity of building this service lies in managing the\nquality of the data available across NOMIS, Delius and OASys, rather than in\nrepresenting that data to users.\n\nChoosing to use a less familar language for developing our applications, on top\nof what we already need to learn, would put us at significant risk of not\ndelivering working software until several months after our first users need it.\n\n### Code reuse\n\nUsing the same language for groups of similar services can make it easier to\nprovide a coherent experience for users by allowing presentation code to be\nshared more easily between services. However, the same HTML structure of pages\ncan be produced by services written in different languages. Since we are\ncommitted to progressive enhancement (see [ADR 0003](0003-use-progressive-enhancement.md)),\nwe will use client-side JavaScript solely to enhance the functionality of those\npages, and that JavaScript can be reused across services regardless of the\nlanguage used on the server.\n\nAs an example of this approach, there is a strong and active cross-government\ncommunity which develops, researches and supports design patterns, styles and\ncomponents which are used on services built in many different languages:\nhttps://design-system.service.gov.uk/\n\nWe will base our user-facing applications on this established design system in\nany case. There is already a variety of design approaches in use across the\nprison-staff-facing services we have, and our best chance of standardising that\nwell is to align ourselves with the cross-government approach.\n\nThat approach is supported by extensive user research over several years and\nacross many services and departments. Using it as our starting point reduces\nthe need for us to undertake duplicate research ourselves to understand the\neffectiveness of alternatives to those existing patterns. We expect that we\nwill need to extend those patterns and develop others inspired by them to meet\nour users' needs, and we will contribute what we learn back to the HMPPS and\ncross-government communities.\n\nSince it has been agreed that all services which need to use a NOMIS API should\nmigrate to the Custody API (see [ADR 0006](0006-use-the-custody-api-to-access-nomis-data.md)),\nany API client library which we build in Ruby can be reused by other Ruby\nservices to ease their migration.\n\n### Operational considerations\n\nThe team has considerable experience of operating live services built in Ruby\nat scale.\n\nWe do not anticipate scaling to be a significant concern for allocation - we\nexpect to have a couple of hundred users a day at most for it.\n\nThe new Cloud Platform makes it easy, quick and cheap for us to scale up if we\nneed to.\n\n## Decision\n\nWe will use Ruby for the new applications we build in London as part of Manage\nOffenders in Custody.\n\n## Consequences\n\nWe will build on the knowledge the team already has of the Ruby ecosystem.\n\nWe will not have to significantly deepen our knowledge of a third language (as\nwell as Ruby and Java), familarise ourselves with a different ecosystem of\nlibraries and decide on and learn another set of tools in order to make\nprogress.\n\nWe will be able to use the GOV.UK Design System as the basis for making our\nservices look consistent with other government and HMPPS services.\n\nWe may not be able to reuse libraries which are built by teams in Sheffield if\nthey are intended for use with particular JavaScript frameworks which we do not\nneed to use.\n\nWe will write client code in Ruby for the Custody API (and any other APIs we\nuse) which we could extract into libraries to be used by other Ruby services\nwhen they migrate to use those APIs.\n\nWe will maintain a strong level of Ruby knowledge within HMPPS, which will help\nus ensure that we can continue to support a significant proportion of our live\nservices in the future.\n\nIf HMPPS wants to ensure that we have civil servants with strong skills in the\nother languages currently used across all its services, we will need to focus\non hiring in those areas rather than expecting our existing developers to be\nable to work equally productively across all those languages.\n"}
{"repositoryUrl": "https://github.com/nhsuk/mongodb-updater.git", "path": "doc/adr/0001-record-architecture-decisions.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-07-05T17:32:01Z", "lastCommit": "2017-07-11T08:24:31Z", "numberOfCommits": 2, "title": "1. Record architecture decisions", "wordCount": 58, "authors": {"name1": 1, "name2": 1}, "content": "# 1. Record architecture decisions\n\nDate: 2017-06-19\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to record the architectural decisions made on this project.\n\n## Decision\n\nWe will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n\n## Consequences\n\nSee Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's _adr-tools_ at https://github.com/npryce/adr-tools.\n"}
{"repositoryUrl": "https://github.com/elifesciences/elife-xpub.git", "path": "docs/adr/0002-pubsweet.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-06-05T14:41:17Z", "lastCommit": "2018-09-25T09:59:34Z", "numberOfCommits": 2, "title": "2. Use of PubSweet", "wordCount": 153, "authors": {"name1": 1, "name2": 1}, "content": "# 2. Use of PubSweet\n\nDate: 2018-06-05\n\n## Status\n\nAccepted\n\n## Context\n\neLife needs to develop their own online journal submission and peer review system.\n\nThere were three options available:\n\n- Build a system from scratch.\n- Use [PubSweet](https://pubsweet.org/) an open source project built by the [Coko Foundation](https://coko.foundation/technology/)\n- Consider using / licensing a closed-source alternative.\n\nBuilding a system from scratch would be ruled out if there was a suitable existing project that could be used.\n\nThe option to use a closed-source was ruled out, even though one of these alternatives had plans to go open source this was considered too great a risk to the project.\n\n## Decision\n\nThe decision was taken on Sept 2016 to use PubSweet. This decision was taken in-part due to another publisher [Hindawi](hindawi.com) also participating in the collaboration.\n\nWe can keep track of dependencies on PubSweet components, all components used are to be recorded in: [PubSweet.md](../developing/PubSweet.md)\n\n## Consequences\n\n- The opportuinty to contribute to PubSweet, thus the [wider community](https://coko.foundation/community/partners-projects/)\n- PubSweet mandates the use of [React](https://reactjs.org/)\n"}
{"repositoryUrl": "https://github.com/cloudfoundry/cf-k8s-networking.git", "path": "doc/architecture-decisions/0010-route-crd-and-kubebuilder-instead-of-metacontroller.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-05-05T17:50:48Z", "lastCommit": "2020-06-26T17:43:51Z", "numberOfCommits": 2, "title": "10. Create Route CRD and use Kubebuilder Instead of cfroutesync/Metacontroller", "wordCount": 438, "authors": {"name1": 1, "name2": 1}, "content": "# 10. Create Route CRD and use Kubebuilder Instead of cfroutesync/Metacontroller\n\nDate: 2020-05-05\n\n## Status\n\nAccepted\n\n## Context\n\n### Proposed Design\nThe proposal and discussion for the Route CRD and design can be found [here](https://docs.google.com/document/d/1DF7eTBut1I74w_sVaQ4eeF74iQes1nG3iUv7iJ7E35U/edit?usp=sharing).\n\n![Proposed RouteControllerDesign](../assets/routecontroller-design.png)\n\n### Summary\nIn order to achieve our scaling targets for the cf-for-k8s networking control plane, we need a \"fast path\" for networking changes to propagate from Cloud Controller to Kubernetes. The periodic sync loop implementation of `cfroutesync` and Metacontroller adds around 7 seconds of latency **on top** of the ~7 seconds of Istio control plane latency which in total exceed our scaling targets<sup>1</sup>.\n\nWe propose:\n1. Replacing our sync loop with a `Route` [Kubernetes custom resource definition (CRD)](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) and a controller that manages networking resources based on these Routes.\n2. Cloud Controller updates Route resources directly on the Kubernetes API as a result of user actions (e.g. `cf map-route`)\n3. Keeping a periodic sync loop will between Cloud Controller's database (CCDB) and the Kubernetes API to ensure consistency.\n\n\n## Decision\n\nWe will introduce a `Route` CRD and build a `RouteController` using the Kubebuilder v2 controller framework.\n\nCloud Controller will be updated to perform CRUD operations on Kubernetes`Route` resources in both its v2 and v3 APIs.\n\nWe will remove `cfroutesync` and dependencies on Metacontroller.\n\n\n## Consequences\nThis reverses the [previous decision (ADR 2)](https://github.com/cloudfoundry/cf-k8s-networking/blob/develop/doc/architecture-decisions/0002-directly-create-istio-resources.md) to use Metacontroller and directly create Kubernetes and Istio resources.\n\n#### Route CRD\n* Decouples cf-k8s-networking from Cloud Controller. Additional routing control planes could be added in the future or developers could use `kubectl` directly.\n* Abstracts the underlying ingress implementation away from Cloud Controller. We could replace Istio with an alternative ingress solution without requiring CC changes.\n* Moves us closer to a more \"Kubernetes native\" design.\n* Potential downside of adding yet another CRD is that it may put more load on the Kubernetes API / etcd. Could become an issue as other teams also move to CRD-based designs.\n\n#### Using Kubebuilder\n* Provides Community buy-in; the `kubebuilder` framework is the encouraged way to engineer a CRD\n* Provides built-in best practices for writing a controller, including: shared caching, retries, back-offs, leader election for high availability deployments, etc...\n\n#### Removal of Metacontroller\nRemoval of Metacontroller alleviates some future problems:\n* As discussed in [ADR 2](https://github.com/cloudfoundry/cf-k8s-networking/blob/develop/doc/architecture-decisions/0002-directly-create-istio-resources.md), Metacontroller did not support the many Route to one VirtualService object relationship which required us to aggregate Routes from Cloud Controller ourselves. With Kubebuilder we can support this relationship and keep the data representations consistent across both the Cloud Controller and Kubernetes APIs.\n* The Metacontroller design, most likely, does not provide necessary metrics for GA\n* Metacontroller itself is [no longer supported](https://github.com/GoogleCloudPlatform/metacontroller/issues/184) and currently presents issues with Kubernetes `v1.16+`\n\n\n### Footnotes\n_<sup>1</sup> Routing changes should take effect in under 10 seconds on an environment with 2000 app instances and 1000 routes._\n\n"}
{"repositoryUrl": "https://github.com/Vodurden/CrossyToad.git", "path": "docs/architecture/decisions/adr-0001-record-architecture-decisions.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-09-30T05:15:00Z", "lastCommit": "2018-09-30T05:15:00Z", "numberOfCommits": 1, "title": "ADR 0001: Record architecture decisions", "wordCount": 146, "authors": {"name1": 1}, "content": "# ADR 0001: Record architecture decisions\n\nDate: 2018-09-30\n\n## Status\n\nAccepted\n\n## Context\n\nI want to be able to remember the reason I chose a particular architecture so I can change my mind!\n\nI also hope the records will be useful to others trying to understand how someone might approach\ncreating a game in Haskell.\n\nNormally I would use this style for a team codebase, but I'm hoping the approach will have value\neven though this is a solo project.\n\nAlso I like architecture decision records and want more projects to use them, so let's lead by\nexample!\n\n## Decision\n\nI will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions\n\n## Consequences\n\n- If I make a decision of consequence. I'll write an ADR outlining why I made that decision.\n- See Michael Nygard's article, linked above, for further consequences of this approach.\n"}
{"repositoryUrl": "https://github.com/alphagov/verify-service-provider.git", "path": "docs/adr/0023-we-will-report-the-version-in-a-saml-extension.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-10-10T11:15:27Z", "lastCommit": "2017-10-10T11:15:27Z", "numberOfCommits": 1, "title": "23. We will report the version in a saml extension", "wordCount": 295, "authors": {"name1": 1}, "content": "# 23. We will report the version in a saml extension\n\nDate: 2017-10-10\n\n## Status\n\nAccepted\n\n## Context\n\nThe verify-service-provider will be deployed to a number of relying parties.\nWhich relying party is using which version could potentially become difficult\nto keep track of.\n\nTo make it as easy as possible for us to manage this we'd like the\nverify-service-provider to report its version in some way.\n\nBecause the verify-service-provider is not intended to be accessible to the\ninternet we can't simply expose an endpoint that reports the version number.\nAlso, since the SAML messages go via the browser we can't use a custom HTTP\nheader.\n\nThere's also a concern about the security implications of reporting a version\nnumber in cleartext.\n\nWe considered a couple of options:\n\n- Requesting metadata from Verify with a custom user-agent string\n- Sending the version in an unencrypted saml extension\n- Sending the version in an encrypted saml extension\n\n## Decision\n\nWe decided to send the version number in the SAML AuthnRequests as an encrypted\nSAML extension. The XML will look roughly like this:\n\n```\n<saml:AuthnRequest>\n  <saml:Issuer>...</saml:Issuer>\n  <saml:Signature>...</saml:Signature>\n  <saml:Extensions>\n  <saml:EncryptedAttribute>...</saml:EncryptedAttribute>\n  </saml:Extensions>\n</saml:AuthnRequest>\n```\n\nOnce decrypted, the Attribute in the Extensions will look like:\n\n```\n<saml:Attribute Name=\"Versions\">\n  <saml:AttributeValue xsi:type=\"metrics:VersionsType\">\n  <metrics:ApplicationVersion>3.4.1</metrics:ApplicationVersion>\n  </saml:AttributeValue>\n</saml:Attribute>\n```\n\n## Consequences\n\nVerify will be able to monitor the versions of connected instances of the\nverify-service-provider.\n\nAuthnRequests sent by the verify-service-provider will be approximately 2,500\nbytes longer than they would be without the version extension. They should\nstill be short enough that this will not cause any validation issues.\n\nVersion numbers will not be visible to malicious third parties as the extension\nwill be encrypted with Verify's public key.\n\n"}
{"repositoryUrl": "https://github.com/nulib/meadow.git", "path": "doc/architecture/decisions/0029-npm.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2021-11-04T16:01:19Z", "lastCommit": "2021-11-04T16:01:19Z", "numberOfCommits": 1, "title": "29. npm", "wordCount": 61, "authors": {"name1": 1}, "content": "# 29. npm\n\nDate: 2021-11-03\n\n## Status\n\nAccepted\n\n## Context\n\nThe latest upgrade of Yarn has introduced issues that we're finding difficult to overcome.\n\nSupersedes [11. Yarn](0011-yarn.md)\n\n## Decision\n\nSwitch back to `npm` instead of `yarn` in all dev, test, and build environments.\n\n## Consequences\n\nThe instructions have changed, but overall the complexity involved in installing and maintaining \nmeadow's JavaScript dependencies should be about the same.\n"}
{"repositoryUrl": "https://github.com/openkfw/TruBudget.git", "path": "docs/developer/architecture/0002-access-control-model.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2021-06-30T15:51:32Z", "lastCommit": "2021-06-30T15:51:32Z", "numberOfCommits": 1, "title": "(sem título)", "wordCount": 507, "authors": {"name1": 1}, "content": "---\nsidebar_position: 2\n---\n\n# Access Control Model\n\nDate: 03/04/2018\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to define our approach to access-control/authorization.\n\n## Decision\n\nSince our users' organizations differ a lot in terms of structures and policies, we\nneed to have a very versatile access control mechanism in place. In the industry, the\nmost used technique employed is role-based access control (RBAC). We base our\nmandatory access control (MAC) mechanism on RBAC, but use the notion of _intent_\ninstead of role (an intent can be executed by exactly one role, thus effectively\nreplacing the role concept).\n\n![Intent + Resource = Permission](./img/0002-access-control-model.png)\n\n### Intents\n\nAn intent is what the user is trying to achieve, for example, \"add a workflow to a\nsubproject\". By using intents rather than roles, we side-step the problem found in\nmany projects, where over time developers create similar roles on-the-fly, as from\ntheir point of view the implications of re-using a role are not always clear.\nConversely, intents are always specific to a use case (examples: \"create project\" or\n\"list a project's subprojects\" rather than \"admin\" or \"user\").\n\nWhile roles _could_ be used to bundle intent-execution rights (e.g. have one role\nthat is allowed to execute all \"view\" intents), we think that those roles would have\nto be managed by organizations themselves (as it will depend on their structure).\nSince in most cases this would mean a 1:1 mapping from role to user group, we skip\nroles altogether.\n\n### User Groups\n\nOrganizations group their users into _user groups_. For any given projects, each\n(resource-specific) intent has a list of user groups assigned to it; all users in the\nassigned groups are then allowed to execute the respective intent.\n\n```plain\n+---+   +---+  +---+\n| User: | is member of  | Group:  | is allowed to  | Intent:   |\n| Alice +-->+ Project Maintainers +--->+ Add workflow  |\n|   |   |   |  | to subproject |\n+---+   +---+  +---+\n```\n\n### Implementation Pattern\n\nThe goal is to enable us to follow a clear pattern for our access control needs:\n\n- HTTP controllers call the domain modules and the authorization module (perhaps using a\n  middleware), but do not deal with intents or groups.\n- Domain modules may interact with the chain to fetch domain objects, and/or prepare\n  closures to be authorized and executed later on. They deal with intents, but not with\n  users or groups.\n- Finally, the authorization module ensures that the _user_ executing the _intent_\n  belongs to a _group_ that is allowed to do that. In order to decide that, the module\n  has to fetch resource-specific ACLs from the chain.\n\nModifying ACLs is done in the same way: each resource's ACL specifies the groups that\nmay execute the \"change this ACL\" intent (to be renamed). This hints at the necessity\nto provide _defaults_ for ACLs when creating resources.\n\n### Resource-specific Access Control Lists (ACLs)\n\nWith each resource/stream on the chain, an ACL stream-item is stored that lists for\neach intent the groups allowed to execute that intent:\n\n```json\n{\n  \"acl\": {\n  \"view project\": [\"all users\"],\n  },\n  ...\n}\n```\n\n## Consequences\n\nWith the proposed changes in place, our users will be able to impose their respective\norganizational structures onto TruBudget's resources in a way that should be flexible\nenough and straight-forward to integrate with their existing directory servers.\n"}
{"repositoryUrl": "https://github.com/robertlcs/react-native-app.git", "path": "doc/adr/0006-make-tabs-swipeable.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-09-12T20:56:30Z", "lastCommit": "2018-10-28T10:44:20Z", "numberOfCommits": 2, "title": "6. Make tabs swipeable", "wordCount": 51, "authors": {"name1": 2}, "content": "# 6. Make tabs swipeable\n\nDate: 2018-09-12\n\n## Status\n\nAccepted\n\n## Context\n\nTo navigate between tabs, users can swipe left or right or click a tab.\n\n## Decision\n\nWe will use NativeBase's component \"Tabs\".\nPreviously used [react-native-tab-view](https://github.com/react-native-community/react-native-tab-view) since it's a cross-platform component that works perfectly on iOS and Android.\n\n## Consequences\n\nIt renders inactive tabs.\n"}
{"repositoryUrl": "https://github.com/buildit/digitalrig-metal-aws.git", "path": "docs/architecture/decisions/0006-create-reference-implementation-repository.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-01-14T21:44:36Z", "lastCommit": "2019-01-14T21:44:36Z", "numberOfCommits": 1, "title": "6. Create Reference Implementation Repository", "wordCount": 220, "authors": {"name1": 1}, "content": "# 6. Create Reference Implementation Repository\n\nDate: 2019-01-14\n\n## Status\n\nAccepted\n\n## Context\n\nThe rig defined at [Bookit Infrastructure](https://github.com/buildit/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.  \nWhilst it's rather generic as it is, it is specific to Bookit's needs.  \nThe AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).  \nThe only way to capture that is via branches which can be hard to discover.  \nFinally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features\n\n## Decision\n\nCreate a digitalrig-metal-aws repo (https://github.com/buildit/digitalrig-metal-aws) that demonstrates all of the options and latest features of the AWS Bare Metal Rig and removes any Bookit specific wording/concepts.\n\n## Consequences\n\nProjects looking to instantiate new AWS Bare Metal Rigs shall be able to clone this reference implementation, make choices nad changes specific to that project, and instantiate their project specific Rig.\nChanges and enhancements will need to be implemented in 2 places:  1) the project specific Rig and 2) the AWS Bare Metal reference implementation Rig so that future implementations will contain the latest features\n"}
{"repositoryUrl": "https://github.com/jmoratilla/devops-challenge.git", "path": "doc/adr/0004-feat-about-orchestration.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-02-17T16:01:16Z", "lastCommit": "2020-02-17T16:01:16Z", "numberOfCommits": 1, "title": "4. feat-about-orchestration", "wordCount": 521, "authors": {"name1": 1}, "content": "# 4. feat-about-orchestration\n\nDate: 2020-02-17\n\n## Status\n\nAccepted\n\n## Context\n\nGoals:\n\n1. To select an orchestration technology suitable for this type of architecture.\n2. The solution must be scalable according to the load.\n\nHere I'm going to talk about Infrastructure as Code (IaC), the way to define a system\n architecture for running applications in the cloud.\n\nBy defining resources in a text file and process them using specific API engines, a \n cloud company can create a set of computing units, networks, users and services in a \n repeatable and under version control way.  That allows to create new ways to manage\n infrastructure, network, storage.\n\nTo read the file we use tools that will contact the APIs and engines to control the\n order and the execution of each task performed by the engines.  These tools are called\n orchestrators.\n\nOrchestration tools exist for creating the resources needed to deploy\n applications.  But there are two models here:\n\n- Those that reuse resources (mutable servers)\n- Those that don't reuse resources (inmutable servers)\n\nFor a microservices architecture, our main purpose is to quickly replace those\n services, with no downtime.\n\nFor this, the inmutable servers philosophy is better, as you can launch an\n instance of the new version, test it, and decide to enable it in production \n without risking the rest of services.\n\nWell, not so valid if we consider database migrations, but it requires to set\n some requirements about database management that are far from the scope of \n this test.\n\nTo scale apps you can upgrade computing resources (vertical scaling), or\n adding more computing instances provisioned with the app (horizontal scaling).\n   Both can be performed easily with inmutable servers.\n\nWhich orchestration tools use the inmutable servers perspective?\n\n- [AWS CloudFormation](https://aws.amazon.com/cloudformation)\n- [Terraform](https://www.terraform.io)\n\nAWS cloud formation uses a json-like text file where resources like computing\n units, vpc, load balancers, databases, etc., are defined to create and \n provision the declared resources.  This file can be versioned and stored in \n a version control system.  This allows you to recreate the resources used in \n each stage of the development of the applications.  The main issue here is \n that is only valid for AWS.\n\nTerraform, by hashicorp, uses a DSL (Domain Specific Language) that makes easy\n to define resources.  Also, it allows to define providers for several \n platforms and services, so it's not limited to AWS, but it can work with \n Google, Azure, IBM, bare metal, vmware, etc.\n\nBut for Kubernetes, there is another tool that works as orchestrator: \n\n* [kops](https://github.com/kubernetes/kops)\n\nThis tool, that it is used to manage and orchestrate clusters, groups and \n secrets in AWS and other cloud platforms, uses terraform internally.  So it \n can be used to generate a terraform set of files and store them in a version\n control system to do the same.\n\n## Decision\n\nI'm going to use kops for the management of the clusters, and I'm going to\n generate templates that can be used to deploy new clusters depending on the\n requirements of an environment like development, staging or production.\n\n## Consequences\n\nThe first thing I change here is that it is not possible to keep using my own\nvirtual servers to create the kubernetes cluster.  Everything now will be\n running on AWS.\n\n"}
{"repositoryUrl": "https://github.com/budproj/unfinished-design-system.git", "path": "docs/adl/004-tokenization-and-static-assets.md", "template": "Nygard", "status": "**DEPRECATED** _check [update 1](#update-1)_", "firstCommit": "2020-10-26T18:47:19Z", "lastCommit": "2020-10-26T18:47:19Z", "numberOfCommits": 1, "title": "ADR 4: Tokenization and Static Assets", "wordCount": 213, "authors": {"name1": 1}, "content": "# ADR 4: Tokenization and Static Assets\n\n* [Table of contents](#)\n  * [Context](#context)\n  * [Decision](#decision)\n  * [Status](#status)\n  * [Consequences](#consequences)\n  * [More reading](#more-reading)\n  * [Updates](#updates)\n\n## Context\n\nTokens have a significant role in theming. They're responsible for defining the primitives of our theme, such as color codes, font names, and others. Tokens are relevant to determining asset locations too.\n\nWe must find a proper way to handle and maintain our tokens.\n\n## Decision\n\nWe've decided to use [Style Dictionary](https://amzn.github.io/style-dictionary/#/) as our framework to handle tokens. It is easy to use since we can define our tickets in standard JSON but empowering them with string interpolation, variables, and other features.\n\nFor our static assets, we're going to host them at [AWS S3](https://aws.amazon.com/s3/), defining the proper CORS rules, and refer the location of those as tokens for our applications to use.\n\nInstead of increasing the size of our Javascript bundle with static assets, we prefer to keep it simple and light by hosting those in an S3 bucket and asking for the application to download it.\n\n## Status\n\n**DEPRECATED** _check [update 1](#update-1)_\n\n## Consequences\n\nTokenization is a complex process. We need to look out and keep it simple. The logic (like mapping those tokens to theme properties) must happen in the design-system itself, keeping the token package just for constant's definition.\n\n---\n\n## More reading\n\n* [Style Dictionary's docs](https://amzn.github.io/style-dictionary/#/)\n\n## Updates\n\n### Update 1\n\nAfter [business/ADR#001](https://github.com/budproj/architecture-decision-log/blob/main/records/business/001-reducing-initial-technical-complexity.md), we've decided to stop the development of a decoupled design system.\n"}
{"repositoryUrl": "https://github.com/nhsuk/connecting-to-services.git", "path": "doc/adr/0015-add-info-page.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-10-21T15:10:49Z", "lastCommit": "2019-10-22T08:44:24Z", "numberOfCommits": 2, "title": "15. Add info page", "wordCount": 73, "authors": {"name1": 2}, "content": "# 15. Add info page\n\nDate: 2019-10-16\n\n## Status\n\nAccepted\n\n## Context\n\nWhen debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.\n\n## Decision\n\nThe application will provide an info page.\n\n## Consequences\n\nDebugging depoyment and time related issues will be made easier.\n"}
{"repositoryUrl": "https://github.com/guttih/island.is-glosur.git", "path": "docs/adr/0009-naming-files-and-directories.md", "template": "Madr", "status": "accepted", "firstCommit": "2020-09-04T09:55:11Z", "lastCommit": "2020-09-04T09:55:11Z", "numberOfCommits": 1, "title": "Unified naming strategy for files and directories", "wordCount": 237, "authors": {"name1": 1}, "content": "# Unified naming strategy for files and directories\n\n* Status: accepted\n* Deciders: devs\n* Date: 2020-07-03\n\n## Context and Problem Statement\n \nAs of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has\ndefaults that differ between schematic types. \nIn order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files \nand directories.  \n\n## Decision Drivers\n\n* Provide consistency when navigating the codebase\n* The earlier we decide on this, the better \n\n## Considered Options\nSome mixture of these:\n* kebab-case\n* PascalCase\n* camelCase\n* snake_case\n\n## Decision Outcome\n\nChosen option: Name files after their default export. If that default export is a React Component, or a class, then the\nfile name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid\nusing kebab-case and snake_case and make sure the name follows the default export of the file.\n\nNaming directories should follow these guidelines:\nOnly use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`:\n`import { Box } from '@island.is/island-ui/core'`\n\nUse PascalCase for directories only containing React components:\n````\ncomponents/CtaButton/index.ts\nimport 'components/CtaButton'\n````\nor:\n````\ncomponents/CtaButton/CtaButton.tsx\nimport 'components/CtaButton/CtaButton'\n````\nrather than\n````\ncomponents/cta-button/CtaButton.tsx\n````\n\nIn all other cases, use camelCase.\n\n### Positive Consequences\n\n* Easier to navigate the codebase\n* File names are more readable, and developers know what to expect\n* This approach is the most common practice, and something most JS and TS developers are familiar with.\n"}
{"repositoryUrl": "https://github.com/Midnighter/structurizr-python.git", "path": "docs/development/adr/0009-use-pydantic-for-json-de-serialization.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-06-09T16:20:39Z", "lastCommit": "2020-11-27T15:18:08Z", "numberOfCommits": 2, "title": "9. Use pydantic for JSON (de-)serialization", "wordCount": 99, "authors": {"name1": 1, "name2": 1}, "content": "# 9. Use pydantic for JSON (de-)serialization\n\nDate: 2020-06-09\n\n## Status\n\nAccepted\n\n## Context\n\nIn order to interact with a remote workspace, for example, at structurizr.com.\nThe remote or local workspace has to be (de-)serialized from or to JSON.\n\n## Decision\n\nIn order to perform these operations we choose\n[pydantic](https://pydantic-docs.helpmanual.io/) which has a nice API, active\ncommunity, good data validation, helpful documentation, and good performance.\n\n## Consequences\n\nWe separate the models representing Structurizr entities and their business\nlogic from how those models are (de-)serialized. That means that for each model\nwe have a corresponding IO pydantic model describing the JSON data model.\n\n"}
{"repositoryUrl": "https://github.com/apache/james-project.git", "path": "src/adr/0019-reactor-netty-adoption.md", "template": "Nygard", "status": "Accepted (lazy consensus) & implemented", "firstCommit": "2020-03-06T02:23:38Z", "lastCommit": "2020-12-14T06:49:38Z", "numberOfCommits": 2, "title": "19. Reactor-netty adoption for JMAP server implementation", "wordCount": 164, "authors": {"name1": 1, "name2": 1}, "content": "# 19. Reactor-netty adoption for JMAP server implementation\n\nDate: 2020-02-28\n\n## Status\n\nAccepted (lazy consensus) & implemented\n\n## Context\n\nAfter adopting the last specifications of JMAP (see \n[new JMAP specifications adoption ADR](https://github.com/apache/james-project/blob/master/src/adr/0018-jmap-new-specs.md)), \nit was agreed that we need to be able to serve both `jmap-draft` and the new `jmap` with a reactive server. \n\nThe current outdated implementation of JMAP in James is currently using a non-reactive [Jetty server](https://www.eclipse.org/jetty/).\n\nThere are many possible candidates as reactive servers. Among the most popular ones for Java:\n\n* [Spring](https://spring.io)\n* [Reactor-netty](https://github.com/reactor/reactor-netty)\n* [Akka HTTP](https://doc.akka.io/docs/akka-http/current/introduction.html)\n* ...\n\n## Decision\n\nWe decide to use `reactor-netty` for the following reasons:\n\n* It's a reactive server\n* It's using [Reactor](https://projectreactor.io/), which is the same technology that we use in the rest of our codebase\n* Implementing JMAP does not require high level HTTP server features\n\n## Consequences\n\n* Porting current `jmap-draft` to use a `reactor-netty` server instead of a Jetty server\n* The `reactor-netty` server should serve as well the new `jmap` implementation\n* We will be able to refactor and get end-to-end reactive operations for JMAP, unlocking performance gains\n\n## References\n\n* JIRA: [JAMES-3078](https://issues.apache.org/jira/browse/JAMES-3078)\n* JMAP new specifications adoption ADR: https://github.com/apache/james-project/blob/master/src/adr/0018-jmap-new-specs.md"}
{"repositoryUrl": "https://github.com/XLab-Tongji/PerformanceTestDocs.git", "path": "failures-adr/0004-sprout-fails-when-transaction-in-progress.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2018-06-17T06:10:27Z", "lastCommit": "2018-06-17T06:10:27Z", "numberOfCommits": 1, "title": "4. Sprout Fails When Transaction In Progress", "wordCount": 72, "authors": {"name1": 1}, "content": "# 4. Sprout Fails When Transaction In Progress\n\nDate: 2018-06-17\n\n## Status\n\nAccepted\n\n## Context\n\nA sprout node fails while a transaction is in progress, the transaction fails.\n\n## Decision\n\nEither the UE will retry automatically or it will display an error to the user, who should retry. Transaction fail can't be routed to sprout2 (As from that point on the P-CSCF will not retry the transaction).\n\n## Consequences\n\nUE/user retries, or transaction fails\n\n## Flow Chart\n\n![sprout-pending-transaction](http://www.projectclearwater.org/wp-content/uploads/2014/02/sprout-pending-transaction.png)"}
{"repositoryUrl": "https://github.com/ahitrin/SiebenApp.git", "path": "doc/adr/0003-dsl-for-creation-and-representation-of-goal-trees.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-06-09T09:29:26Z", "lastCommit": "2020-08-16T05:23:25Z", "numberOfCommits": 5, "title": "3. DSL for creation and representation of goal trees", "wordCount": 84, "authors": {"name1": 5}, "content": "# 3. DSL for creation and representation of goal trees\n\nDate: 2017-05-14\n\n## Status\n\nAccepted\n\n## Context\n\nWe have to work with a lot of goal tree examples during SiebenApp testing.\nCurrent API allows only to create goaltree step-by-step.\nIt makes hard to point a border between test setup and test actions.\n\n## Decision\n\nCreate a declarative [DSL][DSL] that allows to define a goal tree that \"exists before test actions\".\nUse it in all unit tests.\n\n## Consequences\n\nTests will became more readable and clean.\n\n[DSL]: https://en.wikipedia.org/wiki/Domain-specific_language\n"}
{"repositoryUrl": "https://github.com/karlmdavis/fhir-benchmarks.git", "path": "dev/architecture/0003-test-output.md", "template": "unknown", "status": null, "firstCommit": "2020-04-04T02:17:40Z", "lastCommit": "2020-04-04T02:17:40Z", "numberOfCommits": 1, "title": "Architecture Decision: What Should the Test Output Look Like?", "wordCount": 290, "authors": {"name1": 1}, "content": "# Architecture Decision: What Should the Test Output Look Like?\n\nI suspect that a very common workflow for this project later on will be:\n\n1. FHIR server implementor decides to add or improve benchmark for their FHIR server.\n1. Implemetor downloads this project,\n   points it at a local copy of their server,\n   and then starts working to implement and/or improve the benchmarks for their server.\n1. After each change to their server and/or the benchmark,\n   implementor will need to find, view, and understand the results.\n1. Implementor will use results to guide next steps,\n   often re-running the benchmarks and going through this loop for a while.\n\nI'd been thinking it'd be possible for implementors to just review a single\n  JSON output file to inspect the results, and iterate on their servers' performance.\nThat's... likely not true, though?\nIt's entirely likely that some servers will support dozens of benchmarks.\nIt's unlikely that a single JSON file for that much data will be human-readable.\n\nWhat if the benchmark tooling makes it simple for implementors to run only one test at a time, though?\nEven still, I think it's fair to say that they'll eventually need a way to visualize the complete results.\nIn particular, they'll likely often want to compare their server to another one.\n\nI'm going to need to spend time on the visual design of the results.\nI mean... no shit, right?\nWhat I'm realizing now, though, is that getting useful output and comparisons\n  will need to be an **early** concern for the project -- won't be able to defer it for long.\n\nTODO: Not sure I have a concrete answer to how to represent HDR Histogram data in output files."}
{"repositoryUrl": "https://github.com/home-assistant/architecture.git", "path": "adr/0010-integration-configuration.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-04-14T18:43:44Z", "lastCommit": "2020-04-14T18:43:44Z", "numberOfCommits": 1, "title": "0010. Integration configuration", "wordCount": 768, "authors": {"name1": 1}, "content": "# 0010. Integration configuration\n\nDate: 2020-04-14\n\n## Status\n\nAccepted\n\n## Context\n\nHome Assistant has been relying on YAML for its configuration format for a long\ntime. However, in certain cases, this caused issues or did not work at all.\nThese cases are best explained by listing the different categories of\nintegrations that we have in Home Assistant:\n\n- Integrations that integrate devices. Examples include Hue, TP-Link.\n- Integrations that integrate services. Examples include AdGuard, Snapcast.\n- Integrations that integrate transports. These integrations allow users to\n  define their own protocol. Examples include MQTT, serial, GPIO.\n- Integrations that process Home Assistant data and make this available to\n  other integrations. Examples are template, stats, derivative, utility meter.\n- Integrations that provide automations. Examples include automation,\n  device_sun_light_trigger, alert.\n- Integrations that help controlling devices and services.\n  Examples include script, scene.\n- Integrations that expose Home Assistant data to other services.\n  Examples include Google Assistant, HomeKit.\n\nIn all but the first two cases, YAML does just fine. The configuration is static,\nis not discovered and relies on the user setting it up. These cases have been\nsolved by providing a hybrid approach. We offer YAML with a reload service and\nwe offer Storage Collections, which allows the user to create/manage these\nintegrations via the UI.\n\nHowever, in the first two cases, it doesn’t work. Integrations are discovered.\nIntegrations require users logging in on the vendor’s website and authorize\nlinking (OAuth2) or users are required to press buttons on the hub to authorize\nlinking (i.e. Hue).\n\nIn the cases that people can authorize an integration by just putting their\nusername and password in the YAML file, they don’t want to, because it prevents\nthem from sharing their configuration. This is solved currently by using YAML\nsecrets that are substituted during load. This results in one file that provides\nthe structure of your configuration and one file that provides the values.\nSee below for an anonymized example as can be found on GitHub:\n\n```yaml\ncamera:\n platform: onvif\n name: bedroom\n host: !secret camera_onvif_bedroom_host\n port: !secret camera_onvif_bedroom_port\n username: !secret camera_onvif_bedroom_username\n password: !secret camera_onvif_bedroom_password\n```\n\nSo to solve these first two cases, we’ve introduced config entries (a\ncentralized config object) and config flows. Config flows handle creating config\nentries with data from different sources. It can handle a new entry created via\nthe user interface, automatic discovery, but also is able to handle importing\nconfiguration from YAML. Config entries allow for migrations during upgrades,\nlimiting the breaking changes we have to induce on our users.\n\nConfig flows empower users of all knowledge levels, to use and enjoy\nHome Assistant. Since the introduction of config flows we’ve kept it open to\ncontributors of individual integrations to decide if they want to implement\nYAML and/or a user-facing config flow.\n\nSome contributors have decided to drop the YAML import to reduce their\nmaintenance and support burden. A burden that they volunteer to do in their\nspare time. This has sadly resulted in a few pretty de-motivating comments,\ntowards the contributors and the project in general. These comments often\nviolate the code of conduct we have in place to protect the Home Assistant\ncommunity.\n\nThis induces the risk of losing contributors and maintainers, halts our project\ngoals and slows down innovation. As an open source project, maintaining our\ncontributors is our highest priority as they are the creators of the project\nin the first place. They should be highly admired and valued for their\ncontributions.\n\nFrom a project perspective, we have not provided the necessary guidelines on\nthis matter for our contributors to work with and have therefore not managed\nthe expectations of our users to a full extent.\n\n## Decision\n\nTo protect project goals and to provide clarity to our users and contributors,\nwe’re introducing the following rules on how integrations need to be configured:\n\n- Integrations that communicate with devices and/or services are only configured via\n  the UI. In rare cases, we can make an exception.\n- All other integrations are configured via YAML or via the UI.\n\nThese rules apply to all new integrations. Existing integrations that should\nnot have a YAML configuration, are allowed and encouraged to implement a\nconfiguration flow and remove YAML support. Changes to existing YAML\nconfiguration for these same existing integrations, will no longer be accepted.\n\n## Consequences\n\n- Power to our contributors! ❤\n- Removes confusion and questions around the future of the YAML configuration\n  for all users and contributors.\n- Builds upon the goals that have been set out, as presented at the\n  State of the Union 2019.\n- This might impact the number of integrations contributed. This requires\n  configuration flows, which require tests. However, we do provide scaffolding\n  scripts for this (`python3 -m script.scaffold`).\n"}
{"repositoryUrl": "https://github.com/BMMRO-tech/BMMRO.git", "path": "architectural-decision-records/2019-11-20_firestore.md", "template": "unknown", "status": null, "firstCommit": "2020-07-28T12:34:35Z", "lastCommit": "2020-07-29T13:17:29Z", "numberOfCommits": 2, "title": "Firestore", "wordCount": 138, "authors": {"name1": 1, "name2": 1}, "content": "# Firestore\n\n**Status:** Accepted\n\n## Context\n\nWe need a database in order to store the data provided by the user in the different forms.\n\n## Decision\n\nTo reduce the amount of integrations and complexity, we've decided to use one of the databases that Firebase offers.\nUnder the Firebase umbrella are Real-time database and Firestore which are similar in many features.\nGoogle recommends Firestore for new developers, as it is built upon “the successes of Real-time database”. Also, for our purposes, Firestore is a better choice because:\n\n- It offers offline support for iOS, Android and Web clients whereas real-time DB supports only iOS and Android clients.\n- The one-big-JSON-tree structure of real-time can become difficult to maintain as the DB grows. Document-based Firestore offers better organisation provisions using collections.\n- Scaling on firestore is automatic whereas scaling on real-time database requires sharding.\n\n## Links\n\n- [Differences between the Firestore and Real-time Database](https://firebase.google.com/docs/database/rtdb-vs-firestore)\n"}
{"repositoryUrl": "https://github.com/alphagov/verify-service-provider.git", "path": "docs/adr/0002-how-do-we-secure-the-api.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-06-02T15:39:24Z", "lastCommit": "2017-10-09T14:04:26Z", "numberOfCommits": 5, "title": "2. Use dropwizard functionality to secure API", "wordCount": 108, "authors": {"name1": 1, "name2": 2, "name3": 1, "name4": 1}, "content": "# 2. Use dropwizard functionality to secure API\n\nDate: 2017-06-01\n\n## Status\n\nAccepted\n\n## Context\n\nWe need to secure the interaction between the \"client\" code (e.g. node JS)\nand the server side code (which will be a dropwizard app).\n\nDepending on how the users want to run the service provider we may need\ndifferent security solutions.\n\n## Decision\n\nIf possible users can talk to the service provider on the loopback (127.0.0.1)\nIf that doesn't work for some reason then they can use the dropwizard config\nto set up basic auth or tls or something.\n\nSee http://www.dropwizard.io/1.1.0/docs/manual/configuration.html#connectors\n\n## Consequences\n\nWe'll deliver a prototype that allows users to configure dropwizard things.\n\n"}
{"repositoryUrl": "https://github.com/ministryofjustice/opg-use-an-lpa.git", "path": "docs/architecture/decisions/0011-the-same-zend-application-will-be-used-for-both-viewer-and-actor-components.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-05-01T17:10:58Z", "lastCommit": "2019-05-01T17:10:58Z", "numberOfCommits": 1, "title": "11. The same Zend application will be used for both Viewer and Actor components", "wordCount": 134, "authors": {"name1": 1}, "content": "# 11. The same Zend application will be used for both Viewer and Actor components\n\nDate: 2019-05-01\n\n## Status\n\nAccepted\n\n## Context\n\nUse an LPA will be made up of two components - those for use by LPA _actors_, and those used by third \nparty groups who are the _viewers_ of the LPA.\n\nAt present it is expected that these two components will be hosted on two different domains.\n\n## Decision\n\nThat both `Viewer` and `Actor` will both be separate modules of the same Zend application.\n\nNote: it is still expected that they will be deployed separately into two containers.\n\n## Consequences\n\n* The two services will be able to share code much more easily.\n* There will only be a single Zend codebase to manage.\n* The application will be slightly larger overall as the dependencies for both modules will be included.\n"}
{"repositoryUrl": "https://github.com/alphagov/paas-team-manual.git", "path": "source/architecture_decision_records/ADR028-move-logs-to-logit.html.md", "template": "Nygard", "status": "Approved", "firstCommit": "2018-07-13T12:35:57Z", "lastCommit": "2021-03-18T22:01:24Z", "numberOfCommits": 3, "title": "(sem título)", "wordCount": 231, "authors": {"name1": 1, "name2": 1, "name3": 1}, "content": "---\ntitle: ADR028 - Migrate CF platform logs to Logit\n---\n\n# ADR028: Migrate CF platform logs to Logit\n\n## Context\nThe work to ship cloud foundry platform logs to Logit was started in 2018 Q1.\nIt was paused because some IA issues with Logit were not resolved. At one point\nRE recommended that PaaS should host our own logstash as this part of the\nservice was not widely available by market Elastic SaaS providers. The PaaS\nteam was also considering to use Elasticsearch on AWS with our accounts.\n\n## Decision\nAn updated conversation with the RE tool team has confirmed that the IA issues\nhad been resolved, and that GDS can continue to use Logit for now.\n\nIt is a GDS strategy to use a consistent logging solution. Hence, we should\ncontinue our migration of platform logs to logit, including our logstash\nfilters.\n\nThere is considerably less maintenance work for us if we use Logit's logstash\nfilter rather than hosting the bosh release one. In the future if GDS choose to\nuse another vendor that do not have a hosted logstash solution, they would need\nto provide a migration strategy for all the current logstash users.\n\n## Status\nApproved\n\n## Consequences\nWe will continue the migration of platform logs to logit including logstash,\nand take a risk that we may need to spin up our logstash in the future if GDS\nchoose a different platform logs provider.\n"}
{"repositoryUrl": "https://github.com/VirtualProgrammingLab/viplab-websocket-api.git", "path": "docs/adr/0002-use-sha256-with-base64url-encoding.md", "template": "Madr", "status": "proposed", "firstCommit": "2019-11-07T12:25:15Z", "lastCommit": "2019-12-25T11:16:27Z", "numberOfCommits": 2, "title": "Use SHA256 and Base64Url encoding for verifying json", "wordCount": 505, "authors": {"name1": 2}, "content": "# Use SHA256 and Base64Url encoding for verifying json\n\n* Status: proposed\n* Deciders: Leon Kiefer\n* Date: 2019-12-13\n\n## Context and Problem Statement\n\nWe have to transfer json data and verify the integrity of the data.\nThe transfer involves an Authorization server which provides the json, a client which gets the data form that server and pass it to the WebSocket API.\nThe WebSocket API must able to verify the integrity of the json data.\n\n## Decision Drivers <!-- optional -->\n\n* Use standard encodings\n\n## Considered Options\n\n* Transfer complete json via secure channel\n* Send SHA256 hash of Base64Url encoded json\n* Send SHA256 hash of formatted json\n\n## Decision Outcome\n\nChosen option: \"Send SHA256 hash of Base64Url encoded json\", because this method is platform independent and not much session state is required.\n\n### Positive Consequences <!-- optional -->\n\n* The JWT really function as a verification token for the other requests.\n* Can be applied to all json data that must be verified.\n\n### Negative Consequences <!-- optional -->\n\n* The json must be transferred in Base64Url encoding\n\n## Pros and Cons of the Options <!-- optional -->\n\n### Transfer complete json via secure channel\n\nThe complete json data is transferred using a secure channel, like a JWT Token Claim.\nThe secure channel takes care of the Integrity and the transport.\n\n* Good, because well known solutions can be used.\n* Good, because very secure.\n* Bad, because depending on selected secure channel many data have to be stored in a session state.\n* Bad, because there are limitations how many data can be transferred at once using a secure channel.\n\n### Send SHA256 hash of Base64Url encoded json\n\nThe json is encoded with Base64Url encoding (RFC 4648).\nThe Base64 String is then transferred instead of the raw json.\nFor the Base64 string the message digest using SHA256 (FIPS PUB 180-4) is computed and used to verify the integrity.\nThe message digest is a sequence of bytes and should be encoded to a lower-case hex string to transfer it over a secure channel like JWT.\n\n* Good, because the Integrity of the json can be verified simply.\n* Good, because only the small hex encoded message digest must be transferred over a secure channel.\n* Bad, because the json is transferred Base64 encoded and must be decoded before use.\n* Bad, because this must be implemented by hand.\n\n### Send SHA256 hash of formatted json\n\nThe json can be transferred in any format.\nTo verify the integrity, the json is formatted in a way, that the same json info set result in the same json string representation on all platforms.\nThis json string is then hashed using SHA256 (FIPS PUB 180-4) and the message digest can be used the verify the integrity.\nThe message digest is a sequence of bytes and can be Base64Url encoded to transferred it over a secure channel like JWT.\n\n* Good, because only the small Base64 encoded message digest must be transferred over a secure channel.\n* Good, because json can be be transferred as is.\n* Bad, because difficult to get a consistent formatting across platforms.\n* Bad, because heavily depends on the json formatting library\n* Bad, because this must be implemented by hand.\n\n## Links <!-- optional -->\n\n* [ADR-0001](0001-use-json-web-tokens.md)\n* [ADR-0003](0003-transfere-hash-in-jwt-claim.md)\n"}
{"repositoryUrl": "https://github.com/dennisseidel/saas-platform-frontend.git", "path": "adr/0008-use-launchaco-com-to-generate-the-logo-for-free.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2019-03-03T12:02:03Z", "lastCommit": "2019-06-19T12:13:49Z", "numberOfCommits": 2, "title": "8. Use launchaco.com to generate the logo for free", "wordCount": 83, "authors": {"name1": 1, "name2": 1}, "content": "# 8. Use launchaco.com to generate the logo for free\n\nDate: 2019-02-09\n\n## Status\n\nAccepted\n\n## Context\n\nThe landing page need a logo for the name. This could be design by hand but this requires skill and time or an online service could be used.\n\n## Decision\n\nGenerate the logo with [launchaco.com](https://www.launchaco.com/logo/editor). \n\n## Consequences\n\nThis speeds up the logo building process and at the time of writing the service was free. The logos are not very creative but to bootstrap a business this should be enough. \n\n\n## Links\n* https://www.launchaco.com/logo/editor\n"}
{"repositoryUrl": "https://github.com/alphagov/verify-service-provider.git", "path": "docs/adr/0021-we-will-use-http-200-for-valid-saml.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2017-08-16T09:53:55Z", "lastCommit": "2017-10-10T11:15:27Z", "numberOfCommits": 3, "title": "21. We will put verified status inside json object", "wordCount": 144, "authors": {"name1": 1, "name2": 1, "name3": 1}, "content": "# 21. We will put verified status inside json object\n\nDate: 2017-08-16\n\n## Status\n\nAccepted\n\n## Context\n\nWhen communicating with the Verify Service Provider API, we need to decide what status code to respond with\nfor correctly formatted SAML that represents some kind of authentication failure (eg. NO_MATCH).\n\n\n## Decision\n\nAny valid SAML will return a 200 OK response and can be deserialized as a <code>TranslatedRepsonseBody</code>.\nWe will have to define an enum of possible SAML outcomes (<code>Scenario</code>) as we can't use HTTP codes\nInvalid JSON/SAML or internal errors will use a relevant, different HTTP status code.\n\n## Consequences\n\nAPI consumers such as passport-verify will have to handle authentication failures as 200 codes. We will\nhave to document the possible error scenarios. We can add API authentication such as HTTP Basic Auth at a\nlater date without worrying about clashing on HTTP status codes.\n"}
{"repositoryUrl": "https://github.com/cljdoc/cljdoc.git", "path": "doc/adr/0004-utilize-codox-to-read-clojure-script-sources.md", "template": "Nygard", "status": "Accepted Supercedes [2. Don't build on top of Codox](0002-don-t-build-on-top-of-codox.md)", "firstCommit": "2018-01-14T11:41:14Z", "lastCommit": "2018-01-14T16:25:20Z", "numberOfCommits": 2, "title": "4. Utilize Codox to Read Clojure/Script Sources", "wordCount": 176, "authors": {"name1": 2}, "content": "# 4. Utilize Codox to Read Clojure/Script Sources\n\nDate: 2018-01-14\n\n## Status\n\nAccepted\n\nSupercedes [2. Don't build on top of Codox](0002-don-t-build-on-top-of-codox.md)\n\n## Context\n\nI initially thought reading metadata from source files is built into Grimoire but it is not\nand has been implemented separately in projects like [lein-grim](https://github.com/clojure-grimoire/lein-grim). The implementation in `lein-grim` did not work with `.cljs` or `.cljc` files and so copying that\nwas not an option.\n\nIn a previous ADR I decided not to build on top of codox to generate documentation. I still\nbelieve Codox is not what I want to generate final artifacts (HTML, JS Apps) but has\nrelatively solid features when it comes to reading source files and extracting metadata.\n\nCodox' `:writer` option allows us to easily retrieve the raw data in a plain format that\nis easy to understand.\n\n\n## Decision\n\nWe will use Codox to retrieve metadata from source files for now.\nStorage of metadata will be stored in Grimoire as before.\n\n## Consequences\n\nCodox does not provide means to read the source of vars. This will need to be implemented\nseparately, perhaps using `clojure.repl/source-fn` which however has it's own issues.\n"}
{"repositoryUrl": "https://github.com/Alfresco/SearchServices.git", "path": "search-services/alfresco-search/doc/architecture/decisions/0002-search-morelikethis-adr.md", "template": "Nygard", "status": "Investigation Complete", "firstCommit": "2019-01-10T08:42:27Z", "lastCommit": "2019-01-10T08:58:09Z", "numberOfCommits": 4, "title": "2. Search More Like This", "wordCount": 899, "authors": {"name1": 4}, "content": "# 2. Search More Like This\n\nDate: 09/01/2019\n\n## Status\n\nInvestigation Complete\n\n## Context\n\n*Intro (Lucene)*\n\nThe More Like This (MLT from now on) functionality is implemented in Lucene and made available through Solr Rest API.\nThe main implementation code is currently in the Lucene library : org.apache.lucene.queries.mlt.MoreLikeThis_ class .\nIn that class there is the logic to take a document Id (or the document itself) in input and calculate a MLT query based on the significant terms extracted from the document in relation to the corpus.\nCurrently it operates extracting the document from the local Lucene index when standalone(Terms vectors or stored content must be available)\nor using the realtime GET to fetch the input document from adjacent shard.\nThe MLT query is built on the assumption that it is possible to identify significant terms from a document based on the term frequencies of them in document, compared to their occurrence in the corpus.\nAfter the significant terms are extracted, a score is assigned to each one of them and the most important are used to build a boosted query.\nWhich fields to take into account when extracting the terms and building the query is one of the MLT parameters.\nI attach the slides from a presentation I made in 2017[1] detailing the internal of the functionality and some proposed refactor.\n\n*Apache Solr*\n\nApache Solr exposes various ways to interact with the MLT library.\n\n* MoreLikeThisHandler -> a dedicate request handler to return similar results to an input one\n* MoreLikeThisComponent -> to automatically execute more like this query on each document in the result set\n* MoreLikeThisQueryParser -> I tend to consider this the modern approach, that allow you to build MLT queries and debug them more easily\n\n*More Like These*\n\nImplementing the More Like These can be vital to offer advanced functionalities and reccomndetation to the users.\nThe proposed implementation approach will cover different software areas : Lucene, Solr, Alfresco APIs .\n\nI attached an High Level T-Shirt sizing estimation to each part of the developments.\n\n*Lucene - M*\n\nThe Lucene implementation will be the biggest part. It will require to extend the More Like This class with:\n* Additional facade methods to process list of documents or list of document Ids\n* Significant term extraction and scoring from the input set of documents\nIn particular the second bit (significant term extraction and scoring) will be the most important.\nVarious algorithms are available to provide such capability [2] , I recommend a first implementation based on:\nJHL.\nAfter a first investigation KLIP [3] seems a promising implementation as well.\nBeing Alfresco use case very generic additional variants can be added later.\nEach variant could be specific to a specific scenario (big collection, kind of the collection ect)\n\n*Solr - S*\n\nOut of the various ways Apache Solr serves the functionality as a beginning I recommend to extend the 3) MoreLikeThisQueryParser.\nWe’ll re-use all the Solr MLT parameters and in addition we’ll support a different input.\nThese are the classes:\n\norg.apache.solr.search.mlt.CloudMLTQParser\norg.apache.solr.search.mlt.SimpleMLTQParser\n\nThe More Like These Functionality will be compatible out of the box with SolrCloud but for the Alfresco distribution model it will require some work.\n\n*Solr - Alfresco Customisation - M*\n\nTo have the More Like These fully distributed in the Alfresco use case:\nThe Interesting terms extraction and query building is effectively run locally on a single Solr.\nTo have it properly distributed it is needed to:\n* Enable distributed IDF\n* customise the query parser to use the shards parameter to fetch the documents through the get request handler (that already supports it)\nthe customised query parser needs to be compatible with Alfresco name mapping and locale functionalities\n* Acl needs to be preserved\n* the seed document must be fetched from the solr content store, potentially though the realtime GET\n* field parameters must be appropriately rewritten according to Alfresco mappings\n\n*Alfresco Side - Configuration - S*\n\nA specific request handler will be configured in the Alfresco solrconfig.xml to expose an endpoint that will use the More Like This query parser by default.\nIt will take in input as request parameters a set of document Ids and the document fields to use for the document similarity.\nThe rest of the parameters will be hardcoded in the config, only expert admin are invited to touch them (such as the algorithm for term scoring, and all the other MLT params)\n\n*Alfresco Repository and Rest API - M*\n\nA specific query language must be implemented:\norg.alfresco.rest.api.search.impl.SearchMapper\norg.alfresco.repo.search.impl.lucene.LuceneQueryLanguageSPI .\nThe implementation should follow Alfresco best practice and allow to interact with the dedicated request handler for the More Like These\n\n[1] https://www.slideshare.net/AlessandroBenedetti/advanced-document-similarity-with-apache-lucene\n\n[2] https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-significantterms-aggregation.html#_parameters_5\n\n[3] Tomokiyo, T., Hurst, M. (2003). A language model approach to keyphrase extraction. In Proceedings of the\nACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment (Vol. 18,\npp. 33–40) Association for Computational Linguistics.\n\n## Decision\nBased on the results of the investigations the most critical points have been identified in:\n\n1) Alfresco custom storing approach\n2) Alfresco custom ACL filtering\n3) Alfresco sharding model\n\nThe decision is to structure the developments in 3 sequential macro areas:\n1) provide the More Like This functionality via API\n2) provide the More Like These functionality via API\n3) provide the automatic query expansion through API and configuration\n\nThe investigation was done just at API level which means no front end tasks have been taken under consideration.\nYou find the detailed tasks under consequences.\n\n## Consequences\nAppropriate Jiras have been created:\nhttps://issues.alfresco.com/jira/browse/SEARCH-1385\nhttps://issues.alfresco.com/jira/browse/SEARCH-1386\nhttps://issues.alfresco.com/jira/browse/SEARCH-1387\n"}
{"repositoryUrl": "https://github.com/mofeixiaobao/gatemint-sdk.git", "path": "docs/architecture/adr-009-evidence-module.md", "template": "Nygard", "status": "Accepted", "firstCommit": "2020-09-08T03:36:28Z", "lastCommit": "2020-09-08T03:36:28Z", "numberOfCommits": 1, "title": "ADR 009: Evidence Module", "wordCount": 832, "authors": {"name1": 1}, "content": "# ADR 009: Evidence Module\n\n## Changelog\n\n- 2019 July 31: Initial draft\n- 2019 October 24: Initial implementation\n\n## Status\n\nAccepted\n\n## Context\n\nIn order to support building highly secure, robust and interoperable blockchain\napplications, it is vital for the Cosmos SDK to expose a mechanism in which arbitrary\nevidence can be submitted, evaluated and verified resulting in some agreed upon\npenalty for any misbehavior committed by a validator, such as equivocation (double-voting),\nsigning when unbonded, signing an incorrect state transition (in the future), etc.\nFurthermore, such a mechanism is paramount for any\n[IBC](https://github.com/cosmos/ics/blob/master/ibc/2_IBC_ARCHITECTURE.md) or\ncross-chain validation protocol implementation in order to support the ability\nfor any misbehavior to be relayed back from a collateralized chain to a primary\nchain so that the equivocating validator(s) can be slashed.\n\n## Decision\n\nWe will implement an evidence module in the Cosmos SDK supporting the following\nfunctionality:\n\n- Provide developers with the abstractions and interfaces necessary to define\n  custom evidence messages, message handlers, and methods to slash and penalize\n  accordingly for misbehavior.\n- Support the ability to route evidence messages to handlers in any module to\n  determine the validity of submitted misbehavior.\n- Support the ability, through governance, to modify slashing penalties of any\n  evidence type.\n- Querier implementation to support querying params, evidence types, params, and\n  all submitted valid misbehavior.\n\n### Types\n\nFirst, we define the `Evidence` interface type. The `x/evidence` module may implement\nits own types that can be used by many chains (e.g. `CounterFactualEvidence`).\nIn addition, other modules may implement their own `Evidence` types in a similar\nmanner in which governance is extensible. It is important to note any concrete\ntype implementing the `Evidence` interface may include arbitrary fields such as\nan infraction time. We want the `Evidence` type to remain as flexible as possible.\n\nWhen submitting evidence to the `x/evidence` module, the concrete type must provide\nthe validator's consensus address, which should be known by the `x/slashing`\nmodule (assuming the infraction is valid), the height at which the infraction\noccurred and the validator's power at same height in which the infraction occurred.\n\n```go\ntype Evidence interface {\n  Route() string\n  Type() string\n  String() string\n  Hash() HexBytes\n  ValidateBasic() error\n\n  // The consensus address of the malicious validator at time of infraction\n  GetConsensusAddress() ConsAddress\n\n  // Height at which the infraction occurred\n  GetHeight() int64\n\n  // The total power of the malicious validator at time of infraction\n  GetValidatorPower() int64\n\n  // The total validator set power at time of infraction\n  GetTotalPower() int64\n}\n```\n\n### Routing & Handling\n\nEach `Evidence` type must map to a specific unique route and be registered with\nthe `x/evidence` module. It accomplishes this through the `Router` implementation.\n\n```go\ntype Router interface {\n  AddRoute(r string, h Handler) Router\n  HasRoute(r string) bool\n  GetRoute(path string) Handler\n  Seal()\n}\n```\n\nUpon successful routing through the `x/evidence` module, the `Evidence` type\nis passed through a `Handler`. This `Handler` is responsible for executing all\ncorresponding business logic necessary for verifying the evidence as valid. In\naddition, the `Handler` may execute any necessary slashing and potential jailing.\nSince slashing fractions will typically result from some form of static functions,\nallow the `Handler` to do this provides the greatest flexibility. An example could\nbe `k * evidence.GetValidatorPower()` where `k` is an on-chain parameter controlled\nby governance. The `Evidence` type should provide all the external information\nnecessary in order for the `Handler` to make the necessary state transitions.\nIf no error is returned, the `Evidence` is considered valid.\n\n```go\ntype Handler func(Context, Evidence) error\n```\n\n### Submission\n\n`Evidence` is submitted through a `MsgSubmitEvidence` message type which is internally\nhandled by the `x/evidence` module's `SubmitEvidence`.\n\n```go\ntype MsgSubmitEvidence struct {\n  Evidence\n}\n\nfunc handleMsgSubmitEvidence(ctx Context, keeper Keeper, msg MsgSubmitEvidence) Result {\n  if err := keeper.SubmitEvidence(ctx, msg.Evidence); err != nil {\n  return err.Result()\n  }\n\n  // emit events...\n\n  return Result{\n  // ...\n  }\n}\n```\n\nThe `x/evidence` module's keeper is responsible for matching the `Evidence` against\nthe module's router and invoking the corresponding `Handler` which may include\nslashing and jailing the validator. Upon success, the submitted evidence is persisted.\n\n```go\nfunc (k Keeper) SubmitEvidence(ctx Context, evidence Evidence) error {\n  handler := keeper.router.GetRoute(evidence.Route())\n  if err := handler(ctx, evidence); err != nil {\n  return ErrInvalidEvidence(keeper.codespace, err)\n  }\n\n  keeper.setEvidence(ctx, evidence)\n  return nil\n}\n```\n\n### Genesis\n\nFinally, we need to represent the genesis state of the `x/evidence` module. The\nmodule only needs a list of all submitted valid infractions and any necessary params\nfor which the module needs in order to handle submitted evidence. The `x/evidence`\nmodule will naturally define and route native evidence types for which it'll most\nlikely need slashing penalty constants for.\n\n```go\ntype GenesisState struct {\n  Params   Params\n  Infractions  []Evidence\n}\n```\n\n## Consequences\n\n### Positive\n\n- Allows the state machine to process misbehavior submitted on-chain and penalize\n  validators based on agreed upon slashing parameters.\n- Allows evidence types to be defined and handled by any module. This further allows\n  slashing and jailing to be defined by more complex mechanisms.\n- Does not solely rely on Tendermint to submit evidence.\n\n### Negative\n\n- No easy way to introduce new evidence types through governance on a live chain\n  due to the inability to introduce the new evidence type's corresponding handler\n\n### Neutral\n\n- Should we persist infractions indefinitely? Or should we rather rely on events?\n\n## References\n\n- [ICS](https://github.com/cosmos/ics)\n- [IBC Architecture](https://github.com/cosmos/ics/blob/master/ibc/1_IBC_ARCHITECTURE.md)\n- [Tendermint Fork Accountability](https://github.com/tendermint/spec/blob/7b3138e69490f410768d9b1ffc7a17abc23ea397/spec/consensus/fork-accountability.md)\n"}
{"repositoryUrl": "https://github.com/jchaudhu/SAP-Cloud.git", "path": "doc/architecture/decisions/support-kubernetes.md", "template": "unknown", "status": null, "firstCommit": "2018-08-30T10:59:46Z", "lastCommit": "2018-08-30T10:59:46Z", "numberOfCommits": 1, "title": "Support autoscaling using Kubernetes", "wordCount": 230, "authors": {"name1": 1}, "content": "### Support autoscaling using Kubernetes\n\nThe pipeline can leverage certain Kubernetes features to support dynamic scalability.  \n\n#### Solutions assessed\n- Create pod per step\n- Create pod per stage\n- Create pod per pipeline\n\n##### Create pod per step\nThe idea was to create a pod whenever `dockerExecute` step is invoked. \nThough it was a simple solution, it came with a huge overhead of additional stashing and unstashing. \nThis noticeably delayed the pipeline execution. \nHence this solution is not appropriate in our use case.\n\n##### Create pod per stage\nHere we follow the approach of creating a pod per each stage.\nThis addresses the problem we had in the first approach. \nBut, we need to know the `dockerImage`s used in each stage in advance so that a pod can be created with those images. \nSince the list of `dockerImage`s of an individual stage is derived from the `pipeline_config.yaml`, there needs to be an approach which can support the pipeline extensions.\n\n##### Create pod per pipeline\nWe have also analyzed the idea of using a pod per pipeline. \nHowever, this would contradict our basic principle of isolation between stages, because the file system is shared by every container that exists in a pod.\n\n#### Conclusion\nWe have decided to use the second approach where a pod is created per stage. \nTo support the same when pipeline extensions are used, we will create a pod per step for extensions.\n"}
{"repositoryUrl": "https://github.com/bbc/digital-paper-edit-storybook.git", "path": "docs/ADR/adr-28-08.md", "template": "Madr", "status": "accepted", "firstCommit": "2019-08-28T12:54:25Z", "lastCommit": "2019-10-12T07:22:11Z", "numberOfCommits": 3, "title": "Relationship between Storybook and DPE Client", "wordCount": 438, "authors": {"name1": 1, "name2": 2}, "content": "# Relationship between Storybook and DPE Client\n\n- Status: accepted\n- Deciders: Alli and Eimi\n- Date: 2019-08-28\n\nTechnical Story: N/A\n\n## Context and Problem Statement\n\nWe needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.\n\n- From which repo would components be published?\n- Which repos would consume components from NPM?\n- Should the Storybook live inside the Client repo?\n\n## Decision Drivers\n\nN/A\n\n## Considered Options\n\n1. Publish components to NPM from the DPE Client repo, and then consume those components in the Storybook repo\n2. Publish components to NPM from the Storybook repo, and then consume those components in the DPE client\n3. Include the Storybook config and setup within the Client repo\n\n## Decision Outcome\n\nChosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally — before publishing the component to the hosted Storybook and NPM.\n\nThis means that our workflow for populating the Storybook and refactoring the Client code is as follows:\n\n1. Duplicate component code to Storybook repo\n2. Publish completed components to NPM\n3. Remove the original component code from the Client and import via NPM\n\n### Positive Consequences\n\n### Negative consequences\n\nCaveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.\n\nIf possible, also avoid having people working simultaneously on a component that consumes / is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).\n\n## Pros and Cons of the Options\n\n### Option 1: Publish components to NPM from the Client and consume via Storybook\n\nThis workflow would mean that we would need to be refactoring code in the client before publishing individual components to NPM. To modify the components in the Storybook, we would need to re-publish to NPM from the client. This is gross.\n\n### Option 2: Publish components to NPM from the Storybook and refactor to consume in the Client\n\nAlthough this workflow means we are essentially copy-pasting code over from the Client repo to the Storybook, it allows us to:\n\n- Refactor component code while previewing the Storybook locally\n- Reflect changes to the code in the Client by refactoring to import components from NPM\n\nThis is a more sensible workflow than option one.\n\n### Option 3: Same repo for both Storybook and Client\n\nWe didn't discuss this option in detail — mainly because it would likely introduce too many moving parts to the same repo.\n\n## Links\n\nN/A\n"}
