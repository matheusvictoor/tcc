id,repositoryUrl,path,title,content_snippet,my_topic
ADR_001,https://github.com/omair-sajid-confiz/adr-poc.git,doc/adr/0007-use-unity-for-dependency-injection.md,7. Use Unity for dependency injection,"# 7. Use Unity for dependency injection  Date: 2018-09-17  ## Status  Accepted  Extends [6. use dependency injection](0006-use-dependency-injection.md)  Amended By [8. Use Autofac for dependency injection](0008-use-autofac-for-dependency-injection.md)  ## Context  The issue motivating this decision, and any context that influences or constrains the decision.  ## Decision  The change that we're proposing or have agreed to implement.  ## Consequences  What becomes easier or more difficult to do and any risks introduced by the change that will need to be mitigated. ",technology_choice
ADR_002,https://github.com/superwerker/superwerker.git,docs/adrs/notifications.md,Notifications,"# Notifications  ## Context  superwerker creates an OpsItem in Systems Manager OpsCenter for each email received by the [RootMail](rootmail.md) feature. Without notifications for new OpsItems, users need to check the OpsCenter for new items manually and might miss important information regarding their AWS accounts and resources.  ## Decision  - Use CloudWatch Events to trigger an AWS Lambda function whenever a new OpsItem is created.     - OpsCenter / OpsItem supports SNS notifications, but the desired SNS topic Arn needs to be provided explicitly whenever an OpsItem is created. Therefore, we decided against this native feature. - Publish a message to an SNS topic for every new OpsItem.  - Use SNS, since subscriptions and email verification work out-of-the-box with CloudFormation tooling.  - Use native email subscriptions for SNS to notify a specified email address about new messages.  - We decided against using SES for email notifications since this would lead to several additional steps like verifying sender and recipient domains or email addresses. - We decided against (re-)using the existing root email address of the management AWS account since we would need to keep this in sync with the SNS subscription (because the management account root email adress can be changed). And we wanted to keep the notification feature simple. - If no email address is provided, no SNS topic is created.  ## Consequences  - Users can provide an email address for notifications when creating the superwerker CloudFormation stack. - Users need to verify the SNS subscription for the provided email address. - Users will receive an email about every new OpsItem in OpsCenter. - Users need to take care of handling OpsItems (e.g., close them). - SNS email subscriptions do not allow the full range of email capabilities/customizations (which SES would). ",technology_choice
ADR_003,https://github.com/DanilSuits/dddsample-pelargir.git,doc/adr/0003-install-citerus-as-a-git-submodule.md,3. Install citerus as a git submodule,"# 3. Install citerus as a git submodule  Date: 03/06/2018  ## Status  Accepted  ## Context  I can't be bothered to be installing the citerus library into my repositories all the time.  I want to keep my copy of that closely synchronized to the common project, so that any progress that is made can be incorporated into my work.  I want a convenient way to introduce fixes in the existing implementation as I discover the need.  I want to keep the touch on the existing code _light_; I expect to be making some fairly drastic changes in approach, and I don't want to be debating my choices with the maintainers.  I'm not currently comfortable working with maven projects where the elements are not rooted under a common location.  ## Decision  Add the upstream as a git submodule, so that it's clear precisely which version of the upstream is in play at any given time.  Changes that are of general interest can be in that space.  Most radical changes (for a _gentle_ definition of radical) should be maintained elsewhere.  ## Consequences  Well, I have to learn how git submodules work.  I'll need to be careful about how to make global changes, for concerns like formatting, removing unused imports, and so on.  ",governance_and_process
ADR_004,https://github.com/SAP/cloud-sdk-js.git,knowledge-base/adr/0028-public-api-extraction.md,Public API Generation,"# Public API Generation  ## Status  accepted  ## Context  For a typescript library it is common to use [barrels](https://basarat.gitbook.io/typescript/main-1/barrel) to make imports easy for consumers. A barrel is simply an `index.ts` re-exporting content of other source file. As a consumer you can then import all parts of the public API via the root `index.ts`.  In the past we exported everything via multiple barrel file in each folder and a \* in the barrels:  ```ts export * from './some-file.ts'; ```  This was convenient for us, but we had no distinction between:  - This is part of the `public` API where a consumer can rely on stability. - This is part of the `internal` API and not meant for direct usage of the consumer.   The convenience had the drawback that we were often blocked to do a refactoring.   This ADR proposes a way to improve the situation.  ## API Categories  Our new strategy should provide the following options:  - `Exported Public API`: This is a cautious decision and consumer can rely on stable contract for minor versions.   It contains only objects where we see (re)use potential for customers.   This API is reachable via the root level import e.g. `@sap-cloud-sdk/odata-v2`. - `Exported Internal API`: Exported for technical reasons but not meant to be used by consumer.   We keep this API stable for path versions.   This API is accessible via `@sap-cloud-sdk/odata-v2/internal`. - `Not exported`: Should be used wherever possible  Each object like constant, function, interface, type, class **must** be in one of the categories.  ## How to Achieve it  Note that the problem has two sides:  - For TypeScript the `d.ts` files are the source of truth for the available types.   You could use the [stripInternal](https://www.typescriptlang.org/tsconfig#stripInternal) compiler flag to remove internal object from the `d.ts` files.   You could still use `export * from 'ABC""` because the type definition are reduced. - For JavaScript the module exports in the transpiled `index.js` matter.   To have a minimal API exposed here one has to avoid `*` in the export statements.  There are tools like [barrelsby](https://github.com/bencoveney/barrelsby#readme) to create the barrels for you. However, these tools do not create named minimal exports. Hence, we propose the following approach:  - You go over the code and manually maintain the API:   1. Use `@internal` annotation and `stripInternal` compiler option for parts the internal API.      This removes the internal API from the `d.ts` files.   2. Maintain minimal named exports in the root `index.ts` pointing to the objects of the public API.      This creates also minimal module exports in the JavaScript usecase.   3. We create a `internal` export as we did for the [v4 case](https://github.com/SAP/cloud-sdk-js/tree/v1.28.0/packages/core) which can be imported via `@sap-cloud-sdk/odata-v2/internal`.      This internal folder contains all exports.  This is a large manual effort initially and seems to be redundant because the minimal `index.ts` alone would already do the trick. However, the double maintenance makes better check rules possible. We plan to implement the following checks:  - Have a check to avoid any `*` exports in the root `index.ts`. - Use an automatic tool like [barrelsby](https://github.com/bencoveney/barrelsby#readme) to create `index.ts` exporting everything to the `internal` folder. - Have a check to enforce exposed object from `d.ts` match the named exports of the root `index.ts`.   - A missing value in the `index.ts` denotes: You have exported something in the code but not added it to the relevant barrel or missed the `@internal`.   - A missing value in the `d.ts` denotes: You have violated the API contract and marked a previously exported object as `@internal`. - In the integration and E2E tests we enforce root level imports via eslint rule. - [Optiona] Have a check to enforce TsDoc on all exported objects.   Either a full doc if part of the public API or at least `@internal` as minimal value.   The [eslint-plugin-tsdoc](https://github.com/microsoft/tsdoc/issues/209) does not have a `require` rule.   The [eslint-plugin-jsdoc](https://github.com/gajus/eslint-plugin-jsdoc) does have it but does not recognize many TypeScript object as `interface`.  ## Internal Packages  The packages:  - odata-common - generator-common  are not meant for direct usage. Hence, they do not export a public API (root level index is empty). All object intended for the public are re-exported from `odata-v2`,`odata-v4` or the generator packages.  ## Consequences  We have a minimal API exposed to the consumer and most of the exported function are internal. We are able to do refactoring on the internal API methods. We have tooling enforcing to keep the public API minimal and well documented. ",governance_and_process
ADR_005,https://github.com/huifenqi/arch.git,decisions/0030-capacity-evaluation-storage.md,30. 容量评估 - 存储,# 30. 容量评估 - 存储  Date: 2017-05-25  ## Status  Accepted  ## Context  1. 一些需要高 IOPS 的服务，磁盘使用的是普通云盘，如，数据库，备份服务，图片服务等。  ## Decision  1. 明确存储的使用场景； 2. 关注吞吐量，IOPS和数据首次获取时间。  我们的存储都是基于 Aliyun 的，他有以下类别及特点：  * nas(文件存储) 	* 使用场景 		* 负载均衡共享存储和高可用 		* 企业办公文件共享(**不支持本地挂载**) 		* 数据备份 		* 服务器日志共享 	* 价格及吞吐能力 		* 2/G/M SSD性能型 60M/s 		* **0.65/G/M 容量型 30M/s** * disk(块存储、云盘)  	* 使用场景 		* 普通云盘 			* 不被经常访问或者低 I/O 负载的应用场景 		* 高效云盘 			* 中小型数据库 			* 大型开发测试 			* Web 服务器日志 		* SSD 云盘 			* I/O 密集型应用 			* 中大型关系型数据库 			* NoSQL 数据库 	* 价格 		* 普通云盘 0.3/G/M  		* 高效云盘 0.35/G/M 		* SSD 云盘 1/G/M 	* 吞吐量 		* 普通云盘 30MBps 		* 高效云盘 **80MBps** 		* SSD 云盘 256MBps 	* IOPS 		* 普通云盘 数百 		* 高效云盘 3000 		* SSD 云盘 20000 	* 访问延迟 		* 普通云盘 5 - 10 ms 		* 高效云盘 1 - 3 ms 		* SSD 云盘 0.5 - 2 ms * oss(对象存储) 	* 使用场景 		* 图片和音视频等应用的海量存储 		* 网页或者移动应用的静态和动态资源分离 		* 云端数据处理 	* 价格及吞吐能力 		* **0.148/G/M 标准型 吞吐量大，热点文件、需要频繁访问的业务场景** 大概 50M/s，类似高效云盘 		* **0.08/G/M 低频访问型 数据访问实时，读取频率较低的业务场景** Object 存储最低 30 天 		* 0.06/G/M 归档型 数据恢复有等待时间，数据有存储时长要求 Object 存储最低 30 天 * oas(归档存储) 	* 使用场景 		* 低成本备份 		* 数据归档 		* 取代磁带 	* 价格及吞吐能力 		* 0.07/G/M 用户提取数据时能够容忍0-4小时的时间延迟  ## Consequences  1. 加粗的信息可以重点查看下，同类别里比较推荐； 2. 我们选择时还要考虑价格及存储时长要求。  Refs:  * [Server request and upgrade: capacity evaluation][1] * 云盘参数和性能测试方法：[https://help.aliyun.com/document\_detail/25382.html][2]  [1]:	0019-server-request-and-upgrade-capacity-evaluation.md [2]:	https://help.aliyun.com/document_detail/25382.html,data_persistence
ADR_006,https://github.com/noushad-pp/base-react-typescript.git,doc/adr/0001-record-architecture-decisions.md,1. Record architecture decisions,"# 1. Record architecture decisions  Date: 2019-09-25  ## Status  Accepted  ## Context  We need to record the architectural decisions made on this project.  ## Decision  We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).  ## Consequences  See Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's [adr-tools](https://github.com/npryce/adr-tools). ",governance_and_process
ADR_007,https://github.com/zooniverse/front-end-monorepo.git,docs/arch/adr-23.md,ADR-23 Tasks as classifier plugins,"# ADR-23 Tasks as classifier plugins  ## Context We'd like the new classifier to be easily extensible. However, adding new tasks to the classifier involved updating the code in several places: - add new code in three places:   - [task views](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/components/Classifier/components/TaskArea/components/Tasks).   - [task models](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/tasks).   - [annotation models](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/annotations). - import the new modules by name in several places, and register them:   - [registered views](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/components/Classifier/components/TaskArea/components/Tasks/helpers/getTaskComponent.js).   - [import tasks models for workflow steps](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/WorkflowStepStore.js#L5-L18).   - [import all annotations to the classification model](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/Classification.js#L3).   - [register annotations with the classifications store](https://github.com/zooniverse/front-end-monorepo/blob/2e2ac27a442afc8cfaea6f7735b97ebb511367a8/packages/lib-classifier/src/store/ClassificationStore.js#L111-L120).  It was easy to forget one of these steps and a lot of this could be automated in code.  ## Decision  - Keep all the code together. Store task views and models next to each other in the filesystem. (#1212) - Import named modules to a registry object (or similar) then load them in to other code from that register. (#1212) - Delegate responsibility from the classification to individual tasks. (#1228)  ### Implementation  - Task code was moved to `lib-classifier/src/plugins/tasks`. Each task has its own directory, with these subdirectories:   - _components_: React components to render the task.   - _models_: MobX State Tree models for the task. One Task model and one Annotation model. - a _taskRegistry_ object was added, which is described in the [tasks README](https://github.com/zooniverse/front-end-monorepo/blob/master/packages/lib-classifier/src/plugins/tasks/readme.md). - Responsibility for creating new annotations was removed from the classifications store, removing the need for the classifications store to know about different types of tasks and how to create an annotation for each. New methods were added to the task models to delegate responsibility and make tasks more flexible:   - _task.createAnnotation()_ creates a new annotation of the correct type for a specific task.   - _task.defaultAnnotation_ (read-only) returns the default annotation for a specific task.  ## Status  Accepted  ## Consequences  - A similar architecture could be used to register subject viewers with the classifier. - Tasks could be removed completely from the classifier. When a workflow loads, its tasks could be instantiated outside the classifier and only the tasks needed for the workflow could be passed in as props. - The classifier could make better use of the MobX State Tree. A classification could store the tasks used to generate that classification, each task holding a reference to its own annotation. This opens up the possibility of more flexible code for tracking workflow history and handling recursive workflows. We could also take advantage of the tree (via _getParent()_ or _getParentOfType()_) to easily reference the task that generated a specific annotation, or the classification that a task is currently doing work for. - the registry model has no equivalent for `import { SingleChoiceAnnotation, MultipleChoiceAnnotation, TextAnnotation } from '@plugins/tasks/models/annotations'`. It would be helpful, but not necessary, to be able to do this when setting up classifier stores. - the task registry is only available after the task models have been set up and initialised, limiting its usefulness when accessing models in order to set up other models, such as drawing tools. ","architectural_patterns, governance_and_process"
ADR_008,https://github.com/ASethi93/james.git,src/adr/0013-precompute-jmap-preview.md,13. Precompute JMAP Email preview,"# 13. Precompute JMAP Email preview  Date: 2019-10-09  ## Status  Accepted  ## Context  JMAP messages have a handy preview property displaying the firsts 256 characters of meaningful test of a message.  This property is often displayed for message listing in JMAP clients, thus it is queried a lot.  Currently, to get the preview, James retrieves the full message body, parse it using MIME parsers, removes HTML and keep meaningful text.  ## Decision  We should pre-compute message preview.  A MailboxListener will compute the preview and store it in a MessagePreviewStore.  We should have a Cassandra and memory implementation.  When the preview is precomputed then for these messages we can consider the ""preview"" property as a metadata.  When the preview is not precomputed then we should compute the preview for these messages, and save the result for later.  We should provide a webAdmin task allowing to rebuild the projection. The computing and storing in MessagePreviewStore  is idempotent and the task can be run in live without any concurrency problem.  Some performance tests will be run in order to evaluate the improvements.  ## Consequences  Given the following scenario played by 2500 users per hour (constant rate)  - Authenticate  - List mailboxes  - List messages in one of their mailboxes  - Get 8 times the properties expected to be fast to fetch with JMAP  We went from:  - A 7% failure and timeout rate before this change to almost no failure  - Mean time for GetMessages went from 9 710 ms to 434 ms (22 time improvment), for all operation from  12 802 ms to 407 ms (31 time improvment)  - P99 is a metric that did not make sense because the initial simulation exceeded Gatling (the performance measuring tool   we use) timeout (60s) at the p95 percentile. After this proposal p99 for the entire scenario is of 1 747 ms  As such, this changeset significantly increases the JMAP performance.  ## References   - https://jmap.io/server.html#1-emails JMAP client guice states that preview needs to be quick to retrieve   - Similar decision had been taken at FastMail: https://fastmail.blog/2014/12/15/dec-15-putting-the-fast-in-fastmail-loading-your-mailbox-quickly/   - [JIRA](https://issues.apache.org/jira/browse/JAMES-2919) ",performance_and_scalability
ADR_009,https://github.com/alphagov/paas-team-manual.git,source/architecture_decision_records/ADR039-aiven-metrics-for-users.html.md,(sem título),"--- title: ADR039 - Aiven metrics for users ---  # ADR039: Aiven metrics for users  ## Context  We offer our users the following backing services through [Aiven](https://aiven.io):  - Elasticsearch - InfluxDB (private beta)  We want to make sure our users can view metrics for their Aiven backing services so that our users can:  - debug and respond to usage and service performance changes - understand the operational characteristics of their applications and services - make better capacity planning and budgeting decisions  Aiven has service integrations which add extra functionality to an Aiven service. Service integrations are useful for:  - shipping logs to an Elasticsearch/Rsyslog - sending metrics to Datadog - sending metrics to Aiven Postgres/InfluxDB - exposing metrics in Prometheus exposition format  We currently run Prometheus for monitoring the platform, using the [Prometheus BOSH release](https://github.com/bosh-prometheus/prometheus-boshrelease) and have confidence and experience using it.  We will need to think about Prometheus failover. If we load balance Prometheus without sticky sessions, the metrics Prometheus reports will be erratic, as different instances report different metrics.  ## Decision  We will use Prometheus to scrape Aiven-provided services.  We will deploy new Prometheus in the Cloud Foundry BOSH deployment using the Prometheus BOSH release. This will reduce blast radius - tenant usage of metrics will not affect our ability to operate and monitor the platform using Prometheus.  We will need to automate the following tasks:  1. Service discovery: make sure Prometheus has an updated list of Aiven services to scrape. We must colocate this automation with the Prometheus instance. 2. Service integration: make sure every eligible Aiven-provided service uses the Aiven service integration for Prometheus.  ## Initial implementation  In the initial implementation we deploy multiple instances of Prometheus for high availability.  ![architecture](../images/adr450-prometheus-aiven-architecture.svg)  There will be three services on the instance:  - Prometheus - Caddy - Aiven service discovery  We use Gorouter for exposing Prometheus publicly.  We use Caddy for failover and for authentication. Caddy does automatic failover. This means that during regular operation, it proxies all traffic to a single instance, but when the usual instance is unreachable, Caddy will proxy the request to the colocated Prometheus.  The Aiven service discovery process will regularly query the Aiven API to keep an updated list of Aiven Elasticsearch services for which we want to receive metrics.  The Prometheus server instances will be configured with this list and retrieve metrics from the respective Aiven Elasticsearch service instances.  This architecture is temporary and we will review it when we look at how to expose the Prometheus API to all users.  ## Status  Accepted  ## Consequences  We will provide Prometheus as the datastore and interface for (a subset of) tenant metrics.  We will deploy additional stateful instances to our Cloud Foundry BOSH deployment.  We will maintain software to automate the integration of Aiven services and Prometheus. ","technology_choice, observability"
ADR_010,https://github.com/nickvdyck/webtty.git,docs/adr/0003-coding-style-and-enforcement.md,3. Coding Style and Enforcement,"# 3. Coding Style and Enforcement  Date: 21/10/2019  ## Status  Proposed  ## Context  This codebase should be easily approachable by all those contributing to it current and future, consistency of coding style is thus an important aspect.  ## Decision For TypeScript based projects we will use eslint to enforce consistent coding styles and prettier to enforce consistent code formatting.  For C# based projects I have not made a decision yet, this is something that should be addressed in a future extension.  An editorconfig file will be available where needed so that editors can pick up common settings.  These rules should not be written in stone. In the future, it should be possible given consensus to add/remove or override certain rules.  Commits introducing code that does not adhere to the above settings should fail the CI build.  ## Consequences Not having linting an coding style automated setup for C# can haunt me in the future, because of all the styling dept that will be accrued. But I simply don't have the bandwidth or knowledge a the moment to get this setup. ","governance_and_process, technology_choice"
ADR_011,https://github.com/xuyuji9000/adr-playground.git,doc/architecture/decisions/0002-implement-as-unix-shell-scripts.md,2. Implement as Unix shell scripts,"# 2. Implement as Unix shell scripts  Date: 2018-09-13  ## Status  Superceded by [3. Remove Unix Scripts](0003-remove-unix-scripts.md)  ## Context  The issue motivating this decision, and any context that influences or constrains the decision.  ## Decision  The change that we're proposing or have agreed to implement.  ## Consequences  What becomes easier or more difficult to do and any risks introduced by the change that will need to be mitigated. ",others
ADR_012,https://github.com/JulianG/bananatabs.git,doc/adr/0004-using-react-context.md,4. Using React Context,"# 4. Using React Context  Date: 2019-07-01  ## Status  Accepted  ## Context  There was a lot of prop-drilling in the component tree.  ### Old Render Tree -- A lot of prop-drilling  ``` <App/>   <BananaTabs />     <MainView /> (✅session, ❌sessionMutator, ❌windowMutator, ❌tabMutator, ✅browserController)       <Title />       <WindowListView />          (✅windows, ✅sessionMutator, ❌windowMutator, ❌tabMutator)         <WindowView />            (✅window, ❌windowMutator, ❌tabMutator)           <WindowHeader />        (️️⚠️window️, ✅windowMutator, ❌tabMutator)             <DisclosureButton />  (✅window, ✅windowMutator)             <VisibilityIcon />    (✅window, ✅windowMutator, ✅tabMutator)             <WindowTitle />       (✅window, ✅windowMutator)           <TabList />             (✅window, ❌tabMutator)             <TabView />           (✅window, ✅tab, ✅tabMutator)               <TabToolsView />       <MainViewCmdButtons /> (none)  Legend: ✅prop: actually used by component ⚠️prop: only reading id (e.g. window.id, tab.id) ❌prop: only passing it down to children  ```  ## Decision  I'm going to try to use React Context with the `useContext` hook to see if I can reduce or eliminate prop-drilling.  ## Consequences  ### New Render Tree -- No prop-drilling  ``` <App/>   <BananaTabs />     <MainView /> (✅session, ✅browserController)       <Title />       <WindowListView />          (✅windows, ⚛️sessionMutator)         <WindowView />            (✅window)           <WindowHeader />        (️️⚠️window️, ⚛️windowMutator)             <DisclosureButton />  (✅window, ⚛️windowMutator)             <VisibilityIcon />    (✅window, ⚛️windowMutator, ⚛️tabMutator)             <WindowTitle />       (✅window, ⚛️windowMutator)           <TabList />             (✅window)             <TabView />           (✅window, ✅tab, ⚛️tabMutator)               <TabToolsView />       <MainViewCmdButtons /> (none)  Legend: ✅prop: used by component ⚠️prop: only reading id (e.g. window.id, tab.id) ❌prop: only passing it down to children ⚛️context: using context hook ``` ",technology_choice
ADR_013,https://github.com/alphagov/monitoring-doc.git,documentation/architecture/decisions/0012-deploy-alertmanager-to-k8s.md,12. Deploying alertmanager to the new platform,"# 12. Deploying alertmanager to the new platform  Date: 2018-11-06  ## Status  Accepted  ## Context  The Observe team is a part of the new platform team, which is building out kubernetes capability in GDS.  There is a long-term goal that teams in GDS should avoid running bespoke infrastructure, specific to that team, so that any infrastructure we run is run in a common way and supportible by many people.  We also have a desire to migrate off ECS.  ECS is painful for running alertmanager because:    - ECS doesn't support dropping configuration files in place   - ECS doesn't support exposing multiple ports via load balancer for service discovery  Kubernetes does not have either of these limitations.  Currently, we have a plan to migrate everything to EC2, in order to get away from ECS.  We have quite a bit of outstanding pain from the old way of doing things:    - we have two different deploy processes; one using the Makefile and one using the deploy_enclave.sh   - we have two different code styles, related to the above   - we have two different types of infrastructure  We haven't fully planned out how we would migrate alertmanager to EC2, but we suspect it would involve at least the following tasks:    - create a way of provisioning an EC2 instance with alertmanager installed (probably a stock ubuntu AMI with cloud.conf to install software)   - create a way of deploying that instance with configuration added (probably a terraform module similar to what we have for prometheus)   - actually deploy some alertmanagers to EC2 in parallel with ECS   - migrate prometheus to start using both EC2 and ECS alertmanagers in parallel   - once we're confident, switch off the ECS alertmanagers   - tidy up the old ECS alertmanager code  This feels like a lot of work, especially if our longer-term goal is that we shouldn't run bespoke infrastructure and should instead run in some common way such as the new platform.  Nevertheless, we could leave alertmanager in ECS but still ease some of the pain by refactoring the terraform code to be the new module-style instead of the old project-and-Makefile style, even if we leave alertmanager itself in ECS.  (Prometheus is different: we want to run prometheus the same way that non-PaaS teams such as Verify or Pay run it, so that we can offer guidance to them. The principle is the same: we want to run things the same way other GDS teams run them.)  ## Decision  1. We will pause any work migrating alertmanager to EC2 2. We will run an alertmanager in the new platform, leaving the remaining alertmanagers in ECS 3. We will try to migrate as much of nginx out of ECS as possible; in particular, we want paas-proxy to move to the same network (possibly same EC2 instance) as prometheus. 4. We will refactor our terraform for ECS to be module-based rather than the old project-and-Makefile style, so that we reduce the different types of code and deployment style. 5. We will keep prometheus running in EC2 and not migrate it to the new platform (although new platform environments will each have a prometheus available to them)  ## Consequences  We will have to be careful to keep alertmanager configuration in sync between the old and new infrascture.  We will have to keep our ECS instances running longer than we might otherwise choose to. ","infrastructure_and_deployment, observability"
ADR_014,https://github.com/cosmos/cosmos-sdk.git,docs/architecture/adr-046-module-params.md,ADR 046: Module Params,"# ADR 046: Module Params  ## Changelog  * Sep 22, 2021: Initial Draft  ## Status  Proposed  ## Abstract  This ADR describes an alternative approach to how Cosmos SDK modules use, interact, and store their respective parameters.  ## Context  Currently, in the Cosmos SDK, modules that require the use of parameters use the `x/params` module. The `x/params` works by having modules define parameters, typically via a simple `Params` structure, and registering that structure in the `x/params` module via a unique `Subspace` that belongs to the respective registering module. The registering module then has unique access to its respective `Subspace`. Through this `Subspace`, the module can get and set its `Params` structure.  In addition, the Cosmos SDK's `x/gov` module has direct support for changing parameters on-chain via a `ParamChangeProposal` governance proposal type, where stakeholders can vote on suggested parameter changes.  There are various tradeoffs to using the `x/params` module to manage individual module parameters. Namely, managing parameters essentially comes for ""free"" in that developers only need to define the `Params` struct, the `Subspace`, and the various auxiliary functions, e.g. `ParamSetPairs`, on the `Params` type. However, there are some notable drawbacks. These drawbacks include the fact that parameters are serialized in state via JSON which is extremely slow. In addition, parameter changes via `ParamChangeProposal` governance proposals have no way of reading from or writing to state. In other words, it is currently not possible to have any state transitions in the application during an attempt to change param(s).  ## Decision  We will build off of the alignment of `x/gov` and `x/authz` work per [#9810](https://github.com/cosmos/cosmos-sdk/pull/9810). Namely, module developers will create one or more unique parameter data structures that must be serialized to state. The Param data structures must implement `sdk.Msg` interface with respective Protobuf Msg service method which will validate and update the parameters with all necessary changes. The `x/gov` module via the work done in [#9810](https://github.com/cosmos/cosmos-sdk/pull/9810), will dispatch Param messages, which will be handled by Protobuf Msg services.  Note, it is up to developers to decide how to structure their parameters and the respective `sdk.Msg` messages. Consider the parameters currently defined in `x/auth` using the `x/params` module for parameter management:  ```protobuf message Params {   uint64 max_memo_characters       = 1;   uint64 tx_sig_limit              = 2;   uint64 tx_size_cost_per_byte     = 3;   uint64 sig_verify_cost_ed25519   = 4;   uint64 sig_verify_cost_secp256k1 = 5; } ```  Developers can choose to either create a unique data structure for every field in `Params` or they can create a single `Params` structure as outlined above in the case of `x/auth`.  In the former, `x/params`, approach, a `sdk.Msg` would need to be created for every single field along with a handler. This can become burdensome if there are a lot of parameter fields. In the latter case, there is only a single data structure and thus only a single message handler, however, the message handler might have to be more sophisticated in that it might need to understand what parameters are being changed vs what parameters are untouched.  Params change proposals are made using the `x/gov` module. Execution is done through `x/authz` authorization to the root `x/gov` module's account.  Continuing to use `x/auth`, we demonstrate a more complete example:  ```go type Params struct {         MaxMemoCharacters      uint64         TxSigLimit             uint64         TxSizeCostPerByte      uint64         SigVerifyCostED25519   uint64         SigVerifyCostSecp256k1 uint64 }  type MsgUpdateParams struct {         MaxMemoCharacters      uint64         TxSigLimit             uint64         TxSizeCostPerByte      uint64         SigVerifyCostED25519   uint64         SigVerifyCostSecp256k1 uint64 }  type MsgUpdateParamsResponse struct {}  func (ms msgServer) UpdateParams(goCtx context.Context, msg *types.MsgUpdateParams) (*types.MsgUpdateParamsResponse, error) {   ctx := sdk.UnwrapSDKContext(goCtx)    // verification logic...    // persist params   params := ParamsFromMsg(msg)   ms.SaveParams(ctx, params)    return &types.MsgUpdateParamsResponse{}, nil }  func ParamsFromMsg(msg *types.MsgUpdateParams) Params {   // ... } ```  A gRPC `Service` query should also be provided, for example:  ```protobuf service Query {   // ...      rpc Params(QueryParamsRequest) returns (QueryParamsResponse) {     option (google.api.http).get = ""/cosmos/<module>/v1beta1/params"";   } }  message QueryParamsResponse {   Params params = 1 [(gogoproto.nullable) = false]; } ```  ## Consequences  As a result of implementing the module parameter methodology, we gain the ability for module parameter changes to be stateful and extensible to fit nearly every application's use case. We will be able to emit events (and trigger hooks registered to that events using the work proposed in [event hooks](https://github.com/cosmos/cosmos-sdk/discussions/9656)), call other Msg service methods or perform migration. In addition, there will be significant gains in performance when it comes to reading and writing parameters from and to state, especially if a specific set of parameters are read on a consistent basis.  However, this methodology will require developers to implement more types and Msg service methods which can become burdensome if many parameters exist. In addition, developers are required to implement persistence logics of module parameters. However, this should be trivial.  ### Backwards Compatibility  The new method for working with module parameters is naturally not backwards compatible with the existing `x/params` module. However, the `x/params` will remain in the Cosmos SDK and will be marked as deprecated with no additional functionality being added apart from potential bug fixes. Note, the `x/params` module may be removed entirely in a future release.  ### Positive  * Module parameters are serialized more efficiently * Modules are able to react on parameters changes and perform additional actions. * Special events can be emitted, allowing hooks to be triggered.  ### Negative  * Module parameters become slightly more burdensome for module developers:     * Modules are now responsible for persisting and retrieving parameter state     * Modules are now required to have unique message handlers to handle parameter       changes per unique parameter data structure.  ### Neutral  * Requires [#9810](https://github.com/cosmos/cosmos-sdk/pull/9810) to be reviewed   and merged.  <!-- ## Further Discussions  While an ADR is in the DRAFT or PROPOSED stage, this section should contain a summary of issues to be solved in future iterations (usually referencing comments from a pull-request discussion). Later, this section can optionally list ideas or improvements the author or reviewers found during the analysis of this ADR. -->  ## References  * https://github.com/cosmos/cosmos-sdk/pull/9810 * https://github.com/cosmos/cosmos-sdk/issues/9438 * https://github.com/cosmos/cosmos-sdk/discussions/9913 ",governance_and_process
ADR_015,https://github.com/marylly/entusiasme-kotlin.git,doc/adr/000000_entusiasme_application.md,ADR 000000: Entusiasme Application Development,# ADR 000000: Entusiasme Application Development  ### **Prologue/Summary**  In the context of software development professional market for women developers facing the difficult to choose a programming language to study we decided to choose kotlin because of the raise of projects and companies using this tecnologies in their products. __________  ### **Discussion/Context**  In all discussions of technical development communities groups appears the difficult for women to choose a technology to start learn software development. ___________  ### **Decision** * Google's tecnology  * Great company supporting and improving * Good documentation * Small learning curve for Java developer * Easy switch to other OOP language * SO Interoperability * Easy to learn * Sugar Syntax ___________ ### **Status** Accepted ___________ ### **Way/State/Version/Model** N/A ___________ ### **Consequences** N/A ___________ ### **Updates** | Information | From | To | Date | |---|---|---|---| | | | | |,technology_choice
ADR_016,https://github.com/alphagov/app-performance-summary.git,doc/adr/0003-integrate-with-drive.md,3. Integrate with google drive,"# 3. Integrate with google drive  Date: 2018-02-26  ## Status  Accepted  ## Context  - We're using [Google Data Studio](https://datastudio.google.com) to present KPIs to stakeholders. - We're calculating application metrics on a periodic basis - We need a way to get the data into a dashboard  ## Decision  Integrate with google drive so we can load KPI reports into a spreadsheet within the GOV.UK team drive.  Use the [Pygsheets](http://pygsheets.readthedocs.io/) library for this.  The data in the sheet can then be used as a data source within data studio.  ## Consequences  - We will need to manage another [service   account](http://pygsheets.readthedocs.io/en/latest/authorizing.html) - Some of our data infrastructure uses AWS, some of it uses google cloud - Anyone on GOV.UK can view measures of application performance ",technology_choice
ADR_017,https://github.com/vwt-digital/operational-data-hub.git,coding_guidelines/adr/0008-config-and-environment-variables.md,8. Config and environment variables,"# 8. Config and environment variables  Date: 2021-03-15  ## Status  Accepted  ## Context  We feel the need to create guidelines for the use of config variables and environment variables when using cloudbuild.  ## Decision  **In Short**  The code that is going to be executed by the Cloud Function (aka the project that is deployed) should receive its configuration variables from a config file.   The cloudbuild steps use environment variables which can be stored in the cloudbuild.yaml or an external file.  **Elaboration**  When developing a Cloud Function, you should store the variables in a config file. The variables are easy to read and use, and other developers (and you) don’t need to build anything for the project to run (unless something other than variables need a build).  This moves over to when a project is deployed: The variables used in the code of the Cloud Function are all stored within the project: A config file. The project should not receive variables from a cloudbuild, but from a file that is merged into the project by the build (or the other way around).  When building something, you might need variables that you are only going to use for building/deployment. Or you might use an external project that needs to get some variables. These variables should be given as environment variables or CLI flags. These variables could be stored in a file that is placed inside a repository for easy access, but can be put into the build as environment files.  **Examples**  Configuration stored in a repository and merged into the project environment (or the other way around), like this example:  `config.py` ```python INDEX = 0 TOKEN = 'AAAAaaaaBBBBbbbb' ``` `__main__.py` ```python from config import INDEX, TOKEN ```  <br>  Configuration as an environment variable used to build in a specific step, like this example: ```yaml substitutions:   _VENV: '/venv'   - name: 'cloud'     entrypoint: 'bash'     args:       - '-c'       - |         source ${_VENV}/bin/activate ```",governance_and_process
ADR_018,https://github.com/kglazko/innovation-week.git,fftv-categories/docs/architecture/adr-0002-robot-pattern.md,ADR 2: Robot Pattern in UI Tests,"# ADR 2: Robot Pattern in UI Tests ## Context UI tests are notoriously difficult to maintain. A few problems that UI tests have are that they often: 1. Lack a clear architecture 1. Repeat complex logic 1. Are written imperatively  After briefly searching, only one architecture for UI tests comes up: **the Robot pattern.** The Robot pattern separates the ""how"" and the ""what"" concerns of a UI test. The tests handle the what - click this button, assert the state, click that button, assert the state - while the robots handle the ""how"" - how to find the button view, how to click the button, and how to actually assert the state, i.e. the implementation details.  The Robot pattern results in declarative tests like: ```kotlin navigationOverlay {     assertCanGoBack()     goBack()     assertCanNotGoBack()  }.enterUrlAndEnterToBrowser(""https://mozilla.org"") {     assertBodyContent(""Welcome to mozilla.org!"") } ```  In the above example, the top-level functions are screen transitions while the inner scope functions are interactions on a given screen.  To learn more about the Robot pattern, see these resources: - [Brief introduction (missing screen transitions)](https://medium.com/android-bits/espresso-robot-pattern-in-kotlin-fc820ce250f7) - [Presentation introducing the pattern](https://academy.realm.io/posts/kau-jake-wharton-testing-robots/) - [Slide deck from presentation](https://jakewharton.com/testing-robots/)  Pros of the Robot pattern: - Declarative test files - Discourages repetition of complex ""how"" logic by centralizing in robots - Reduces the number of places test code needs to change for UX changes, e.g.:   - If only one screen changes, only one robot, and perhaps the reliant tests, needs to change   - Small UX changes (e.g. polish) generally only change the robot code - Clearly separates which interactions occur on which screens - Reduced scope when implementing functionality through robot abstractions  Cons of the Robot pattern: - To write robots and debug test failures, it's a pattern that must be learned - Relies on Kotlin features so probably won't be readable in Java - Test failure call stacks are less clear due to nested function calls to support the test DSL - Robot screen transition implementations use uncommon Kotlin syntax to support the test DSL - Hard to write generic re-usable code, as required by the robots  Neutral notes on the Robot pattern: - Doesn't define how to handle interactions that don't fit into a screen (e.g. mutating internal state, clicking hardware remote buttons)  ## Decision We will architect our UI tests using the Robot pattern: there are no obvious alternatives and the pros significantly outweigh the cons when addressing our specific problems.  Status: Accepted  ## Consequences - Tests written with the Robot pattern should be more maintainable but are expected to need more upfront design and brain power - All new UI tests will be written using the Robot pattern and Kotlin - All existing UI tests will be refactored to use the Robot pattern (to reduce code duplication) and Kotlin - Test developers need to learn about the robot pattern in order to debug tests and modify the robots - The QA teams, in order to maintain these tests, need to familiarize themselves with Kotlin - The test harness is not coupled to the robot pattern so if we have issues we can easily go back to writing tests without an architecture as before ","testing_strategy, architectural_patterns"
ADR_019,https://github.com/alphagov/publishing-api.git,docs/arch/adr-005-documents-and-editions.md,Decision Record: Resolve consistency and uniqueness in content,"# Decision Record: Resolve consistency and uniqueness in content  ## Context  Publishing API has adopted the open/closed principle where the domain model is based around a central ContentItem model, where there are numerous entities that store data related to the ContentItem - however the ContentItem does not have knowledge about them.  Over time this data structure revealed a number of problems to developers and users of the Publishing API. Such as:  * Difficulty in maintaining uniqueness constraints, as these were in multiple   tables - particularly problematic with concurrent requests. * Long verbose queries that were inconsistent with how queries are normally   authored in Ruby-on-Rails. * Slow performing queries due to the need for many joins. * Large amounts of code to try resolve problems identified.  A number of non-exclusive options were considered on how to resolve the issues:  * **Option 1**: Merge user_facing_version and locale into ContentItem table, as   this would allow us to set a single uniqueness constraint that could raise an   error on a concurrent request; * **Option 2**: Merge user_facing_version, locale, base_path, and state into   ContentItem table, as this would allow multiple uniqueness constraints but   breaks from open/closed principal; * **Option 3**: Split ContentItems into Document and Edition models, with   Edition being a particular version of a Content Item and a Document spanning   all versions; * **Option 4**: Split a ContentItem model into two separate models, one that   focuses on the uniqueness and relationships, the latter on the content; * **Option 5**: ContentStore specified directly in the database rather than as a   byproduct of the state value; * **Option 6**: Store state history, this would change state from being a single   field that is updated to a collection that is stored for a ContentItem; * **Option 7**: Take a location centric approach to storing how base_path values are   stored for ContentItems. This would involve a table storing which items use a   particular base_paths and would open the door to storing other items (such as   redirects) that require a base_path.  ## Decisions  Each of the options was considered and a selection of them were chosen to be implemented:  ### Option 1  This was rejected in favour of Option 2 which effectively superseded it.  ### Option 2  This was accepted as it was felt the additional complexity introduced by using the open/closed principal was a greater cost to us than the potential complexity of increasing the concerns of a ContentItem model.  There is concern that the model could become a ""god"" model as the initial proposal tried to avoid. We agreed that we should not use the model layer for logic where possible and instead use supplementary classes. This would allow us to keep our models ""thin"".  ### Option 3  This option was accepted with some uncertainty of the naming that should be used. It was decided to use the original proposal of ""Documents"" and ""Editions"" as these are concepts already in use in GOV.UK publishing - with a synonymous meaning. There was concern that not all content the Publishing API stores is considered a ""document"", however it was felt that the use of the term ""document"" was already in use within Publishing and this would not be introducing a fresh problem.  The key aspects that influenced choosing this was: * It offered a simple means to lock requests for concurrency * It provided a simpler interface for someone to look up which content is   in draft or in live * It made the distinctions between translations of a piece of content clearer.  ### Option 4  This was rejected. It was felt that this option was letting our application concerns influence our schema too greatly. There was also not a clear difference between which model would have responsibility for what data, which we felt would make it a challenging abstraction to explain.  ### Option 5  This was initially delayed for further investigation. However it transpired that to have a unique index between base_path and state in PostgreSQL we would need this. Thus it was accepted and implemented.  ### Option 6  This was rejected due to it not being a current concern, it is an idea that may be revisited as part of work to include workflow history and/or to support a greater array of workflow states.  ### Option 7  This was rejected since we are not at a point where we are concerned with different entities sharing the concept of base_path. It was felt that until these are a concern this idea offered an increase in complexity without any clear benefits.  This will be reconsidered if we pursue ideas such as ""redirects as first-class citizens"".  ",data_persistence
ADR_020,https://github.com/cosmos/cosmos-sdk.git,docs/architecture/adr-008-dCERT-group.md,ADR 008: Decentralized Computer Emergency Response Team (dCERT) Group,"# ADR 008: Decentralized Computer Emergency Response Team (dCERT) Group  ## Changelog  * 2019 Jul 31: Initial Draft  ## Context  In order to reduce the number of parties involved with handling sensitive information in an emergency scenario, we propose the creation of a specialization group named The Decentralized Computer Emergency Response Team (dCERT).  Initially this group's role is intended to serve as coordinators between various actors within a blockchain community such as validators, bug-hunters, and developers.  During a time of crisis, the dCERT group would aggregate and relay input from a variety of stakeholders to the developers who are actively devising a patch to the software, this way sensitive information does not need to be publicly disclosed while some input from the community can still be gained.  Additionally, a special privilege is proposed for the dCERT group: the capacity to ""circuit-break"" (aka. temporarily disable)  a particular message path. Note that this privilege should be enabled/disabled globally with a governance parameter such that this privilege could start disabled and later be enabled through a parameter change proposal, once a dCERT group has been established.  In the future it is foreseeable that the community may wish to expand the roles of dCERT with further responsibilities such as the capacity to ""pre-approve"" a security update on behalf of the community prior to a full community wide vote whereby the sensitive information would be revealed prior to a vulnerability being patched on the live network.    ## Decision  The dCERT group is proposed to include an implementation of a `SpecializationGroup` as defined in [ADR 007](./adr-007-specialization-groups.md). This will include the implementation of:  * continuous voting * slashing due to breach of soft contract * revoking a member due to breach of soft contract * emergency disband of the entire dCERT group (ex. for colluding maliciously) * compensation stipend from the community pool or other means decided by    governance  This system necessitates the following new parameters:  * blockly stipend allowance per dCERT member * maximum number of dCERT members * required staked slashable tokens for each dCERT member * quorum for suspending a particular member * proposal wager for disbanding the dCERT group * stabilization period for dCERT member transition * circuit break dCERT privileges enabled  These parameters are expected to be implemented through the param keeper such that governance may change them at any given point.  ### Continuous Voting Electionator  An `Electionator` object is to be implemented as continuous voting and with the following specifications:  * All delegation addresses may submit votes at any point which updates their    preferred representation on the dCERT group. * Preferred representation may be arbitrarily split between addresses (ex. 50%    to John, 25% to Sally, 25% to Carol) * In order for a new member to be added to the dCERT group they must    send a transaction accepting their admission at which point the validity of    their admission is to be confirmed.     * A sequence number is assigned when a member is added to dCERT group.      If a member leaves the dCERT group and then enters back, a new sequence number      is assigned.   * Addresses which control the greatest amount of preferred-representation are    eligible to join the dCERT group (up the _maximum number of dCERT members_).    If the dCERT group is already full and new member is admitted, the existing    dCERT member with the lowest amount of votes is kicked from the dCERT group.     * In the split situation where the dCERT group is full but a vying candidate      has the same amount of vote as an existing dCERT member, the existing      member should maintain its position.     * In the split situation where somebody must be kicked out but the two      addresses with the smallest number of votes have the same number of votes,      the address with the smallest sequence number maintains its position.   * A stabilization period can be optionally included to reduce the    ""flip-flopping"" of the dCERT membership tail members. If a stabilization    period is provided which is greater than 0, when members are kicked due to    insufficient support, a queue entry is created which documents which member is    to replace which other member. While this entry is in the queue, no new entries    to kick that same dCERT member can be made. When the entry matures at the    duration of the  stabilization period, the new member is instantiated, and old    member kicked.  ### Staking/Slashing  All members of the dCERT group must stake tokens _specifically_ to maintain eligibility as a dCERT member. These tokens can be staked directly by the vying dCERT member or out of the good will of a 3rd party (who shall gain no on-chain benefits for doing so). This staking mechanism should use the existing global unbonding time of tokens staked for network validator security. A dCERT member can _only be_ a member if it has the required tokens staked under this mechanism. If those tokens are unbonded then the dCERT member must be automatically kicked from the group.    Slashing of a particular dCERT member due to soft-contract breach should be performed by governance on a per member basis based on the magnitude of the breach.  The process flow is anticipated to be that a dCERT member is suspended by the dCERT group prior to being slashed by governance.    Membership suspension by the dCERT group takes place through a voting procedure by the dCERT group members. After this suspension has taken place, a governance proposal to slash the dCERT member must be submitted, if the proposal is not approved by the time the rescinding member has completed unbonding their tokens, then the tokens are no longer staked and unable to be slashed.  Additionally in the case of an emergency situation of a colluding and malicious dCERT group, the community needs the capability to disband the entire dCERT group and likely fully slash them. This could be achieved though a special new proposal type (implemented as a general governance proposal) which would halt the functionality of the dCERT group until the proposal was concluded. This special proposal type would likely need to also have a fairly large wager which could be slashed if the proposal creator was malicious. The reason a large wager should be required is because as soon as the proposal is made, the capability of the dCERT group to halt message routes is put on temporarily suspended, meaning that a malicious actor who created such a proposal could then potentially exploit a bug during this period of time, with no dCERT group capable of shutting down the exploitable message routes.  ### dCERT membership transactions  Active dCERT members  * change of the description of the dCERT group * circuit break a message route * vote to suspend a dCERT member.  Here circuit-breaking refers to the capability to disable a groups of messages, This could for instance mean: ""disable all staking-delegation messages"", or ""disable all distribution messages"". This could be accomplished by verifying that the message route has not been ""circuit-broken"" at CheckTx time (in `baseapp/baseapp.go`).  ""unbreaking"" a circuit is anticipated only to occur during a hard fork upgrade meaning that no capability to unbreak a message route on a live chain is required.  Note also, that if there was a problem with governance voting (for instance a capability to vote many times) then governance would be broken and should be halted with this mechanism, it would be then up to the validator set to coordinate and hard-fork upgrade to a patched version of the software where governance is re-enabled (and fixed). If the dCERT group abuses this privilege they should all be severely slashed.  ## Status  Proposed  ## Consequences  ### Positive  * Potential to reduces the number of parties to coordinate with during an emergency * Reduction in possibility of disclosing sensitive information to malicious parties  ### Negative  * Centralization risks  ### Neutral  ## References    [Specialization Groups ADR](./adr-007-specialization-groups.md) ",others
ADR_021,https://github.com/KIT-SOC4S/ftd-scratch3-offline.git,docs/architecture/decisions/0010-use-travis-ci-for-continuous-integration.md,10. Use travis ci for continuous integration,"# 10. Use travis ci for continuous integration  Date: 2019-10-20  ## Status  Accepted  ## Context  We want to use continuous integration to make sure that at any time the build is working.   CI will check every commit and PR.   Possible choices are: Travis CI, CircleCI or AppVeyor.   Travis CI offers Linux and Mac builds. Windows is in beta.   CircleCI supports all 3 platforms.   AppVeyor supports Linux and Windows.   The authors have already used Travis CI.    ## Decision  We will use Travis CI for continuous integration.    ## Consequences  Possible vendor lock-in. ","infrastructure_and_deployment, technology_choice"
ADR_022,https://github.com/wikimedia/mediawiki-extensions-Popups.git,docs/adr/0009-utilize-browser-caching.md,9. Utilize browser caching,"# 9. Utilize browser caching  Date: 2017-05-24  ## Status  Accepted  ## Context  We needed to make sure we're not overloading the servers with excessive requests. We wanted to find a way to serve fresh resources while keeping the back-end happy.  ## Decision  Page Previews will leverage the browser's cache rather than maintaining its own. We rely on Grade A browsers implementing HTTP caching correctly and their vendors making accessing them as efficient as possible in order to avoid incurring the incidental complexity of writing our own cache in JavaScript.  We'll set appropriate `Cache-Control` HTTP headers for both the MediaWiki API, via [the `maxage` and `smaxage` main module parameters][0], and the RESTBase page summary endpoint with the help of the Services team.  ## Consequences  Resources fetched from the MediaWiki API [will be cached for 5 minutes in public caches and the browsers's cache][1]. Unlike the MediaWiki API, resources fetched from the RESTBase endpoint, [will be cached for 14 days in public caches][2].  [0]: https://www.mediawiki.org/wiki/API:Main_module#Parameters [1]: https://github.com/wikimedia/mediawiki-extensions-Popups/blob/86075fba/src/gateway/mediawiki.js#L15 [2]: https://github.com/wikimedia/mediawiki-services-restbase-deploy/blob/9a86d4ce/scap/templates/config.yaml.j2#L100-L101 ",data_persistence
ADR_023,https://github.com/akvo/akvo-product-design.git,FLOW/arch/ADR-003.md,ADR 003: Browser-based submission of survey form responses,"## ADR 003: Browser-based submission of survey form responses  ## Context  In certain cases organisations would like to take advantage of the AkvoFLOW data gathering capabilities but due to their (organistions') nature, they do not require the mobile app to fill out survey forms.  They instead require the possibility to fill out and submit survey forms through a web browser.  The implementation of this feature occurs at a moment in which we are in the process of migrating the entire Akvo FLOW backend functionality away from being hosted on GAE infrastructure.  The requirement of the possibility to fill in and submit responses through a browser-based interface is also applicable beyond AkvoFLOW and is relevant for other products developed by Akvo.  ## Decision  * We will avoid as much as possible making any modification to the GAE component and  implement this functionality as a service that interacts with the GAE component.  * We will split the implementation of the functionality into two components, a client and a backend server component.  * The server component will read the XML representation of the form definition (as currently used by the Akvo FLOW mobile app), and expose an API that delivers the form definition in JSON format.   * The client will read the flow representation from the server API, and render the forms in html.  * The client will store responses locally until submit time. At submit time, the client delivers the responses to the server.  * The server component will receive the responses submitted by the client, transform the responses to the correct format as used by GAE, and use the regular GAE data ingestion process, as used by the device.  ## Status  In progress.  ## Consequences  * We will create minimal to no code modifications within the GAE component also avoiding the possible need for reimplementation at a later stage.  * Storing the responses locally till submission implies the possibility of data loss in case a browser/computer crashes. ",api_and_contracts
ADR_024,https://github.com/alphagov/paas-team-manual.git,source/architecture_decision_records/ADR006-rds-broker.html.md,(sem título),"--- title: ADR006 - RDS broker ---  # ADR006: RDS broker  ## Context  We need to provide tenants with the ability to provision databases for use in their applications. Our first iteration of this will be using RDS. We investigated some implementations of a service broker which supported RDS   - [cf platform eng](https://github.com/cf-platform-eng/rds-broker)  - [18F](https://github.com/18F/rds-service-broker)  ## Decision  We will use the [cf platform eng](https://github.com/cf-platform-eng/rds-broker) rds broker. As this is not a supported product, we will fork this and maintain this and implement new features ourselves.  ## Status  Accepted  ## Consequences  We will be maintaining a new service broker, but have a head start on creating it by basing it on an existing service broker. ","technology_choice, data_persistence"
ADR_025,https://github.com/threefoldtech/js-sdk.git,docs/architecture/decisions/0007-payment-for-3bot-deployer.md,7. payment_for_3Bot_deployer,"# 7. payment_for_3Bot_deployer  Date: 2020-09-15  ## Status  Accepted  ## Context  The deployment of 3Bot is not guaranteed to succeed due to network failures or misbehaving of nodes on the grid. Users who pay for a 3Bot reserve capacity and in case of failing to initialize the solution they lose their money (and that capacity won't be usable after payment)  ## Decision  - 3Bot deployer to start with a funded wallet  - when the user wants to create an instance, we create a pool for this instance using the funded wallet for 15 mins (that should be enough for the initialization step). - When we manage to deploy and initialize the 3Bot, we ask the user to extend the lifetime of the 3Bot in the same chatflow - In case of pool extension failure they will be refunded from the explorer   ## Consequences  - Users will guarantee their payment won't be lost. - A small amount of tokens might be lost from our wallet but it guarantees the safety of users payments and successful deployment of their solutions. ",technology_choice
ADR_026,https://github.com/etienneleba/TDD-hexagonal-project.git,Docs/ADRS/2020_12_02_9_35_TECHNICAL.md,Context,"### Context This is the beginning on the project and for the moment all the commit are done on the master branch  ### Decision  Use Gitflow to handle code source and project management.  ### Consequences  Install the gitflow plugin, explain how to contribute to the project with gitflow in the how to contribute documentation file.    ","governance_and_process, technology_choice"
ADR_027,https://github.com/contiamo/operational-visualizations.git,docs/adr/0001-about-cursors.md,1. about-cursors,"# 1. about-cursors  Date: 2019-07-08  ## Status  2019-07-08 proposed  ## Context  In the grid in cell function we recieve `FragmentFrame` which is vertical slice of original `DataFrame` and then when we render visualaisation we iterate through raw data with `mapRows` method of `IterableFrame` and then we need to access some exact value in raw row. One of the ideas on how to do this universally was to use some kind of **cursor**.  Current implementation of **cursor** looks like this  ```tsx export interface ColumnCursor<Name extends string, ValueInRawRow = any> {   (row: RowCursor): ValueInRawRow;   column: Name;   index: number; } ```  so we can access value like this `cursor(row)` or more hacky and optimised way `row[cursor.index]`. Second option is fragile, in case we would want to change implementation of `row` (for example, from row-oriented storage to column-orinted) it will brake in many places (at least we have types, which will make this refactoring easier).  Cursor at the moment can be recieved from ""root"" `DataFrame`, which we think of as source of all derivative `PivotFrame`s and `FragmentFrame`s. Cursors are cached for referential transparency, so if they got passed down to React components, they will not trigger rerender.  **Question** raised in [one of PR](https://github.com/contiamo/operational-visualizations/pull/70/files#diff-2044c7c7ba6c3fbf04dd49cf3cfa68b9R38): Shall we provide `getCursor` functionality in `FragmentFrame`? From one side it may make code simpler, on the other side we need to make sure that `FragmentFrame`s are referentially transperent (which is not the case, for example, for `cell` method) and it will create a lot of copies of the same cursors. We can as well pass reference to ""root"" `DataFrame` along all derivative structures and ""proxy"" `getCursor` method call to it.  ## Decision  At the moment we decided to use proposed version of cursors and adjust it in the future depending if we like DX or not. We will provide `getCursor` method only for `DataFrame` for now, but we can chnage this in the future.  ## Consequences  This decision mainly affects DX. And it is hard to foresee, without trying out in real project ",others
ADR_028,https://github.com/psu-libraries/scholarsphere.git,doc/architecture/decisions/0001-record-architecture-decisions.md,1. Record architecture decisions,"# 1. Record architecture decisions  Date: 2019-11-04  ## Status  Accepted  ## Context  We need to record the architectural decisions made on this project.  ## Decision  We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).  ## Consequences  See Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's [adr-tools](https://github.com/npryce/adr-tools). ",governance_and_process
ADR_029,https://github.com/alphagov/content-data-api.git,docs/arch/adr-007-etl-publishing-api-content-store.md,ADR 007: ETL to populate Content Items' dimension.,"# ADR 007: ETL to populate Content Items' dimension.  26-01-2018  ## Context  As [per this Trello card][1], we want to populate the Content Items' dimension with the latest changes that result of editing content.       ## Decision  Addressing the ETL process for Content Items this way:  1. The first time, we load all the content items via [publishing-api][3]. We retrieve all the content-ids and paths of all the content items that are live. 2. For each Content Item we will get the JSON from the [Content Store][2], and we will extract the attributes that we need to fulfil the immediate needs. We will also persist all the JSON to be able to extract other attributes in the future. 3. On a daily basis, we will be listening to [publishing-api][3] events, in the same way that [Rummager][4] or [Email Alert Service][5] do. Once we receive a change for the content item, we will automatically update the content items dimension with the new approach.  ### Benefits:  1. This is more aligned with GOV.UK architecture. 1. This is very light and efficient. It also embrace simple code as the ETL process for Content Items is almost trivial.  ## Status  Accepted.  [1]: https://trello.com/c/zqcU0x3s/28-3-content-items-find-source-for-content-items [2]: http://github.com/alphagov/content-store [3]: http://github.com/alphagov/publishing-api [4]: http://github.com/alphagov/rummager [5]: https://github.com/alphagov/email-alert-service ",governance_and_process
ADR_030,https://github.com/mahanhz/clean-architecture-example.git,doc/adr/0003-use-spring-framework.md,3. Use Spring framework,"# 3. Use Spring framework  Date: 20-July-2017  ## Status  Accepted  ## Context  We need to build the how part of the application  ## Decision  We will use Springs ecosystem to implement how our application works.  More explicitly we will use Spring Boot, Spring Data and Spring WebMvc.  ## Consequences  Since developers are already familiar with the technology and like ot use it, there is no delay in being productive.  ## Made by  Mahan Hashemizadeh",technology_choice
ADR_031,https://github.com/miquecg/heroes-board-game.git,docs/adr/0003-broadcast-messages.md,Broadcast Messages,"# Broadcast Messages  ## Context and Problem Statement  Heroes can kill nearby enemies on a certain range, but there is no central place in the game that knows every hero position on the board at any time. Each one of them is the source of truth. In order to do that a hero must broadcast the intent to the rest of players. How to send messages to all heroes at once?  ## Decision Drivers  * Less possible changes * Performance is not a concern  ## Considered Options  * `:pg2` - Implement broadcast on top of an `:all_heroes` group * Registry for PubSub * Manager GenServer - Stores and monitors heroes PID * `Supervisor.which_children/1`  ## Decision Outcome  Chosen option: ""`Supervisor.which_children/1`"", because it comes out best.  ## Pros and Cons of the Options  ### `:pg2`  * Good, because it's designed for such use cases * Good, because it's optimized for speed * Bad, because it's an extra application in the release  ### Registry for PubSub  Example: https://hexdocs.pm/elixir/Registry.html#module-using-as-a-pubsub  * Good, because it allows multiple use cases * Bad, because of some extra complexity  ### Manager GenServer  * Good, because it's simple and focused * Bad, because it reimplements existing solutions  ### `Supervisor.which_children/1`  Docs: https://hexdocs.pm/elixir/Supervisor.html#which_children/1  * Good, because it can be done with existing parts of the system * Bad, because of uncertainties if system was to be scaled ",others
ADR_032,https://github.com/guttih/island.is-glosur.git,docs/adr/0001-use-nx.md,Use NX,"# Use NX  - Status: accepted - Deciders: devs - Date: 03.05.2020  ## Context and Problem Statement  We want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI/CD.  ## Decision Drivers  * Low complexity and overhead in development. * Fit for our stack. * Optimize CI/CD with dependency graphs and/or caching. * Flexible.  ## Considered Options  * [Bazel] * [Nx] * [Lerna]  ## Decision Outcome  Chosen option: ""Nx"", because:  * It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS). * It's relatively easy to learn with focused documentation. * It has schematics to generate apps, libraries and components that includes all of our tools. * It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.  ## Pros and Cons of the Options  ### [Bazel]  * Good, because it's created and maintained by Google * Good, because it supports multiple programming languages and platforms. * Bad, because it's difficult to learn, with a custom BUILD files. * Bad, because JS support is hacky.  ### [Nx]  * Good, because it has built in support for our stack. * Good, because it's been around for a while, originating in the Angular world. * Good, because it's designed with best practices for large scale web applications. * Good, because it supports elaborate code generation. * Good, because it helps optimise CI/CD for large projects. * Bad, because it's fairly opinionated. * Bad, because it's an open source project maintained by an agency.  ### [Lerna]  * Good, because it integrates with NPM and Yarn. * Good, because it's used by a lot of open source projects. * Bad, because it's primarily designed for managing and publishing open source projects, not building and deploying large scale applications.  ## Links  * [Why you should switch from Lerna to Nx](https://blog.nrwl.io/why-you-should-switch-from-lerna-to-nx-463bcaf6821)  [Pants]: https://www.pantsbuild.org/ [Bazel]: https://bazel.build/ [Nx]: https://nx.dev/ [Lerna]: https://lerna.js.org/ ",technology_choice
ADR_033,https://github.com/ebi-uniprot/uniprot-rest-api.git,doc/architecture/decisions/0003-spring-framework.md,3. Spring Framework,"# 3. Spring Framework  Date: 2018-08-02  ## Status  Accepted  ## Context  Programming frameworks can promote the productivity of a project; producing smaller code bases, added reliability, additional features (than one would otherwise write themselves), etc.   ## Decision  We have used the [Spring framework](https://spring.io/) within the team for a number of years, and its recent advances in the domain of REST applications makes it an ideal choice.  ## Consequences  As stated by Spring themselves, they are an opinionated framework that guides the design of certain aspects of a codebase. However, these 'opinions' are generally there for the benefit of a project, making it possible to achieve your goal. ",technology_choice
ADR_034,https://github.com/alphagov/content-data-api.git,docs/arch/adr-008-focus-on-english-content.md,ADR 008: Focus on english content in the first iterations.,"# ADR 008: Focus on english content in the first iterations.  28-01-2018  ## Context  Some Content Items are written in different languages, so [Publishing-api][1] will return the `content_id` along with all locales assigned to the Content Item.    ## Decision  Focus on content written in English.  The main reason is that we would need different algorithms and libraries to make our application consistent among all the languages / locales.  If this is a real need, we will support it in future iterations of the Data Warehouse.  ### Benefits:  This makes the codebase simpler.     ## Status  Accepted.  [1]: http://github.com/alphagov/publishing-api ",others
ADR_035,https://github.com/alphagov/email-alert-api.git,docs/adr/adr-005-record-architecture-decisions.md,5. Record architecture decisions,"# 5. Record architecture decisions  Date: 2020-10-21  ## Context  While we have already recorded architectural decisions for this project, there is no guidance to follow for writing new ones. Recently we found that creating a new ADR was [hindered](https://github.com/alphagov/email-alert-api/pull/1441#discussion_r508729384) by wanting to be consistent with the structure of previous ADRs.  ## Decision  We will continue to use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions.  We will adopt a structure for future ADRs that matches this document - the exemplar in [our Rails app conventions](https://docs.publishing.service.gov.uk/manual/conventions-for-rails-applications.html#documenting-your-decisions). We will not change how the files are named, to avoid breaking links that were never expected to change.  ## Status  Accepted  ## Consequences  ADRs preceding this one will have an inconsistent format to the ones that follow this. ",governance_and_process
ADR_036,https://github.com/globtec/phpadr.git,docs/arch/0001-documenting-architecture-decisions.md,1. Documenting architecture decisions,"# 1. Documenting architecture decisions  Date: 2018-02-11  ## Status  Accepted  ## Context  Record certain design decisions for the benefit of future team members as well as for external oversight.  ## Decision  Use Architecture Decision Records (ADR), that is a technique for capturing important architectural decisions, along with their context and consequences as described by [Michael Nygard](https://twitter.com/mtnygard) in his article: [Documenting Architecture Decisions](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).  ## Consequences  See Michael Nygard's article, linked above.",governance_and_process
ADR_037,https://github.com/ministryofjustice/hmpps-interventions-ui.git,doc/architecture/decisions/0012-use-feature-flags-for-in-progress-features.md,12. Use feature flags for in-progress features,"# 12. Use feature flags for in-progress features  Date: 2021-09-13  ## Status  Accepted  ## Context  There are a few reasons we might not want to put a new feature in front of users as soon as it's been merged into the `main` branch:  - The API might not be up-to-date with the latest version of the UI (and vice versa) - because we're building the UI and API side of the service independently, there are times when the two may be out of sync: an endpoint may not yet be providing all the data we need; the backend functionality may not be finished at the time of writing the UI code. - We want to satisfy the Pact contracts between the two sides of the service but not use the new data structure until the UI has been updated. - The new functionality may need to be further tested (either with users by developers) and iterated upon before release. - We want to keep Pull Requests as small as possible so they're quick to review and it's easy to make changes - this means we'd want to merge smaller chunks of work at a time, which might not be ready for users. - We want to test interactions between systems (e.g. the Community API) on the Development environment but not release these changes to the public.  ## Decision  Any features or behaviour that isn't ready to be interacted with by users will be placed behind a config-based feature flag, configured in `server/config.ts`, e.g. as below: ``` features: {   previouslyApprovedActionPlans: get('FEATURE_PREVIOUSLY_APPROVED_ACTION_PLANS', 'false') === 'true', } ```  This can then be turned on for each environment by adding the environment variable (e.g. `FEATURE_PREVIOUSLY_APPROVED_ACTION_PLANS`) to the intended environment.  We'll usually want to enable this for the development environment and test environment, possibly the pre-prod environment but not the production environment.  Before deploying the changes to the production environment, it's a good idea to double check the configuration is as expected e.g. by checking it's hidden in the pre-production environment.  Once the feature is ready to be interacted with by users, we'll remove the feature flag from the UI configuration.  ## Consequences  - Pull requests can be kept small and self-contained, as we don't need to release a whole feature at once, and there's no risk of it being visible to users. - We can test functionality on the development environment without these changes being deployed to production. - We can release changes to the contracts between the UI and API but keep existing functionality working until both are in sync. - We can test functionality in User Research sessions without it being on the production environment.  There's a small overhead involved in setting up the config and testing it's visible or not on the desired environment. ",governance_and_process
ADR_038,https://github.com/ASethi93/james.git,src/adr/0002-make-taskmanager-distributed.md,2. Make TaskManager Distributed,"# 2. Make TaskManager Distributed  Date: 2019-10-02  ## Status  Accepted (lazy consensus)  ## Context  In order to have a distributed version of James we need to have an homogeneous way to deal with `Task`.  Currently, every James nodes of a cluster have their own instance of `TaskManager` and they have no knowledge of others, making it impossible to orchestrate task execution at the cluster level. Tasks are scheduled and ran on the same node they are scheduled.  We are also unable to list or access to the details of all the `Task`s of a cluster.  ## Decision  Create a distribution-aware implementation of `TaskManager`.  ## Consequences   * Split the `TaskManager` part dealing with the coordination (`Task` management and view) and the `Task` execution (located in `TaskManagerWorker`)  * The distributed `TaskManager` will rely on RabbitMQ to coordinate and the event system to synchronize states ","technology_choice, microservices_and_modularity"
ADR_039,https://github.com/scheduleonce/once-ui.git,docs/adr/0005-publish-the-library-on-npm.md,5. Publish the library on npm,# 5. Publish the library on npm  Date: 2018-12-21  ## Status  Accepted  ## Context  Need to find a way for developers to import this library into their own projects.  ## Decision  We will use npm to publish the library privately in [MyGet](https://www.myget.org) (a 3rd party npm compatible registry)  ## Consequences  Importing the library is as easy as running `npm install @oncehub/ui`,technology_choice
ADR_040,https://github.com/dante-ev/docker-texlive.git,docs/adr/0002-provide-all-packages.md,Provide all packages,"# Provide all packages  ## Context and Problem Statement  Should the Docker image include all packages or a subset of the packages?  ## Considered Options  * Provide all packages * Provide a subset of packages * Provide a minimal set of packages and use [texliveonfly](https://ctan.org/pkg/texliveonfly)  ## Decision Outcome  Chosen option: ""Provide all packages"", because   * texliveonfly does not work on all packages * speeds-up compilation time (because no additional download)  We accept that the final image is ~2GB of size. ",others
ADR_041,https://github.com/avniproject/openchs-adr.git,decisions/0012-create-a-generic-relationship-framework-to-link-between-mother-and-child.md,12. Create a generic relationship framework to link between mother and child,"# 12. Create a generic relationship framework to link between mother and child  Date: 2018-05-28  ## Status  Accepted  ## Context  During a delivery for a mother in the mother program, we will need to create one or more new children. Filling in details during delivery and during PNC visits will need switching between them easy.   At the same time, we are also thinking of creating the concept of a family. Here, the individuals of a family will be linked to the head of the household through a relationship.   We need the modeling of a relationship to be a generic structure that can support both these use cases.   ## Decision  Create an option to map relationships between individuals, with relationship being a concept orthogonal to families. Relationships between individuals can be anything (family relationships, or even relationships to ASHA worker etc if required).   Relationships will be two-way with different values between the two. We will not build (atleast for now) the ability to automatically deduce transitive relationships.   ## Consequences  Support for family can be turned off if it is not required for an organisation.  Development for the family feature and mother-child linking can happen in parallel without dependencies.  There will be individuals that are related, but are not part of the same family. It is possible that some of them are not part of any family. The user is expected to do this manually.  ",others
ADR_042,https://github.com/openmrs/openmrs-rfc-frontend.git,text/0030-link-system.md,Link System,"# Link System - Start Date: 2021/10/21 - RFC PR: https://github.com/openmrs/openmrs-rfc-frontend/pull/30  ## Decision, including impact on distributions  We will extend the extension system to have support specific to links.  We will allow a new property of the `setupOpenMRS` return object, called `links`. It will be an array of objects which contain - `id`: Identifies the link - `label`: The text that should appear on the link - `to` (optional): The target path or URL. Can include `${variables}` which will be interpolated from `openmrsBase`, `openmrsSpaBase`, or the extension props - `onClick` (optional): A click handler. Receives extension props as its second argument - `menu` or `menus` (optional): Names of menus to attach the link to - `offline`, `online`, and `meta` (optional): As in extensions.  ```ts function setupOpenMRS() {   return {     pages: [],     extensions: [],     links: [{       id: ""Patient list link"",        label: () => t(""patientList"", ""Patient List""),       to: ""${openmrsSpaBase}/patient-list"",       onClick: (e) => { console.log(""Clicked button "" + e.button); }       menu: 'App menu',     }]   } } ```  We will define a new React component, `Menu` (along with framework-independent functions that would support it). `Menu` would work exactly like `ExtensionSlot`, but only for Links.  ```ts function Menu(props: { name: string, state?: object }); ```  We will extend the configuration system so that every Menu supports the same configuration options as ExtensionSlots.  The `configure` value will support only the key `openInNewTab`. `openInNewTab` will default to `true` for external links, `false` otherwise. When `true`, it will add `target=""_blank""` to the link. If the link is an external link, it will also add `rel=""noopener noreferrer""`.  Menu configuration will also support one additional key, `links`, which would allow an implementer to add arbitrary links to the Menu:  ```json {   ""@openmrs/esm-primary-navigation-app"": {     ""menus"": {       ""App menu"": {         ""add"": [""Patient list link""],         ""remove"": [""Provider management link""],         ""order"": [""Home page link"", ""Patient list link""],         ""links"": [{           ""label"": ""Home"",           ""to"": ""${openmrsSpaBase}/home""         }],         ""configure"": {           ""Patient list link"": {             ""openInNewTab"": true           }         }       }     }   } } ```  For links created using `links`, the link label is the ID.  ## Reason for decision  - We have a proliferation of trivial ""link"" extensions, which all share   approximately the same simple structure. - Link styling should be left up to the slot/menu. This enforces that   links are not bringing their own CSS classes. - We already have an extension system wiring things together, so this   won't add much complexity to it. - All menu-like UI elements should be configurable. At present, developers   add configurability to menus in various ad-hoc ways, or they leave the   menu unconfigurable.  ## Alternatives  - Continue making link extensions and solving the configurability problem   each time. - To solve the configurability problem, you could create a feature in   a module (or a new module) where you can create new   link-like extensions via the config schema. In the config, give it a   label, a target, and a slot (or slots) to attach to, and it will create   the link as an extension and attach it to that slot. This solves the   configurability problem for menu-like slots. It does not, however,   create a common abstraction for defining link-like extensions in code.   It also would be defining new extensions based on the config, which   would create interdependency between config and extensions in the load   process. This may have consequences for performance. - To create a common abstraction for defining link-like extensions in code,   you could simply define a generic link-like extension generator in   esm-framework, which produces the expected type of extension. - Another solution to solve the configurability problem would be to create   some abstraction for use at the slot level, which both generates part of a   config schema, and wraps the slot. This would probably not be very elegant,   and config schema generation is a kind of abstraction and complexity that   we have not yet broached.  ## Common practices (not enforced)  In the patient chart and the offline tools, we create pages and links together as extensions. The link is the extension component, and the page is created using the extension meta. In this paradigm, those extensions would use the `createLinkExtension` function to create their link components, and would otherwise be unaffected. ",others
ADR_043,https://github.com/wangyyovo/oklog.git,doc/arch/adr-001-multitenancy.md,ADR 1: Multitenancy,"# ADR 1: Multitenancy    We propose to add features OK Log to make it suitable for multi-tenant  environments.    ## Context    System operators often run shared infrastructure for different workloads. For  example, a single Kubernetes cluster may serve multiple departments in an  organization, none of whom should need to know about the others.    OK Log as it exists today (v0.3.2) assumes all users (both producers and  consumers) are part of the same global namespace, and provides no way to  segment ingestion or querying.    We have at least one interesting use case, with one potential user, where  multi-tenanancy would be a requirement.    ## Decision    Motivated by this new use case, we judge that adding multi-tenant features, in  addition to a handful of other longstanding feature requests, would push OK Log  in a useful direction.    The initial set of issues include:    - [Add first-class concept of topics](https://github.com/oklog/oklog/issues/113)  - Separate indexer layer for faster queries  - [Move to length-delimited records](https://github.com/oklog/oklog/issues/112)  - [Extend record identifier](https://github.com/oklog/oklog/issues/114)  - [Long-term storage](https://github.com/oklog/oklog/issues/115)    Additional issues may be filed, and these issues may be refactored or dropped  outright depending on the result of experimentation.    ## Status    April 3, 2018: Accepted.    ## Consequences    We hope adding these features will make OK Log more broadly useful, even in  non-multi-tenant environments. However, some features will likely introduce  incompatibility between older and newer versions of OK Log, making in-place  upgrades difficult or impossible. We will mark any such changes with a major  version bump.  ",microservices_and_modularity
ADR_044,https://github.com/guardian/dotcom-rendering.git,dotcom-rendering/docs/architecture/004-emotion.md,Emotion,"# Emotion  ## Context  Using a CSS-in-JS library will allow us to scope styles to a component. Popular CSS-in-JS libraries include Styled Components, Emotion and Styletron.  Styletron generates atomic CSS, guaranteeing the smallest possible CSS output. However, it does not support language features that cannot be atomised, such as nesting and sibling selectors.  Styled Components and Emotion support more complex features and have a similar API (composing styled components using the `styled()` method). Emotion additionally supports adding styles to the `css` prop on React components. It is also smaller (5.66KB Gzipped vs 16.27KB for styled-components).  ## Decision  We will use Emotion as our CSS-in-JS library.  ## Status  Approved ",technology_choice
ADR_045,https://github.com/jmoratilla/devops-challenge.git,doc/adr/0010-feat-ci-cd-with-circleci.md,10. feat-ci-cd-with-circleci,"# 10. feat-ci-cd-with-circleci  # 8. feat-about-cicd  Date: 2020-02-20  ## Status  Draft  ## Context  Time to work on the CI/CD solution.  I need a job manager to build, test and deploy the apps to the kubernetes  cluster.  I know there is a lot of documentation about jenkins, but I have been working in the last years with other solutions like:  * SolanoCI (now closed) * CircleCI  So I need to see if there is a way to use my knowledge in CircleCI or not.   Besides, CircleCI has a free plan very useful for testing.  I know there is a jenkins-x product, but I don't have a clear idea about  it.  I installed it and spent couple of hours to make it work (jx) but I got  an error downloading kops, and I couldn't go further.    ## Decision  Try first with CircleCI to see if I can deploy the apps to kubernetes.  As I'm using a monorepo, all the apps are within the same repository, so we  cannot separate the building process of each microservice.  All them will be  build and deployed as one.  But if nothing has change in the app, then the building process will be faster.  Testing will be peformed on all apps secuentially, and some tests could be  performed on all services without implementing mockups.  This way, when a event is received by the CI, a script will be executed to   execute the actions on each app.     ## Consequences  ","infrastructure_and_deployment, technology_choice"
ADR_046,https://github.com/jdanil/skunkworks.git,docs/decision-log/fonts.md,Fonts,"# Fonts  ## Decision  Prefer system fonts, otherwise use variable fonts.  ```css @font-face {   font-display: fallback; } ```  ## Rationale  ## Resources  - [Controlling Font Performance with Font Display](https://developers.google.com/web/updates/2016/02/font-display) ",others
ADR_047,https://github.com/open-apparel-registry/open-apparel-registry.git,doc/arch/adr-002-decide-how-to-display-more-facilities.md,Determine How to Display All Facilities on the Map,"# Determine How to Display All Facilities on the Map  ## Context  The Open Apparel Registry currently includes more than 18,000 facilities. For performance reasons, we have paginated the facilities data API endpoint data so that it will [return a maximum of 500 results][pagination-pr] for any single request. In turn this means that the frontend client will only ever display a maximum of 500 facilities at a time, rendered as clustered Leaflet markers via React-Leaflet. Facilities API requests are currently filtered using Django querysets whose inputs are querystring parameters included in the API requests.  To enable users to view all of the OAR's facilities on the map simultaneously, we'll need to update how the API returns facilities for display and how the client renders them on the map. At present this means updating the application so that it can display 18,000+ facilities simultaneously. Following upcoming MSI integration work, we anticipate that the number of OAR facilities will increase to around 100,000 -- which the application should be able to map. In addition, we also want users to be able to filter these vector tiles by query parameters like contributor, facility name, and country, along with the map bounding box.  To accomplish this we have decided to use vector tiles generated, ultimately, by PostGIS's [`ST_AsMVT`][st-asmvt] function, rendering them in the frontend with [Leaflet Vector Grid][leaflet-vector-grid] (possibly via [react-leaflet-vector-grid][react-leaflet-vector-grid]). We've decided to have the vector tiles cluster facilities by zoom level, which would limit the number of actual points the frontend needs to display at any given time.  This ADR documents a subsequent decision between setting up a dedicated `ST_AsMVT`-based vector tile server, like [Martin][martin] or adding a new vector tile endpoint to the existing Django web application which would make the `ST_AsMVT` query.  ## Four Rejected Options  Before landing on an `ST_AsMVT`-based solution, we did consider a few other options which we ultimately rejected outright:  ### Reusing Existing /facilities API Endpoint  In theory we could remove the `MAX_PAGE_SIZE` limit on the `/facilities` API endpoint. In practice this would cause performance problems as the size of the GeoJSON response -- and the number of Leaflet markers -- increased. The web app would have to serialize tens of thousands of markers for each request, the GeoJSON payload for each request could be several megabytes in size, and the client would have to put tens of thousands of Leaflet markers in browser memory.  ### Using Windshaft  While we could potentially use a combination of [Windshaft][windshaft] and [Leaflet.utfgrid][leaflet-utfgrid] to render facilities, there wasn't much enthusiasm for setting up and maintaining a Windshaft tiler for a few specific reasons:  - using Windshaft requires adding another service - Windshaft is pretty costly to configure and maintain - Windshaft's documentation isn't great  ### Creating Static Vector Tiles  We ruled out the idea of creating a static set of vector tiles because the OAR's facilities data changes frequently.  ### Using a Lambda Function Tiler  Azavea has undertaken some research work to determine the viability of using a tiler based on a Lambda function which can connect to PostGIS and call `ST_AsMVT`. However, the research has discovered a few limits on this approach, such as dealing with function warmup times and handling concurrent database connections.  ## Two `ST_AsMVT`-Based Approaches  An `ST_AsMVT`-based approach to generate vector tiles dynamically seemed to be promising. While the vector tiles working group's report did note some uncertainty around how performant it would be to generate tiles in PostGIS, OAR's traffic is such that it may not encounter performance problems which could emerge for a higher traffic site.  We considered two ways to generate tiles using `ST_AsMVT`:  - using a dedicated vector tile server like [Martin][martin], [t-rex][trex], or [tegola][tegola] - adding a vector tiles endpoint to the existing Django web app  ### Using Martin (or t-rex or Tegola) as a Vector Tile Server  Martin, t-rex, and Tegola are open-source vector tile servers which can connect directly to PostGIS and render vector tiles. Judging by their documentation, each of them appear to be fairly straightforward to configure and operate. Each has a slightly different API.  We considered Martin most seriously as an option in part because it had good documentation around how to write PL/pgSQL functions for requesting tiles with data filtered by a set of query parameters. Here's [Martin's function sources example][martin-function-sources]:  ```plpgsql CREATE OR REPLACE FUNCTION public.function_source(z integer, x integer, y integer, query_params json) RETURNS BYTEA AS $$ DECLARE   bounds GEOMETRY(POLYGON, 3857) := TileBBox(z, x, y, 3857);   mvt BYTEA; BEGIN   SELECT INTO mvt ST_AsMVT(tile, 'public.function_source', 4096, 'geom') FROM (     SELECT       ST_AsMVTGeom(geom, bounds, 4096, 64, true) AS geom     FROM public.table_source     WHERE geom && bounds   ) as tile WHERE geom IS NOT NULL;    RETURN mvt; END $$ LANGUAGE plpgsql IMMUTABLE STRICT PARALLEL SAFE; ```  #### Pros  ##### Configuration  Martin appears fairly straightforward to configure and its documentation encompassed most of what we'd want to do.  ##### Performance  Martin touts being ""suitable for large databases"" which indicates that it might obviate some of the performance concerns around using `ST_AsMVT`.  #### Cons  ##### PL/pgSQL Function Sources  Since Martin uses PL/pgSQL functions for its filtering, we would have to rewrite some facility filtering logic that currently exists in Django querysets in the web app to work in PL/pgSQL. Moreover, each time we added a new filter or search option to the web application, we'd have to write a version of the same query in PL/pgSQL for the tiler.  ##### Security  Martin's `query_params` appear to be passed in to the database as strings, which opens a security hole. While we could create a PostGIS role or user with a limited, readonly set of permissions to use solely for the Martin instance, doing so requires taking on some additional risk and complexity.  Likewise, adding PostGIS-based security just for the tiler may also compel us to have to figure out how to duplicate features like API key authentication or facilities-data request logging -- which we've already written once in Django.  ##### Unfamiliarity  We don't have any experience running Martin in production. We've also got limited experience using Rust, the language in which Martin is written. Together this means a Martin-based tile server may be difficult to operate and debug.  ### Adding a Vector Tile Endpoint to the Existing Django Web App  Adding a vector tile endpoint to the existing web app seemed like a promising approach, since it would enable trying out `ST_AsMVT` while reusing the app's database connection and Django's querysets for filtering. In this approach we would add a `/tile/{layer}/{z}/{x}/{y}/` endpoint to the Django application, then update the client to make tile requests there rather than rendering the `/facilities` GeoJSON response as Leaflet markers.  #### Pros  ##### Provides Access to the Existing Django Queryset Apparatus  While using Martin (or a similar solution) would compel writing new versions of the facilities queries in PL/pgSQL, placing a vector tile endpoint in Django lets us reuse some of the existing query code and also provides access to Django models and querysets. Likewise, we would not have to write new code for new filter and search options in two different languages.  ##### Already Has a Secure Database Connection  The Django web application already has a secure database connection, so we would not have to create a solution for securing Martin or another PostGIS-backed tile server.  ##### Enables Switching from `ST_AsMVT` to Another Python Vector Tile Option  There remains some question about the viability of using `ST_AsMVT`. If it turns out that this is not a performant solution, having the tile endpoint in Django makes it possible to drop out of using `ST_AsMVT` altogether and to switch to an alternate Python library for generating vector tiles.  ##### Doesn't Require Creating & Deploying a Different Service  Adding Martin or another vector tile server would increase the number of different kinds of services running as part of the OAR, which adds to the application's complexity. Keeping the tile endpoint in Django does not require adding a new service.  ##### Allows Scaling by Increasing the Number of App Instances  Adding a tile endpoint to the Django app also enables continuing to scale the application in the usual way: by increasing the number of app instances available to serve requests.  #### Cons  ##### Mingles Tile Request Traffic with Other App Traffic  The biggest downside of adding a vector tile endpoint to the Django app is that it would mean mingling tile request traffic with other app traffic. While we plan to limit the size of tile request responses by clusting facilities at different zoom levels, tile request traffic will likely be more frequent and sustained than requests to the current `/facilities` endpoint.  ## Decision  We have decided to add a vector tile endpoint to the existing Django app.  While Martin, in particular, seemed like a compelling solution, we had enough open questions about it to discourage us from taking on the complexity of using it here.  Our main apprehension about adding a tile endpoint to the existing web app is that it'll mingle tile requests with other requests in a way that could cause performance problems. However, given the size of the OAR's traffic and the possibility of addressing traffic increases by scaling the number of app instances, this seemed like an acceptable tradeoff.  ## Consequences  As a consequence of this decision, we will need to:  - add a new `/tile` endpoint to the API. - determine an aggregation strategy for clustering facilities at different zoom levels - adjust the Leaflet map to use this tile endpoint and determine symbology - make necessary adjustments to the frontend for sending selected filters and searches to the `/tile` endpoint. - determine whether to adjust the Gunicorn configuration to change the number of workers per instance or the worker type - decide whether to add caching HTTP headers to `/tile` and to replicate any changes in CloudFront  [pagination-pr]: https://github.com/open-apparel-registry/open-apparel-registry/pull/509 [st-asmvt]: https://postgis.net/docs/ST_AsMVT.html [leaflet-vector-grid]: https://github.com/Leaflet/Leaflet.VectorGrid [react-leaflet-vector-grid]: https://github.com/mhasbie/react-leaflet-vectorgrid [windshaft]: https://github.com/CartoDB/Windshaft [leaflet-utfgrid]: https://github.com/danzel/Leaflet.utfgrid [trex]: https://github.com/t-rex-tileserver/t-rex [tegola]: https://tegola.io/ [martin]: https://github.com/urbica/martin [martin-function-sources]: https://github.com/urbica/martin#function-sources ",api_and_contracts
ADR_048,https://github.com/joejag/wikiindex.git,doc/arch/adr-003-testing_library.md,Testing library,"# Testing library  ## Context  * We want to write programmer tests to support a TDD workflow. * We want to be able to mock out functions.  ## Decision  * We will use Midje to test our code. * Despite it's heavy macro design, it allows you to write expressive code and easily makes mocks  ## Alternatives Considered  * The inbuilt (deftest). We felt it was less expressive than Midje. ","testing_strategy, technology_choice"
ADR_049,https://github.com/ministryofjustice/opg-digideps.git,docs/architecture/decisions/0002-use-amazon-aurora-for-application-database.md,2. Use Amazon Aurora Serverless for ephemeral environments,"# 2. Use Amazon Aurora Serverless for ephemeral environments  Date: 2020-02-14  ## Status  Accepted  ## Context  DigiDeps uses a Postgres database to store persistent information. Since the project was first set up, we have been using an Amazon RDS hosted instance of Postgres for this. However, this hosting option lacks scalability so we have to pay for a full database host 24/7 for each environment.  We have several environments which do not need to be highly available. This includes ""ephemeral"" development environments, the ""main"" environment used in CI, and the training environment. We do not need to run a database for these environments outside of working hours, and often inside of them too.  ## Decision  We will use Amazon Aurora Serverless for environments which do not need to always be on. Aurora automatically scales with usage, including pausing completely if the database isn't in use.  ## Consequences  We will need to stop running regular healthchecks in these environments, since this prevents the database from pausing.  Our database infrastructure will vary between accounts, meaning we cannot be certain that code which worked in development will work in production. Smoke tests in preproduction will indicate any infrastructure failures before release to production.  We will consider upgrading our production database to a matching Postgres version (10.7) and/or hosting it with Provisioned Aurora to further align in the future.  As Aurora identifies itself as Postgres, no application changes are needed to support this. ","data_persistence, technology_choice"
ADR_050,https://github.com/openkfw/TruBudget.git,docs/developer/architecture/0005-workflowitem-ordering.md,(sem título),"--- sidebar_position: 5 --- # Workflowitem-ordering  Date: 04/05/2018  ## Status  Draft  ## Context  Workflowitems are sorted by their creation time by default, but there needs to be some mechanism that allows for manual sorting as well (mainly relevant for the UI). Previously, each workflowitem would hold a pointer to the previous item in the list. However, this approach cannot prevent an inconsistent state if there is a data race between two concurrent requests: it may happen that two workflowitems share the same pointer (turning the list into a tree).  ## Decision  We solve this by maintaining the ordering as a list, stored with the subproject the workflowitems belong to:  ```plain subproject stream:   stream item ""workflowitem_ordering"" => { data: [id1, id2, ...], log: [], permissions: {}} ```  Note that we use the resource structure here simply to be able to treat the record like any other, but `log` and `permissions` have no meaning at the time of writing.  Since Multichain doesn't offer transactions for stream operations, we cannot guarantee that a newly created workflowitem would always be recorded in the list, so we apply the following trick when computing the ordering:  - Workflowitems that are included in the workflowitem-ordering are included in the result exactly in that ordering; - all remaining workflowitems are sorted by their creation time and appended to the result.  Because workflowitems are sorted by their creation time by default, newly created items _do not_ have to be added to the ordering, so no inconsistencies can occur.  ## Consequences  Using this approach, we get the following properties:  - Without setting an ordering through an API call, the ordering list is empty and all items are sorted by their creation time. - When an ordering is set, it is respected when returning workflowitems. - The case of concurrent requests:   - Concurrent creation causes both items to be appended to the list, ordered by their creation time, or in arbitrary order in case the creation times are equal.   - Concurrent updates of the ordering is a race with all-or-nothing semantics: whoever finished the update last wins, and there can never be any inconsistencies. - If an update to the ordering does not include a workflowitem that was not present when the request was issued, when returning the ordered list of workflowitems, the missing workflowitem is simply set as the last element (which makes sense: it is the newest workflowitem, after all). ",others
ADR_051,https://github.com/buildit/bookit-api.git,docs/architecture/decisions/0009-use-jpa.md,9. Use JPA,"# 9. Use JPA  Date: 2017-12-27  ## Status  Accepted  Amends [8. Database](0008-database.md)  ## Context  Originally, we utilized a Spring's JdbcTemplate to quickly CRUD our entities against the data store.  While this was quick and easy, we did most of our filtering in the application as opposed to SQL WHERE clauses.  As we continued, each addition to our entities required a lot of boilerplate code.    Spring has great JPA support and Boot uses Hibernate out of the box.  Our entity models are still relatively simple, but using JPA reduces a lot of the boilerplate code, and opens up a lot of additional features ""for free.""  Specifically, we can utilize JPA to manage our schema updates (naively, later if we need something more robust we can look to Liquibase or Flyway).  It also simplifies joins, where clauses, and gives us more database independence.  ## Decision  * Use JPA to map objects into database tables     * Use Hibernate as the JPA implementation - Spring Boot's default * Leverage Spring Data's JPA support to implement queries via Repository interface patterns  ## Consequences  * While the tests will ensure the code is correct, we will need to ensure the code performs appropriately and does not encounter N+1 or combinatorial query explosion. * Will still need to figure out the best approach to support our local datetime overlapping interval logic in WHERE clauses * Consider utilizing Spring Data's RestRepository support to expose repositories directly (very fast time to market, should have started with this) ","technology_choice, data_persistence"
ADR_052,https://github.com/crypto-com/thaler.git,architecture-docs/adr-001.md,ADR 001: Storage Consolidation,"# ADR 001: Storage Consolidation  ## Changelog * 10-12-2019: Initial Draft * 11-12-2019: Extra comments * 03-01-2020: Refinement of the decision based on early experiments + discussion on Slack  ## Context Currently (not counting Tendermint's internal storage or wallets), two processes maintain their internal storage:  * chain-abci: stores the node state, Merkle trie of staking states, transaction metadata (whether spent or not), validator tracking, etc. * tx-validation enclave (TVE): sealed transaction data (of valid obfuscated transactions that have outputs)  The reason for having two processes is that SGX SDK compilation is different and needs Intel SGX SDK tooling (and the subsequent process execution requires Intel SGX PSW tooling, such as AESM service), so for the development convenience, the transaction validation code that needs to execute in an enclave is isolated. (For example, one can build and run chain-abci on any platform (e.g. macOS),  and run the enclave parts inside a docker container or on a remote Linux host.) The inter-process communication is over a simple REQ-REP 0MQ socket.  *Problem 1: These two storage locations need to be ""in sync""*:  when an obfuscated transaction arrives that spends some transaction outputs, chain-abci will do a basic validation and check if they are unspent and forward it to TVE (assuming its storage contains sealed transaction data of respective outputs). There is currently a naive check that TVE stores the latest app hash provided by chain-abci; and upon a startup, chain-abci cross-checks if TVE is in sync with it. This leads to various errors and annoyances that are usually resolved by removing all storage and syncing from scratch (in certain cases, there may be a better mechanism, but wasn't implemented).  *Problem 2: Transaction querying*:  As wallet / client-* may be lightweight client and not have access to TEE directly, it will connect to one remotely. For this purpose, there is transaction query enclave (TQE). See [this document](https://github.com/crypto-com/thaler-docs/blob/master/plan.md#transaction-query-enclave-tqe-optional--for-client-infrastructure) for more details.  There are two flows (over an attested secure channel):  1. retrieving transactions: client submits transaction identifiers signed by its view key, and TQE replies with matching transaction data. For this workflow, TQE contacts TVE over REQ-REP 0MQ socket to retrieve data.  2. submitting new transactions: client submits a new transaction, TQE forwards it to TVE that checks it (so that it doesn't obfuscate random / invalid data) and if it's valid, it encrypts it with the obfuscation key (currently compile-time mock, but planned to be periodically regenerated by another type of enclave) and returns the obfuscated transaction to TQE that forwards it to the client.  In the first flow, TQE only talks to the TVE's application wrapper that handles the persistence -- it can unseal the transaction data, because the key policy is MRSIGNER.   In the second flow, TVE holds the obfuscation key inside the enclave memory, so the payload goes to TVE. Currently, TVE cannot check everything, e.g. staked state or if a transaction output was spent or not -- in the future, it may internally have app hash components and at least require some lightweight proofs for these things.  For the first flow, it's unnecessary for TQE to talk to TVE. For the second flow, it'll be desirable to do a more complete verification (currently there are a few hacks and workarounds).  ## Decision This will be a bit big change, so it can be done in several steps:  * separate out the storage functionality from chain-abci into chain-storage crate https://github.com/crypto-com/chain/issues/753   * This should decouple the existing storage functionality from state machine logic   * The result should be more high-level APIs that can then be used or extended by the embedded tx-validation enclave app wrapper     (getSealedTx, insertSealedTx...) to encapsulate some of the low-level storage choices  * embed tx-validation enclave app wrapper in chain-abci Based on early experiments in: https://github.com/crypto-com/chain/pull/738 It will include several changes (in several sub-PRs):   * make more general SGX library loader (currently, it's been fixed to enclave.signed.so)   * extract out enclave-only tx-validation core functionality into a separate crate that would allow *optional* mock version (replacing the enclave-bridge)   * build process modifications: consolidating common functionality, making chain-abci's build process to expose URTS to tx-validation enclave   * chain-abci starting up tx-validation's zMQ server for the sole purpose of preserving current tx-query workflows (tx-query changes are out of scope of this ADR, e.g. having ""insecure"" test-only connection)   * replace the ""embedded"" tx-validation's sled storage with chain-storage (*addressing Problem 1*):     * store sealed transaction payloads in `COL_BODIES` (or a dedicated column if desired)     * for serving tx-query requests, just use chain-abci's latest committed state (for last block's time etc.)   * remove the redundant enclave-protocol variants of zMQ inter-communication message payloads (unused by tx-query):     * CheckChain: no need for the latest app hash checking (only one storage), can do the sanity check (the mainnet/testnet *.signed.so will be different) with direct call     * VerifyTx: can call directly (IntraEnclave)     * EndBlock: can call directly      * CommitBlock: no need (both information stored / handled during normal chain-abci execution)   * modifications in tx-query protocol and client to allow complete verification verification of encryption requests (*addressing Problem 2*):     * `EncryptionRequest::WithdrawStake` doesn't need to include the StakedState information:       * tx-query will obtain the address from the signature payload and pass that to chain-abci / embedded tx-validation       * chain-abci (before calling the tx-validation ecall) should look up the staked state (if it doesn't exist, it'll return an error)   * move `SGX_TEST` tx-validation's SGX unit test (unfortunately the normal Rust unit tests don't work in Apache SGX SDK) under an optional flag in chain-abci   * along the way, update documentation, integration tests and integration test environment set ups    ## Status  Accepted  ## Consequences  ### Positive  * Only one place to store transaction data -- no need to keep storage of two processes in sync * Decoupling state machine logic (chain-abci) from the storage * Full validation of (existing) TQE requests * As TQE (and other non-yet-implemented enclave logic) evolves, it'll be beneficial to have one canonical Chain storage place (chain-storage / chain-abci) which TQE etc. can rely on  ### Negative * More complex chain-abci building process   ### Neutral * A few more crates (storage crate + some of the enclave functionality may be extracted and abstracted out into existing or new crates) * Coupling TQE process to chain-abci * Storage space shared between chain-abci and ""sub-abci"" enclave applications: perhaps an extra column in RocksDB-like storage * Depending on how the tx-validation embedding into chain-abci and mocking is done, it may be a source of ""silent"" enclave only errors a developer would discover only after the full integration test, or worse at longer runs in SGX env (as it's not every edge case is covered by integration test and there's no fuzzing yet) * many documentation and script changes (as tx-validation wrapper app has been there for quite some time)  ## References  * moving app wrapers to chain-abci: https://github.com/crypto-com/chain/pull/665#discussion_r356377869 * https://github.com/libra/libra/tree/master/storage/storage-service * more discussion pointing out other concerns in tx-query: https://github.com/crypto-com/chain/pull/741 * early embedding tx-validation into chain-abci experiment: https://github.com/crypto-com/chain/pull/738 ","data_persistence, microservices_and_modularity"
ADR_053,https://github.com/Tripwire/octagon.git,doc/architecture/decisions/0003-we-test-via-visual-screenshots.md,3. we test via visual screenshots,"# 3. we test via visual screenshots  date: 2018-01-25  ## status  accepted  ## context  how to test UI code is a long, complicated, and contested topic.  unit test code is welcome in this project, but outside the scope of this decision.  - visual changes are subtle & difficult to detect - small CSS changes can have large, cascading effects across a web app  due to high risks in subtle changes, we test our components visually, and track changes.  ## decision  - we shall take screenhots of components we develop and components we import - we shall check in our screenshots into change control - we shall test screenshot changes onchange - we shall capture screenshots out of real browser engines, not mock browsers  completeness of component coverage is not defined in this decision.  tooling used to realize this decision is not defined in this decision.  ## consequences  - cost of ownership is high   - tooling is generally complicated   - tests are often slow   - browser coverage may be limited  ",testing_strategy
ADR_054,https://github.com/nationalarchives/tdr-dev-documentation.git,architecture-decision-records/0001-multi-account-terraform.md,1. Use Terraform across multiple AWS accounts,"# 1. Use Terraform across multiple AWS accounts  **Date:** 2020-02-20  The actual decision was made earlier than this, between December 2019 and January 2020.  ## Context  We have created multiple AWS accounts to host the different TDR environments, and we're happy with the decision we made during the Alpha to use Terraform to configure the infrastructure (see [terraform.md][alpha-terraform] and [terraform_vs_cloudformation.md][tf-vs-cf]).  The Alpha only ran in one AWS account. We used Terraform workspaces so that we could create multiple environments if we wanted, but in the end we only used one environment for prototyping and user testing.  In Alpha, we ran Terraform scripts from our dev machines. For Beta, we'd prefer to run Terraform in a controlled environment like Jenkins. This has a few advantages:  - Makes it easier to control access to production infrastructure: a developer   can have permission to run Jenkins jobs (and therefore make   version-controlled, peer-reviewed changes to production) without having   credentials giving them full access to the production account - Makes it easier to deploy changes to each environment in turn - Provides a record of (recent) Terraform runs, and who started them  But there's a chicken-and-egg problem with running Terraform from a hosted system like Jenkins, because it would be helpful if we could use Terraform to create the Jenkins infrastructure itself.  [alpha-terraform]: ../technology-considerations/terraform.md [tf-vs-cf]: ../technology-considerations/terraform_vs_cloudformation.md  ## Decision  Create multiple Terraform projects, which are run in different stages:  - A backend project which is run against the management account and bootstraps   the rest of the infrastructure. It is run from a development machine and   creates the IAM roles and Terraform state storage (S3 and DynamoDB) that will   be used by the environment-specific Terraform scripts. This project's own   Terraform state is stored in a manually-created S3 bucket and DynamoDB table. - A CI project which is run against the management account when the backend   bootstrap script has been run. It is also run from a development machine, but   the Terraform state is saved in the storage set up by the bootstrap script. - An environment project which is run against each TDR environment account   (integration, staging, production). This script is run from Jenkins.  Since the environment-specific Terraform is run from Jenkins, which is in AWS, we can use IAM roles to give Jenkins permission to run Terraform. This means that we don't need to generate an AWS secret key (which could be used from outside our environment if it was stolen) for Jenkins to run Terraform. ","technology_choice, infrastructure_and_deployment"
ADR_055,https://github.com/mateuszmidor/ArchStudy.git,ADR/decisions/0003-stop-recording-architecture-decisions.md,3. Stop recording architecture decisions,"# 3. Stop recording architecture decisions  Date: 2021-01-16  ## Status  Accepted  Supercedes [1. Record architecture decisions](0001-record-architecture-decisions.md)  ## Context  The issue motivating this decision, and any context that influences or constrains the decision.  ## Decision  The change that we're proposing or have agreed to implement.  ## Consequences  What becomes easier or more difficult to do and any risks introduced by the change that will need to be mitigated. ",governance_and_process
ADR_056,https://github.com/archivematica/archivematica-architectural-decisions.git,0000-use-markdown-architectural-decision-records.md,(sem título),"--- layout: page status: accepted adr: ""0000"" title: Use Markdown Architectural Decision Records deciders: date: ---  ## Context and problem statement  We want to record architectural decisions made in this project. Which format and structure should these records follow?  ## Considered options  * [MADR][0] 2.1.2 – The Markdown Architectural Decision Records * [Michael Nygard's template][1] – The first incarnation of the term ""ADR"" * [Sustainable Architectural Decisions][2] – The Y-Statements * Other templates listed at @joelparkerhenderson's [repository][3]. * Formless – No conventions for file format and structure  ## Decision outcome  Chosen option: ""MADR 2.1.2"", because  * Implicit assumptions should be made explicit.   Design documentation is important to enable people understanding the decisions   later on. See also [A rational design process: How and why to fake it][4]. * The MADR format is lean and fits our development style. * The MADR structure is comprehensible and facilitates usage & maintenance. * The MADR project is vivid. * Version 2.1.2 is the latest one available when starting to document ADRs.  [0]: https://adr.github.io/madr/ [1]: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions [2]: https://www.infoq.com/articles/sustainable-architectural-design-decisions [3]: https://github.com/joelparkerhenderson/architecture_decision_record [4]: https://doi.org/10.1109/TSE.1986.6312940 ",governance_and_process
ADR_057,https://github.com/dxw/dalmatian-frontend.git,doc/architecture/decisions/0005-use-brakeman-for-security-analysis.md,5. use-brakeman-for-security-analysis,# 5. use-brakeman-for-security-analysis  Date: 2020-04-03  ## Status  Accepted  ## Context  We need a mechanism for highlighting security vulnerabilities in our code before it reaches production environments  ## Decision  Use the [Brakeman](https://brakemanscanner.org/) static security analysis tool to find vulnerabilities in development and test  ## Consequences  - Brakeman will be run as part of CI and fail the build if there are any warnings - Brakeman can also be run in the development environment to allow developers to address issues before committing code to the repository - Brakeman will help developers learn about common vulnerabilities and develop a more defensive coding style - Use of Brakeman in development & test environments should reduce or eliminate code vulnerabilities that would be exposed in a penetration test,"security, technology_choice"
ADR_058,https://github.com/mozilla/fxa.git,docs/adr/0001-isolating-payment-content-with-third-party-widgets-from-general-account-management.md,Isolating payment content with third-party widgets from general account management,"# Isolating payment content with third-party widgets from general account management  * Deciders: Ben Bangert, Ian Bicking, Wil Clouser, Les Orchard, Shane Tomlinson * Date: 2019-05-06  ## Context and Problem Statement  In the implementation of payment features for subscription services, we've decided to use third-party JavaScript for payment widgets.  Best practices established by our security team indicate that third-party JS should not be included on the highly-sensitive pages - i.e. such as those used for general account management on Firefox Accounts.  So, we need a way to isolate the pages responsible for subscription sign-up and management from the rest of Firefox Accounts.  ## Decision Drivers  * Security when dealing with financial transactions. * Security when including with third-party JS code for payment widgets. * Simplicity in user experience flows. * Delivering against the subscription services deadline.  ## Considered Options  * Option A - Payment pages as separate app supplied with pre-generated access   token * Option B - Payment pages as separate app and normal OAuth Relying Party * Option C - Payment-related content embedded in iframes with pre-generated   access token passed via postMessage * Option D - Payment pages as normal FxA content using iframes to isolate   third-party widgets  ## Decision Outcome  Chosen option: ""Option A - Payment pages as separate app supplied with pre-generated access token"", because  * Further refinements to access token delivery mechanism in Option A do not   significantly affect the rest of the payments app. * Doesn't preclude an upgrade to Option B in the future - i.e. once [Issue   #640](https://github.com/mozilla/fxa/issues/640) is resolved. * Doesn't preclude Option C as a future option - e.g. offering embedded   subscription widgets to third-parties.  * Fastest practical option given existing record of reviews by security & UX and   work completed so far. * Fresh start with a more modern web stack (i.e. React).  ## Pros and Cons of the Options  ### Option A - Payment pages as separate app supplied with pre-generated access token  * Description   * Payments pages as standalone web app (i.e. payments.firefox.com)   * Access token generated via fxa-content-server (i.e. accounts.firefox.com) to     access fxa-auth-server subscription APIs.   * Access token conveyed to payments pages directly via URL parameter     * Alternatively, access token conveyed indirectly via code exchange or other       more secure mechanism. * Pros   * Can effectively isolate third-party widgets by virtue of living on a     separate origin and receiving a scoped access token.   * Our original plan of record reviewed by security and UX teams - i.e. time     already spent.   * Allows us to build something from scratch using a more modern framework like     React. * Cons   * Building something with React is novel for the project overall.   * We need to stand up yet another server.  ### Option B - Payment pages as separate app and normal OAuth Relying Party  * Description:   * Payments pages as standalone web app (i.e. payments.firefox.com).   * Payments pages as full OAuth Relying Party with the usual login flow     requesting scope to access fxa-auth-server subscription APIs.   * Access token acquired via standard OAuth mechanisms. * Pros   * Can effectively isolate third-party widgets by virtue of living on a     separate origin and receiving a scoped access token.   * Allows us to build something from scratch using a more modern framework like     React. * Cons   * There is [an open issue to support     `?prompt=none`](https://github.com/mozilla/fxa/issues/640) for OpenID     Connect login. Left unresolved, this issue means accessing payment pages can     result in a redundant login prompt in UX flows.   * Building something with React is novel for the project overall.   * We need to stand up yet another server.  ### Option C - Payment-related content embedded in iframes with pre-generated access token passed via postMessage  * Description   * Payments content hosted on separate domain (i.e. payments.firefox.com)     embedded in iframes on fxa-content-server pages (i.e. accounts.firefox.com)   * Access token generated via fxa-content-server (i.e. accounts.firefox.com)   * `iframe.postMessage()` API used to pass access token to payments content * Pros   * This can effectively isolate third-party widgets by virtue of living on a     separate origin and receiving a scoped access token. * Cons   * We need to stand up yet another server - i.e. at least for the separate     origin.   * Security story around iframes on the same origin as user management pages     has not been reviewed - i.e. additional time needed.   * Seems an awkward fit with UX flows drafted so far, for both subscription     management and sign-up.   * Would most likely be more content built using Backbone requiring     modernization later.     * (unless we combined React & Backbone on fxa-content-server, which is not       as clean as a separate app)  ### Option D - Payment pages as normal FxA content using iframes to isolate third-party widgets  * Description   * Payments pages hosted within fxa-content-server     * (i.e. only accounts.firefox.com involved - no payments.firefox.com host       created)   * iframes used to embed and isolate third-party widgets   * Session token used as usual for API authentication, no access token     required. * Pros   * Requires the fewest novel technology choices.   * Can reuse the existing fxa-content-server. * Cons   * Security story around iframes on the same origin as user management pages     has not been reviewed - i.e. additional time needed.   * Would most likely be more content built using Backbone requiring     modernization later.     * (unless we combined React & Backbone on fxa-content-server, which is not       as clean as a separate app)  ## Links  * Shane's earlier ""[Securing the payment page][securing]"" Google Doc.  [securing]: https://docs.google.com/document/d/17NItC2sWtMH4iGfyaxo_WLxWmKAKpfWOw3N14tQY7I8/edit?usp=sharing",api_and_contracts
ADR_059,https://github.com/kgrzybek/modular-monolith-with-ddd.git,docs/architecture-decision-log/0017-implement-archictecture-tests.md,17. Implement Architecture Tests,"# 17. Implement Architecture Tests  Date: 2019-11-16  ## Status  Accepted  ## Context  In some cases it is not possible to enforce the application architecture, design or established conventions using compiler (compile-time). For this reason, code implementations can diverge from the original design and architecture. We want to minimize this behavior, not only by code review.  ## Decision  We decided to implement Unit Tests for our architecture. </br> We will implement tests for each module separately and one tests library for general architecture. We will use _NetArchTest_ library which was created exactly for this purpose.  ## Consequences - We will have quick feedback about breaking the design rules - Unit tests for architecture are documenting our architecture to some level - We will have dependency to external library - We need to implement some _""reflection-based""_ code to check some rules, because library does not provide everything what we need - This kind of tests are a bit slower than normal unit tests (because of reflection) - More tests to maintain","testing_strategy, technology_choice"
ADR_060,https://github.com/serienator/serienator.git,adr/cicd-tool.md,Tool for CI/CD,# Tool for CI/CD  ## Issue  We would like to have a tool for allowing our ci/cd process.   ## Decision  Jenkins,"infrastructure_and_deployment, technology_choice"
ADR_061,https://github.com/apache/james-project.git,src/adr/0043-avoid-elasticsearch-on-critical-reads.md,43. Avoid ElasticSearch on critical reads,"# 43. Avoid ElasticSearch on critical reads  Date: 2020-11-11  ## Status  Accepted (lazy consensus) & implemented  Scope: Distributed James  ## Context  A user willing to use a webmail powered by the JMAP protocol will end up doing the following operations:  - `Mailbox/get` to retrieve the mailboxes. This call is resolved against metadata stored in Cassandra.  - `Email/query` to retrieve the list of emails. This call is nowadays resolved on ElasticSearch for Email search after  a right resolution pass against Cassandra.  - `Email/get` to retrieve various levels of details. Depending on requested properties, this is either  retrieved from Cassandra alone or from ObjectStorage.  So, ElasticSearch is queried on every JMAP interaction for listing emails. Administrators thus need to enforce availability and good performance for this component.  Relying on more services for every read also harms our resiliency as ElasticSearch outages have major impacts.  Also we should mention our ElasticSearch implementation in Distributed James suffers the following flaws:  - Updates of flags lead to updates of the all Email object, leading to sparse segments  - We currently rely on scrolling for JMAP (in order to ensure messageId uniqueness in the response while respecting limit & position)  - We noticed some very slow traces against ElasticSearch, even for simple queries.  Regarding Distributed James data-stores responsibilities:  - Cassandra is the source of truth for metadata, its storage needs to be adapted to known access patterns.  - ElasticSearch allows resolution of arbitrary queries, and performs full text search.  ## Decision  Provide an optional view for most common `Email/query` requests both on Draft and RFC-8621 implementations. This includes filters and sorts on 'sentAt'.  This view will be stored into Cassandra, and updated asynchronously via a MailboxListener.  ## Consequences  A migration task will be provided for new adopters.  Administrators would be offered a configuration option to turn this view on and off as needed.  If enabled, given clients following well defined Email/query requests, administrators would no longer need to ensure high availability and good performances for ElasticSearch to ensure availability of basic usages (mailbox content listing).  Given these pre-requisites, we thus expect a decrease in overall ElasticSearch load, allowing savings compared to actual deployments. Furthermore, we expect better performances by resolving such queries against Cassandra.  The expected added load to Cassandra is low, as the search is a simple Cassandra read. As we only store messageId, Cassandra dataset size will only grow of a few percents if enabled.  ## Alternatives  Those not willing to adopt this view will not be affected. By disabling the listener and the view usage, they will keep resolving all `Email/query` against ElasticSearch.  Another solution is to implement the projecting using a in-memory datagrid such as infinispan. The projection would be computed using a MailboxListener and the data would be first fetched from this cache and fallback to ElasticSearch. We did not choose it as Cassandra is already there, well mastered, as disk storage is cheaper than memory. InfiniSpan would moreover need additional datastore to allow a persistent state. Infinispan on the other hand would be faster and would have less restrictions on data filtering and sorting. Also this would require one more software dependency.  ## Example of optimized JMAP requests  ### A: Email list sorted by sentAt, with limit  RFC-8621:  ``` [""Email/query"",  {    ""accountId"": ""29883977c13473ae7cb7678ef767cbfbaffc8a44a6e463d971d23a65c1dc4af6"",    ""filter: {        ""inMailbox"":""abcd""    }    ""comparator"": [{      ""property"":""sentAt"",      ""isAscending"": false    }],    ""position"": 30,    ""limit"": 30  },  ""c1""] ```  Draft:  ``` [[""getMessageList"", {""filter"":{""inMailboxes"": [""abcd""]}, ""sort"": [""date desc""]}, ""#0""]] ```  ### B: Email list sorted by sentAt, with limit, after a given receivedAt date  RFC-8621:  ``` [""Email/query"",  {    ""accountId"": ""29883977c13473ae7cb7678ef767cbfbaffc8a44a6e463d971d23a65c1dc4af6"",    ""filter: {        ""inMailbox"":""abcd"",        ""after"": ""aDate""    }    ""comparator"": [{      ""property"":""sentAt"",      ""isAscending"": false    }],    ""position"": 30,    ""limit"": 30  },  ""c1""] ```  Draft: Draft do only expose a single date property thus do not differenciate sentAt from receivedAt. Draft adopts sentAt to back the date property up, thus the above request cannot be written using draft syntax.  ### C: Email list sorted by sentAt, with limit, after a given sentAt date  Draft:  ``` [[""getMessageList"", {""filter"":{""after"":""aDate"", ""inMailboxes"": [""abcd""]}, ""sort"": [""date desc""]}, ""#0""]] ```  RFC-8621: There is no filter properties targeting ""sentAt"" thus the above request cannot be written.  ## Cassandra table structure  Several tables are required in order to implement this view on top of Cassandra.  Eventual denormalization consistency can be enforced by using BATCH statements.  A table allows sorting messages of a mailbox by sentAt, allows answering A and C:  ``` TABLE email_query_view_sent_at PRIMARY KEY mailboxId CLUSTERING COLUMN sentAt CLUSTERING COLUMN messageId ORDERED BY sentAt ```  A table allows filtering emails after a receivedAt date. Given a limited number of results, soft sorting and limits can be applied using the sentAt column. This allows answering B:  ``` TABLE email_query_view_sent_at PRIMARY KEY mailboxId CLUSTERING COLUMN receivedAt CLUSTERING COLUMN messageId COLUMN sentAt ORDERED BY receivedAt ```  Finally upon deletes, receivedAt and sentAt should be known. Thus we need to provide a lookup table:  ``` TABLE email_query_view_date_lookup PRIMARY KEY mailboxId CLUSTERING COLUMN messageId COLUMN sentAt COLUMN receivedAt ```  Note that to handle position & limit, we need to fetch `position + limit` ordered items then removing `position` firsts items.  ## References  * [JIRA](https://issues.apache.org/jira/browse/JAMES-3440) * [PR discussing this ADR](https://github.com/apache/james-project/pull/259)",data_persistence
ADR_062,https://github.com/vwt-digital/operational-data-hub.git,architecture/adr/0021-messages-are-in-json-format.md,21. Messages are in JSON format,# 21. Messages are in JSON format  Date: 2020-09-21  ## Status  Accepted  Implements [16. Pub/Sub implements Event sourcing](0016-pub-sub-implements-event-sourcing.md)  Implemented by [19. Single schema per topic](0019-single-schema-per-topic.md)  ## Context  It is preferred to use a single message type for the business events. This makes it easier to handle messages on the pub/sub system in a standerdized way.  ## Decision  All business events on the ODH platform topics are formatted as [JSON](https://tools.ietf.org/html/rfc7159)  ## Consequences  ### Disadvantages  Messages deliverd to the ODH or messages for systems conneted to the ODH might need other message formats. In these cases the messages need to be transformed from one layout to the other.,api_and_contracts
ADR_063,https://github.com/customcommander/project-blueprint.git,adr/0003-test-distributed-files-only.md,3. Test Distributed Files Only,# 3. Test Distributed Files Only  Date: 2021-02-08  ## Status  Accepted  Relates [2. Use Google Closure Compiler](0002-use-google-closure-compiler.md)  ## Context  It is not uncommon to have successful tests against development sources failing against the production bundle. We must make sure that code distributed to the public works as intended and avoid false positive in testing. The advanced compilation mode of the Google Closure Compiler makes this class of errors more likely to happen as it transforms the development sources radically.  ## Decision  Testing will be made against the production bundle to catch compilation errors before they reach our users.  ## Consequences  - Longer testing feedback loop as sources must be recompiled before each test run. - Stronger focus on testing the public interface as internal functions and helpers are out of reach.,testing_strategy
ADR_064,https://github.com/buildit/bookit-api.git,docs/architecture/decisions/0010-jpa-manages-schema.md,10. JPA manages schema,"# 10. JPA manages schema  Date: 2017-12-27  ## Status  Accepted  Amends [8. Database](0008-database.md)  ## Context  Originally, we used Spring Boot's Database Initialization support to automatically create and intialize our database via schema.sql and data.sql scripts.  Each deployment (application initialization) would execute these scripts.  Our implementation would drop the database and recreate it each time.  While this accelerated our development (avoid data migrations), it's not sustainable  ## Decision  * Leverage Hibernate's (our JPA implementation) ddl-auto feature to update the staging/production databases (we will continue to drop/recreate all other databases....local, integration).     * recreating in integration ensures a clean database for each run.  In addition, it validates that we can recreate a database from scratch  ## Consequences  * Hibernate does warn against utilizing this feature ""in production."" * We can ""test"" the update in staging.  There is a risk that Hibernate updates a schema naively which may result in dropped data * This approach won't address how we can add static data (locations & bookables for now) over time. * Hibernate won't drop columns that are no longer used.  Add only.   * Also won't migrate data for you.  Schema only. * If any of the above prove onerous, we can address by introducing a database migration tool such as Liquibase or Flyway. * This did cause us to hit a memory limit (previously set to 256MB).  Had to bump to 512MB. ",data_persistence
ADR_065,https://github.com/vwt-digital/operational-data-hub.git,coding_guidelines/adr/0002-repo-naming-conventions.md,2. repo naming conventions,"# 2. repo naming conventions  Date: 2021-02-01  ## Status  Accepted  ## Context  We feel the need to use a naming convention for github repos.  ## Decision  We identify three kinds of repositories:  ### 1. General rules: * Use hyphens ('-') between words in the name because:     * words written together without something inbetween are unclear.     * ""\_"" is harder to type than ""-""  [stack overflow](ttps://stackoverflow.com/a/11947816). * Make repo names not longer than needed. (Because GCP project names are also limited in length).  ### 2. config VWT DAT repositories Config repositories are repositories containing configurations of a specific Google Cloud Project (GCP) project.  * Should have the same name as the GCP project they are connected to minus the customer, environment and location. * Name ends with `-config`.  ### 3.  solutions VWT DAT repositories Solutions repositories are repositories containing solutions, they can belong to multiple domains. * Their names should always start with the domain they belong to. * If the repository will handle multiple facets of the service, the name should end in `-handlers` * Sometimes, two repositories are connected because they are the frontend and backend of a service. Their names should be the same except for the ending. Frontend repositories should end in `-tool` and backend repositories should end in `-api`.  ### 4. ""normal"" VWT DAT repositories ""Normal"" repositories are repositories not belonging to a solution. They contain code used specifically for the Operational Data Hub (ODH). * Repository naming is equal to naming convention for solution repositories. Domains for these reposiitories is limited to `dat` and `odh`. * If the repository is forked from another repository, its name should contain the name of the repository it forked from.  ## Consequences ",governance_and_process
ADR_066,https://github.com/ASethi93/james.git,src/adr/0031-distributed-mail-queue.md,31. Distributed Mail Queue,"# 31. Distributed Mail Queue  Date: 2020-04-13  ## Status  Accepted (lazy consensus)  ## Context  MailQueue is a central component of SMTP infrastructure allowing asynchronous mail processing. This enables a short  SMTP reply time despite a potentially longer mail processing time. It also works as a buffer during SMTP peak workload to not overload a server.   Furthermore, when used as a Mail Exchange server (MX), the ability to add delays to be observed before dequeing elements allows, among others:   - Delaying retries upon MX delivery failure to a remote site.  - Throttling, which could be helpful for not being considered a spammer.  A mailqueue also enables advanced administration operations like traffic review, discarding emails, resetting wait  delays, purging the queue, etc.  Spring implementation and non distributed implementations rely on an embedded ActiveMQ to implement the MailQueue.  Emails are being stored in a local file system. An administrator wishing to administrate the mailQueue will thus need  to interact with all its James servers, which is not friendly in a distributed setup.  Distributed James relies on the following third party softwares (among other):   - **RabbitMQ** for messaging. Good at holding a queue, however some advanced administrative operations can't be  implemented with this component alone. This is the case for `browse`, `getSize` and `arbitrary mail removal`.  - **Cassandra** is the metadata database. Due to **tombstone** being used for delete, queue is a well known anti-pattern.  - **ObjectStorage** (Swift or S3) holds byte content.  ## Decision  Distributed James should ship a distributed MailQueue composing the following softwares with the following  responsibilities:   - **RabbitMQ** for messaging. A rabbitMQ consumer will trigger dequeue operations.  - A time series projection of the queue content (order by time list of mail metadata) will be maintained in **Cassandra** (see later). Time series avoid the  aforementioned tombstone anti-pattern, and no polling is performed on this projection.  - **ObjectStorage** (Swift or S3) holds large byte content. This avoids overwhelming other softwares which do not scale  as well in term of Input/Output operation per seconds.   Here are details of the tables composing Cassandra MailQueue View data-model:   - **enqueuedMailsV3** holds the time series. The primary key holds the queue name, the (rounded) time of enqueue  designed as a slice, and a bucketCount. Slicing enables listing a large amount of items from a given point in time, in an  fashion that is not achievable with a classic partition approach. The bucketCount enables sharding and avoids all writes  at a given point in time to go to the same Cassandra partition. The clustering key is composed of an enqueueId - a  unique identifier. The content holds the metadata of the email. This table enables, from a starting date, to load all of the emails that have ever been in the mailQueue. Its content is never deleted.  - **deletedMailsV2** tells wether a mail stored in *enqueuedMailsV3* had been deleted or not. The queueName and  enqueueId are used as primary key. This table is updated upon dequeue and deletes. This table is queried upon dequeue  to filter out deleted/purged items.   - **browseStart** store the latest known point in time from which all previous emails had been deleted/dequeued. It  enables to skip most deleted items upon browsing/deleting queue content. Its update is probability based and  asynchronously piggy backed on dequeue.   Here are the main mail operation sequences:   - Upon **enqueue** mail content is stored in the *object storage*, an entry is added in *enqueuedMailsV3* and a message   is fired on *rabbitMQ*.  - **dequeue** is triggered by a rabbitMQ message to be received. *deletedMailsV2* is queried to know if the message had already been deleted. If not, the mail content is retrieved from the *object storage*, then an entry is added in  *deletedMailsV2* to notice the email had been dequeued. A dequeue has a random probability to trigger a browse start update. If so, from current browse start, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2* until the first non deleted / dequeued email is found. This point becomes the new browse start. BrowseStart can never  point after the start of the current slice. A grace period upon browse start update is left to tolerate clock skew. Update of the browse start is done randomly as it is a simple way to avoid synchronisation in a distributed system: we ensure liveness while uneeded browseStart updates being triggered would simply waste a few resources.  - Upon **browse**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*, starting from the  current browse start.  - Upon **delete/purge**, *enqueuedMailsV3* content is iterated, and checked against *deletedMailsV2*. Mails matching  the condition are marked as deleted in *enqueuedMailsV3*.  - Upon **getSize**, we perform a browse and count the returned elements.   The distributed mail queue requires a fine tuned configuration, which mostly depends of the count of Cassandra servers,  and of the mailQueue throughput:  - **sliceWindow** is the time period of a slice. All the elements of **enqueuedMailsV3** sharing the same slice are  retrieved at once. The bigger, the more elements are going to be read at once, the less frequent browse start update  will be. Lower values might result in many almost empty slices to be read, generating higher read load. We recommend  **sliceWindow** to be chosen from users maximum throughput so that approximately 10.000 emails be contained in a slice. Only values dividing the current *sliceWindow* are allowed as new values (otherwize previous slices might not be found).  - **bucketCount** enables spreading the writes in your Cassandra cluster using a bucketting strategy. Low values will  lead to workload not to be spread evenly, higher values might result in uneeded reads upon browse. The count of Cassandra  servers should be a good starting value. Only increasing the count of buckets is supported as a configuration update as decreasing the bucket count might result in some buckets to be lost.  - **updateBrowseStartPace** governs the probability of updating browseStart upon dequeue/deletes. We recommend choosing  a value guarantying a reasonable probability of updating the browse start every few slices. Too big values will lead to uneeded update of not yet finished slices. Too low values will end up in a more expensive browseStart update and browse iterating through slices with all their content deleted. This value can be changed freely.  We rely on eventSourcing to validate the mailQueue configuration changes upon James start following the aforementioned rules.  ## Limitations  Delays are not supported. This mail queue implementation is thus not suited for a Mail Exchange (MX) implementation. The [following proposal](https://issues.apache.org/jira/browse/JAMES-2896) could be a solution to support delays.  **enqueuedMailsV3** and **deletedMailsV2** is never cleaned up and the corresponding blobs are always referenced. This is not ideal both from a privacy and space storage costs point of view.  **getSize** operation is sub-optimal and thus not efficient. Combined with metric reporting of mail queue size being  periodically performed by all James servers this can lead, upon increasing throughput to a Cassandra overload. A configuration parameter allows to disable mail queue size reporting as a temporary solution. Some alternatives had been presented like  [an eventually consistent per slice counters approach](https://github.com/linagora/james-project/pull/2565). An other  proposed solution is [to rely on RabbitMQ management API to retrieve mail queue size](https://github.com/linagora/james-project/pull/2325) however by design it cannot take into account purge/delete operations. Read  [the corresponding JIRA](https://issues.apache.org/jira/browse/JAMES-2733).  ## Consequences  Distributed mail queue allows a better spreading of Mail processing workload. It enables a centralized mailQueue management for all James servers.  Yet some additional work is required to use it as a Mail Exchange scenario. ","technology_choice, data_persistence, microservices_and_modularity"
ADR_067,https://github.com/dante-ev/docker-texlive.git,docs/adr/0001-do-not-base-on-any-tex-image.md,Do not base on any tex image,"# Do not base on any tex image  ## Context and Problem Statement  The Docker image can be made from scratch or base (`FROM`) on an existing one. When choosing an existing one, which one should be taken?  ## Decision Drivers  * Self-maintaining an image (from scratch) is hard and one repeats the mistakes, others have done * Patching an existing image (via PRs) might lead to rejections  ## Considered Options  * Do not base on any tex image * Use <https://github.com/scottkosty/install-tl-ubuntu> - Ubuntu and TeXLive 2019 * base on <https://github.com/reitzig/texlive-docker> - alpine different flavours can be selected * base on https://github.com/janweinschenker/docker-texlive/blob/master/Dockerfile - Ubuntu 17.10, texlive-full * base on https://github.com/thomasWeise/docker-texlive/blob/master/image/Dockerfile - Ubuntu 16.04, small installation * <s>base on https://github.com/sumandoc/TeXLive-2017/blob/master/Dockerfile - Debian sid, install-tl-unx.tar.gz, LaTeXML</s> - does not exist anymore * base on https://github.com/rchurchley/docker-texlive/blob/latest/Dockerfile - Debian latest, texlive.iso; fetches config files via wget * base on https://github.com/cdlm/docker-texlive - some more tools * base on https://github.com/mtneug/texlive-docker/blob/master/Dockerfile - Ubuntu 17.04, texlive-full; nice badges; build.sh * base on https://github.com/shuichiro-makigaki/docker-texlive-2017/blob/master/Dockerfile - Fedora latest * base on https://hub.docker.com/r/ctarwater/docker-texlive/~/dockerfile/ - debian Jessie * base on https://github.com/adinriv/docker-texlive/blob/master/Dockerfile - tlmgr -> based on docker-texlive-minimal - switched from install-tl-ubuntu to custom solution at https://github.com/adinriv/docker-texlive/commit/4c573e09bafff8da1ac121dd769a17bbbe0ca53b * base on https://github.com/adinriv/docker-minimal-texlive/blob/master/Dockerfile - ubuntu:rolling, install-tl-unx, minimal setup * base on https://github.com/harshjv/docker-texlive-2015/blob/master/Dockerfile - ubuntu 14.04 * base on https://github.com/chrisanthropic/docker-TeXlive/blob/master/Dockerfile - debian:jessie, texlive 2015, ISO * base on https://github.com/camilstaps/docker-texlive/blob/master/Dockerfile - debian:jessie, config file in repo; `latexmk` does not work there. https://github.com/camilstaps/docker-texlive/issues/1 * base on https://github.com/dc-uba/docker-alpine-texlive - [alpine linux](https://hub.docker.com/_/alpine/), minimal texlive 2016 * base on https://github.com/blang/latex-docker (blog entry: https://ljvmiranda921.github.io/notebook/2018/04/23/postmortem-shift-to-docker/) - not maintained any more  ## Decision Outcome  Chosen option: ""Do not base on any tex image"", because  * breaking changes on ""base images"" could come in * could become unmaintained (e.g., <https://github.com/adinriv/docker-texlive>) * the `install-tl-ubuntu` script could get unmaintained  We accept that  * We are based on debian sid, which constantly changes * We will have to monitor the upstream repository if texlive 2019 is released and possibly adapt our Dockerfile. * We get a large image - more than 4 GB. * We have to install font packages separatetly for each font (e.g., [fonts-texgyre](https://packages.debian.org/sid/fonts/fonts-texgyre)) ",governance_and_process
ADR_068,https://github.com/alphagov/content-data-api.git,docs/arch/adr-014-track-attribute-changes-per-basepath.md,Track attribute changes through time for basepaths,"# Track attribute changes through time for basepaths  21-09-2018  ## Context:  The Data Warehouse tracks changes to Content Items through time so, for example, if we change the organisation attached to a Content Item, we should know WHEN that change happened and WHAT changed.  Tracking changes not only applies to other entities associated to the Dimension Items, but also to core attributes like the `title`, `description`, `rendering-app`, or any other attribute. This is the foundation of our Data Warehouse: track changes and track performance at the same time.  In GOV.UK we can identify a Content Item on a unique way by their `content_id`.  It is the combination of a `content_id` and a `locale` what makes a Content Item unique. So on our first iteration of the DW, when we decided to use the Content Item as the `grain`, hoping to be able to track all our changes by `content_id`.  Shortly after we noticed that most of our metrics are not related to a Content Item but the `base_path` of the item; the is due to some Content Items like Guides or Travel Advice, has multiple `base_paths` and we need to track metrics at that level (sub item). This was a business requirement driven by user research.  So we decided to reduce the scope of the `grain` to the `base_path` of the Content Item, which simplified and fulfilled our user needs. It was accepted by the business that if the `base_path` changes, then we won't be able to track the series backwards. We decided to postpone dealing with this issue when we had more information about it.  As of today, we have learned more about our real needs for querying metrics. Aggregating metrics through time for a Content Item with a base_path, need to be done via its `content_id`, and there some edge cases to handle to be accurate. Unfortunately, to address this issues, we need to make our queries more complex, and the codebase is way more complicated to maintain and reason about.  The underlying problem is that we don't have a way to track our grain (base_path) through time because all the attributes can be changed, and overcoming this limitation using `content_id` and `locales` it is just not worth the effort.  In summary, we would need to provide an efficient way to:  Given a `base_path` in their latest version, select all the metrics through time, regardless of the changes to the item.   At the very end, what we are asking for is for a real unique identifier for the item, because we actually don't have it, because the `content_id` is not playing that role.  ## Decision  Associate a unique identifier to each item that is tracked in the dimensions item.   Each `base_path`, which represents the grain of Data Warehouse, will have a unique identifier that will never change. This ID will allow us to perform queries through time in an efficient way, and also to be accurate with our results.  On the same go, all the Contnet Items that share a locale will also be uniquely referenced in the Data Warehouse.  ### Note  We would not need to add this unique identifier if   1. The sub-items for Content with multiple paths have a unique UID for each `base_path`.  Unfortunately, to implement this feature we would need to modify each publishing  application; which would impact our immediate delivery, so it needs a wider conversation.  2. All the content items that have different locales but same ID have a different `content_id`.  ## Consequences  It has a low impact in our codebase; we only need to generate it once (on creation), then it will be cloned with each version, so at the very end, a few lines of code with an important impact in our delivery.  ~  ",others
ADR_069,https://github.com/eclipse/winery.git,docs/adr/0008-no-support-for-local-git-source-clones.md,No support for local git source clones,"# No support for local git source clones  A user wants to edit source files locally in his favourite IDE. Therefore, he wants to use the usual ways to retrieve source files. Typically, this is a `git clone` from a git repository having the respective source files.  A user does not want to clone the whole winery repository, as this might a) be too large b) not focused enough. It would be beneficial to have the source of an artifact template available as git checkout.  The source files of an artifact implementation are currently directly editable in the winery once they are uploaded.  The only way to edit sources locally is to download and upload them again. The solution for the user should be: - easy to use - scalable in terms of storage required in Winery's repository   ## Considered Alternatives  * No support for local clones * Git repositories as submodules * Using filter-branch (https://help.github.com/articles/splitting-a-subfolder-out-into-a-new-repository/) * Using git sparse-checkout to create a local clone (https://gist.github.com/sumardi/5559896)  ## Decision Outcome  * Chosen Alternative: no support for local edit  Since all alternatives require either too many additional git repositories or are very inconvenient to apply for the user, we decided to not support any clone/push functionality.   ## Pros and Cons of the Alternatives   ### No support for local edit * `+` no changes needed * `-` no local edit support  ### Git repositories as submodules * `+` simple git cloning possible * `+` additional repositories can be cloned into winery-repository as submodules * `+` separate version history * `-` one repository for each implementation * `-` each separate repository has to be created on the git remote of winery (e.g., GitHub)  ### Using filter-branch on sever's side * `+` no changes needed in the existing repositories * `+` `git filter-branch --prune-empty --subdirectory-filter` allows to skip any subdirectory * `-` server needs to execute very large filter commands for each user for each requested artifact template * `-` the mapping back from the filtered repository to the full repository is cumbersome. * `-` merge conflicts are not resolved by git tooling automatically  ### Using filter-branch on user's side * `+` no changes needed in the existing repositories * `+` `git filter-branch --prune-empty --subdirectory-filter` allows to skip any subdirectory * `-` user needs to execute very large filter commands * `-` the mapping back from the filtered repository to the full repository is cumbersome. * `-` requires the user to type the commands manually  ### git sparse checkout * `+` no changes needed in the existing repositories * `-` requires the user to type the commands manually   ## License  Copyright (c) 2017 Contributors to the Eclipse Foundation  See the NOTICE file(s) distributed with this work for additional information regarding copyright ownership.  This program and the accompanying materials are made available under the terms of the Eclipse Public License 2.0 which is available at http://www.eclipse.org/legal/epl-2.0, or the Apache Software License 2.0 which is available at https://www.apache.org/licenses/LICENSE-2.0.  SPDX-License-Identifier: EPL-2.0 OR Apache-2.0 ",others
ADR_070,https://github.com/dxw/react-template.git,docs/adr/0008-use-styled-jsx.md,8. Use styled-jsx,"# 8. Use styled-jsx  Date: 2019-10-10  ## Status  Accepted  ## Context  We want to be able to style our components in a way that reduces chances of clashes, or unwanted styles.  We could use a convention like [BEM](http://getbem.com/), but they are generally hard to enforce.  [`styled-jsx`](https://github.com/zeit/styled-jsx) is an alternative that scopes styles to individual components, and is integrated into Next.js by default. We can still fall back to global stylesheets if required (for instance if we are using GOV.UK styles).  ## Decision  We will use `styled-jsx` for internal component styling.  ## Consequences  Using `styled-jsx` means styles for components are contained within those components, ensuring they never leak out unintentionally. We can share styles between components via [`styled-jsx/css`](https://github.com/zeit/styled-jsx#external-css-and-styles-outside-of-the-component) as needed.  `styled-jsx` also allows styles to be dynamic without dealing with stateful classes. ",technology_choice
ADR_071,https://github.com/hmrc/play-frontend-hmrc.git,docs/maintainers/adr/0004-add-contact-and-welsh-information-links-into-footer.md,Add contact HMRC and Welsh information links to standard footer,"# Add contact HMRC and Welsh information links to standard footer  * Status: accepted * Date: 2020-12-04  Technical Story: PLATUI-854  ## Context and Problem Statement  In the context of classic services' requirement for contact HMRC and Welsh information links in their footer, facing the  fact that these links are missing from hmrcStandardFooter, should we add them?  The additional links needed are:  * ""Contact"", linking to https://www.gov.uk/government/organisations/hm-revenue-customs/contact * ""Rhestr o Wasanaethau Cymraeg"", linking to https://www.gov.uk/cymraeg  ## Decision Drivers  * The need for consistency across HMRC services. * Our belief that including them is likely to improve the user experience for tax users. * We can see no good reason for not including them as standard because they are applicable across HMRC services. * We have a time sensitive opportunity of acting now while teams are in the process of uplifting their frontend libraries. * The HMRC design community have been consulted on multiple public Slack channels and two successive design system working group meetings, with no objections noted. * Classic services support multiple live services. Not including these links as standard would mean their having to duplicate these links, and associated English and Welsh content, across tens of repositories.  ## Considered Options  * Add the links to hmrcStandardFooter * Create an hmrcExtendedFooter component containing the additional links * Do nothing  ## Decision Outcome  Chosen option: ""Add the links to hmrcStandardFooter"", because this  will benefit tax users, and we have a unique window of opportunity to act now.  ### Positive Consequences  * Tax users have better information provided to them * Teams do not need to duplicate content and URLs across hundreds of repositories * We can more easily maintain the content and links in a central repository  ### Negative Consequences  * Teams currently using a Welsh link as their language toggle will likely need to switch to using one of the standard components for language switching e.g. hmrcLanguageSelect. * Teams already including a contact link manually will need to remove it when upgrading.   ## Pros and Cons of the Options  ### Add the links to hmrcStandardFooter  * Good, because it's more likely the additional links will become standard benefiting users * Good, because we can standardise the content for English and Welsh * Good, because hyperlinks can be maintained in a central place and do not have to be corrected in 100s of separate services * Bad, because it's possible some teams may not want a contact or Welsh link for valid business reasons  ### Create an hmrcExtendedFooter component containing the additional links  * Good, because new links will be added consistently for teams adopting it * Good, because we can standardise the text for English and Welsh * Good, because the hyperlinks can be maintained in a central repository * Bad, because uptake unlikely to be as high as making the links standard * Bad, because teams may become confused about which footer to use, increasing delivery friction  ### Do nothing  * Good, because teams have more flexibility * Bad, because tax users will not benefit from these additional information links * Bad, because teams may add the additional links inconsistently with different text, different URLs, or   in a different order * Bad, because there is no standardisation of the content in English and Welsh * Bad, because if changes need to be made, they will need to be made in 100s of individual services rather than in one library * Bad, because teams will have to rollback these changes if they adopt a new standard footer with the contact and Welsh links ",others
ADR_072,https://github.com/nulib/meadow.git,doc/architecture/decisions/0017-preservation-strategy.md,17. Preservation Strategy,"# 17. Preservation Strategy  Date: 2019-10-11  ## Status  Accepted  ## Context  Having a ""preservation first"" mindset has been decided upoan as a stated goal for Meadow. Generally, this means that digital preservation policies, processes and deliverables should be planned and implemented in tandem with development of the applications features, processeses and infrastructure.  ## Decision  The digital preservation lifecycle for a Work and its FileSets begin as soon as an Ingest Sheet is ""approved"". Actions in the ingest pipeline are used to add digital preservation ""artifacts"" to Work and FileSet metadata (such as checksum and timestamps) and move objects to preservation storage buckets in S3. Additionally, the success and failure outcomes of these actions are added as AuditEntries that can be used verification, future audits and problem resolution.  ## Consequences  The riskiest aspect of this strategy is that the application is rapidly evolving and we may have to alter aspects of our digital preservation strategy as we learn more about our chosen infrastructure's offerings and limitations over time. ",others
ADR_073,https://github.com/rhurkes/sware-server.git,docs/adr/0002-misc.md,Miscellaneous small decisions,"# Miscellaneous small decisions  * Status: accepted * Date: 2020-04-14  ## Context and Problem Statements  There are multiple small decisions that need to be made: - What process should be used to create keys?     - Whatever method, having the store own the key generation is preferred as it allows individual loaders to not care about key generation.     - We used to use ingestion time in microseconds as the ID. In practice this was monotonic, and I was never able to simulate duplicate system times or times that jumped backwards in sequence. Thread-safety came for free when using ZMQ, but would require some extra work in a simplified solution. This work could have been a Mutex or using an MPSC queue (channels), but didn't provide a lot of value as the ingest time was metadata that is only useful in evaluating lag between ingestion and event dissemination.     - Based on the previous statement, I tried implementing using an AtomicUsize as an ID. This worked well, except it made the API contract a little more gross, and made the store considerably more complicated. Keyless fetches had to iterate backwards through all keys, deserializing the data, and checking if the ingest time was less than the threshold. Having this performance constraint on reads seemed bad. I ended up using a mutex and a while loop to prevent collisions of ingest timestamps as the ID. A mutex and db.get() aren't ideal, but write performance shouldn't be an issue.     - It would have been nice to leverage some sort of CAS/transaction in RocksDB, but the Rust bindings didn't allow for it.     - Tested the write performance in release builds at 6.4μs/put for the mutex/CAS-free code, and 9.3μs/put for the final implementation. The mutex performance hit was negligable, so most of the hit came from the CAS behavior. Even on the busiest of weather days, you're still going to be limited in your writes by the time over-the-wire from the sources which will measure in the dozens of milliseconds if not more. A 3μs difference would not be noticeable. sware v1 did writes in 30μs, so this is considerably faster.     - Can I create a collision? I took 4 threads and put 1M events with each using no throttling. I expected to see 4M keys in RocksDB, and saw 2216421 - almost a 50% collision rate. - I used to roll up all errors into a WxError type, which essentially just persisted the message from each. It was boilerplate that didn't add a ton of value, so I changed most of these functions to return `()` as an `Err` and log errors where they occur. I also switched from `slog` to `log` as I never really used all the structured logging features. - On 64 test events, gzipping shrunk the payload down to 14% of its original size. There were further savings by creating an optimized Event struct that doesn't serialize None values, but they were only about 5% smaller. I'll leave it for now, as it's not that much extra work, and compression won't be available in `warp` for a little while. ",others
ADR_074,https://github.com/ensemblejs/ensemblejs.git,doc/decisions/0003-use-immutablejs-for-immutability.md,1. Use ImmutableJS for immutability,"# 1. Use ImmutableJS for immutability  Date: 15/06/2016  ## Status  Accepted  ## Context  The cost of cloning JSON, while fast degrades with object size and as we're doing it serveral times per frame the cost is too much.  ## Decision  Implement ImmutableJS to avoid the need to clone data. I'm hoping the internal behaviour of ImmutableJS is smart enough to avoid the cost of cloning by cleverly moving references around. The data has to be created once, but I am hopeful that that is the only time.  ## Consequences  `JSON.parse(JSON.stringify(data))` is easy to reason about as well as being fast enough for most uses. The use of standard JavaScript objects is also easy to reason about. ImmutableJS enforces it's own API and it's another thing for someone to learn.",technology_choice
ADR_075,https://github.com/island-is/handbook.git,docs/adr/0001-use-nx.md,Use NX,"# Use NX  - Status: accepted - Deciders: devs - Date: 03.05.2020  ## Context and Problem Statement  We want a monorepo tool to help us to scale development up for multiple projects and teams. It should not be too much in the way, but help us manage code, dependencies and CI/CD.  ## Decision Drivers  * Low complexity and overhead in development. * Fit for our stack. * Optimize CI/CD with dependency graphs and/or caching. * Flexible.  ## Considered Options  * [Bazel] * [Nx] * [Lerna]  ## Decision Outcome  Chosen option: ""Nx"", because:  * It's specially designed around our stack (TypeScript, React, Node.JS, NPM, ESLint, Prettier, Cypress, Jest, NextJS). * It's relatively easy to learn with focused documentation. * It has schematics to generate apps, libraries and components that includes all of our tools. * It is opinionated, which gives us a good base to start developing faster. Many things can still be configured or extended.  ## Pros and Cons of the Options  ### [Bazel]  * Good, because it's created and maintained by Google * Good, because it supports multiple programming languages and platforms. * Bad, because it's difficult to learn, with a custom BUILD files. * Bad, because JS support is hacky.  ### [Nx]  * Good, because it has built in support for our stack. * Good, because it's been around for a while, originating in the Angular world. * Good, because it's designed with best practices for large scale web applications. * Good, because it supports elaborate code generation. * Good, because it helps optimise CI/CD for large projects. * Bad, because it's fairly opinionated. * Bad, because it's an open source project maintained by an agency.  ### [Lerna]  * Good, because it integrates with NPM and Yarn. * Good, because it's used by a lot of open source projects. * Bad, because it's primarily designed for managing and publishing open source projects, not building and deploying large scale applications.  ## Links  * [Why you should switch from Lerna to Nx](https://blog.nrwl.io/why-you-should-switch-from-lerna-to-nx-463bcaf6821)  [Pants]: https://www.pantsbuild.org/ [Bazel]: https://bazel.build/ [Nx]: https://nx.dev/ [Lerna]: https://lerna.js.org/ ","technology_choice, infrastructure_and_deployment"
ADR_076,https://github.com/ec-europa/europa-component-library.git,docs/decisions/004-datepicker.md,Datepicker component,"# Datepicker component  | Status        | proposed                                             | | ------------- | ---------------------------------------------------- | | **Proposed**  | 25/11/2019                                           | | **Accepted**  | (the date the proposal was accepted/rejected)        | | **Driver**    | @emeryro                                             | | **Approver**  | (who will make the decision and merge the PR)        | | **Consulted** | (who you worked with on this decision)               | | **Informed**  | (who should be informed if the proposal is accepted) |  ## Decision  (Describe the decision that you propose, ideally in a single sentence)  ## Context  A datepicker component has been requested for ECL2.   This component was available on ECL1 but has not been ported yet.   We have to find a good way to provide such feature.  The component has to:  - respect the styling provided [in the specifications](https://webgate.ec.europa.eu/CITnet/confluence/x/fqvBN) - be translatable - offer a no-js behavior (still to be defined) - propose different date format - be as accessible as possible  ## Consequences  (Describe the pros and cons of the proposed decision. Think about the people in the **Informed** line of the DACI table above. How will this decision affect them?)  ## Alternatives Considered  ### External library with custom style  #### Pikaday  On ECL1 we used [Pikaday](https://github.com/Pikaday/Pikaday) to handle datepicker.   We could rely on it again, as it seems to follow most of the requirements.  **Pros**  - quick to implement - similar to ECL1 - code maintained by the library owner  **Cons**  - less control over the styling and behavior. Some specs may not be applied fully - not updated for more than a year - extra dependency for ECL2  ### Home made script and style  We could write a script from sctrach to handle datepicker.  **Pros**  - full control over the behavior, styling and accessibility  **Cons**  - will require far more time to implement - code has to be maintained on our side ",technology_choice
ADR_077,https://github.com/yext/edward.git,doc/adr/0002-implement-in-go.md,2. Implement in Go,"# 2. Implement in Go  Date: 2016-03-02  ## Status  Accepted  ## Context  Edward will be provided as a command-line tool, ideally across multiple operating systems. It will need a simple means of installation and updating.  ## Decision  Edward shall be implemented using Go.  ## Consequences  Go applications are cross-platform and can be distributed using `go get`. This means that there is no additional configuration or scripting required. Alternate distribution mechanisms can be built and enabled separately from this repo.  Go also provides built-in support for executing other processes and a basic system for providing command-line interfaces. ",technology_choice
ADR_078,https://github.com/raster-foundry/raster-foundry.git,docs/architecture/adr-0000-architecture-documentation.md,0000 - Architecture Documentation,"# 0000 - Architecture Documentation  ## Context We need a way to document major architecture decisions; in the past we have used the [Architecture Decision Record (ADR) format](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions). On past projects, we have found the ADR format to be a useful way to write and manage architecture decisions.  We have written ADRs using both reStructuredText and Markdown formats on past projects. Certain documentation generators, such as Sphinx, can only use one of RST / Markdown. It is currently unknown which documentation generators we are likely to use for this project. The team is somewhat more comfortable writing in Markdown than RST.  ## Decision We will continue to use the ADR format for writing architecture decisions for this project. We will use Markdown for formatting ADR documents.  ## Consequences Major architectural decisions will need to be documented; changes to architectural decisions made via ADR will need to be documented in a superseding ADR.  If we choose to use a documentation generator that does not support Markdown, we may need to convert existing ADRs to that tool's preferred format. ",governance_and_process
ADR_079,https://github.com/alphagov/gsp.git,docs/architecture/adr/ADR024-soft-multitenancy.md,ADR024: Soft Multi-tenancy,"# ADR024: Soft Multi-tenancy  ## Status  Accepted  ## Context  One Programme has many Service Teams.  One Service Team has many Environments.  Some Service Teams have separate AWS accounts for separate environments. (i.e. Staging, Production)  Many Service Teams have micro-service architectures that run on many machines.  Some Service Teams have unique programme specific network isolation requirements that may be hard to implement in a shared environment.  Separate programme level accounts would enable separation of billing.  Sharing the infrastructure within a programme will lower hosting costs.  To ensure network/compute isolation between Service Teams it may be necessary to isolate resources.   ## Decision  We will design for a ""soft multi-tenancy"" model where each programme shares a single GSP cluster with service teams within that programme.  This will:  * Maintain clear separation of billing at the programme level by isolating cluster to programme's own AWS account * Maintain clear separation of programme specific policies and risk assessments by not forcing all users to adhere to the strictest rules? * Minimize costs by sharing infrastructure, control plane and tooling between teams/environments * Minimize support burden by reducing the amount of configuration  ## Consequences  * Less efficient than one big cluster * Less isolated than millions of clusters ",microservices_and_modularity
ADR_080,https://github.com/jvdub/disc-golf-statistics.git,doc/architecture/decisions/0002-use-es2016-modules.md,2. Use ES2016 Modules,"# 2. Use ES2016 Modules  Date: 2017-12-01  ## Status  Accepted  ## Context  ES2016 introduced native support for the concept of modules. These are scoped files that expose some public functions. Modules are a way of organizing and sharing code.  ## Decision  We will use ES2016 modules to organize and share code. More information can be found here: http://exploringjs.com/es6/ch_modules.html#sec_modules-in-javascript  ## Consequences  Because of the nature of this project, this decision is made in lieu of choosing a framework. I want to be able to learn some of the new ES2016 features without a framework getting in the way. ",technology_choice
ADR_081,https://github.com/UKHomeOffice/drt-v2.git,doc/architecture/decisions/0013-use-lihaoyi-s-autowire.md,10. use lihaoyi's autowire,"# 10. use lihaoyi's autowire  Date: 31/06/2016  ## Status  Accepted  ## Context  We've got a Single Page app, it needs to talk to the server. Our use of scala and scalajs means we can use [lihaoyi's autowire macros](https://github.com/lihaoyi/autowire) Although this is essentially a 0 on the [Richardson maturity model](https://martinfowler.com/articles/richardsonMaturityModel.html) it has huge benefits in terms of speed of change. We also (at the moment) only have the one client of the SPA so we can afford the tight coupling.  It doesn't preclude moving toward something more restful, as we can just add routes when we recognise a need.  ## Decision  Use autowire for now.    ## Consequences  + \+ Compile time safety between front and back.  - \- macro magic that has cause a few headaches here and there (beware immutable.Seq) ",technology_choice
ADR_082,https://github.com/tsobe/lobiani.git,doc/adr/0009-develop-admin-tool-as-spa.md,9. Develop admin tool as SPA,"# 9. Develop admin tool as SPA  Date: 2021-01-11  ## Status  Accepted  ## Context  As part of the functional requirements, we need to have a means of managing the content through an intuitive web  interface. There are a couple of options to achieve this  ### 1. As a ""traditional"" web application in the MVC architectural style, embedded within the backend (monolith) #### Pros  - Server-side rendering sometimes can provide better user experience since content is immediately visible once  the page is loaded - Compatibility issues between the frontend and backend can be detected at the earlier stage  #### Cons  - Tools and libraries for the backend and frontend are mixed in the same project, making it a bit messy and mentally harder to grasp - Strong coupling between web layer and backend discourages us to design general-purpose API for other types of potential consumers - Provides limited level of interactivity  ### 2. As a Single Page Application in the MVVM architectural style, packaged and deployed separately from the backend #### Pros  - Frontend is more decoupled from backend technologies since they interact with each other via API - Backend API can be potentially used by other consumers too: CLI, Native & Mobile (as long as it is general-purpose) - Frontend and backend can be delivered independently (and hence faster) from each other - Provides greater level of interactivity  #### Cons  - More pipelines need to be maintained in CI - Compatibility issues between the frontend and backend may be detected later, during the integration stage - Complete client-side rendering may degrade the user experience a bit  ## Decision  We will go for the SPA approach  ## Consequences  - Initial development pace might be slower, since we will have to learn the new frontend technologies as we go - We will explore modern frontend technologies and approaches ",architectural_patterns
ADR_083,https://github.com/huifenqi/arch.git,decisions/0006-replace-svn-with-git.md,6. SVN 迁移至 Git,"# 6. SVN 迁移至 Git  Date: 06/04/2017  ## Status  Accepted  ## Context  当前我们用的是 SVN 做代码、产品文档、UI 设计图的管理，在此说说其中的代码管理  1. 代码仓库过大，新分支，新Tag 代码都是一份完整的拷贝; 2. 必须联网提交； 3. SVN 代码合并方式低效，目前使用 Beyond Compare 做代码合并，分支使用方式落后； 4. 无法很好的做 code review（只能 patch 或第三方工具）； 5. 面试者看到是这么落后，以此类别其他技术栈，综合理解就是，我能学到啥。  ## Decision  使用全球最流行的分布式管理工具 Git 及平台 Github，其特点为分布式，目录结构简单，代码无冗余，可 review with PR。    ### 方式一：    git svn clone `svn project url` -T trunk  将 SVN 项目的 trunk 转为 git 项目的 master，仅保留了 trunk 分支的提交记录，此方式适用于所有代码都规整到了一个分支，并且不需要其他分支的提交记录。  ### 方式二：  使用命令 `https://github.com/nirvdrum/svn2git`，它可以保留所有branch, tags，以及所有分支的提交历史。  svn2git http://svn.example.com/path/to/repo --trunk trunk --tags tag --branches branch  git push --all origin  git push --tags  use `--revision number` to reduce the commit history.  目前生产环境使用的 centos 版本过低，导致 git 也无法升级的处理方法：  yum install http://opensource.wandisco.com/centos/6/git/x86\_64/wandisco-git-release-6-1.noarch.rpm  yum update git  ## Consequences  * 工作流的调整  Refs:  * https://help.github.com/articles/what-are-the-differences-between-subversion-and-git/ * http://stackoverflow.com/questions/871/why-is-git-better-than-subversion * [https://www.atlassian.com/git/tutorials/migrating-overview][1] * [https://www.atlassian.com/git/tutorials/svn-to-git-prepping-your-team-migration][2] * [https://www.git-tower.com/learn/git/ebook/cn/command-line/appendix/from-subversion-to-git][3] * [https://tecadmin.net/how-to-upgrade-git-version-1-7-10-on-centos-6/][4]  [1]:	https://www.atlassian.com/git/tutorials/migrating-overview [2]:	https://www.atlassian.com/git/tutorials/svn-to-git-prepping-your-team-migration [3]:	https://www.git-tower.com/learn/git/ebook/cn/command-line/appendix/from-subversion-to-git [4]:	https://tecadmin.net/how-to-upgrade-git-version-1-7-10-on-centos-6/",technology_choice
ADR_084,https://github.com/nulib/meadow.git,doc/architecture/decisions/0016-ingest-pipeline-spec.md,16. ingest-pipeline-spec,"# 16. ingest-pipeline-spec  Date: 2019-09-17  ## Status  Accepted  ## Context  Per [issue #1104](https://github.com/nulib/next-generation-repository/issues/1104): Developers need a (basic/nothing fancy) general, conceptual/overall understanding/plan of what form the ingest pipeline will take so that different pieces may effectively be worked on by different people.  ## Decision  We developed a specification for a flexible, message-driven [Ingest Pipeline](../specs/ingest_pipeline.md).  ## Consequences  Using this spec, we will be able to break down the ingest process into a series of atomic actions, with consistent progress tracking and error reporting. Further refinements may be required, and will be handled by subsequent ADRs. ",others
ADR_085,https://github.com/dxw/support-rota.git,doc/architecture/decisions/0003-use-dotenv-for-managing-environment-variables.md,3. use-dotenv-for-managing-environment-variables,"# 3. use-dotenv-for-managing-environment-variables  Date: 2019-09-19  ## Status  Accepted  ## Context  Accessing ENV directly without a wrapper is limited and can introduce problems.  We want our tooling to help us guard against missing environment variables. When `nil` is accidentally provided during the start up process it is preferable to fail fast with an explicit message. Without this `nil` can be passed down through the stack and cause strange behaviour where the code has been designed with it as a dependency. Instead of adding `nil` guards throughout the codebase for required environment variables (eg. `ENV.fetch('FOO', 'default')`, this should be managed centrally.  We have previously used Figaro for this purpose but it was deprecated in 2016 https://github.com/laserlemon/figaro. We should only use supported gems to ensure we get support in the form of fixes and security patches.  We also want to be able to stub our environment variables in our test suite. An easy example of this is when we use environment variables as a feature flag mechanism. We want to stub the value to test both scenarios without being influenced by real values being loaded. Mutating the actual ENV value (eg. ` allow(ENV).to receive(:[]).with('BOX_ID').and_return(""1234"")`) is possible but may have unexpected consequences where more than 1 part of the same process under test uses the same variable. Figaro used to be a handy abstraction layer that we could stub eg. `allow(Figaro).to receive(:env).with(:foo).and_return('bar')`. We should then consider how we can stub environment variables.  ## Decision  Use DotEnv to load our environment variables.  ## Consequences  Should Docker and Docker Compose be added to the project the environment variables will need to be loaded with `docker-compose up --env-file=.env.development` rather than `docker-compose.env` which is a pattern we have used. Having 2 files for managing environment variables such as `.env*` and `docker-compose.env*` is undesirable due to the overhead in keeping these in sync.  DotEnv loads environment variables but doesn't offer an interface as Figaro did. For DotEnv you'd access by writing `ENV['foo']` rather than `DotEnv.foo`. We will need to make a supporting decision to use [Climate Control](https://thoughtbot.com/blog/testing-and-environment-variables#use-climate-control) to support testing. ",technology_choice
ADR_086,https://github.com/openkfw/TruBudget.git,docs/developer/architecture/0001-record-architecture-decisions.md,(sem título),"--- sidebar_position: 1 ---  # Record architecture decisions  Date: 03/04/2018  ## Status  Accepted  ## Context  We need to record the architectural decisions made on this project.  ## Decision  We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions  ## Consequences  See Michael Nygard's article, linked above. ",governance_and_process
ADR_087,https://github.com/commercetools/commercetools-adyen-integration.git,docs/adr/0002-matching-adyen-notification.md,2. Matching Adyen Notification with commercetools payment.,"# 2. Matching Adyen Notification with commercetools payment.  Date: 2020-12-18  ## Status  [Deprecated](https://github.com/commercetools/commercetools-adyen-integration/pull/395)  ## Context  The Adyen notification needs to be matched by its commercetools payment equivalent. We are using the custom field for the merchantReference and fetching the commercetools payment object with query `custom(fields(merchantReference=""${merchantReference}""))`. The alternative for that is the native payment `key` field.  ## Decision  - We will use the native payment key for matching payment for notification. - The extension module will validate the reference field makePaymentRequest#reference before handling the payment to avoid unnecessary calls to Adyen. - The payment key will be set by the make payment handler, also makePaymentRequest#reference should be validated to avoid mismatches. - The notification will use the native payment key to fetch payment. It first finds the payment by `key` where `key=${merchantReference}` and then it finds in this payment the corresponding transaction by `interactionId` where `interactionId=${pspReference}`.   ## Consequences  - It is easier to fetch a key rather than using a custom field, also a key is an indexed field, so with a key, it's more performant. - The payment key is unique for all payments. - It's not possible to set key with my-payments endpoint. This prevents by default changing/removing the key accidentally. It is more secure than custom fields as the custom field might be changed with my-payment endpoint. Check for more details: https://docs.commercetools.com/api/projects/me-payments  Please refer to the [0011-matching-adyen-notification](./0011-matching-adyen-notification.md) for the latest change regarding matching Notification with payment.",others
ADR_088,https://github.com/omair-sajid-confiz/adr-poc.git,doc/adr/0003-write-help-file.md,3. write help file,"# 3. write help file  Date: 2018-09-17  ## Status  Superceded by [4. Generate Help file](0004-generate-help-file.md)  ## Context  The issue motivating this decision, and any context that influences or constrains the decision.  ## Decision  The change that we're proposing or have agreed to implement.  ## Consequences  What becomes easier or more difficult to do and any risks introduced by the change that will need to be mitigated. ",others
ADR_089,https://github.com/ijmccallum/life-dashboard.git,docs/arch decisions/20191028 api spec.md,Which API Spec to use,"# Which API Spec to use  Going to try an use some kind of spec standard to define and generate the api, this doc is to decide which spec to use.  ## Choices - pros cons   - OpenAPI 3.0 (Yaml or JSON)    - Pros: Turns out this _is_ swagger, just newer    - cons: is there much point thinking about cons - haven't come across any competitors.  - Swagger (Yaml or JSON)    - Pros: More stuff works with it maybe?    - Cons: it's the older one!   - JSON:     - Pros: My code can read it directly (assuming I'm in JS)    - Cons: More syntax to write  - YAML:    - Pros: It's not as tempting to try generating my own & it's more concice    - Cons: It's not able to be read nativly by my code.  ## Decision  OpenAPI.YAML  Newer spec, yaml is prettier. Simple as!",technology_choice
ADR_090,https://github.com/guardian/editions.git,docs/04-✅-paths.md,Issue Paths,"# Issue Paths  ## Status: ?  ## Context  The editions lambda needs to be able to identify specific versions of an issue.  ## Decision  To have two deployments of the backend, one for previewing, and a second for published issues.  The published issues deployment will replace the issueid path parameter with source/issueid.  `source` will identify which file in the published bucket will be retreived to form the issue on.  ## Alternatives  The issue could be identified with a get parameter and they could both run in the same deployment.  ## Consequences  The published issues lambda will not be directly callable from the app. ",infrastructure_and_deployment
ADR_091,https://github.com/phpactor/amp-fswatch.git,adr/0002-do-not-use-callback.md,(sem título),"Do not use a callback in the public API =======================================  Context -------  In 0001 it was decided to use a callback to handle modified files. When used with real code:  ```php $this->watcher->watch($this->paths, function (ModifiedFile $file) {     asyncCall(function () use ($file) {         $job = $this->indexer->getJob($file->path());          foreach ($job->generator() as $file) {             yield new Delayed(1);         }     }); }); ```  Which is just odd. The callback is not part of the co-routine. Using a promise improves this:  ```php while (null !== $file = yield $watcher->wait())     $job = $this->indexer->getJob($file->path());      foreach ($job->generator() as $file) {         yield new Delayed(1);     } }); ```  Decision --------  Refactor the code to yield promises for modified files.  Whilst initially I thought this would be quite difficult, it didn't take long. Each watcher has an async co-routing which builds a queue of modified files which are then subsequently yieled when `->wait()` is called on the `Watcher` (if there are no files, then we pause the co-routine for some milliseconds then try again).  Consequences ------------  It should be easier to integrate this library into Amp projects. On the downside it does mean coupling Amp to the public API - but seeing as this package is called AmpFsWatch, that's acceptable. ",api_and_contracts
ADR_092,https://github.com/PakkuDon/pixel-art-gallery.git,doc/adr/0002-store-pixel-art-in-github-repository.md,2. Store pixel art in Github repository,# 2. Store pixel art in Github repository  Date: 2020-11-06  ## Status  Accepted  ## Context  I've recently gotten into pixel art. I wanted a place to host these so I could share them later if I felt so inclined. But I didn't want to create a new account or use an existing account that had any PII associated with it.  I also figured I could make another side project out of this.  ## Decision  - Pixel art will be stored in a git repository that will be hosted on Github. - Link to image and associated details will be stored in a text file so they can be displayed in a UI later.  ## Consequences  - Images and their associated metadata will be stored under version control so that they can be revised or reverted with relative ease. - As new images will be added in git commits my Github contribution graph may still appear green even if I haven't coded at all.,technology_choice
ADR_093,https://github.com/vwt-digital/operational-data-hub.git,architecture/adr/0054-coding-guidelines.md,54. Coding guidelines,"# 54. Coding guidelines  Date: 2020-09-21  ## Status  Accepted  Implements [4. Create software defined everything](0004-create-software-defined-everything.md)  ## Context  Coding conventions are a set of guidelines for a specific programming language that recommend programming style, practices, and methods for each aspect of a program written in that language. These conventions usually cover file organization, indentation, comments, declarations, statements, white space, naming conventions, programming practices, programming principles, programming rules of thumb, architectural best practices, etc. These are guidelines for software structural quality. Software programmers are highly recommended to follow these guidelines to help improve the readability of their source code and make software maintenance easier.  Coding guidelines result in enhanced efficiency, reduced risk, mininized complexity, maintainability, bug rectification, comprehensive look and cost efficiency.  ## Decision  We will use coding guidelines for all languages used on the platform.  ## Consequences  Coding guidelines have to be defined, learned and maintained. However, a lot of common standardized coding guidelines exist, coming with tools to simplify implementation.  ## References  * https://en.wikipedia.org/wiki/Coding_conventions, retrieved 3 November 2020 * https://www.multidots.com/importance-of-code-quality-and-coding-standard-in-software-development, retrieved 3 November 2020 ",governance_and_process
ADR_094,https://github.com/alphagov/monitoring-doc.git,documentation/architecture/decisions/0009-use-cloud-init-to-build-prometheus-server.md,9. Use Cloud Init to build Prometheus Server,"# 9. Use Cloud Init to build Prometheus Server  Date: 2018-08-15  ## Status  Accepted  ## Context  The Prometheus Server needs to be built in a reproducible way within AWS. Reproducible in this context means that the server can be built, destroyed and rebuilt. The rebuilt server will be identical to the original server and the is no external intervention required (i.e. logging into the server to make changes to configuration)  ## Decision  Cloud init will be used to build a reproducible server.  ## Consequences  The cloud init was chosen over other strategies such as creating a machine image because there is prior art for building a Prometheus server with cloud init and building machine images requires additional tools. It was felt that cloud init will be the fastest way to achieve the short term goals. The use of cloud init should be reviewed at the earliest opportunity. ","technology_choice, infrastructure_and_deployment"
ADR_095,https://github.com/alphagov/verify-matching-service-adapter.git,doc/adr/0001-record-architecture-decisions.md,1. Record architecture decisions,"# 1. Record architecture decisions  Date: 15/05/2017  ## Status  Accepted  ## Context  We need to record the architectural decisions made on this project.  ## Decision  We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions  ## Consequences  See Michael Nygard's article, linked above. ",governance_and_process
ADR_096,https://github.com/thomvaill/log4brains.git,docs/adr/20200925-use-prettier-eslint-airbnb-for-the-code-style.md,Use Prettier-ESLint Airbnb for the code style,"# Use Prettier-ESLint Airbnb for the code style  - Status: accepted - Date: 2020-09-25  ## Context and Problem Statement  We have to choose our lint and format tools, and the code style to enforce as well.  ## Considered Options  - Prettier only - ESLint only - ESLint with Airbnb code style - ESLint with StandardJS code style - ESLint with Google code style - Prettier-ESLint with Airbnb code style - Prettier-ESLint with StandardJS code style - Prettier-ESLint with Google code style  ## Decision Outcome  Chosen option: ""Prettier-ESLint with Airbnb code style"", because  - Airbnb code style is widely used (see [npm trends](https://www.npmtrends.com/eslint-config-airbnb-vs-eslint-config-google-vs-standard-vs-eslint-config-standard)) - Prettier-ESLint enforce some additional code style. We like it because the more opinionated the code style is, the less debates there will be :-)  In addition, we use also Prettier to format json and markdown files.  ### Positive Consequences <!-- optional -->  - Developers are encouraged to use the [Prettier ESLint](https://marketplace.visualstudio.com/items?itemName=rvest.vs-code-prettier-eslint) and [Prettier](https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode) VSCode extensions while developing to auto-format the files on save - And they are encouraged to use the [ESLint VS Code extension](https://marketplace.visualstudio.com/items?itemName=dbaeumer.vscode-eslint) as well to highlight linting issues while developing ",technology_choice
ADR_097,https://github.com/Accenture/sfpowerscripts.git,decision records/002-parallel-development-process.md,Process for creating parallel development streams,"# Process for creating parallel development streams  * Status: Rejected  <!-- optional -->   ## Context and Problem Statement  When creating a parallel development streams (e.g. release), there are a list of manual steps that must be performed:  - package versions may need to be updated so that packages between streams do not share the same version-space - a new artifact feed or npm tags need to be created - a set of artifacts needs to be created for the new development stream  ## Options 1. **CLI helper tool for creating parallel dev streams**     - Create a CLI tool that automates the tasks required when creating parallel development streams:         - fetch last set of artifacts from source feed and publish them to the target feed; or         - create NPM tags that point to the latest artifact versions from parent stream         - increments package versions in the parent stream         - create a new git branch 2. **Document the process**     - Document the process for creating parallel development streams in Gitbook  ## Decision  All this issues arised from the fact that with the assumption of using multiple feeds. As we are moving to ask users to utilize a single feed/artifact repository and how it is in most platforms like GitHub or GitLab, there is no specific need for a helper tool. Users should be versioning their artifacts using semantic version when dealing with multiple development streams ",governance_and_process
ADR_098,https://github.com/hmrc/tech-radar.git,architecture-decision-records/adr-1_create-hybrid-tech-radar.md,ADR 1: Create Hybrid Tech Radar,"# ADR 1: Create Hybrid Tech Radar  ## Context  Technical material is not unified across the different Architecture spaces.  Additionally material is not consistently broken down and represented even within a single one of those spaces.  We need to increase consistency, accuracy and re-usability, and remove ambiguity to permit a better technical toolkit to be created to decentralise architecture, and implementation across the delivery centres.  There has been no shared understanding on what the pieces that comprise a technical toolkit should be.  Some material has not been maintained/updated and the work has not been opened to a large enough pool of contributors.  There is not a high enough level of engagement with technical material across many distributed teams to achieve a high enough level of quality according to a set of standards designed to support operational strategies.  ## Decision  We will create a hybrid Tech Radar, fully describing a set of lego building blocks for forming architectural material usable across the entire architecture.  We will reach a shared understanding on the level of granularity is with our building blocks, and in terms of the definitions of what those building blocks are.  The tech radar will be dropped periodically with updates to increase and maintain engagement with the material.  Material will be labelled across several axes (TBC) to provide indications on maturity of content, and suitability or relevance to different audiences etc.  ## Status  Accepted.  ## Consequences  Requires some complex activity in de-duplicating artefacts across the different areas of the architecture.  Requires a decision making process to reach consensus on how to edit material so it is compatible as we collide it in a single repository. ",others
ADR_099,https://github.com/learnitmyway/my-notes.git,adr/create-react-app.md,Fork and extend Create React App,"# Fork and extend Create React App  ## April 13 2019  ### Context  It would be nice to have `babel-plugin-jsx-remove-data-test-id` but I don't know of a way to add it to Create React App. it would be possible to fork CRA and adjust the config there. However, that seems to involved the following:  1. Fork CRA 2. Add plugin to babel-preset-react-app 3. Publish developerdavo-babel-preset-react-app 4. Point to developerdavo-babel-preset-react-app in react-scripts 5. Publish developerdavo-react-scripts  Not to mention, local development requires package linking and updating dependencies in developerdavo-babel-preset-react-app would require publishing two packages.  ### Decision  It doesn't seem to be worth the maintenance cost ",others
ADR_100,https://github.com/yldio/asap-hub.git,docs/decision/02-email-provider.md,Email provider,"# Email provider  Status: Final  Date: 2020-05-19  Author: Filipe Pinheiro <filipe@yld.io>  Reviewed-by: Tim Seckinger <tim.seckinger@yld.io>  ## Context  The majority of web applications uses email as a communication channel. The ASAP Hub is no different, and emails are a way to keep the communication with users.  The requirements for an email service are the following:  - **HTTP API**. The API allows us to send emails to our users programmatically. The API also works as the integration point and an excellent candidate to decouple sending emails from the other responsibilities of the service. - Provide a **template** engine. To create a consistent experience and mitigate misconfigured and poorly formatted emails, it's a good practice to have a template and send the data to fill the model to the email provider. The creation of the templates also enables us to decouple their production and test them in isolation. - Provide an **SMTP** configuration. [Auth0](../spike/0016-auth0.md) integrates seamlessly with several email services, but SMTP is also an option for the others. - (Optional) Track events about the emails sent  ## Options  The providers we looked into that fit he previous requirements were:  - [Amazon SES](https://aws.amazon.com/ses/) - [Postmark](https://postmarkapp.com/) - [SendGrid](https://sendgrid.com/) - [Mailchimp](https://mailchimp.com/) - [Mailgun](https://www.mailgun.com/) - [SparkPost](https://www.sparkpost.com/)  ## Decision  At this stage of development, we decide to use Amazon SES. The disadvantage of using Amazon SES is the lack of a dashboard and the visibility of the emails sent. Still, we believe that the ease of integration at this stage of the project is important to deliver the first user stories quickly. We will make sure that the email provider can be changed with reasonable effort in the future. ",technology_choice
ADR_101,https://github.com/dennisseidel/saas-platform-frontend.git,adr/0010-use-aws-amplify-coginito-for-login-and-the-identity-provider.md,10. Use aws amplify and coginito for login and the Identity Provider,"# 10. Use aws amplify and coginito for login and the Identity Provider  Date: 2019-03-02  ## Status  Accepted  ## Context  A user expect that he can create an account and login into our application. This should be standard compliant, economical (cheap) and easy to implement.  ## Decision  We use AWS Amplify (as a frontend lib) and AWS Cognito as the Identity Provider.  ## Consequences  This makes it easy to setup within minuites in the app with the react pre defined components, as well as an mostly standard compliant IDP with AWS Amplify that has a ok pricing model - that is better then Auth0. I have have to check if the application gets more users if it is not more economic to switch to a self hosted version like Keyloak.",technology_choice
ADR_102,https://github.com/theaiscope/GDD-app.git,doc/adr/0003-use-bitrise-for-ci.md,3. Use Bitrise for CI,# 3. Use Bitrise for CI  Date: 2019-06-04  ## Status  Accepted  ## Context  We need an easy way to integrate and test out code that is fast and reliable.  ## Decision  We choose Bitrise because it came from a suggestion from more senior devs and provides an easy interface to manage the workflows with good support for Android apps and testing. It also allows us to notify users in a easy way and with different roles.   ## Consequences  Bitrise does not allow us to use Pipelines-as-code 100% of the way. There is some configuration that needs to be done of the web workflow editor. Instructions followed to setup: https://devcenter.bitrise.io/tips-and-tricks/use-bitrise-yml-from-repository/  It was very easy to setup UI testing as the default configurations of the steps are already working,"technology_choice, infrastructure_and_deployment"
ADR_103,https://github.com/benjamminj/portfolio.git,docs/architecture/decisions/0002-use-nextjs.md,2. Use NextJS,"# 2. Use NextJS  Date: 2020-09-03  ## Status  Superceded by [14. Migrate to SvelteKit](0014-migrate-to-sveltekit.md)  Superceded by [13. Migrate to SvelteKit](0013-migrate-to-sveltekit.md)  ## Context  There's a few large architectural decisions that were made before I started using ADRs, so I'm backfilling them a little.  [NextJS](https://nextjs.org/) is a fantastic framework for building scalable, enterprise-grade applications with React.  NextJS has a few notable contrasts to [Gatsby](https://www.gatsbyjs.com/) (which this blog was previously built on) which factored into the decision to use it here. Some of these are captured below, some of my thoughts can be found in [this article by Jared Palmer](https://jaredpalmer.com/gatsby-vs-nextjs).  Here's some loose bullet points behind my reasoning behind using NextJS for this site.  - NextJS comes baked in with routing, webpack configurations, TypeScript support, and a whole host of other things. Less fiddling with tools, more writing meaningful code. - In general, Gatsby is more ""plugin-oriented"", NextJS is more ""recipe-oriented"". While plugins are nice to get a ton of functionality, each plugin introduces another dependency that you have to maintain, upgrade. Each new dependency introduces another point of failure. - NextJS doesn't have a GraphQL layer baked into it. For a website of this size I felt like being required to use the GraphQL layer to access markdown files was overkill. - NextJS allows you to build ""hybrid"" applications—ones where there's a combination of server-rendered (SSR) pages, static-rendered pages, and prerendered pages. There's not likely to be any SSR pages in this website, but NextJS sets you up for flexibility as time goes on in regards to rendering strategies. - NextJS is a super in-demand, ""hot"" skill in the React community, so having my website built on top of it both keeps me fresh on the latest patterns and showcases my abilities. - I like working with NextJS! I like the framework and how they've chosen the APIs!  ## Decision  I will use [NextJS](https://nextjs.org/) as the framework on top of React to build this website  ## Consequences  This will influence some of the patterns throughout the application. Stuff like routing, file structure, etc. ",technology_choice
ADR_104,https://github.com/theupdateframework/python-tuf.git,docs/adr/0009-what-is-a-reference-implementation.md,Primary purpose of the reference implementation,"# Primary purpose of the reference implementation  * Status: accepted * Date: 2021-08-25  ## Context and Problem Statement  The original goal for the reference implementation refactor was to provide an implementation which is both an aid to understanding the specification and a good architecture for other implementations to mimic.  During refactoring efforts on the metadata API and ngclient, several friction points have arisen where a safe object-oriented API would result in a less direct mapping to the [Document formats] in the specification.  The archetypal example friction point is that [Timestamp] lists snapshot _only_ in a `meta` dictionary of `METAPATH` -> attribute fields. The dictionary will only ever contain one value and creates an extra level of indirection for implementations which try to map to the file format.  When presented with such cases, we have considered multiple options: * Strict mapping to the [Document formats] * Simple and safe API in preference to mapping to the [Document formats] * Strict mapping to the [Document formats] with additional convenience API   which is documented as the preferred interface for users  So far implementation has tended towards the final option, but this is unsatisfying because: * the API contains traps for the unsuspecting users * two code paths to achieve the same goal is likely to result in inconsistent   behaviour and bugs  Therefore, we would like to define our primary purpose so that we can make consistent decisions.  [Document formats]: https://theupdateframework.github.io/specification/latest/#document-formats [Timestamp]: https://theupdateframework.github.io/specification/latest/#file-formats-timestamp  ## Decision Drivers  * The reference implementation is often the starting point for new   implementations, porting architecture of the reference implementation to new   languages/frameworks * Reading reference implementation code is a common way to learn about TUF * The TUF formats include non-intuitive JSON object formats when mapping to OOP   objects * Multiple code paths/API for the same feature is a common source of bugs  ## Considered Options  Primary purpose of the reference implementation is: * a learning resource to aid understanding of the specification (pedagogical reference) * a good architecture for other implementations to mimic (exemplary reference)  ## Decision Outcome  Primary purpose of the reference implementation is as an exemplary reference: providing a safe, consistent API for users and a good architecture for other implementations to mimic.  ## Links  * Discussed [on Slack](https://cloud-native.slack.com/archives/C01GT17AC5D/p1629357567021600) * Discussed in the [August 2021 TUF community meeting](https://hackmd.io/jdAk9rmPSpOYUdstbIvbjw#August-25-2021-Meeting) ",governance_and_process
ADR_105,https://github.com/linagora/linshare-mobile-android-app.git,adr/0002-image-loading-with-glide.md,7. Image Loading with Glide,"# 7. Image Loading with Glide  Date: 2019-12-17  ## Status  Accepted  ## Context  In the android linshare application, we implement the list file from the space of user. Proposed user interface design includes the thumbnail of file preview when user wants to show It is necessary to have an library to process first part is image thumbnail and preview. Over the best practice of image processing there are 2 libraries is very commons by android developer community: Glide and Picasso To compare between them, the commonly usage is the same, but Glide have much more strengthen rather than Picasso. It process the image source and have method to generate the thumbnail natively, it consume less the memory than Picasso and the library is have smaller packer with much more APIs to help process image, witch could be useful later when we implement more functionality in the application  ## Decision  We decided to use Glide instead of Picasso.  ## Consequences  Base on our implementation, the application is working well and for the developer it should be also save time to develop the feature base on Glide lib  ## References  [Comparison](https://medium.com/@multidots/glide-vs-picasso-930eed42b81d). [Source code](https://github.com/bumptech/glide). ",technology_choice
ADR_106,https://github.com/NERC-CEH/datalab.git,architecture/decisions/0002-ansible-for-provisioning-tool.md,2. Ansible for provisioning tool,# 2. Ansible for provisioning tool  Date: 2017-11-04  ## Status  Accepted  ## Context  We need a tool to provision servers and software for the datalabs project.  ## Decision  We will use Ansible as our provisioning tool as both the JASMIN DevOps team and Tessella team have experience using it.  ## Consequences  Ansible does not support Windows meaning that additional work will be required to allow administration from Windows laptops. This is not seen to be an issue as the project team have been provided with Linux workstations on the client network.,technology_choice
ADR_107,https://github.com/umd-lib/caia.git,docs/adr/0011-pin-mbtest-library-to-v2.5.1.md,"0011 - Pin ""mbtest"" library to v2.5.1","# 0011 - Pin ""mbtest"" library to v2.5.1  Date: March 26, 2021  ## Context  On March 22, 2021, it was discovered that the ""caia"" Jenkins builds were failing. All the failing tests were failing with the following error, related to a ""get_actual_requests"" method call:  ``` TypeError: 'generator' object is not subscriptable ```  The ""caia"" build was last successful in Jenkins on October 7, 2020. No builds were performed again until March 22, 2021, as there was no development work being done on the project.  Builds were made on March 22, 2021 because of a move to the ""GitHub organization"" pipeline in LIBITD-1880, which triggered rebuilds in all existing projects.  When the last successful build was made in October, the ""mbtest"" library ([https://github.com/brunns/mbtest](mbtest)) was at v2.5.1. In v2.5.2, the ""src/mbtest/server.py"" file was modified, changing the ""get_actual_requests"" method signature (see [this commit e398f2f1f32420](mbtest_commit)). from:  ``` def get_actual_requests(self) -> Mapping[int, JsonStructure]: ```  to  ``` def get_actual_requests(self) -> Iterable[Request]: ```  The change from a Mapping to an Iterable is the cause of the error in the tests.  ## Decision  The simplest solution for the moment is to ""pin"" the version of the ""mbtest"" library to v2.5.1 in the ""setup.py"" file. This will preserve the current behavior, until further ""caia"" development warrants additional testing.  ## Consequences  This decision improves the stability of the ""caia"" builds by pinning the ""mbtest"" dependency to a specific version. Since this library is only used for testing, keeping up with the latest updates (i.e. for security fixes) is not a concern.  If significant additional development of the ""caia"" project is performed, it would likely be worthwhile to update to the lastest ""mbtest"" version, and update the tests appropriately.  [mbtest]: https://github.com/brunns/mbtest [mbtest_commit]: https://github.com/brunns/mbtest/commit/e398f2f1f324209500506cc72fa0a045b2d420f4#diff-f6d8bc80c4ba5a033a4d011f675c4b43767a86fcd51b4463bdad275911ef95b6L159-R161 ",testing_strategy
ADR_108,https://github.com/ministryofjustice/modernisation-platform.git,architecture-decision-record/0010-terraform-module-strategy.md,10. Terraform Module Strategy,"# 10. Terraform Module Strategy  Date: 2021-06-18  ## Status  ✅ Accepted  ## Context  The Modernisation Platform uses [Terraform](https://www.terraform.io/) for its infrastructure as code. To make infrastructure reusable, or to simply tidy up code you can use [Terraform Modules](https://developer.hashicorp.com/terraform/language/modules). There are different use cases in the platform for using modules, and this ADR outlines how we plan to use them.  ## Decision  Modules used only by the Modernisation Platform core infrastructure will remain in the [terraform/modules](https://github.com/ministryofjustice/modernisation-platform/tree/main/terraform/modules) folder where they are currently located. These modules are mainly single use modules but created to keep the code tidier and easier to maintain. Modules used only by the core which currently have their own repository will remain where they are.  Modules used by users will have their own repository per module which we link to from the main repo. These modules will be versioned with GitHub releases, and tested with [Terratest](https://terratest.gruntwork.io/) against a test AWS account.  ## Consequences  ### General consequences  * new user modules will be created in their own repository * a new template for module repositories should be created for consistency, similar to [Cloud Platform Module Template](https://github.com/ministryofjustice/cloud-platform-terraform-template) * we will name modules using the format `modernisation-platform-terraform-module-name` * core platform modules can stay where they are currently located * modules should be tested with Terratest * we will need to create a testing environment for Terratest to run  ### Advantages  * we can version user modules with GitHub releases to avoid breaking existing infrastructure when updating modules * testing gives us confidence * keeping our core modules together makes it easier for us to navigate the platform code * we don't need to re-version core modules whenever we make a change to them  ### Disadvantages  * user modules are scattered in various repositories (we will signpost them from the main repo to make it easier to find them) ",governance_and_process
ADR_109,https://github.com/jmoratilla/devops-challenge.git,doc/adr/0006-feat-add-autoscaling-policy.md,6. feat-add-autoscaling-policy,"# 6. feat-add-autoscaling-policy  Date: 2020-02-20  ## Status  Accepted  ## Context  Goal: The platform must be scalable according to the load  We need to autoscale nodes in case we need to cope with a high load.  I have researching, and I have found that we have to scale the nodes  as well as the pods.  For some loads, scaling pods can be enough, but for other workloads it  can be better to scale the cluster nodes.  Issues then:  1. Autoscaling pods 2. Autoscaling nodes     ## Decision  For the first issue, I'm going to use:  * [Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)  This object is a controller and an API resource that is included in kubectl.  It  allows to set a period of time to check the load of a pod (cpu or custom metrics)  and increase the number of pods running.  For the second issue, I'm going to use:  * [Cluster Autoscaler Addon](https://github.com/kubernetes/kops/tree/master/addons/cluster-autoscaler)  With this addon, you can set a policy in AWS IAM and attach it to the previously  defined autoscaling group for nodes.  I haven't found another way to set the AutoScale of the nodes instanceGroup    ## Consequences  This is complex to me, as I use to manually manage the load of the system and  plan the scaling based on schedules.  A problem it can occurs is that by any mean, somebody attacks the services so  the cluster begins to grow uncontrolled and spend money and resources.  ","governance_and_process, infrastructure_and_deployment, technology_choice"
ADR_110,https://github.com/elastic/cloud-on-k8s.git,docs/design/adr-template.md,[short title of solved problem and solution],"# [short title of solved problem and solution]  * Status: [proposed | rejected | accepted | deprecated | … | superseded by [ADR-0005](0005-example.md)] <!-- optional --> * Deciders: [list everyone involved in the decision] <!-- optional --> * Date: [YYYY-MM-DD when the decision was last updated] <!-- optional -->  Technical Story: [description | ticket/issue URL] <!-- optional -->  ## Context and Problem Statement  [Describe the context and problem statement, for example, in free form using two to three sentences. You may want to articulate the problem in form of a question.]  ## Decision Drivers <!-- optional -->  * [driver 1, for example, a force, facing concern, …] * [driver 2, for example, a force, facing concern, …] * … <!-- numbers of drivers can vary -->  ## Considered Options  * [option 1] * [option 2] * [option 3] * … <!-- numbers of options can vary -->  ## Decision Outcome  Chosen option: ""[option 1]"", because [justification. For example, only option, which meets k.o. criterion decision driver | which resolves force force | … | comes out best].  ### Positive Consequences <!-- optional -->  * [For example, improvement of quality attribute satisfaction, follow-up decisions required, …] * …  ### Negative Consequences <!-- optional -->  * [For example, compromising quality attribute, follow-up decisions required, …] * …  ## Pros and Cons of the Options <!-- optional -->  ### [option 1]  [example | description | pointer to more information | …] <!-- optional -->  * Good, because [argument a] * Good, because [argument b] * Bad, because [argument c] * … <!-- numbers of pros and cons can vary -->  ### [option 2]  [example | description | pointer to more information | …] <!-- optional -->  * Good, because [argument a] * Good, because [argument b] * Bad, because [argument c] * … <!-- numbers of pros and cons can vary -->  ### [option 3]  [example | description | pointer to more information | …] <!-- optional -->  * Good, because [argument a] * Good, because [argument b] * Bad, because [argument c] * … <!-- numbers of pros and cons can vary -->  ## Links <!-- optional -->  * [Link type] [Link to ADR] <!-- example: Refined by [ADR-0005](0005-example.md) --> * … <!-- numbers of links can vary --> ",governance_and_process
ADR_111,https://github.com/KIT-SOC4S/ftd-scratch3-offline.git,docs/architecture/decisions/0008-use-scratch-saves-as-input-for-conversion-to-c.md,8. Use scratch saves as input for conversion to C,# 8. Use scratch saves as input for conversion to C  Date: 2019-10-20  ## Status  Accepted  Amended by [19. Allow direct input of project.json without a .sb3 file](0019-allow-direct-input-of-project-json-without-a-sb3-file.md)  ## Context  We need a way to get the used scratch blocks from a scratch program.   We could directly use the format scratch internally (when running in the browser) uses to represent the program.   This would mean that more code would have to be written in Javascript to interface with scratch.   This would also couple us tightly to the internal representation scratch uses.   We could use the format scratch uses to save the program.   Its format is defined [here](https://en.scratch-wiki.info/wiki/Scratch_File_Format) and it is stable for scratch3.   Using the scratch saves as input is also more flexible than using an internal representation.    ## Decision  We will use scratch's .sb3 save files as input.  ## Consequences  -,technology_choice
ADR_112,https://github.com/pulibrary/figgy.git,architecture-decisions/0003-preservation.md,3. Preservation,"# 3. Preservation  Date: 2019-04-02  ## Status  Accepted  ## Context  We have agreed that we will preserve digital objects by saving resources in Google Cloud Storage in a directory structure which preserves both the binaries the resource is made up of as well as the JSON serialization of the resource itself.  ## Decisions  1. Preserving    1. We will preserve materials in Google Cloud Coldline Storage with       `versioning` enabled. Versions will be kept indefinitely and without       limit. All files will go in a single bucket.       - Staging bucket is configured with the following command:         ```         gsutil mb -c regional -l us-west1 -p pulibrary-figgy-storage-1 gs://figgy-staging-preservation         echo '{""rule"": [{""action"": {""type"": ""Delete""}, ""condition"": {""age"": 2}}]}' > lifecycle.json         gsutil lifecycle set lifecycle.json gs://figgy-staging-preservation         rm lifecycle.json         gsutil bucketpolicyonly set on gs://figgy-staging-preservation         gsutil iam ch serviceAccount:figgy-staging@pulibrary-figgy-storage-1.iam.gserviceaccount.com:objectAdmin gs://figgy-staging-preservation         ```       - Production bucket is configured with the following command:         ```         gsutil mb -c coldline -l us-west1 -p pulibrary-figgy-storage-1 gs://figgy-preservation         gsutil bucketpolicyonly set on gs://figgy-preservation         gsutil iam ch serviceAccount:figgy-preservation-production@pulibrary-figgy-storage-1.iam.gserviceaccount.com:objectAdmin gs://figgy-preservation         gsutil versioning set on gs://figgy-preservation         ```    1. When a resource is `complete` and marked with the `cloud` preservation       policy it will save itself and all resources contained in `member_ids` in       a directory structure in Google Cloud Storage that looks like the       following:       ```       - <resource-id>         - data           - <child-id>             - <child-id>.json             - <binary.tif>         - <resource-id>.json       ```    1. Children are preserved on save if their parents are preserved.    1. Related objects such as collections, Ephemera Terms, etc. will not be       packaged inside the preserved object. If it's important they be preserved,       those objects should be marked with the `cloud` preservation policy.    1. When a FileSet is added to a resource which is already complete and marked       with the `cloud` preservation policy, it will upload the new binary       content to both the repository and to Google Cloud Storage.    1. If a child is marked to be preserved, but its parent is not, it will still       save in a nested directory structure, but will not automatically create       backups of its parents.    1. This behavior will be attached to the ChangeSetPersister. 1. Packaging Details    1. When preserved a `PreservationObject` will be created in Figgy with a       `preserved_object_id` property which points to the object it's preserving.    1. Each `PreservationObject` will contain `FileMetadata` for the binary       object as well as a serialized JSON file of the resource it's preserving.       On upload to preservation, those items' checksums will be calculated and       stored on the `PreservationObject`.    1. JSON metadata will have the use `pcdm:PreservedMetadata` and binary       content will have the use `pcdm:PreservationCopy`    1. We will only keep the most recent version of any file, overwriting any       files which match the same file name, but relying on versioning to go back       if necessary.    1. When a preserved resource is deleted, we will delete its directory from       preservation storage. If we need to get it again, we will look at Google       Cloud Storage's stored versions.    1. If a child's hierarchy changes (it moves parents), we will move the       content in the preservation storage to match.    1. When a file's binary content is replaced on disk, we will upload a new       copy of the file to preservation and calculate a new checksum. 2. Fixity Checks    1. Technical details of fixity checking will occur in a later ADR.    1. A random subset of the preserved copies will have their files pulled down       from preservation storage, their checksums calculated as they're streamed,       and then compared to the checksum of the object stored locally.    1. In the case of a failure it will be reported to Figgy and displayed in a       dashboard for further follow-up and repair.  ## Consequences  1. Structure    1. The resultant structure will not be in a format that is expected by       outside vendors. However, we have a BagIt packager for those use cases,       and this structure can be easily converted to a BagIt bag by creating       manifests using the checksums in the metadata files. 1. ""State at Time of Preservation""    1. As we are not preserving ""related"" resources as part of the resource, we       are not preserving the values of controlled vocabularies at time of       preservation. As of now, we do not have this use case, and are more       concerned with our material not being lost in the event of a technical       failure. 1. Storage Format    1. Storing items in individual files means we will be unlikely to move to       another cloud storage with a delay on reads, like AWS Glacier. 1. Finding a child resource    1. Storing in a nested structure means if somebody needs to find a child       resource we need information about its parent in order to find it. We       expect this to not be a problem - requests are often ""can I get page 6 of       X"", not ""can I get the file with this ID."" However, if necessary we can       iterate over the file listing in cloud storage to find it. 1. Versioning    1. Versioning everything may mean keeping copies of things that are never       used, and wasting space. We don't expect this to be a big problem as files       don't move around a lot post-complete. If it is, we can re-evaluate our       versioning strategy. ",governance_and_process
ADR_113,https://github.com/xebia-france/xebikart-infra.git,doc/adr/002-use-rabbitmq-with-mqtt-plugin-to-make-devices-communicate-with-each-other.md,Use RabbitMQ with MQTT plugin as message broker for devices so they can communicate with each other,"# Use RabbitMQ with MQTT plugin as message broker for devices so they can communicate with each other  * Status: Accepted - Date: 2019-03-11 - Deciders:     - achotard     - blacroix     - jmartinsanchez  ## Context and Problem Statement  We want a message broker so the devices and other applications can communicate with the backend.  What broker and protocol should we use?  ## Decision Drivers <!-- optional -->  - Applicability regarding IoT projects : low-resources clients, etc - Possibility to use it to stream frames/images coming from cars cameras - Ease of deployment on Kubernetes - Existing knowledge of the team  ## Considered Options  - [RabbitMQ](https://www.rabbitmq.com/), optionally with [MQTT plugin](https://www.rabbitmq.com/mqtt.html) - [VerneMQ](https://vernemq.com/) - Non-MQTT brokers     - [Kafka](https://kafka.apache.org/)     - [NATS](https://nats.io/)     - [AWS Kinesis](https://aws.amazon.com/kinesis/)     - [Google Cloud Pub/Sub](https://cloud.google.com/pubsub/)  ## Decision Outcome  Chosen option: **[RabbitMQ](TODO) with [MQTT plugin](https://www.rabbitmq.com/mqtt.html)**, because:  - It is already well-known among the team - It has some [existing ""official"" Helm chart](https://github.com/helm/charts/tree/master/stable/rabbitmq) - It seems like a good fit to iterate fast  We **do not exclude switching to another MQTT broker such as VerneMQ in the future**, depending on our ability to dsitribute it cleanly on Kubernetes.  We also **do not exclude using another broker such as Kafka or NATS for appropriate use cases**.  ## Pros and Cons of the Options <!-- optional -->  ### RabbitMQ  Pros:  - Existing [""official"" Helm chart](https://github.com/helm/charts/tree/master/stable/rabbitmq) ready to be deployed on Kubernetes - Another [""official"" Helm chart supporting High   Availability](https://github.com/helm/charts/tree/master/stable/rabbitmq-ha/)   RabbitMQ clusters on Kubernetes - Official [MQTT plugin](https://www.rabbitmq.com/mqtt.html) supported in core distribution  Cons:  - Too obvious :D - Doesn't scale as well as alternatives  ### VerneMQ  Pros:  - Is the most performant MQTT broker out there - Is known to be scalable - Is pretty cool  Cons:  - No ""official"" Helm chart ready to be deployed on Kubernetes - Not existing knowledge about it  ### Non-MQTT brokers  Non-MQTT message brokers such as NATS, Kafka, AWS Kinesis and Google Cloud Pub/Sub were quickly put aside due to their lack of support for the MQTT protocol.  **Managed services** such as Kinesis or Pub/Sub are also eliminated because they would lock us with a given provider and we want to be able to host the entire XebiKart infrastructure anywhere (envisionning going multi-cloud). This is also the reason behind the lack of consideration for things such as **AWS IoT** or equivalent MQTT managed solutions.  **Kafka** didn't look like a good candidate for our use-case (also not supporting MQTT), as well as native NATS even if this one is the most ready to be distributed on Kubernetes. **We are still considering both for a later stage** when we will be deeper in the project and when we will be able to split brokers according to their usage. ",technology_choice
ADR_114,https://github.com/Alfresco/SearchServices.git,search-services/alfresco-search/doc/architecture/decisions/0008-content-store-replication.md,1. (Solr)ContentStore Replication,"# 1. (Solr)ContentStore Replication  Date: 2019-24-09  ## Status Accepted  ## Context Solr Index Replication distributes a copy of a master index to one or more slave servers.  The master server is in charge to manage updates to the index while all querying is handled by the slaves.  This division of labor enables Solr to scale to provide adequate responsiveness to queries against large search volumes.          ![Solr Replication](https://lucene.apache.org/solr/guide/6_6/images/index-replication/worddav2b7e14725d898b4104cdd9c502fc77cd.png)  _(Source: Apache Solr Reference Guide)_        SearchServices, being a set of extensions written on top of Apache Solr, leverages this mechanism for replicating the index.  However, there's an external part of data which is managed out of the plain Lucene index: the ContentStore.        A ContentStore is composed by several files, specifically one for each indexed document; each of them is a SolrInputDocument instance stored on filesystem using a  serialized and compressed form.     ![ContentStore - Filesystem view](images/filesystem.png)  The ContentStore is where SearchServices maintains the input data in its original form, before entering in Solr and therefore before  applying any text analysis. This source of data is needed for managing some search-related capabilities like:   - fingerprinting - highlighting - clustering  - stored (i.e. original) field value retrieval at query time   In a master/slave(s) infrastructure, the ContentStore must be managed on the master and it must be replicated on each slaves.     Unfortunately, the default built-in Solr replication mechanism considers only:   - the Lucene index datafiles - the configuration files under the core ""conf"" folder (e.g. schema.xml. stopwords.txt)   As consequence of that, if we use the default Solr ReplicationHandler the ContentStore won't be replicated on slaves.    ### How the built-in Solr ReplicationHandler works The index replication mechanism is provided by org.apache.solr.handler.ReplicationHandler.  The component is quite complex because it contains the logic needed on both sides (master and slave) and therefore plays a different workflow depending on the role of the Solr node:  - **MASTER**: the node is responsible to manage the index as consequence of deletes, adds, updates. The replication handler doesn't play an active role here; it just takes care about incrementing the index version, other than providing the required endpoints for retrieving the index datafiles  - **SLAVE**:  the node maintains a local index version and (when the replication is enabled) periodically polls the master in order to detect index changes that need to be replicated - **REPEATER**: a node that acts at the same time as a MASTER and SLAVE. It is used as an intermediate node for replicating the index when slaves are on a different datacenter    The _CommitVersionInfo_, which drives the replication process, is composed by two different values:   - **index version**: a number indicating the index version; it is timestamp-based and it changes every time an update (i.e. add, delete) is applied to the index - **generation**: a lucene index is composed by several segments. Each of them has a progressive identification number called ""generation"". The one with the largest generation is the active one; from a replication perspective, the generation number doesn't actually indicate the ""active"" segment, but instead the current 'replicateable' one.  #### Available services There is only one single endpoint (/replication) provided by the ReplicationHandler.     Then, through a ""command"" request parameter we can indicate the logical ""endpoint"" we want to execute.     The available commands are listed in the following table; an additional column reports if the customisation we are going to implement has some impact on it.   The following table lists the available sharding methods and the associated mnemonic codes (i.e. the value of the ""shard_method"" attribute we need to configure in solrcore.properties).  |Command|Description| |----|-----| |indexversion|Returns the version (index version + generation) of the latest replicatable index on the specified master or slave.| |filecontent|Returns the stream reference for a given file. It is used for replicating a file that needs to be synched. | |filelist|Returns the list of index files associated with a given generation and all configuration files included in the replication | |backup|An asynchronous call, which will backup data from the latest index commit point.| |restore|Restores an index snapshot into the current core. | |status|Backup status| |restorestatus|Restore status| |deletebackup|Deletes an index snapshot previously created. | |fetchindex|Forces the specified slave to fetch a copy of the index from its master.| |enablepoll|(Slave only) enables the polling on a slave node| |disablepoll|(Slave only) disables the polling on a slave node| |enablereplication|(Master only) Enables replication on the master for all its slaves.| |disablereplication|(Master only) Disables replication on the master for all its slaves.| |details|Returns the replication status and the current node configuration| |commits|Gets the commit points for the index.|  #### Workflow  As briefly mentioned, the runtime logic of the ReplicationHandler depends on the role of the hosting Solr node.  ##### Master  When the node is a master, the main responsibilities, from a replication perspective, are:   * **update the index version and generation**: The index version is the actual trigger of the replication mechanism. The index version is incremented / updated each time a change (or a set of changes) is applied to the index, while the generation follows a similar path but it directly maintained at low level by Lucene.   * **expose the relevant endpoints** needed and called by the slaves for replicating the data.   ##### Slave   In the ""Slave"" mode, other than providing more or less the same endpoints seen above (with a slightly different semantic, indeed), the main difference is the ""active"" role played by a background thread which, simplifying, performs the following operations:   * get the latest index version and generation (command=indexversion) from the master and from the slave  * if the versions are the same then no replication happens, otherwise  * a temporary directory which will hold the downloaded data is created (index.<timestamp>)  * the list of files to be replicated is fetched (command=filelist); this includes datafiles and configuration files  * for each file that needs to be replicated retrieve and download the content (command=filecontent)  Internally, the set of downloaded data files could change, because Solr could decide to download only a part of the index or a full copy.   ## Decision The SearchServices replication mechanism needs to include also the ContentStore as part of the synchronization workflow. In order to do that, the additional behaviour requires a customisation of the workflow provided by the default Solr ReplicationHandler, as explained below.   ### Alfresco Replication Handler Design The Alfresco ReplicationHandler is, as the name suggests, a customisation of the default Solr ReplicationHandler described above.  The customisation injects some additional logic on both side of the replication mechanism in order to efficiently replicate the content store.     #### Master The master maintains, for each core, a versioning history of the owned data which consists of:  - the underlying Lucene index - configuration files (if they have to be versioned/replicated) - content store (if the content store replication is enabled)       The master replication handler configuration will declare the new ReplicationHandler class in the solrconfig.xml and it will have an additional attribute  which indicates the absolute path of the content store root folder.  The ""version"" number associated with a set of changes is incremented after a persistent change (i.e. hard commit) in the Lucene index.     Note this behaviour is already implemented in the default ReplicationHandler, so that means we will inject the additional logic required for the content store as part of such mechanism.     Specifically, we will use the version number (aka indexversion*) also for storing content store changes.     The content store changes will be stored in a separate Lucene Index, with must respect the following requirements:  - it must be present only on master nodes  - it must be initialised on-demand, even from scratch, in case of absence   - it must associate the indexversion to a set of content store changes (deletes and adds) - it must answer efficiently to the following query: given two index versions S and M, where S < M, give me all documents (i.e. content store changes) between S and M, sorted by commit timestamp asc   The ideal candidate class where this index should be facaded is SolrContentStore. The class, other than dealing with filesystem changes, would also manage the Lucene index by executing RW operations. The SolrContentStore would execute and collect all changes that will be flushed in a new document of that internal index when the CommitTracker executes a commit.     The existing endpoints on the master side described in SEARCH-1838 would be slightly changed in the following way:      |Command|Notes|Impact| |----|----|----| |indexversion|Returns the version (index version + generation) of the latest replicatable index on the specified master or slave.|NO| |filecontent|The command itself won't change. The difference will be in the additional content store stream handler|YES| |filelist|There will be an additional ""content store"" section which would list the content store changes that must be synchronized. Specifically, the ""deletes"" section will be a list of items, while the ""upserts"" would be a virtual file (something like contentstore://blalbalba) that corresponds on the master side to the optimized stream used for transferring upserts (new adds or updates)   |YES| |backup| |NO*| |restore| | NO* | |status| | NO*| |restorestatus| |NO*| |deletebackup| |NO*| |fetchindex|We will include also a configurable option where the fetch will include also the content store.  |YES| |enablepoll| |NO| |disablepoll| |NO| |enablereplication| |NO| |disablereplication| |NO| |details|We will include some detail about the content store.|NO| |commits| |NO|  The content store is an Alfresco concept and it is currently manually backed up.    In terms of design we would want it to the backup as well; however, the design described in this document is not related with that set of functionalities;  although the backup management consists of a set of services provided by the ReplicationHandler, we believe it's better to create a dedicated issue that will focus on that complex aspect.          #### Slave     The slave configuration (from the Alfresco replication perspective) would be exactly the same on slave nodes. Other than providing the same capabilities of the built-in handler, it will also indicate the absolute path of the content store root folder.  The workflow on the slave node would follow the existing path:   * a slave should persist the latest applied content store version somewhere (e.g. a file called .version). If that version doesn't exist then the slave needs to get the whole content store.  * at a scheduled intervals (see the ""pollInterval"" above) the slave polls the master using the ""indexversion"" command, in order to get the latest ""replicateable"" version of the master data  * if the version on the slave is equal to the version on the master then nothing happens  * if the slave version is lesser than the master version, then the replication starts**  * the slave issues a ""filelist"" commands for retrieving the list of changes (i.e. index changes, configuration files changes and content store changes)  * the slave asks and downloads index files  * the slave asks and downloads configuration files  * the slave downloads and applies the content store adds   * the slave executes the deletes on the content store  * the content store version number is updated in the slave     _* the Solr replication handler actually uses 2 numbers for indicating the version: the first is called ""indexversion"" and it is mainly responsible for triggering the replication process. The second is the Lucene generation, that is: the number of the last active segment in the lucene index. For our purposes, we can ignore the generation and use only the indexversion._        ** _if the master version is lesser than the slave version, then something strange happened between the two indexes: the only thing we can do is to replicate the whole content store (the same thing happens with the Lucene index)_      ## Consequences The new replication mechanism will be able to properly synchronize, from master to slave(s), the whole logical ""index"" belonging to  the SearchServices master node. As explained above, that includes:  - the Lucene index datafiles  - the configuration files under the ""conf"" folder - the ContentStore  ",governance_and_process
ADR_115,https://github.com/hmcts/cnp-design-documentation.git,doc/adr/0012-pipeline-metrics.md,Pipeline Metrics,"# Pipeline Metrics  ## Status  Proposed  ## Context  To determine the success and health of the pipeline, the following metrics will be used.  ## Key Metrics  ### Deployment Stability  Tracks how effective the Pipeline is in supporting recovery from issues via deployment of remediating changes. This value is a factor of the pipeline's entire processing time and should be kept as low as possible whilst maintaining trust.  A combination of:  * Production Deployment Failure Rate   * Track the number of production changes which require remediation   * Sourced from issue system at HMCTS (what is this?) * Production Recovery Time   * The time it takes to remediate a Production a failed change   * Sourced from issue system at HMCTS (what is this?)  ![Deployment Stability](../../img/deploy-stability.png)  ### Deployment Throughput  Tracks how effective the pipeline is at supporting frequent releases (and therefore reduction of unreleased code and increased ability to react to change. This value is an indicator of the ability to deploy to production frequently. It should be kept low and ideally less than a day.  A combination of:  * Deployment Lead Time   * Time from code landing on the non-live blue/green and the switch to release   * Source: deploy to non-live and blue/green switch from jenkins (or wherever we trigger this) * Deployment Interval   * Time between blue/green switch releases   * Sourced from the tool making the blue/green switch  ![Deployment Throughput](../../img/deploy-throughput.png)  ### Build Stability  Indicates the team's ability and diligence of maintaining the application in a “potentially deployable” state from the master branch - a core tenant of Continuous Delivery. This value should be kept low.  A combination of:  * Build Failure Rate   * Percentage of builds that fail to reach deployment to non-live (due to compilation, testing, any checks in the pipeline)   * Sourced from Jenkins * Build Failure Recovery Time   * Average time between a failed build and the next successful build that deploys to pre-release   * Sourced from Jenkins  ![Build Stability](../../img/build-stability.png)  ### Build Throughput  Indicates the frequency of change in an application.  In an actively developed application this value should be relatively high - frequent changes being built and deployed to pre-release - at least once or twice a day. As artefacts aren't used, we'll approximate to the deploy to non-live of blue/green.  A combination of:  * Build Lead Time   * Time from “merge to master” to deploy to non-live, i.e. before blue/green switch   * Sourced from Jenkins * Build Interval   * Time between deploys to pre-release   * Sourced from Jenkins  ![Build Throughput](../../img/build-throughput.png)  ### Mainline (master branch) Throughput  Tracks the rate at which code is committed to master - an indicator of Continuous Integration and whether code is hanging around in unmerged branches.  A combination of:  * Mainline Lead Time   * Time between a commit and its merge to master   * Sourced from github * Mainline Interval   * Average between master commits   * Sourced from Github  ![Mainline Throughput](../../img/mainline-throughput.png) ",infrastructure_and_deployment
ADR_116,https://github.com/cloudfoundry/cloud_controller_ng.git,decisions/0009-services-orphan-mitigation.md,Context:,"# Context:  The aim of this document is to record how Orphan Mitigation (OM) for service instances and service bindings is implemented  and the reasoning behind the decisions taken.  [OSBAPI](https://github.com/openservicebrokerapi/servicebroker/blob/v2.15/spec.md#orphan-mitigation) defines  Orphan Mitigation as the attempt(s) made by the platform (Cloud Controller) to clear up any resources that may have been  created by a Service Broker during an operation that eventually failed. Consequences of having lingering resources in the  broker may include higher costs, resource quota consumption, etc.  In this scenario, one of two things can happen: 1. The platform has no record of such resources.     In this case there is no way for the platform to list failed operations of resources that are not tracked by it.     As a result, it is possible the operator ignores such resources may have been created.     Even if the operator realizes the failed operation actually created some resources in the service broker,     destroying such resources would include direct interaction with the service broker as the platform does not     provide tools to delete resources it doesn't track.     1.  The platform still keeps a record of failed service resources.      In this case the platform can choose whether to perform OM or defer the choice of when to clean up failed resources      to the operator. Deferring to the operator can be beneficial as it allows for more troubleshooting.  As of this document, the Cloud Controller aims to comply with the OSBAPI 2.15 specification.  The specification states the platform may choose to leave the decision of when OM happens to the operator in case they  need to troubleshoot for any scenario. So whether Cloud Foundry performs OM in the scenarios OSBAPI outlines as requiring  it or not is our choice, as long as there is another mechanism in the platform that the operator can use to remove failed  resources at a later stage (i.e. `DELETE /v3/service_instances` or `DELETE /v3/service_credential_bindings`).  Cloud Foundry shouldn’t perform OM in the scenarios OSBAPI says it is not needed.  ## V2  v2 is not fully compliant with OSBAPI 2.15 version regarding OM. Changes needed to be compliant are not backwards compatible  so it is difficult to introduce them without releasing a major version. The OM spec changed on OSBAPI v2.15 and thus, even  when v2 was made compatible with that same version, not all scenarios behave the same. Also, in some scenarios the choice  was made to give the opportunity to the operator to troubleshoot and not automatically mitigate orphan resources. In other  scenarios the choice was taken not to implement complicated OM logic in places where commands where available to delete  resources and so clean up was deferred to the user.  There is also [this document](https://docs.google.com/document/d/11iXxAciCIQpCvrnzmGoEqQIbIVxpn6VDYlm_SVuq9TU/edit?usp=sharing)  that specifies what CC is doing in v2 and what it should be doing that has been shared with the community.  Although effort has been put over time to keep it accurate, it is not 100% a reflection of what V2 really does.  ## V3  In v3 API all operations that require broker communication are done asynchronously. This is one of the main differences  from v2 API. As a result of this approach CF always has a record of the resource that is being created or deleted.  When the broker operation fails, CF still keeps a record of the resource e.g. if a create binding operation fails the CF will have keep a record of that resource and set the state to `create failed`. This allows the operator to manually  remove the failed resource and it means that there should not be any resources that the service broker has created and  CF does not have a record of.  However, we have chosen to keep performing orphan mitigation in many cases in order to keep some level of consistency  with the expected behaviour in v2 and to satisfy requirements and issues raised by users.   # Provisioning  ## Scenarios when CF will perform OM:  Status code | Response Body |  OSBAPI advices OM |Notes :------------:| --------------|:--------------:| -------- 201 | malformed |  Yes | 202 | malformed | No | If this happens, CF would not be able to record the broker response that might include important properties for continuing the async flow (e.g. operation_id). 2xx | - |  Yes | 422 | unexpected error | No | No resource should have been created, however attempting OM does not have any risks. 5xx | - | Yes | Client Timeout | - | Yes |  ## Scenarios when CF will NOT perform OM:  Status code | Response Body |  OSBAPI advices OM |Notes :------------:| --------------|:--------------:| -------- 200 | Malformed | No | 201 | Other (not malformed) | No | 202 | Other (not malformed) | No | 422 | Requires app/Async/Concurrency error | No | 4xx | - | No |  # Binding   ## Scenarios when CF will perform OM:  Status code | Response Body |  OSBAPI advices OM |Notes :------------:| --------------|:--------------:| --------  200 | bad data | No | If this happens, CF would not be able to record the broker response. Safest assumption is to delete the resource from the broker and allow the operator to start over.  201 | malformed |  Yes |  201 | bad data | No | If this happens, CF would not be able to record the broker response. Safest assumption is to delete the resource from the broker and allow the operator to start over.  202 | malformed | No | If this happens, CF would not be able to record the broker response that might include important properties for continuing the async flow (e.g. operation_id).  2xx | - |  Yes |  410 | - | No | This is not a valid error code for a `POST` request. No resource should have been created, however attempting OM does not have any risks.  422 | unexpected error | No | No resource should have been created, however attempting OM does not have any risks.  5xx | - | Yes |  Client Timeout | - | Yes |  ## Scenarios when CF will NOT perform OM:  Status code | Response Body |  OSBAPI advices OM |Notes :------------:| --------------|:--------------:| --------  200 | Malformed | No |  201 | Other (not Malformed or Bad data) | No |  202 | Other (not Malformed or Bad data) | No |  422 | Requires app/Async/Concurrency error | No |  4xx | - | No |    In v3, in the case of any other CF internal error not related to the Broker response, CF does not perform OM.  Even in case of failure, there is a record of the resource in the DB and the user is able to delete the resource after failure.   # Handling service instances and bindings last operation broker responses  Cloud Foundry will not attempt any OM for any of the responses from instances or bindings Last Operation requests.  We decided to give the operator the possibility of troubleshooting and delete the resource when they see fit, in line with  [SI behaviour](https://github.com/cloudfoundry/cloud_controller_ng/issues/1842) our users depend on.  ## Deprovisioning and Unbinding  In event of failure, Cloud Foundry will keep the record of the resource and the user can attempt to delete again.  There is not a clear benefit of implementing any OM logic for such straightforward scenario.   ## Changes from v2 to v3 When possible we have kept the same OM implementation in v2 and v3. Cases when we have diverged have been documented in this doc.  In v3 all types of bindings, including service credentials bindings for apps and keys and service route bindings,  have the same OM behaviour.  # Status Accepted  # Consequences: This document is a description of our reasoning about OM and its current implementation at the time of writing.  The behaviour for each use-case might change if OSBAPI advices new behaviour or our customers request other changes. ",others
ADR_117,https://github.com/MITLibraries/timdex.git,docs/architecture-decisions/0003-follow-twelve-factor-methodology.md,3. Follow Twelve Factor methodology,"# 3. Follow Twelve Factor methodology  Date: 2018-10-16  ## Status  Accepted  ## Context  Designing modern scalable cloud based applications requires intentionally designing the architecture to take advantage of the cloud.  One leading way to do that is [The Twelve Factor](https://12factor.net) methodology.  ## Decision  We will follow Twelve Factor methodology.  ## Consequences  Our application will be deployable in the cloud in a scalable efficient manner.  We will leverage services for some aspects of applications that previously would have relied on a Virtual Machine, such as storage for files and logs. ",architectural_patterns
ADR_118,https://github.com/DFE-Digital/buy-for-your-school.git,doc/architecture/decisions/0008-use-brakeman-for-security-analysis.md,8. use-brakeman-for-security-analysis,# 8. use-brakeman-for-security-analysis  Date: 2020-04-03  ## Status  ![Accepted](https://img.shields.io/badge/adr-accepted-green)  ## Context  We need a mechanism for highlighting security vulnerabilities in our code before it reaches production environments  ## Decision  Use the [Brakeman](https://brakemanscanner.org/) static security analysis tool to find vulnerabilities in development and test  ## Consequences  - Brakeman will be run as part of CI and fail the build if there are any warnings - Brakeman can also be run in the development environment to allow developers to address issues before committing code to the repository - Brakeman will help developers learn about common vulnerabilities and develop a more defensive coding style - Use of Brakeman in development & test environments should reduce or eliminate code vulnerabilities that would be exposed in a penetration test,"security, technology_choice"
ADR_119,https://github.com/guardian/editions.git,docs/00-✅-adr.md,To record decisions as Architectural Decision Records,# To record decisions as Architectural Decision Records  ## Status: Busy  ## Context  We need to document our decisions on this new project.  ## Decision  To write ADRs for all major technical decisions. Naming each with a monotonically increasing counter.  Rejected ADRs will have -🚯- after the counter.  Accepted ADRS will have -✅- after the counter.  ## Consequences  🎉🎉🎉,governance_and_process
ADR_120,https://github.com/zooniverse/front-end-monorepo.git,docs/arch/adr-25.md,ADR 25: Drawing Sub-task,"# ADR 25: Drawing Sub-task   Created: January 6, 2020 Updated: April 30, 2020  ## Context  A drawing mark's sub-task is designed to support volunteers answering additional questions for each drawing mark annotation. It allows single choice or multiple choice question task, text task, dropdown task, and slider task.  ### Annotation JSON structure  The current sub-task annotation JSON structure is:  ```json // a point with sub-task consisting of a question task and a dropdown task {   ""annotations"": [     {       ""task"": ""T0"",       ""value"": [         {           ""frame"": 0,           ""tool"": 0,           ""x"": 452.18341064453125,           ""y"": 202.87478637695312,           ""details"": [             {""value"": 0},             {""value"": [               {""value"": ""option-1""},               {""value"": ""option-2""},               {""value"": null}             ]}           ]         },         {           ""frame"": 0,           ""tool"": 0,           ""x"": 374.23454574576868,           ""y"": 455.23453656547428,           ""details"": [             {""value"": 1},             {""value"": [               {""value"": ""option-3""},               {""value"": ""option-4""},               {""value"": ""option-5""}             ]}           ]         },         {           ""frame"": 0,           ""tool"": 1,           ""x"": 404.61279296875,           ""y"": 583.4398803710938,           ""details"": [             {""value"": 1},             {""value"": [               {""value"": ""option-3""},               {""value"": ""option-4""},               {""value"": ""option-5""}             ]}           ]         }       ]     }   ] } ```  The annotation structure for the sub-task, under `details`, has a few issues because it solely relies on an array index to relate back to the original sub-task. This makes it difficult to make downstream analysis and aggregation scripts. The aggregation code now has to parse the details array and make a ""mock annotation"" of the correct structure to be passed along to the next reducer.  ### Sub-task UI  The sub-task UI positioned itself fixed below relative to the position of the mark. Notably transcription projects have commented that this interferes with being able to transcribe successfully since the dialog may cover up part of the subject and cannot be moved without moving the drawing mark.  ## Decision  For initial support, we will support the single and multiple choice question tasks and the text task in the sub-task. Slider task may be deprecated and dropdown task may be changing in ways we do not have a plan for yet, so they can be supported later if it makes sense to add them.  ### Annotation JSON structure  The annotations in the details array will be updated to be an object that just contains a reference to the sub-task's unique identifier. The task annotation itself will be stored in the classification's annotations array flattened.   The main benefit of this reorganization will be with downstream analysis and aggregation. When aggregating drawn shapes the first step is clustering. Once the clusters are found the subtasks need to be aggregated within each cluster. This will be easier to do if the structure of each subtask annotation is the same as if that task was asked on its own. The code can just take all subtask annotations within a cluster and just pass it to the reducer as if it is a list of main task annotations without having to reshape them.  An addition of `markIndex` is being added for the sub-task annotations for the purpose of having an identifier relating it back to the parent drawing task annotation value array which represents marks.   An example of the new sub-task annotation JSON structure at classification submission:  ```json {   ""annotations"": [     {       ""task"": ""T0"",       ""taskType"": ""drawing"",       ""value"": [         {           ""frame"": 0,           ""toolIndex"": 0,           ""toolType"": ""point"",           ""x"": 452.18341064453125,           ""y"": 202.87478637695312,           ""details"": [             {""task"": ""T0.0.0""},             {""task"": ""T0.0.1""}           ]         },         {           ""frame"": 0,           ""toolIndex"": 0,           ""toolType"": ""point"",           ""x"": 374.23454574576868,           ""y"": 455.23453656547428,           ""details"": [             {""task"": ""T0.0.0""},             {""task"": ""T0.0.1""}           ]         },         {           ""frame"": 0,           ""toolIndex"": 1,           ""toolType"": ""point"",           ""x"": 404.61279296875,           ""y"": 583.4398803710938,           ""details"": [             {""task"": ""T0.1.0""},             {""task"": ""T0.1.1""}           ]         }       ]     },     {       ""task"": ""T0.0.0"",       ""taskType"": ""single"",       ""markIndex"": 0,       ""value"": 0     },     {       ""task"": ""T0.0.1"",       ""taskType"": ""dropdown"",       ""markIndex"": 0,       ""value"": [         {""value"": ""option-1""},         {""value"": ""option-2""},         {""value"": null}       ]     },     {       ""task"": ""T0.0.0"",       ""taskType"": ""single"",       ""markIndex"": 1,       ""value"": 1     },     {       ""task"": ""T0.0.1"",       ""taskType"": ""dropdown"",       ""markIndex"": 1,       ""value"": [         {""value"": ""option-3""},         {""value"": ""option-4""},         {""value"": ""option-5""}       ]     },     {       ""task"": ""T0.1.0"",       ""markIndex"": 2,       ""taskType"": ""single"",       ""value"": 1     },     {       ""task"": ""T0.1.1"",       ""markIndex"": 2,       ""taskType"": ""dropdown"",       ""value"": [         {""value"": ""option-3""},         {""value"": ""option-4""},         {""value"": ""option-5""}       ]     }   ],   ""metadata"": {     ""classifier_version"": ""2.0""   } } ```  The sub-task identifiers follow a convention of `TASK_KEY.TOOL_INDEX.DETAILS_INDEX`.  Note that this is the structure at classification submission. The classifier's internal store models may have differences for the purposes of keeping track of in-progress annotations and marks being made.  ### Drawing sub-task UI  The UI will change to adopt the design of Anti-Slavery Manuscripts (ASM) with this [generalized design](https://projects.invisionapp.com/d/main#/console/12923997/396381420/preview). It will be a pseudo-modal, but with a few notable differences from a true modal:  - The initial position will be near the associated mark made - Interactions will be allowed with the image toolbar to allow zoom, rotate, as well as opening of the tutorial, field guide, and task help. Submission of the classification should not be allowed.   - If the sub-tasks are required, the modal should not be closeable until the required annotations are made or the mark is deleted if cancelled - The dialog can be moved and resized  To support movability and resizing, we will leverage [react-rnd](https://github.com/bokuweb/react-rnd) which is the same library ASM used. Grommet's `Layer` cannot be used since it is intended for actual modal or side panel use and cannot be arbitrarily positioned or moved.  ## Status  Accepted  ## Consequences  Flattening the annotations array for drawing task sub-tasks is conceptually consistent with the move to using workflow steps to flatten the combo task. It is however a breaking change and this change will have to be communicated to project builders. As with other classifications from the new classifier, this can be checked by the presence of `classifier_version: 2.0` in the classification metadata. In the future, we would also like to include a link to JSON schema for each annotation type as recommended in [ADR 07](adr-07.md).  Flattening also has added benefits for Panoptes when generating classification exports. It can parse through a flattened array to convert machine readable strings to human readable strings for each task without having to check for values in a nested `details` array and then traverse it.   In the raw classification export, this also benefits researchers that want to analyze the outputted CSV directly and prefer a flatter JSON structure. Flat structures facilitate research teams being able to load data without JSON-based manipulation. There are many teams who would benefit from the ability to read-in a CSV to Excel and start analyzing their results, as opposed to needing to first parse JSON. There will still be some JSON structure in CSV exports, but this will contribute toward minimizing it.   The transcription task is a automatically configured drawing task with a sub-task and will be using a new variant of a text task that includes auto-suggestions from caesar reductions. Its sub-task should use the suggested changes from this ADR as well.  ### Aggregation   The aggregation for caesar code needed to be updated to accommodate the new annotation structure for sub-tasks. PR [289](https://github.com/zooniverse/aggregation-for-caesar/pull/289) implements these changes. Here is a sample extractor and reducer using the new code:  #### Setting up the extractor To set up and extractor you need to URL encode the keywords  ```json {     ""task"": ""T0"",     ""shape"": ""point"",     ""details"": {         ""T0_toolIndex0_subtask0"": ""question_extractor"",         ""T0_toolIndex0_subtask1"": ""dropdown_extractor"",         ""T0_toolIndex1_subtask0"": ""question_extractor"",         ""T0_toolIndex1_subtask1"": ""dropdown_extractor""     } } ```  and that looks like `https://aggregation-caesar.zooniverse.org/extractors/shape_extractor?task=T0&shape=point&details=%7B%27T0_toolIndex0_subtask0%27%3A+%27question_extractor%27%2C+%27T0_toolIndex0_subtask1%27%3A+%27dropdown_extractor%27%2C+%27T0_toolIndex1_subtask0%27%3A+%27question_extractor%27%2C+%27T0_toolIndex1_subtask1%27%3A+%27dropdown_extractor%27%7D`  although I expect the decoded URL would also work (not tested) `https://aggregation-caesar.zooniverse.org/extractors/shape_extractor?task=T0&shape=point&details={'T0_toolIndex0_subtask0':+'question_extractor',+'T0_toolIndex0_subtask1':+'dropdown_extractor',+'T0_toolIndex1_subtask0':+'question_extractor',+'T0_toolIndex1_subtask1':+'dropdown_extractor'}`  These keywords define the task ID, the shape used for the drawing tool, and the extractors to use for each of the subtasks. In this example there are two point tools on task `T0` and they each have a question subtask followed by a dropdown subtask.  Any subtasks not explicitly defined in this `details` keyword are ignored an will not be extracted.  #### Setting up the reducer  The reducer's URL prams also have the `details` section in the same format as the extractor. As an example for the dbscan reducer the keywords would look like (using default cluster params):  ``` json {     ""shape"": ""point"",     ""details"": {         ""T0_toolIndex0_subtask0"": ""question_reducer"",         ""T0_toolIndex0_subtask1"": ""dropdown_reducer"",         ""T0_toolIndex1_subtask0"": ""question_reducer"",         ""T0_toolIndex1_subtask1"": ""dropdown_reducer""     } } ```  The encoded URL would be `https://aggregation-caesar.zooniverse.org/reducers/shape_reducer_dbscan?shape=point&details=%7B%27T0_toolIndex0_subtask0%27%3A+%27question_reducer%27%2C+%27T0_toolIndex0_subtask1%27%3A+%27dropdown_reducer%27%2C+%27T0_toolIndex1_subtask0%27%3A+%27question_reducer%27%2C+%27T0_toolIndex1_subtask1%27%3A+%27dropdown_reducer%27%7D`  or the decoded URL `https://aggregation-caesar.zooniverse.org/reducers/shape_reducer_dbscan?shape=point&details={'T0_toolIndex0_subtask0':+'question_reducer',+'T0_toolIndex0_subtask1':+'dropdown_reducer',+'T0_toolIndex1_subtask0':+'question_reducer',+'T0_toolIndex1_subtask1':+'dropdown_reducer'}` ",others
ADR_121,https://github.com/embvm/embvm-core.git,docs/architecture/decisions/0019-virtual-platform-takes-in-thwplatform-type.md,19. Virtual Platform Takes in THWPlatform Type,"# 19. Virtual Platform Takes in THWPlatform Type  Date: 2020-10-20  ## Status  Accepted  Caused by [18. Driver Registration in HW Platform](0018-driver-registration-in-hw-platform.md)  ## Context  As a consequence of [ADR 0018](0018-driver-registration-in-hw-platform.md), we moved the [Driver Registry](../components/core/driver_registry.md) definition to the hardware platform and removed the global singleton from the platform. We also want the platform APIs to forward to the hw platform. However, we needed a way to access the hw platform object for successful forwarding. This requires the platform base class to know about the type.  ## Decision  The hardware platform type is now a template parameter for the Virtual Platform base class. A local variable will be declared (`hw_platform_`), and a `protected` API will be provided to access that variable as well.  ## Consequences  - Users do not have control over how the hardware platform variable is named - Users may not know that they need to access the hardware platform through specific variables/functions: now there is an education challenge     + We can show example code to mitigate this - Only one hardware platform can be used with a platform, which would *potentially* break board ID and board revision selection for instantiating one of many hardware platforms.      + However, this can be handled in the hardware platform logic itself, if multiple revisions need to be supported.  ## Related Documents  Please see the [associated development log](../../development/logs/20201020_driver_registry_redesign.md) for detailed information about the changes that were made as part of this effort. ",others
ADR_122,https://github.com/Informatievlaanderen/publicservice-registry.git,docs/adr/0004-sqlstreamstore.md,4. Use `SqlStreamStore`,"# 4. Use `SqlStreamStore`  Date: 2017-09-12  ## Status  Accepted  ## Context  Since we decided to use event sourcing, we need a way to store events in our database.  In `Wegwijs` we stored events in `MSSQL`, which allows easy debugging of events. All sql statements to save/read events were hand-written.  **However**, since we decided on async event handlers in a previous ADR, we would benefit a lot from having catch-up subscriptions for our event handlers. Catch-up subscriptions allow event handlers to be in charge of what events they are interested in, and give event handlers more autonomy over their own rebuilds.  While `GetEventStore` supports this, and is most likely a top-notch choice for storing events, this would require us to take care of hosting this. We also have doubts about the support for storing business-critical data outside of `MSSQL` in `AIV`.  We currently host no VMs for business-critical concerns, and we feel that hosting `GetEventStore` ourselves, would add a significant burden.  As an alternative, `SqlStreamStore` is an OSS library on GitHub which supports storing events into `MSSQL`, and has support for catch-up subscriptions. It has an active community, and has been used in several production systems successfully according to that community.  ## Decision  We will use the `SqlStreamStore` library as our event store. We will keep an eye on ongoing developments from `SqlStreamStore`.  ## Consequences  We will no longer have to implement saving/reading events ourselves.  We will have the option to use catch-up subscriptions, giving event handlers more autonomy.  We will need to keep an eye on `SqlStreamStore` developments, through github updates and the `#`sqlstreamstore` `channel on https://ddd-cqrs-es.slack.com.  We will be able to use constantly updated best practices from the community.  It will be harder to customize saving/reading the event store, though we don't see the need for that at this point. ",technology_choice
ADR_123,https://github.com/cfe84/roster.git,adr/general-004.password-derivation.md,Decision,# Decision  Using a simple derivation function for now  # Reason  Have something sufficiently good rapidly.,others
ADR_124,https://github.com/MITLibraries/dos-server.git,docs/adr/adr-1-metadata.md,Metadata,"# Metadata  ## Context  Digital objects need to have associated metadata for various use cases (refer to  the requirements documentation for details). Metadata of these objects can be descriptive, administrative, and structural.  To avoid ""duplication"" of descriptive metadata, it is desired that DOS not store descriptive metadata.  ## Decision  Descriptive metadata will not be stored by DOS.  ## Status  Accepted  ## Consequences  Some use cases or features (such as bulk download of objects) that were originally gathered in DOS business analysis Phase 1 may not be fulfilled by DOS. It is possible that further business analysis may need to be done for these use cases and  to see if any systems are already fulfilling that role in some capacity.",observability
ADR_125,https://github.com/commonality/archetypes-rules.git,docs/adr/adr-0001-architecture-decision-record-use-adrs.md,Architecture Decision Record: Use ADRs,"# Architecture Decision Record: Use ADRs  [![Decided on][octicon-calendar]][adr-0001] <time datetime=""2019-03-08"">2019-03-08</time>  ## Status  [![adr: accepted][adr-accepted-badge]][adr-0001]  ## Context  The **archetypes-rules** team has several explicit goals that make the practice and discipline of architecture very important:  -   We want to think deeply about all our architectural decisions, exploring all     alternatives and making a careful, considered, well-researched choice.  -   We want to be as transparent as possible in our decision-making process.  -   We don't want decisions to be made unilaterally in a vacuum. Specifically,     should **archetypes-rules** ever fall under the purview of a Technical     Steering Committee (TSC), we would want to give our TSC colleagues the     opportunity to review every major decision.  -   Despite being a geographically and temporally distributed team, we want our     contributors to have a strong shared understanding of the technical rationale     behind decisions.  -   We want to be able to revisit prior decisions to determine fairly whether they     still make sense, and if the motivating circumstances or conditions have     changed.  ## Decision  We will document every architecture-level decision for **archetypes-rules** and its core modules with an Architecture Decision Record. These are a well structured, relatively lightweight way to capture architectural proposals. They can serve as an artifact for discussion, and remain as an enduring record of the context and motivation of past decisions.  ### Our Workflow for Making and Recording Architecture Decisions[^1]  ```mermaid graph TD  subgraph ADR Workflow  title[""ℹ️  High-level flow chart showing how we distribute design authority with architecture decision records""] title-->A style title fill:#FFF,stroke:#FFF linkStyle 0 stroke:#FFF,stroke-width:0;    A((""start"")) --> B(Significant requirement, challenge, or change)   B --> C(""Create GitLab Issue (with the ADR Template)"")   C --> D(""Field Comments and Discussions"")   D --> E(""Vote (lazy consensus)"")   E --> F{""Tally 👍  👎  reactions""}   F --> |""👍 majority 'accepted'""| G(""Apply badge 'ADR accepted'"")   F --> |""👎 majority 'rejected'""| H(""Apply badge 'ADR rejected'"")   G --> I   H --> I(""Merge ADR into `master`"")   I --> J((""stop"")) end ```  1.  A contributor creates an ADR document outlining an approach for a particular     question or problem. All ADRs have an initial status of ""proposed.""     Maintainers and Trusted Committers must apply the <kbd>adr: proposed</kbd>     Issue label.  1.  We track architecture decisions on the ""[ADR (Architecture Decision     Records)][adr-issue-board]"" Issue Board.  1.  The Maintainer, Trusted Committer, Contributors, and Consumers consider the     ADR through _public_ GitLab Discussions. These GitLab Discussions must be     resolved by (lazy) consensus.  1.  During the discussion period, the ADR should be updated to reflect     additional context, concerns raised, and proposed changes. Maintainers and     Trusted Committers must apply the <kbd>adr: discussion-underway</kbd> label     the associated Issue or Pull Request.  1.  Once consensus is reached, the ADR will be marked as either an ""accepted"" or     ""rejected"". The Maintainer or Trusted Committer must likewise apply the     label ""adr: accepted"" or ""adr: rejected"" to the original issue.  1.  The Maintainer or Trusted Committer will also update the **""Status""**     section of the ADR with the badge that reflects the ADR's state.[^2] \(The     Style Guide for Images has     [""how-to"" instructions for badges](https://github.com/archetypes-rules/signatures/wikis/Style-Guides/Images).)  1.  Only after an ADR is accepted should implementing code be committed to the     `master` branch of a relevant application, library, or module.  1.  All ADRs should be merged into the `master` branch, no matter what their     ultimate status is.  1.  If a decision is revisited and a different conclusion is reached, a new ADR     should be created documenting the context and rationale for the change. The     new ADR should reference the old one, and once the new one is accepted, the     old one should (in its ""status"" section) be updated to point to the new one.     The old ADR should not be removed or otherwise modified except for the     annotation pointing to the new ADR.  ## Consequences  1.  Contributors must write an ADR and submit it for review before selecting an     approach to any architectural decision; that is, any decision that affects     the way **archetypes-rules** or an **archetypes-rules** application is put     together at a high level.  1.  We will have a concrete artifact around which to focus discussion, before     finalizing decisions.  1.  If we follow the process, decisions will be made deliberately, as a group.  1.  The master branch of our repositories will reflect the high-level consensus     of the steering group.  1.  We will have a useful persistent record of why the system is the way it is.  ## References  [^1]: Swindle, G. (2019) _Architecture Decisions · Wiki ·   archetypes-rules/signatures · GitLab_. Retrieved March 08, 2019, from   <https://github.com/archetypes-rules/signatures/wikis/Governance/Architecture-Decisions>  [^2] Swindle, G. (2019) _Images · Wiki · archetypes-rules/signatures · GitLab_.   Retrieved March 08, 2019, from   <https://github.com/archetypes-rules/signatures/wikis/Style-Guides/Images>  <!-- Do not remove this line or anything under it. -->  [adr-0001]: docs/adr/adr-0001-architecture-decision-record-use-adrs.md  [adr-issue-board]: https://github.com/archetypes-rules/signatures/boards/980468?&label_name[]=type%3A%20adr  [adr-accepted-badge]: https://flat.badgen.net/badge/ADR/accepted/44AD8E  [adr-proposed-badge]: https://flat.badgen.net/badge/ADR/proposed/AC900D  [adr-rejected-badge]: https://flat.badgen.net/badge/ADR/rejected/D9534F  [adr-deprecated-badge]: https://flat.badgen.net/badge/ADR/deprecated/7F8C8D  <!-- Octicon image definition list -->  [octicon-calendar]: https://cdnjs.cloudflare.com/ajax/libs/octicons/8.3.0/svg/calendar.svg ",governance_and_process
ADR_126,https://github.com/UKGovernmentBEIS/beis-report-official-development-assistance.git,doc/architecture/decisions/0002-use-bullet-to-catch-nplus1-queries.md,2. use-bullet-to-catch-nplus1-queries,"# 2. use-bullet-to-catch-nplus1-queries  Date: 2019-09-19  ## Status  Accepted  ## Context  It can be easy to miss an inefficient database query during code review. These can build up and have detremental performance on the application and effect the user experience.  ## Decision  Add an automatic check to the test suite to ensure (through CI) that these are fixed before being deployed.  ## Consequences  - Code reviews are depended upon less to catch this type of error, removing the risk of human error - Application response times should no longer be affected by this type of issue - Requires protected branch configuration to ensure that CI must succeed before being able to be merged ",testing_strategy
ADR_127,https://github.com/cloudfoundry-attic/copilot.git,docs/decisions/0001-record-architecture-decisions.md,1. Record architecture decisions,"# 1. Record architecture decisions  Date: 2018-07-25  ## Status  Accepted  ## Context  We need to record the architectural decisions made on this project.  ## Decision  We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).  ## Consequences  See Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's [adr-tools](https://github.com/npryce/adr-tools). ",governance_and_process
ADR_128,https://github.com/katarzyna-starachowicz/my_budgeting.git,doc/adl/0003_rails_event_store.md,3. Rails Event Storm,"# 3. Rails Event Storm  ## Status  Accepted  ## Context  As I'm learning DDD with Arkency course, I'm going to start with solution propsed by them.  ## Decision  [Rails Event Storm](https://railseventstore.org/) is a library for publishing, consuming, storing and retrieving events. According to its creators from Arkency – it's your best companion for going with an Event-Driven Architecture for your Rails application.   ## Consequences  It is going to be the base for event-driven architecture of My Budgeting app. ","architectural_patterns, technology_choice"
ADR_129,https://github.com/JulianG/bananatabs.git,doc/adr/0001-record-architecture-decisions.md,1. Record architecture decisions,"# 1. Record architecture decisions  Date: 2019-05-09  ## Status  Approved  ## Context  We need to record the architectural decisions made on this project.  ## Decision  We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).  ## Consequences  See Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's [adr-tools](https://github.com/npryce/adr-tools).  Use the `adr` command to manage ADRs.  Try running `adr help`.  ADRs are stored in a subdirectory of your project as Markdown files.  The default directory is `doc/adr`, but you can specify the directory when you initialise the ADR log.  1. Create an ADR directory in the root of your project:          adr init doc/architecture/decisions      This will create a directory named `doc/architecture/decisions'      containing the first ADR, which records that you are using ADRs     to record architectural decisions and links to      [Michael Nygard's article on the subject][ADRs].  2. Create Architecture Decision Records          adr new Implement as Unix shell scripts      This will create a new, numbered ADR file and open it in your     editor of choice (as specified by the VISUAL or EDITOR environment     variable).      To create a new ADR that supercedes a previous one (ADR 9, for example),     use the -s option.          adr new -s 9 Use Rust for performance-critical functionality      This will create a new ADR file that is flagged as superceding     ADR 9, and changes the status of ADR 9 to indicate that it is     superceded by the new ADR.  It then opens the new ADR in your     editor of choice.      3. For further information, use the built in help:          adr help ",governance_and_process
ADR_130,https://github.com/WikiWatershed/model-my-watershed.git,doc/arch/adr-002-geoprocessing-caching.md,002 - Geoprocessing Caching,"# 002 - Geoprocessing Caching  ## Context  Geoprocessing calls for large shapes can take a long time to complete. While users can draw custom shapes, they can also pick from a list of predefined shapes in the system. Geoprocessing the same shape over and over when multiple users select it is wasteful and unnecessary. By caching the geoprocessing results of predefined shapes, or Well Known Areas of Interest (WKAoIs), we can improve user experience and application performance.  Outside of geoprocessing, there are also [some](https://github.com/WikiWatershed/model-my-watershed/blob/dea12ff9a6fb234e30978e97ed1d0f6266a406c9/src/mmw/apps/modeling/calcs.py#L28) [database](https://github.com/WikiWatershed/model-my-watershed/blob/dea12ff9a6fb234e30978e97ed1d0f6266a406c9/src/mmw/apps/modeling/calcs.py#L51) [calls](https://github.com/WikiWatershed/model-my-watershed/blob/dea12ff9a6fb234e30978e97ed1d0f6266a406c9/src/mmw/apps/modeling/calcs.py#L97), but these are much faster in comparison and are not likely to see significant improvement by caching, especially since the cache could be a table in the database itself.  In this ADR we consider the following questions:   * What will be cached?  * How will it be cached?  * Where will it be cached?  * When will it be cached?  * How will the cache be invalidated?  We also sketch out an implementation plan, and consider consequences and side-effects.  ## Decisions  ### What will be cached?  The output of the geoprocessing service for WKAoIs will be cached. This is almost always a JSON blob of key-value pairs, where the key is a combination of overlaid cell values in a set of rasters, and the value is a count of such cells. The inputs are a GeoJSON shape and a [set of related arguments](https://github.com/WikiWatershed/model-my-watershed/blob/develop/src/mmw/mmw/settings/base.py#L412) that specify rasters, operation type, CRS, etc.  While we could cache the entire output of a MapShed or TR-55 run, an update to some of the constituent raster or vector data would force recalculation of the whole. By caching only the time-consuming geoprocessing results, we ensure that any updates to constituent rasters would invalidate only that specific cached result, leaving the others still current. And since vector data results would never be cached, they will always be current upon update as well.  In case of MapShed, the modifications do not change any of the geoprocessing queries, thus those requests can be cached easily. For TR-55, the modifications _do_ change the geoprocessing queries. For these cases, we will cache only the Current Conditions (modification-less) run, not other Scenarios, since storing arbitrary shapes can balloon the size of the cache very quickly. This decision may be revisited in the future.  In case we do not foresee updating the rasters very often, it may be beneficial to consider caching the entire JSON response of the API.  ### How will it be cached?  The current stack already has [Django Caching](https://github.com/WikiWatershed/model-my-watershed/blob/a49cc115d18d2c67cbd0721cf62faa08e98d13fc/src/mmw/mmw/settings/base.py#L76-L90) setup. This allows us to cache with a single line of code:  ```python from django.core.cache import cache  cache.set(key, value, None) ```  where the `key` is a unique identified consisting of WKAoI id and the geoprocessing operation, `value` is the result of the geoprocessing operation, and `None` is the timeout value which ensures the values don't ever expire.  Retrieval is as simple as:  ```python value = cache.get(key) if not value:     # Calculate and cache the value  return value ```  The `key` should be prefixed with `geop` to namespace it from other cache entries in the app, composed of the WKAoI id, which consists of a table name and an integer id, and the [geoprocessing operation name](https://github.com/WikiWatershed/model-my-watershed/blob/a49cc115d18d2c67cbd0721cf62faa08e98d13fc/src/mmw/mmw/settings/base.py#L387). For example, `geop__boundary_huc08__1__nlcd_soils`. In practice, the `key` may actually be something like `:1:geop__boundary_huc08__1__nlcd_soils`, with a configurable app-wide prefix (which defaults to `''`) and a version number are prefixed to our key. However, as long as we always access the cache via `django.core.cache.cache`, we shouldn't need to worry about that.  ### Where will it be cached?  Django Caching Framework can be configured to use a number of cache backends, including Redis and Database. It is currently setup to use Redis / [ElastiCache](https://aws.amazon.com/elasticache/redis/) in production.  The advantages of using Redis are:   * It is already configured  * It allows us to take advantage of the Django Caching Framework which is configured to use Redis  * Redis is really fast, and a good candidate for storing key/value pairs like we intend to  * It is designed to be a cache, and thus comes with mechanisms for timeout, LRU, and cache misses out of the box  The disadvantages of using Redis are:   * In the case of system failure, the cached values will be lost, and will need to be cached again  * If it purges least recently used values, it might not be a good candidate for storing hard-to-process large WKAoIs that are rarely used  ### When will it be cached?  When a user selects a WKAoI, and a request is made to `/analyze` or `/modeling`, if the WKAoI results haven't already been cached, we will run and cache them. Over time, we will build up the cache, so that when new users request the same WKAoIs, they will get the cached results. These will be cached in the Celery Tasks where calculation would otherwise happen.  For a select few WKAoIs which are too large to process in the production infrastructure, such as HUC-8s which time-out during MapShed gathering phase, we can run their geoprocessing steps on more powerful infrastructure with longer timeouts in a batch process, cache the results, and then decommission it. This will make them available to regular users of the production app without needing the extra power to render them.  For this purpose, we'll need a pair of new Django management commands. The first should run the geoprocessing steps for given WKAoIs and save the results to a file. This will be run in the super environment. The second should take a file of pre-processed results for given shapes and operations, and add them to the cache. This will be run in the production environment.  ### How will the cache be invalidated?  Since every cache entry is tagged with its type, if a certain raster is updated, we can remove all related cache entries. For example, if there are updates to `us-percent-slope-30m-epsg5070` raster, which is used in the [`nlcd_slope`](https://github.com/WikiWatershed/model-my-watershed/blob/a49cc115d18d2c67cbd0721cf62faa08e98d13fc/src/mmw/mmw/settings/base.py#L449) and [`slope`](https://github.com/WikiWatershed/model-my-watershed/blob/a49cc115d18d2c67cbd0721cf62faa08e98d13fc/src/mmw/mmw/settings/base.py#L462) requests, we could simply run  ```python cache.delete_pattern('geop__*__nlcd_slope') cache.delete_pattern('geop__*__slope') ```  To refresh a specific shape we could do  ```python cache.delete_pattern('geop__boundary_huc08__1__*') ```  These could be management commands as well.  ## Implementation Plan  1. See if TR-55 geoprocessing, currently done via [`SummaryJob`](https://github.com/WikiWatershed/mmw-geoprocessing/blob/develop/summary/src/main/scala/SummaryJob.scala), can be done via [`MapshedJob`](https://github.com/WikiWatershed/mmw-geoprocessing/blob/develop/summary/src/main/scala/MapshedJob.scala), for consistency. And if so, rewrite the geoprocessing bits to use `MapshedJob` instead. 2. Split [`nlcdSoilCensus`](https://github.com/WikiWatershed/model-my-watershed/blob/develop/src/mmw/mmw/settings/base.py#L388) into three requests: `nlcd`, `soil`, and `nlcd_soil`. The first two can be used for `/analyze` and will be much faster, while the second can be used for `/modeling/tr-55`. 3. Update the geoprocessing submodule to take GeoJSON shape or WKAoI id, and geoprocessing type, and return the output. Update the Celery Tasks or Django Views which use the geoprocessing submodule to use the new interface. 4. Add caching support to the geoprocessing submodule. All geoprocessing is done in two parts: `_start` and `_finish`. In the case of a cache hit, there should be a signal passed from `_start` to `_finish` that instructs it to fetch the value from the cache instead of `sjs_retrieve`. 5. Update the UI to send WKAoI id instead of GeoJSON for predefined shapes. RWD and user defined shapes should still be sent as GeoJSON.  ## Consequences  This will make the worst case scenario, a cache miss, slightly longer than it currently is, because we'll be checking the cache before doing the actual geoprocessing.  In case of ElastiCache failures, the cache would have to be rebuilt.  Large WKAoIs that are processed out-of-band may get pushed out of the cache if they are not used and the cache exceeds its maximum size.  User defined shapes will still not be cached, and their runtime will not be improved at all by this. Since the longest time taking activity in the geoprocessing is fetching tiles from S3, adding a network cache there may help improve those runtimes. ",data_persistence
ADR_131,https://github.com/actions/runner.git,docs/adrs/0274-step-outcome-and-conclusion.md,ADR 0274: Step outcome and conclusion,"# ADR 0274: Step outcome and conclusion  **Date**: 2020-01-13  **Status**: Accepted  ## Context  This ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.  This allows downstream a step to run based on whether a previous step succeeded or failed.  Reminder, currently the steps contains `steps.<id>.outputs`.  ## Decision  For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:  - `success` - `failure` - `cancelled` - `skipped`  When a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`.  ### Example  ```yaml steps:    - id: experimental     continue-on-error: true     run: ./build.sh experimental    - if: ${{ steps.experimental.outcome == 'success' }}     run: ./publish.sh experimental ```  ### Terminology  The runs API uses the term `conclusion`.  Therefore we use a different term `outcome` for the value prior to continue-on-error.  The following is a snippet from the runs API response payload:  ```json       ""steps"": [         {           ""name"": ""Set up job"",           ""status"": ""completed"",           ""conclusion"": ""success"",           ""number"": 1,           ""started_at"": ""2020-01-09T11:06:16.000-05:00"",           ""completed_at"": ""2020-01-09T11:06:18.000-05:00""         }, ```  ## Consequences  - Update runner - Update [docs](https://help.github.com/en/actions/automating-your-workflow-with-github-actions/contexts-and-expression-syntax-for-github-actions#steps-context)",api_and_contracts
ADR_132,https://github.com/architecture-topography/topo.git,doc/adr/0003-graphql-as-api-query-language-via-appolo-server.md,3. GraphQL as API Query Language via Apollo Server,"# 3. GraphQL as API Query Language via Apollo Server  Date: 2020-06-06  ## Status  Accepted  ## Context  We need a way to populate our Neo4J database via an API layer.  ## Decision  Use GraphQL libraries for the server API.   GraphQL has been increasing in popularity and maturity lately. It provides a more flexible API query layer compared to REST based interactions. Specifically being able to perform arbitrarily structured queries, with optional sub-elements and with query parameters.  [Apollo Server](https://www.apollographql.com/) will be used as it's a popular well documented implementation.  ## Consequences  We will need to write the API server in Javascript or similar. (We're using TypeScript) ","api_and_contracts, technology_choice"
ADR_133,https://github.com/guardian/manage-frontend.git,docs/04-node.md,To use Node and Express for the server,"# To use Node and Express for the server  ## Context  It's not intended for this app to have a significant server-side component, instead it will directly send calls from the front end to a service layer. This service layer will be discussed in its own project, but presently comprises solely of [members data API](https://github.com/guardian/members-data-api), the same back end that currently provides account management functionality.  ## Decision  The back end for this app will have two responsibilities. Firstly, it will act as a proxy for calls to the service layer. Secondly, it will provide server side rendering capabilities to improve user experience.  ## Status  Accepted ",technology_choice
ADR_134,https://github.com/SparksNetwork/backend-services.git,adr/adr-003-sending-notifications.md,ADR 3: Sending notifications,"# ADR 3: Sending notifications  * Jeremy Wells  ## Status  **proposal** | ~~accepted~~ | ~~depreciated~~ | ~~superceded~~  ## Context  ### Definitions  * **Command message**: This is a message on a stream that has been generated by  a user action. * **Command service**: This is a service that listens to command messages. * **Data message**: This is a message on a stream that has been generated by  another service asking to persist data.  The application needs to send notifications to end users about certain events.  For example, send the volunteer an email to say that their application has  been accepted.  However, there are rules around the sending of notifications. With the above  example, the volunteer should receive the accepted application when the   following is true:  * Engagement isAccepted = true * Opp confirmationsOn = true  Further, there are notifications with time based restrictions. For example, it  may be desirable with the above email to wait for an amount of time before   sending the notification so that an event coordinator can move an engagement   around without causing a flood of emails.  The above rules mean that if an engagement is accepted, but the confirmations  are not turned on, and then the confirmations get turned on, then the  notifications for existing accepted engagements get sent. Any engagements  that are accepted from that point in time are also sent.  ## Options  1. Create the emails in the command services. For the above example, this would require code in the engagements service and the opps service. The engagements  code would check isAccepted && confirmationsOn. The opps service would  check confirmationsOn and then loop over all engagements.      In order to implement the timing rules the service would add a sendAt to the email record. The email service itself would need to do something with this.      @startuml actor User participant engagements control emails control data  User -> engagements: isAccepted = true engagements -> engagements: confirmationsOn? alt true     engagements -> emails: Send accepted end engagements -> data: Update @enduml  @startuml actor User participant opps control emails control data  User -> opps: confirmationsOn = true loop isAccepted engagements     opps -> emails: Send accepted end opps -> data: Update @enduml      **Pros** * Natural way to write it  **Cons** * Services have too many responsibilities * Duplication * Complexity - loop over opp engagements * Hard to prevent duplicate emails  2. Create a separate service that listens on data topics (engagements, opps)  and when it sees an update to isAccepted or confirmationsOn, performs the above  rules and sends to the emails topic.   (for brevity user omitted):    @startuml participant engagements control data participant ""engagements accepted"" control emails  engagements -> data: Update isAccepted = true data -> ""engagements accepted"" ""engagements accepted"" -> ""engagements accepted"": confirmationsOn? ""engagements accepted"" -> emails: Send accepted @enduml  @startuml participant opps control data participant ""engagements accepted"" control emails  opps -> data: Update confirmationsOn = true data -> ""engagements accepted"" loop accepted engagements     ""engagements accepted"" -> emails: Send accepted end @enduml  **Pros** * Services have single responsibility * Testing is easier  **Cons** * Complexity - loop over opp engagements * Hard to prevent duplicate emails  3. Introduce a notification model to the services in option 2. Thus instead of emitting email specific messages, the services emit normal data messages. In  the given example, when an engagement is accepted an accepted notification is  created.      The accepted notification model / service will deal with the logic. As      engagements are accepted the notifications will be collected. When the opp     confirmationsOn flag is set the existing notifications will be sent.          The implementation of a notifications service would need to run on a      schedule or loop.      @startuml participant engagements participant ""engagements accepted"" as accepted control data database database  engagements -> data: Update isAccepted = true data -> database: Update engagement data -> accepted accepted -> data: create notification data -> database: create notification @enduml  @startuml participant notifications database database control emails  [--> notifications: scheduler notifications -> database: load database -> notifications loop notifications     notifications -> notifications: condition check     alt true         notifications -> emails: Send email         notifications -> database: Remove notification     end end @enduml      **Pros** * No duplication * Services have single responsibility * Notification logic is separated * Notifications are tracked * Notifications are generic - will support more than email  **Cons** * More code * Notification logic is separated * Notification model or service requires logic  ## Decision  Option 3 is my preference  ## Consequences  ",others
ADR_135,https://github.com/adaptris/interlok.git,docs/adr/0006-workflow-callback.md,(sem título),"--- layout: page title: 0006-workflow-callback --- # Add a new method to AdaptrisMessageListener  * Status: ACCEPTED * Deciders: Aaron McGrath, Lewin Chan, Gerco Dries, Matt Warman, Sebastien Belin, (Paul Higginson) * Date: 2019-11-06  ## Context and Problem Statement  When you enable [Dead Letter Queues](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html) in SQS; the contract is that messages read from a source queue are failed over to the dead letter queue once the redrive policy is fired (i.e. after a max number of attempts to deliver the message). From testing, this is predicated on the fact that the message is not deleted. Of course, our sqs-polling-consumer deletes the message after submitting it to the workflow.  Since we can't rely on the JMS semantics either (since onMessage() traditionally doesn't throw exceptions, and RuntimeExceptions aren't propagated by Interlok) we then have to think about some way of having an asynchronous callback.   ## Considered Options  * Do Nothing * Modify AdaptrisMessageListener to have callbacks.  ## Decision Outcome  Chosen option: Modify AdaptrisMessageListener to have callbacks.  ## Pros and Cons of the Options  ### Do Nothing   * Good, because no change to code. * Bad, because we can't use SQS DLC behaviours. * Bad, because we might not preserve all message attributes (depending on configuration). * Neutral, error handling is still fully within the purview of Interlok (which is predictable)  ### Modify AdaptrisMessageListener to have callbacks.  If we change AdaptrisMessageListener to be this :  ``` default void onAdaptrisMessage(AdaptrisMessage msg) {   onAdaptrisMessage(msg, (s)-> {}, (f)->()); }  void onAdaptrisMessage(AdaptrisMessage msg, java.util.function.Consumer<AdaptrisMessage> success, Consumer<AdaptrisMessage> failure); ```  Then we can effectively make our SQS polling consumer this : ``` for (Message message : messages) {   try {     AdaptrisMessage adpMsg = AdaptrisMessageFactory.defaultIfNull(getMessageFactory()).newMessage(message.getBody());     // stuff skipped for brevity.     final String handle = message.getReceiptHandle();     // on success we delete the message, on failure we leave it in situ.     // Might need to make that configurable, since we don't want poison messages     retrieveAdaptrisMessageListener().onAdaptrisMessage(adpMsg, (s) -> {       sqs.deleteMessage(new DeleteMessageRequest(queueUrl, handle))     }, (f) -> {});     if (!continueProcessingMessages(++count)) {       break messageCountLoop;     }   }   catch (Exception e){     log.error(""Error processing message id: "" + message.getMessageId(), e);   } }  ```  * Good, because this makes the behavour controllable from the consumers perspective (e.g. fs-consumer could wait until the workflow completed before deleting the file...) * Good, because this will have the side effect of enabling callbacks for all consumers if they want it; which means we can get rid of the object monitors and things that we do Jetty + pooling workflow... * Bad, because it's a callback, and it happens at some point in the future... is the session/queue whatever still valid. * Bad, because threadsafe is hard. * Neutral, all workflows have to change (and message listener stub implementations).  ### Note 2020-02-12  The API change is going to be `void onAdaptrisMessage(AdaptrisMessage msg, Consumer<AdaptrisMessage> success);` since the semantics of how the failure will be handled (or where it should be fired) isn't clear at the moment. As of the next major release (v4); we are proposing a different way of handling things like this, since asynchronous callbacks are becoming ever more popular.",others
ADR_136,https://github.com/alphagov/verify-stub-idp.git,docs/adr/0001-record-architecture-decisions.md,1. Record architecture decisions,"# 1. Record architecture decisions  Date: 2018-02-15  ## Status  Accepted  ## Context  We need to record the architectural decisions made on this project.  ## Decision  We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions  ## Consequences  See Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's _adr-tools_ at https://github.com/npryce/adr-tools. ",governance_and_process
ADR_137,https://github.com/nats-io/nats-architecture-and-design.git,adr/ADR-10.md,JetStream Extended Purge,"# JetStream Extended Purge  |Metadata|Value| |--------|-----| |Date    |2021-06-30| |Author  |@aricart| |Status  |Implemented| |Tags    |server, client, jetstream|  ## Context  JetStream provides the ability to purge streams by sending a request message to: `$JS.API.STREAM.PURGE.<streamName>`. The request will return a new message with the following JSON:  ```typescript {     type: ""io.nats.jetstream.api.v1.stream_purge_response"",      error?: ApiError,     success: boolean,     purged: number } ```  The `error` field is an [ApiError](ADR-7.md). The `success` field will be set to `true` if the request succeeded. The `purged` field will be set to the number of messages that were purged from the stream.  ## Options  More fine-grained control over the purge request can be achieved by specifying additional options as JSON payload.  ```typescript {     seq?: number,     keep?: number,     filter?: string } ```  - `seq` is the optional upper-bound sequence for messages to be deleted   (non-inclusive) - `keep` is the maximum number of messages to be retained (might be less   depending on whether the specified count is available). - The options `seq` and `keep` are mutually exclusive. - `filter` is an optional subject (may include wildcards) to filter on. Only   messages matching the filter will be purged. - `filter` and `seq` purges all messages matching filter having a sequence   number lower than the value specified. - `filter` and `keep` purges all messages matching filter keeping at most the   specified number of messages. - If `seq` or `keep` is specified, but `filter` is not, the stream will   remove/keep the specified number of messages. - To `keep` _N_ number of messages for multiple subjects, invoke `purge` with   different `filter`s. - If no options are provided, all messages are purged.  ## Consequences  Tooling and services can use this endpoint to remove messages in creative ways. For example, a stream may contain a number of samples, at periodic intervals a service can sum them all and replace them with a single aggregate. ",others
ADR_138,https://github.com/arachne-framework/architecture.git,adr-009-datomic-config-ontology.md,Architecture Decision Record: Configuration Ontology,"# Architecture Decision Record: Configuration Ontology  ## Context  In [ADR-003](adr-003-config-implementation.md) it was decided to use a Datomic-based configuration, the alternative being something more semantically or ontologically descriptive such as RDF+OWL.   Although we elected to use Datomic, Datomic does not itself offer much ontological modeling capacity. It has no built-in notion of types/classes, and its attribute specifications are limited to what is necessary for efficient storage and indexing, rather than expressive or validative power.  Ideally, we want modules to be able to communicate additional information about the structure and intent of their domain model, including:  - Types of entities which can exist - Relationships between those types - Logical constraints on the values of attributes:     - more fine grained cardinality; optional/required attributes     - valid value ranges     - target entity type (for ref attributes)  This additional data could serve three purposes:  - Documentation about the intended purpose and structure of the configuration defined by a module. - Deeper, more specific validation of user-supplied configuration values - Machine-readable integration point for tools which consume and produce Arachne configurations.  ## Decision  - We will add meta-attributes to the schema of every configuration, expressing basic ontological relationships. - These attributes will be semantically compatible with OWL (such that we could conceivably in the future generate an OWL ontology from a config schema) - The initial set of these attributes will be minimal, and targeted towards the information necessary to generate rich schema diagrams   - classes and superclass   - attribute domain   - attribute range (for ref attributes)   - min and max cardinality - Arachne core will provide some (optional) utility functions for schema generation, to make writing module schemas less verbose.  ## Status  PROPOSED  ## Consequences  - Arachne schemas will reify the concept of entity type and the possible relationships between entities of various types. - We will have an approach for adding additional semantic attributes in the future, as it makes sense to do so. - We will not be obligated to define an entire ontology up front - Modules usage of the defined ontology is not technically enforced. Some, (such as entity type relationships) will be the strong convention and possibly required for tool support; others (such as min and max cardinality) will be optional. - We will preserve the possibility for interop with OWL in the future. ",data_persistence
ADR_139,https://github.com/ministryofjustice/cloud-platform.git,architecture-decision-record/004-use-kubernetes-for-container-management.md,Use kubernetes for running containerised applications,"# Use kubernetes for running containerised applications  Date: 10/04/18  ## Status  ✅ Accepted  ## Context  MOJ Digital's approach to infrastructure management and ownership has evolved over time, and has led to the following outcomes:  - Unclear boundaries on ownership and responsibilities between service teams and the cloud platforms team - Significant variation in deployment, monitoring and lifecycle management across products - Inefficient use of AWS resources due to the use of virtual machine-centric architecture, despite our standardisation on Docker containers  The last few years has seen the advent of several products specifically focused on the problem of running and managing containers in production:  - Kubernetes - Mesos / Mesosphere / DC/OS - Docker Swarm - AWS ECS - CloudFoundry  Given the technology landscape within MOJ, we require a container management platform that can support a wide range of applications, from ""modern"" cloud-native 12-factor applications through to ""legacy"" stateful monolithic applications, potentially encompassing both Linux- and Windows-based applications; this removes CloudFoundry from consideration, given its focus on modern 12-factor applications and reliance on buildpacks to support particular runtimes.  From the remaining list of major container platforms, Kubernetes is the clear market leader:  - Rapid industry adoption during 2017 establishing it as the emerging defacto industry standard - Managed Kubernetes services from all major cloud vendors - Broad ecosystem of supporting tools and technologies - Increasing support for Kubernetes as a deployment target for commercial and open-source software projects  There is also precedent for Kubernetes use within MOJ, as the Analytical Platform team has been building on top of Kubernetes for around 18 months.  ## Decision  Use Kubernetes as the container management component and core technology for our new hosting platform.  ## Consequences  1. Several technical spikes looking at Kubernetes itself and associated components - logging, monitoring, application deployment, etc. 2. Requirement to use an identity broker for Kubernetes user authentication, given the decision to use Github as an identity provider and the lack of a common auth protocol between Kubernetes and Github ","technology_choice, infrastructure_and_deployment"
ADR_140,https://github.com/Trendyol/ios-architecture-decision-logs.git,adr/0005-inject-ab-config-global-values-toPresenter.md,Inject AB / Config / Global Values to Presenters For Testablity,"# Inject AB / Config / Global Values to Presenters For Testablity  * Status: accepted * Deciders: iOS Team * Date: 2020-07-21  ## Context  We faced a problem that missing test cases on some presenters because of not injectable variables like ab tests, config or global values on presenters. So we want to cover all of these missings.  ## Decision  We decided to inject this variables to related presenters from their constructors.  ## Consequences  Injecting this variables to presenters from their constructors are giving us to more testable presenters. So we can tests all production code cases which is involving business logics. ",testing_strategy
ADR_141,https://github.com/elifesciences/tech-team.git,adr/0002-use-containers-for-foreign-languages.md,1. Use containers to deploy tools from foreign languages source code,"# 1. Use containers to deploy tools from foreign languages source code  Date: 2017-10-25  ## Status  Proposed  ## Context  eLife has a small set of supported languages: PHP, Python, JavaScript; in-house developers are present for them. All other languages are defined as **foreign**.  There are tools that are peculiar to our infrastructure, such as [goaws](https://github.com/p4tin/goaws) for AWS simulation, but are written in languages no one is an expert in (Go).  There are also tools that were originally written in another language but are being adopted by us, like [INK](https://gitlab.coko.foundation/INK/ink-api) for document conversion, written in Ruby.  These tools are usually distributed as source code. The operational overhead of writing formulas for the environment to build them is a form of waste.  Some tools written in Java instead have a very stable runtime platform (ElasticSearch, even Jenkins), as they are distributed as binaries.  ## Decision  We will use existing Docker containers to deploy tools that require building from source in a foreign language, in testing or production environments.  ## Consequences  We should not see too many runtimes being supported in builder-base-formula.  We should not allocate time to resolve versioning or building issues for languages such as Go or Ruby, but reuse existing container.  We should tag the container images we use and push them onto [https://hub.docker.com/u/elifealfreduser/] for reproducibility. ",technology_choice
ADR_142,https://github.com/ec-europa/europa-component-library.git,docs/decisions/005-themes.md,Handle `themes` in ECL v3,"# Handle `themes` in ECL v3  | Status        | accepted                                              | | ------------- | ----------------------------------------------------- | | **Proposed**  | 07/10/2020                                            | | **Accepted**  | 19/10/2020                                            | | **Driver**    | @kalinchernev                                         | | **Approver**  | [@team](https://github.com/orgs/ec-europa/teams/inno) | | **Consulted** | @emeryro, @planctus                                   | | **Informed**  | [@team](https://github.com/orgs/ec-europa/teams/inno) |  ## Decisions  - Themes will be managed as separate packages (described in [this point](#option-4-themes-are-multiple-packages-multiple-presets-generate-multiple-themes-output)) - Resources will be managed as separate packages (described in [this point](#option-1-separate-package-per-system-or-per-theme)) - Decisions can be revised after actual iterations  ## Context  - ECL v2 was made in order to generate two independent design systems, EC and EU. - ECL V3 starts from the assumption that there is only one source template and one design system with multiple `themes`. - The number of themes to accommodate is not set as of the time of the document writing, thus the magnitude could vary between 2 (previous systems) to 20+.  ## Prerequisite notes  The term ""code duplication"" used in this document follows the [conventional definition](https://en.wikipedia.org/wiki/Duplicate_code). Duplicate code example in ECL v2 can be illustrated with source code for EC and EU which is completely identical and exists in several occassions. However, splitting a module file of hundreds of lines of code into multiple files containing portions of the same code is not considered code duplication but code separation or code splitting.  ## Consequences  - There are continous internal discussions for the definition of a ""theme"" and whether it's overlapping with the concept of presets of ECL v2. - The CSS source code needs an improved organization. It must scale with time and reduce complexity and code duplication. - Release packages will require changes. The question on publishing ""themes"" is unclear. - Storybook application(s) (ECL Playground website) will most probably also need changes accordingly. It is unclear whether a single instance of a Storybook application can accommodate the new themes architecture or there will be multiple instances. - There is an strong uncertainty on how to consistently manage all other assets such as JavaScript, markdown files, icon and logo resources, templates, etc. Environment variables have been considered as a global contextual information but above-mentioned ambiguous topics are unclear at this stage.  ## Scopes  The topics below are categorized with focus on separation of subjects rather than specific type of code.  ### Package organization  Although the concept of ""presets"" was put under question during the analysis of v3, they are still used throughout this document in terms of code organization, not ECL release packages. This means that regardless of the usage of presets below, they may not be part of the resulting release of ECL v3.  #### Option 1: ECL v2 is preserved  This option has been discarded from the very beginning because it cannot achieve the goal without increased complexity and code duplication.  #### Option 2: ""themes"" is single package, single preset generates multiple themes output  This option has been discarded as too complex for the worth of the result. The complexity comes from SASS language limitations as dynamic imports are not supported. Every dynamic theme generation approach with SASS involves workarounds which were not acceptable for the team.  #### Option 3: ""themes"" is single package, multiple presets generate multiple themes output  - `@ecl/ecl-base` - common variables - `@ecl/ecl-theme` - set of variables related to the themes (global parameters), one set per theme - `@ecl/ecl-preset-{ec-core}` - base + theme ec core + components - `@ecl/ecl-preset-{eu-core}` - base + theme eu core + components - `@ecl/ecl-preset-{ec-standardised}` - base + theme ec standardised + components - `@ecl/ecl-preset-{eu-standardised}` - base + theme eu standardised + components - `@ecl/ecl-preset-{custom}` - template for theme generation  #### Option 4: ""themes"" are multiple packages, multiple presets generate multiple themes output  Same as option 3 with the following difference in theme-related packages which are multiple:  - `@ecl/ecl-theme-{ec-core}` - set of variables - `@ecl/ecl-theme-{eu-core}` - set of variables - `@ecl/ecl-theme-{ec-standardised}` - set of variables - `@ecl/ecl-theme-{eu-standardised}` - set of variables  In this version, each target bundle is composed of a pair of a theme + preset.  ### Resource organization  Resources are: logo, favicons, icons and other similar types of assets which are not SCSS or JavaScript and are used by consumers of ECL.  #### Option 1: separate package per system or per theme  Current organization of v2.  - `@ecl/resources-ec-logo` - `@ecl/eu-resources-logo`  #### Option 2: single package  - `@ecl/resources`  #### Option 3: include resources in themes  This option does imply that each theme comes in a dedicates package ([see above](#option-4-themes-are-multiple-packages-multiple-presets-generate-multiple-themes-output)). Resources useful to display a theme are stored directly in it. To limit duplication, some global resources may still be put in dedicated packages. ",others
ADR_143,https://github.com/azavea/franklin.git,docs/0001-record-architecture-decisions.md,(sem título),"--- id: 0001-record-architecture-decisions title: 1 - Recording Architecture Decisions --- Date: 2019-12-30  ## Status  Accepted  ## Context  We need to record the architectural decisions made on this project.  ## Decision  We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).  ## Consequences  See Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's [adr-tools](https://github.com/npryce/adr-tools). ",governance_and_process
ADR_144,https://github.com/ebi-uniprot/uniprot-rest-api.git,doc/architecture/decisions/0004-lombok.md,4. Lombok,"# 4. Lombok  Date: 2018-08-02  ## Status  Accepted  ## Context  Java projects often contain a large amount of boilerplate code, e.g., defining data/value classes, builders, etc. All such code follows a certain pattern and needs testing -- and writing both of these can be error prone. A library that enables cutting down boilerplate code, and which generates tested code would be beneficial to the project.  ## Decision  We will use the [Lombok](https://projectlombok.org/) library to reduce the amount of boilerplate code we need to write.  ## Consequences  With less code to maintain, we envisage a more succint codebase whose function is more readable. ",technology_choice
ADR_145,https://github.com/cerner/beadledom.git,adr/health/default_dependencies_primary.md,**Title**: Dependencies Default to Primary,"## **Title**: Dependencies Default to Primary  ## **Status**: accepted  ## **Context**:  Primary Health checks were not checking if dependencies were primary or not before checking their health. This was not the intended behavior of the primary health check endpoint, as it should only check the primary dependencies of any given project.  ## **Decision**:  In order to do this passively, we changed the dependency class to default to primary unless otherwise specified. We did this because we don't know if our consumers rely on the previous behavior of meta/health checking dependencies if they were of unspecified importance.   ## **Consequences**:  Any system that wants a dependency to be secondary must explicitly state it as such.  ",others
ADR_146,https://github.com/Voronenko/runbooks-mkdocs.git,docs/architecture/decisions/0003-use-plantuml-for-diagramming.md,3. Use PlantUML for diagramming,"# 3. Use PlantUML for diagramming  Date: 2020-11-03  ## Status  Accepted  Amended by [5. Use PlantUML for diagramming with use of stdlib](0005-use-plantuml-for-diagramming-with-use-of-stdlib.md)  ## Context  The issue motivating this decision, and any context that influences or constrains the decision.  ## Decision  The change that we're proposing or have agreed to implement.  ## Consequences  What becomes easier or more difficult to do and any risks introduced by the change that will need to be mitigated. ",governance_and_process
ADR_147,https://github.com/AbhishekJoshi/ionic-dummy-repo.git,@docs/adr/ADR-001.md,ADR-001<br/> Should the seed template be opinionated on state management framework choice?,"# ADR-001<br/> Should the seed template be opinionated on state management framework choice?   ## Context Given the speed at which state-management libraries, preferences and trends change, should this starter seed should assert an opinion via dependencies in this regard?   Developers bring different skills and ideas to the table, should they be free to make a decision on the best tool for the job, based upon requirements, complexity etc?   ### Who Was Involved in This Decision - Alex Ward - Chris Weight   ### Relates To - N/A   ## Decision The Hybrid seed template will _not_ express an opinion via pre-determined dependencies on what state management frameworks (if any) should be used. This can be decided on a per-project basis. Though there are positives and negatives either way, it is felt that the ability to rapidly implement changes to approach over the course of time and projects is a powerful plus.   ## Status  :bulb: Proposed on 2018-01-24 :white_check_mark: Accepted on 2017-06-23   ## Consequences  ### - Positives  + Developers are free to work to their strengths and pre-existing knowledge when starting a project + There is flexibility should a 'better solution' become prevalent in *the future* + Reduces maintenance overhead in the starter seed + Developers not familiar with a particular state-management approach, framework, library etc have the opportunity to study how it is used 'in the wild' and gain expertise  ### - Negatives  - A Developer on-boarding onto an existing project may not be familiar with the design pattern expressed by a chosen state-management framework or library. This will increase on-boarding time - Consistency across projects is not guarenteed - An unknown chosen state-management approach may ultimately prove to be a bad choice (hard to reason, difficult to maintain, overly complex etc)  ",others
ADR_148,https://github.com/HHS/TANF-app.git,docs/Technical-Documentation/Architecture-Decision-Record/008-deployment-flow.md,8. Deployment Flow,"# 8. Deployment Flow  Date: 2021-01-27 (_Updated 2022-07-18_)  ## Status  Accepted  ## Context  Our Cloud.gov organization currently has three Spaces -- `tanf-dev`, `tanf-staging`, and `tanf-prod`. The vendor team currently has access to the tanf-dev space only.  Since the recent changes to our [Git workflow](https://github.com/HHS/TANF-app/blob/main/docs/Technical-Documentation/Architecture-Decision-Record/009-git-workflow.md) we believe our current deploy strategy should be updated to more closely match the workflow. Previously, since we had approvals on two different repositories we decided that it made sense to maintain [two separate staging sites](https://github.com/HHS/TANF-app/blob/837574415af7c57e182684a75bbcf4d942d3b62a/docs/Architecture%20Decision%20Record/008-deployment-flow.md). We would deploy to one with approval in the raft-tech repository, and another with approval to HHS. Since we now have all approvals made in raft-tech, the deploy after approval serves the same purpose as deploying to the Government staging site would have.  Additionally, as of January 2021, the project has only a single deployment environment in the `tanf-dev` space on Cloud.gov. This poses challenges to the vendor development team. The team works on multiple features or fixes at any one time, but only has a single environment to test deployed code. This is leading to ""crowding"", where multiple in-progress features by different devs all want to be deployed to the same environment for testing.   As of Spring 2022, following [ADR 018](https://github.com/HHS/TANF-app/blob/main/docs/Technical-Documentation/Architecture-Decision-Record/018-versioning-and-releases.md), the project needs more than one deployment environment in the `tanf-staging` space on Cloud.gov to ensure that there is a dedicated environment for release-specific features.     ## Decision  Deploy Environment | Cloud.gov Space | Cloud.gov Dev Access | Role                                             | Deploys when ...                                  | -------------------|-----------------|----------------------|--------------------------------------------------|---------------------------------------------------| Dev                | Tanf-Dev        | Vendor & Gov      | Deploy code submitted for gov review                | Relevant github label assigned as shown below     | Develop            | Tanf-Staging    | Vendor & Gov      | Deploy code once gov-approved                       | Code merged to `raft-tech/TANF-app:develop` | Staging            | Tanf-Staging    | Gov               | Deploy code once gov-approved                       | Code merged to `HHS/TANF-app:main` | Production         | Tanf-Prod       | Gov               | Deploy code tested in staging & ready for prod      | Code merged to `HHS/TANF-app:master`                |    ### Gitflow and Deployments We will be following the Gitflow process which is an industry standard. You can read more about it [in our ADR](./018-versioning-and-releases.md). I will just highlight the parts relevant for our deployment strategy. Release branches will be merged to `HHS/TANF-app:master` which will deploy to our production sites. Code merged to `raft-tech/TANF-app:develop` will be deployed to our staging sites.  ### Dev deployments Within the dev space, there is no correlation for branch to environment as these feature or bugfix branches will constantly vary:  | Dev Site | Frontend URL | Backend URL | Purpose                                          | | -------- | -------- | -------- |--------------------------------------------------| | A11y | https://tdp-frontend-a11y.app.cloud.gov | https://tdp-backend-a11y.app.cloud.gov/admin/ | Space for accessibility testing                  | | QASP | https://tdp-frontend-qasp.app.cloud.gov | https://tdp-backend-qasp.app.cloud.gov/admin/ | Space for QASP review                            | | raft | https://tdp-frontend-raft.app.cloud.gov | https://tdp-backend-raft.app.cloud.gov/admin/ | Space for Raft review                          |  ## Consequences  **Pros** * 3-space strategy instead of 4 aligns better with our current gitflow.  * Code deploys automatically upon gov approval, but does not deploy immediately to prod, leaving room for further gov testing. Any mistakes that make it past gov review will not deploy immediately to prod. * Vendor dev team ""crowding"" should be reduced through this solution. * Only need to maintain/monitor vendor dev access to a single Cloud.gov Space using this solution -- least privilege, simplified account management. * Frees up memory and disk space in the dev environment. * Dedicated environment for release-specific features.  **Risks** * None that we can see at this time  ## Notes  - As of June 2022, CircleCI supplies environment variable key-value pairs to multiple environments (e.g. vendor's CircleCI deploys applications to dev and staging environments). The values from CircleCI are expected to be unique per environment, so until [#1826](https://github.com/raft-tech/TANF-app/issues/1826) is researched and addressed, these values will need to be manually corrected in cloud.gov immediately following the execution of the execution of the [`<env>-deployment` CircleCI workflow](../../.circleci/config.yml) CircleCI workflow. This workaround applies to backend applications in the TDP staging environment. ",infrastructure_and_deployment
ADR_149,https://github.com/eclipse/winery.git,docs/adr/0019-version-in-the-name.md,Versions of TOSCA elements in the name,"# Versions of TOSCA elements in the name  In order to enable the versioning of TOSCA elements, the version corresponding to one element must be saved in a TOSCA compliant way.   Forces: - TOSCA compliant - The version identifier must be detectable in the XML file   ## Considered Options  * Version in the name * Version in the namespace * Save version externally  ## Decision Outcome  * Chosen Option: version in the name/id because it is compliant to the TOSCA specification and shows the version    directly in the XML file. * Easiest and best fit regarding compliance  ## Pros and Cons of the Options  ### Version in the name  * Good, because it is consistent to the TOSCA specification * Good, because even from outside of the winery, definitions can be detected in the specific version on first sight * Good and bad, because it requires a deep copy of all files and definitions on creating a new version* * Bad, because Introduces naming conventions to the naming of components: '_' are not allowed anymore*  ### Version in the namespace  * Good, because it is easy and well established method in XML * Good, because the definition’s name/id stays intact * Bad, because it implies that all elements in the corresponding namespace have the same version * Bad, because it is usually used to specify the version of the XML’s vocabulary only  ### Save version externally  * Good, because it requires less disk space than * Bad, because the version is not detectable in the XML   ## License  Copyright (c) 2017 Contributors to the Eclipse Foundation  See the NOTICE file(s) distributed with this work for additional information regarding copyright ownership.  This program and the accompanying materials are made available under the terms of the Eclipse Public License 2.0 which is available at http://www.eclipse.org/legal/epl-2.0, or the Apache Software License 2.0 which is available at https://www.apache.org/licenses/LICENSE-2.0.  SPDX-License-Identifier: EPL-2.0 OR Apache-2.0 ",others
ADR_150,https://github.com/anton-liauchuk/educational-platform.git,docs/architecture-decisions/0009-architecture-tests.md,9. Architecture tests.,# 9. Architecture tests. Date: 2020-06-27  ## Status Accepted  ## Context We need to have the mechanism for supporting and validating common architecture principles in all application.  ## Decision Architecture tests with using Archunit should be implemented.,testing_strategy
ADR_151,https://github.com/mozmeao/infra.git,docs/architecture/decisions/0007-service-dns-patterns.md,7. Service dns Patterns,"# 7. Service dns Patterns  Date: 2021-01-04  ## Status  Accepted  ## Context  Our current dns naming follows a couple of very similar patterns. Sometimes using 'gcp', 'frankfurt', 'oregon-b' as ways to separate different environments.  We should have one pattern and stick to it the best we can.  Things the pattern needs to solve for:  * Should be 'the same' for all of the meao services (for example: nucleus/bedrock/snippets). * should allow for multiple 'environments' of a service to be deployed in the same region ('prod'|'stg'|'dev') * should allow for multiple regions/deployments of the same service + environment ('or', 'fr', 'ia') * should also have a good 'user facing' pattern, that is not the same as the above pattern. (www.mozilla.org -> 'bedrock' 'prod' 'or' && 'bedrock' 'prod' 'fr' with some mechanism for choosing between the two deployments.)  ## Decision  For the backend deployments follow this pattern: 'service'.'environment'.'region'.'domain'. An incomplete list of each of examples of values for those variables:  | Service  | |----------| | bedrock  | | nucleus  | | snippets | | prom     |  | Environments | |--------------| | dev          | | stg          | | prod         | | demo1        |  | Region | Description           | |--------|-----------------------| | or     | oregon eks cluster    | | fr     | frankfurt eks cluster | | ia     | iowa gcp cluster      |  | Domain     | |------------| | moz.works  | | mozmar.org | | ramzom.org |  This leads to a few examples:  | Examples                 | |--------------------------| | bedrock.dev.or.moz.works | | prom.prod.fr.mozmar.org  | | nucleus.stg.ia.moz.works |   Note that these are for 'internal' use primarily.  The user facing domains will stay as they are.  A few examples, nucleus.mozilla.org (prod) and nucleus.allizom.org (stg), www.mozilla.org (bedrock prod) www.allizom.org (bedrock stg).  The connection between the new dns entries and the user facing will stay the same. (If we're using a r53 traffic policy now, we will continue to after this change, if we're just using cname/alias records we will again after this change,etc, including cloudflare vs cloudfront etc.)    ## Consequences  All services will live at new addresses. Old addresses will need to be deleted. ",governance_and_process
ADR_152,https://github.com/holochain/holochain-rust.git,doc/architecture/decisions/0017-capabilities.md,Capabilities & Security ADR,"# Capabilities & Security ADR  Date: 2019-02-26  ## Status Draft  ## Context  A unified security model for Holochain applications: * Each zome must be able to represent and enforce its own security modeling because that is the appropriate place to do so. (Push the intelligence to the edges.) * Developers must be able to build in granualar specificity and revokability of access to functions and entries. * We must be able to distinguish between application security model, and architectural and code security model.  I.e. what is the security model application developers build into their apps, and the security model of Conductor/Core, etc.  E.g. we have to ensure that zome calls aren't subject to replay attacks in general, and also allow zome developers to declare and create security policies in specific.  ## Decision  Holochain will use a variant of the [capabilities](https://en.wikipedia.org/wiki/Capability-based_security) security model. Holochain DNA instances will grant revokable, cryptographic capability tokens which are shared as access credentials. Appropriate access credentials must be used to access to functions and private data.  This enables us to use a single security pattern for:  - connecting end-user UIs,  - calls across zomes within a DNA,  - bridging calls between different DNAs,  - and providing selective users of a DNA the ability to query private entries on the local chain via send/receive.  Each capability grant gets recorded as a private entry on the grantor's chain, and are validated against for every zome function call.  ## Elements: ### CapabilityGrant The CapabilityGrant is by default a private entry recorded on the chain because this does not need to be published to the DHT (though it may be in some cases). The Address (i.e. hash) of these entries serves as the capability token credential that is part of an access request.  The instance can look up the grant by that address and confirm if the request conforms to the grant type.   DNA developers can do this manually using `grant` and `verify_grant` api calls for custom access use-cases, but this is also done automatically to verify both zome function calls as well as bridge and cross zome calls.  #### Grant attributes: 1. **Asignees:** a list of agents to whom the grant applies.  If the list is not specified then, then the grant is assumed to be transferable, i.e. possesion of the token is sufficient and serves as a password.  Note that ""anonymous"" and assigned grants are mutually exclusive. 2. **Pre-filled Parameters:** A zome-function call grant may also specify a template for parameter values of a the function being granted access.  This allows for ""currying"" type behavior on grants, where the grant itself forces a function parameter to specifc value.  *Comments:* In the past we have talked about the ""public"" capability. All access must be signed.  ""public"" access comes from publishing an unassigned token publicly.  #### Special Case -- Agent Grant: There is one special case grant, the **Agent Grant**.  So far in Holochain, we have been saying that the second entry on the chain, after the DNA, is the AgentID entry, that identifies the agency by public-key that ""owns"" the chain.  We have also talked about (and implemented in proto along with the revoking and re-issue of the AgentID entries). In the capabilities security model we can unify this with capability grants where that second entry can be thought of as ""super-user/admin/root"" capability which should indeed be granted to the agent who ""owns"" the chain, but the token of that agency (the public key) may need to be revoked and replace, just like any other capability token.  ### CapabilityRequest A capability request is a structure for making a request referring to a particular capability grant.  Request attributes: 1. **Token:** the address of the capability grant entry committed to the chain 2. **Provenance:** the address of the requester and the signature of request contents. 3. **Contents:** the exact data of the contents of the request that is signed.  In the case of zome function call requests this is the function name, the function parameters, and a nonce for preventing replay attacks.  *Comments:* Core has to be able to do three things with a CapabilityRequst: 1. Load the grant from the address (or detect that this is a special case grant) 2. Confirm that the Contents matches the the Provenance. 3. Extract the data from the contents that's needed for the purpose, i.e. actually get the function name and parameters of the zome-call.  ## Processes ### Zome Function Calls  Capabilities allow developers to specify control access to zome function calling, either from exterior calls or via bridging or even cross-zome calling.  This latter may seem odd, but it's important for enabling more secure development patterns for zome mix-ins.  See Consequences for details.  A broad overview of how zome function call from an outside source would flow under the capabilities model:  1. Agent bundles function name, parameters and a timestamp into a call request block, and signs it with the agent's private key, and sends it to Conductor along with any other parameters necessary for routing to the correct instance over what ever interface is being used. 2. Conductor creates a CapabilityRequest structure and passes it into holochain Core. 3. Core loads CapabilityGrant from chain by it's address, and checks validity according to the grant's parameters, returning a CapabilityCheckFailed error, or calling the zome function if successfull.  It may also check the timestamp of the call to make-sure it's within a reasonble window to prevent some re-play attacks.  Additionally, if complete security from replay attacks is necessary we may implement an additional handshake where the agent makes a ""pre-call"" indicating the desire to make a zome call.  In that case the Conductor would have to pass this request into core where a nonce could be generated that the client has to include in the call request block. Note that this also requires implementing an ephemeral store in core, something that's on our development path.  *Comments:*  1. The ""agent"" above could be a web-UI that holds the private key (in some Holo cases) or could be an extended Conductor that is an electron app.  In the former case, the necessary token grants have to be passed to the UI.  If the agency in the UI is the ""owner"" of the chain, then it can effectively use the special case agent grant to make any zome call it wants.  Otherwise, it will have to have received the public token, or an assigned or transferable token.  See Consequences below.  #### Calling Elements/Structs We will use a simple JSON structure for what gets signed by which ever component of the system has the private key:  TBD. Draft structures: https://hackmd.io/cvXMlcffThSpN-C5WrfGzg?view#  ### Genesis  - We use the convention of using a reserved-trait name (""hc_public"") to identify functions for which such a public grant can be created at genesis time, and be made available to the Conductor to send to UIs (or proxy on their behalf in the various use-cases i.e. as a web-proxy) for creating provenance for public access.  *Comments:* this has been implemented in `capabilities-3`  ### Bridging  TBD  ## Consequences  ### Exposing Tokens  #### Public Token We need to add a method/convention for agents to be able to access the public token generated at genesis time.  In `capabilities-3` the public token is returned as part of the results of initialization during genesis.  That initialization data strcture from genesis should be made available in the HDK through a new PUBLIC_TOKEN global, and to the conductor for additions to the conductor_api.  #### Conventions & examples for generated tokens We need to provide some examples of zome functions/patterns of how to request tokens, generate them and return them for use by calling agents, both at the level of UI zome function calling, and at the application level for use in node-to-node send & receive communications.  ### Zome mix-in security The capability model is not only useful for extra-membrane security, but also intra-membrane security for Cross-zome calls.  Because our composibility model includes drop-in zomes, for which the developer may not be able to see the source code (i.e. they only get the WASM), it is important to create the ability to make calling functions in other zomes subject to a capability request on a specific grant.  For this to work, we may need to expand the expressivity of the `sign` API call.  i.e. we may need to limit under what agency that call can be made for certain zomes. ",security
ADR_153,https://github.com/simpledotorg/simple-server.git,doc/arch/014-region-level-sync.md,Region level sync,"# Region level sync September 2020  _This ADR has been added retroactively in Apr 2021 to capture our switch to block-level syncing._  ## Status Accepted. This feature was released to all users by Feb 2020.  ## Context [PRD](https://docs.google.com/document/d/1Cflct0Y-44IRUVw_5-NptcnNSX1UgAPBiXqoXHq22io/edit)  Users in large districts reported that the Simple app was running very slow, making the app near-unusable. The slowdown was caused by the volume of patient data synced to the user’s phone. We realised that the amount of data being stored on the device had to be reduced for better long-term performance.  Currently we sync the entire district's records to a user's phone. Some of the large districts have upto 50,000 patients, which can amount to 400-500 MB of data. On lower-end phones we noticed the app started slowing down when the DB size grew beyond 250 MB.  A district typically has between 1-20 blocks. From trends in IHCI, we found it's uncommon for patients to visit facilities across blocks. Patients that have a BP taken in more than 1 block is around 2%, with the exceptions of: Sindhudurg (9.8%), Hoshiarpur (5.3%), Bathinda (3.1%). This means that we can sync only a block's data to the user's phone and be reasonably confident about finding patients on the app.  ## Decision - The server will sync records from the user's block instead of the entire district.   Specifically the following patients will be synced:   - patients that registered at a facility in the same block,   - patients that are assigned to a facility in the same block, and   - patients that have an appointment scheduled at a facility in the same block. - All other sync resources will return records belonging to these patients only. - The sync mechanism should support the ability to adjust the sync radius to any sync region.   This is important in case we need to change the kind of records that are synced to the app in the future.   See the [wiki entry on Region level sync](../wiki/adjusting-sync-boundaries.md) for how it works.  ### On the app - Users can continue selecting any facility in their district when switching facilities. - Users can continue selecting any facility in their district when scheduling a patient’s next visit or preferred facility.  - It is possible that a patient will visit a facility outside their block and their record will not be found on the user’s device. In this case the user should      - Scan the patient’s BP passport if they have one.     - Register the patient again, as if they were new. Make sure to attach their existing BP passport to the registration.     - The duplicate patient records will be merged by the Simple team later.  ## Consequences - The Simple app will not be able to find patients who moved from one block to another. - Block is currently a freeform text field on the `Facility` model.   It needs to be a first-class entity to make block-level syncing possible.   This is introduced through the `Region` model.- Block is currently a freeform text field on the `Facility` model.   It needs to be a first-class entity to make block-level syncing possible.   This is introduced through the `Region` model. - When a patient gets registered across blocks as a duplicate, we will need to identify them and merge their data.    We plan to implement online patient lookup for the case where a patient is not found locally. ",performance_and_scalability
ADR_154,https://github.com/embvm/embvm-core.git,docs/architecture/decisions/0009-event-driven-framework-design.md,9. Event Driven Framework Design,"# 9. Event Driven Framework Design  Date: 2018-07-06  ## Status  Accepted  ## Context  * Embedded systems are highly event-driven, as they are responding to external stimuli and reacting in a planned way * Event-driven APIs reduce coupling, as the various objects don't need to know anything about other objects that they work with * We can reduce the number of threads used by relying on event-driven behavior  ## Decision  The framework will be defined with interfaces and processing models that support event-driven development (callbacks, events, register/unregister for events).  Dispatch queues will be provided to assist with the event driven model.  Platform examples will default to dispatch-based processing models.  ## Consequences  * All of our APIs should support an event driven interface * Consideration of callback functions & notification registering needs to be included at all stages * Threading will still be allowed and usable, maintaining flexibility * Dispatch queues will be used to handle generic callbacks without blocking high-priority operations  ## Further Reading  * [Observer Pattern](../../patterns/observer.md) ",architectural_patterns
ADR_155,https://github.com/Flowminder/FlowKit.git,docs/source/developer/adr/0010-prefect-for-autoflow.md,Prefect for workflow definition and execution in AutoFlow,"# Prefect for workflow definition and execution in AutoFlow  Date: 22 November 2019  # Status  Pending  ## Context  The first prototype of AutoFlow used [Apache Airflow](https://airflow.apache.org/) (as used in FlowETL) to define and execute workflows. However, this proved to be problematic in some respects - Airflow has limited support for parametrising DAG runs and sharing data between tasks, and re-running a DAG for an execution date for which it has already run is complicated.  [Prefect Core](https://docs.prefect.io/) is an alternative open-source workflow engine, which allows DAGs to be parametrised and run simultaneously for multiple sets of parameters, and allows data exchange between tasks. Prefect also allows the creation of dynamically-generated tasks mapped over the outputs from running another task, which makes it easier for AutoFlow to spawn multiple runs of a workflow when the sensor finds multiple days of data for which the workflow has not previously run.  ## Decision  AutoFlow will use Prefect to define and run workflows.  ## Consequences  The process of parametrising and dynamically mapping workflow runs is simpler than it would be with Airflow.  Unlike Airflow, Prefect Core is not a full workflow management system - it provides the functionality for defining workflows and running them individually, but the full workflow orchestration and monitoring system is left to the proprietary Prefect Cloud platform. As a result, AutoFlow cannot benefit from a UI such as Airflow provides, and if in the future we want AutoFlow to run multiple sensor workflows we will need to write our own code to do this concurrently. ","governance_and_process, technology_choice"
ADR_156,https://github.com/zendesk/link_platform.git,doc/architecture/decisions/0014-use-redux.md,14. Use redux,# 14. Use redux  Date: 2018-08-01  ## Status  Accepted  ## Context  Redux is a state container for JavaScript applications. It helps to manage state across an application.  [This article](https://hackernoon.com/the-react-state-museum-a278c726315) provides a nice non-comprehensive listing of various alternative state management libraries.  ## Decision  We will use Redux in `link_platform` for state management.  ## Consequences  Redux introduces a layer of complexity when introducing components and handling user interactions. This complexity may be undesirable if our application is simple or does not deal with many changes in state.,technology_choice
ADR_157,https://github.com/home-assistant/architecture.git,adr/0016-home-assistant-core.md,0016. Installation method: Home Assistant Core,"# 0016. Installation method: Home Assistant Core  Date: 2020-07-01  ## Status  Accepted  ## Context  Define a supported installation method as per [ADR-0012](https://github.com/home-assistant/architecture/blob/master/adr/0012-define-supported-installation-method.md).  ## Decision  This is for running just the Home Assistant Core application directly on Python. It does not provide the full Supervisor experience and thus does not provide the Supervisor panel and add-ons.  ### Supported Operating Systems and versions  - All major Linux distributions, latest stable and major versions. - Windows; only using WSL. - macOS; Python via Homebrew.  ### Supported Python versions  Running Home Assistant Core is only supported when running the application using the official Python virtual environment. Running Home Assistant Core without a virtual environment, system/globally installed Python packages, is not supported.  Details on the supported Python versions are defined in [ADR-0020](./0020-minimum-supported-python-version.md).  ### Documentation  Some operating systems will require extra libraries or packages to be installed prior to installing the Python requirements. In this case our documentation shall link to the installation instructions of the Python requirement that requires them.  In case that is not available or possible, we will name the libraries or packages that need to be installed. We do not aim not include installation instructions for every OS.  ### Required Expertise  - **Installation**   Requires installing Python 3 with venv support (the default except on Debian based systems). Then create a virtual environment and install Home Assistant Core via pip.    For packages that require compilation, the user will need to install compilers and other development packages. If those development packages are not the same as provided by the operating system, you can break your system.  * **Start when the system is started:** This is the responsibility of the user. It is based on their operating system. * **Run with full network access:** Works, is the only option. * **Access USB devices:** This works out of the box.  * **Maintaining the Home Assistant installation**   Maintenance requires more time, effort, skills, and experience than the other methods.    - **Python upgrades:** Home Assistant upgrades Python every year. It can happen that your current operating system doesn’t support the new minimum required version out of the box. In that case, you need to find unofficial Python packages for your system or compile Python from source.   - **Installing Python dependencies:** Some Python packages need compilation. Users are responsible for having the right compilers and development packages installed.   - **Updating Home Assistant:** Updating happens via the pip command-line tool.  - **Maintaining the Operating System**   Home Assistant Core runs in a Python virtual environment. Anything outside of that is the responsibility of the user.  * **Security updates for OS:** Responsibility of the user.  * **Maintaining the components required for the Supervisor:** No supervisor, so N/A  **Conclusion:** This is an expert installation method. Based on the integrations that you’re running, you will need a lot of extra packages installed.  ## Consequences  Update documentation on how to install this method, the required experience and the expected maintenance.  Move existing documentation that does not match supported installation methods to the community guides wiki.  Notify user during onboarding of expected maintenance for their installation method. ",others
ADR_158,https://github.com/poorprogrammer/cfo.git,doc/adr/0001-record-architecture-decisions.md,1. Record architecture decisions,"# 1. Record architecture decisions  Date: 2020-05-07  ## Status  Accepted  ## Context  We need to record the architectural decisions made on this project.  ## Decision  We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).  ## Consequences  See Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's [adr-tools](https://github.com/npryce/adr-tools). ",governance_and_process
ADR_159,https://github.com/alphagov/gsp.git,docs/architecture/adr/ADR011-build-artefacts.md,ADR011: Build Artefacts,"# ADR011: Build Artefacts  ## Status  Pending  ## Context  As part of our pipelines we will be building artefacts that will be used to test and deploy our applications. We will be deploying applications to Kubernetes. We will need to build a container image of some kind.  There are some competing container image formats, namely:  * [OCI] * [ACI]  The OCI image format is [based on the Docker v2][oci-standard] image format.  The Kubernetes project appears to [prefer Docker/OCI][k8s-preferance] images over ACI.  [rkt is moving to OCI][rkt-oci] and away from ACI. OCI will become the preferred image format.  Docker has wide industry adoption and appears to have wide understanding within GDS.  Docker is the default container runtime for Kubernetes.  ## Decision  We will build and store OCI images built using Docker.  ## Consequences  * It may be tricky to deploy apps outside of a container orchestrator of some   kind. * This means that the images will need to be pre-built and stored in accessible   way. * We will be unable to use container runtimes that do not support OCI images.  [OCI]: https://github.com/opencontainers/image-spec [ACI]: https://github.com/appc/spec/blob/259c2eebc32df77c016974d5e8eed390d5a81500/spec/aci.md#app-container-image [oci-standard]: https://blog.docker.com/2017/07/oci-release-of-v1-0-runtime-and-image-format-specifications/ [k8s-preferance]: https://kubernetes.io/blog/2015/05/docker-and-kubernetes-and-appc/ [rkt-oci]: https://github.com/rkt/rkt/blob/03285a7db960311faf887452538b2b8ae4304488/ROADMAP.md#oci-native-support ","technology_choice, infrastructure_and_deployment"
ADR_160,https://github.com/wikimedia/mediawiki-extensions-Popups.git,docs/adr/0006-factories.md,6. Factories,"# 6. Factories  Date: 2016-11-08  ## Status  Accepted.  This ADR was accepted implicitly by the (current) primary maintainer of this repository, Sam Smith. The date of this ADR was changed to reflect this.  ## Context  Given that the majority of the codebase is going to be rewritten, there's a need for a consistent style for building the system anew.  The [Reading Web team](https://www.mediawiki.org/wiki/Reading/Web/Team), historically, has tended towards taking an object oriented approach to building software. However, a typical result of this approach are classes that have many varied concerns, share – not specialise – behaviour via inheritance rather than via composition. These issues are evidenced by a lack of unit tests, i.e. the classes become increasingly hard to test and even harder to test in isolation to the point where high-level integration tests are relied on for validation of the design.  Unless attention is paid, these classes have all of their members exposed by default due to a lack of support for visibility modifiers from either the JavaScript language or our (current) tooling. Like other teams, the [Reading Web team](https://www.mediawiki.org/wiki/Reading/Web/Team) tends to follow the convention of prefixing private member names with an underscore.  Moreover, while planning the rewrite of the codebase, [the decision to use Redux to maintain state](./0002-contain-and-manage-state.md) was made very early on. A significant part of the codebase will be written in the style that Redux requires: functions that return objects, or _factories_.  What's needed, then, is a general rule that, when applied, leads the Reading Web team to produce a codebase that's easier to maintain (verify and modify) and is familiar. This rule must also acknowledge that it must be broken now and again.  ## Decision  1. Favour factories over classes – however we wish to define them, e.g. with    [OOjs](https://www.mediawiki.org/wiki/OOjs) – by default. 2. Favour classes when the performance benefits of prototypal inheritance far    outweigh the benefits of consistency and simplicity.  ## Consequences  The most obvious consequence of this decision is the easy portability of the codebase: there's no requirement for a framework to help define classes and manage inheritance, e.g. [OOjs](https://www.mediawiki.org/wiki/OOjs). Moreover, the `new` operator in `new Foo()`, is simply replaced with the `createFoo` factory function, and the `instanceof` operator is rendered useless.  The more subtle consequence is that behaviour must be shared via composition since the use of inheritance is strongly discouraged. The most important positive consequence of this is is that the system will be more flexible as it'll be composed of implementations of small interfaces. The most important negative consequence is that more effort will be required when sharing behaviour between parts of the system, which is trivial using inheritance, as attention must be paid when designing these interfaces.  Despite being the negative consequence, requiring more attention to be paid when defining behaviour should make it harder to write – and easier to spot – components ""that have many varied concerns"" and, hopefully, result in components that are easier to test. ",architectural_patterns
ADR_161,https://github.com/UST-MICO/docs.git,adr/0013-source-to-image-workflow.md,Source-to-Image Workflow,"# Source-to-Image Workflow  Technical Story: [Evaluate Knative build](https://github.com/UST-MICO/mico/issues/49)  ## Context and Problem Statement  We want to have a Source-to-Image workflow to import services based on a GitHub repository. It should run inside our Kubernetes cluster, however currently Kubernetes doesn't have a resource build-in that is able to build container images. Therefore another technology is required.  ## Decision Drivers  * MUST run on our Kubernetes cluster * MUST run completely in userspace (no root access required) * MUST be sufficient to provide a single URL to a GitHub repository (with included Dockerfile) * SHOULD be independent of any cloud service provider  ## Considered Options  * Gitkube * Google Cloud Build * Azure Container Registry Tasks (ACR Tasks) * OpenShift Source-to-Image (S2I) * Knative Build  ## Decision Outcome  Chosen option: *Knative Build*, because it meets all of our criterion decision drivers. It allows us to implement a Source-to-Image workflow on our Kubernetes cluster independently to any cloud service provider.  ### Positive Consequences  * By using *Knative Build* we have the choice to use different kinds of `Builders`, follow-up decision is required: [Building OCI images](./0015-building-oci-images.md)  ### Negative consequences  * *Nothing known*  ## Pros and Cons of the Options  ### Gitkube  [GitHub: Gitkube](https://github.com/hasura/gitkube)  * Good, because it is easy to use (only git and kubectl are required) * Good, because it is designed to build and deploy images to Kubernetes * Bad, because it uses Docker-in-Docker approach (see [Building OCI Images](./0015-building-oci-images.md)) * Bad, because it is build for a different purpose: Helping developers to trigger the build and deployment of an image by using `git push`  ### Google Cloud Build  [Cloud Build documentation](https://cloud.google.com/cloud-build/docs/)  * Good, because it is easy to use, nothing has to be installed (only a call to the `Cloud Build API` is required) * Good, because it is a managed service therefore it doesn't consume our own resources * Good, because it allows us to use different `Builder` technologies (see [Building OCI Images](./0015-building-oci-images.md)) * Bad, because we depend on the continuity of a third-party service (Google Cloud Build) * Bad, because it runs only on Google Cloud, that forces vendor lock-in * Bad, because it leads to additional costs (first 120 builds-minutes per day are free, see [Pricing](https://cloud.google.com/cloud-build/pricing)  ### Azure Container Registry Tasks (ACR Tasks)  [Tutorial: Automate container image builds with Azure Container Registry Tasks](https://docs.microsoft.com/en-us/azure/container-registry/container-registry-tutorial-build-task)  * Good, because it is a managed service therefore it doesn't consume our own resources * Good, because we already run on the Azure Cloud * Bad, because it uses the Docker daemon internally, no other `Builders` can be used (see [Building OCI Images](./0015-building-oci-images.md)) * Bad, because there is no public API available, only usable with the Azure CLI * Bad, because we depend on the continuity of a third-party service (ACR Tasks) * Bad, because it runs only on Azure, that forces vendor lock-in * Bad, because it leads to additional costs (100 minutes are included for free per month, see [Pricing calculator](https://azure.microsoft.com/en-us/pricing/calculator/#)  ### OpenShift Source-to-Image (S2I)  [GitHub: Source-To-Image (S2I)](https://github.com/openshift/source-to-image)  * Good, because it is the way to go for creating a Source-to-Image workflow on a OpenShift cluster * Bad, because it is designed for the OpenShift Container Platform (additional effort is required to use it on plain Kubernetes) * Bad, because it uses the Docker daemon internally (in future *Buildah*), no other `Builders` can be used (see [Building OCI Images](./0015-building-oci-images.md))  ### Knative Build  [GitHub: Knative Build](https://github.com/knative/build)  * Good, because it has the backing of industry giants (Google, Red Hat, IBM, SAP) * Good, because it is designed for Kubernetes * Good, because it provides a standard, portable, reusable, and performance optimized method for defining and running on-cluster container image builds * Good, because it allows us to use different `Builder` technologies (see [Building OCI Images](./0015-building-oci-images.md)) * Good, because *Knative Build* consumes little resources (2 pods a ~11 MB). * Bad, because it is still work-in-progress ",technology_choice
ADR_162,https://github.com/nulib/meadow.git,doc/architecture/decisions/0008-api-documentation.md,8. api-documentation,"# 8. api-documentation  Date: 2019-07-01  ## Status  Accepted  ## Context  We want our [API](./0004-api.md) to be self-documenting and testable as we go.  ## Decision  Use an [OpenAPI hex package](https://hexdocs.pm/open_api_spex) to automate OpenAPI tasks.  ## Consequences  Generating, validating, coding, and testing our APIs is much faster and easier. ","governance_and_process, technology_choice"
ADR_163,https://github.com/alphagov/verify-proxy-node.git,doc/adr/20190327-hashing-attributes.md,Hashing the identity information,"# Hashing the identity information  Date: 2019-03-27  ## Status  Accepted  ## Context  The SAML profile used by the eIDAS scheme requires that SAML messages are sent directly to the receiving proxy node via a user's browser. This introduces a host of security concerns, especially when we take the history of SAML exploits and XML security in general into account. Due to many of the attacks being centred around breaking XML parsing, the main concerns raised were of SAML being parsed by a public-facing service or the service responsible for communicating with the hardware security module (HSM). With all this considered, the role of the SAML parsing has been assigned to the Verify Service Provider (VSP).   We need to have confidence that the identity information we are sending back to the Member State is the same as the identity information which was originally issued by the identity provider (IDP). The Gateway service will receieve a SAML response back from the Hub which is sent to the Translator service, before being passed onto the VSP which will parse the response. Due to the exploits surrounding XML parsing, we need to be confident that the VSP hasn't been compromised, as it could result in manipulated attributes being sent to the Translator which would ultimately result in fraudulent identities being sent back to a Member State.  ## Decision  To ensure that we are confident that the identity information we are sending back to the Member State is the same as what was originally issued by the IDP, we will LOG to monitoring a secure one-way SHA-256 hash of the user attributes in the Verify Hub and within the Translator service in the eIDAS Proxy Node. Any mismatches between the two Hashes will trigger an alert.   ## Consequences  - This would be more beneficial to be implemented as a technical control, to have a higher confidence that no malicious transactions are approved.","technology_choice, observability"
ADR_164,https://github.com/Alfresco/alfresco-anaxes-shipyard.git,docs/adrs/0009-using-one-db-per-service.md,9. Using one database per service,"# 9. Using one database per service  Date: 2018-01-10  ## Status  Accepted  ## Context  Many of the components the Digital Business Platform offers need a database for storing information. Currently Content Services, Process Services and Sync Service do this but there might be more to come in the future. All three are using the same version of the database so the obvious question was raised: Can we just use one database for all our services?   We identified several advantages of using one database per service over one for all the services.  Choosing one database per service would put us in line with the current best practices in the microservices world, as well as allowing us to change versions of the DB individually, and as an added benefit it kills off the possibility of one service being able to impact performance on other services.  If we would choose one database for all our components however we would have fewer components to configure, keep track of, monitor and backup.  ## Decision  We will proceed with the one database per service for existing and future components as there are a number of advantages over having all our services fight over the same database.  ## Consequences  With each new component that has the need for database storage, we would automatically have one more configuration to manage, monitor and backup. ",microservices_and_modularity
ADR_165,https://github.com/sul-dlss-labs/infrastructure-adrs.git,0002-extract-only-useful-technical-metadata.md,(sem título),"--- layout: default title: ADR-0002 nav_order: 5 permalink: records/0002/ --- # Extract Only Useful Technical Metadata  * Status: drafted * Decider(s): <!-- required -->   * Andrew Berger   * Hannah Frost   * Vivian Wong   * Infrastructure Team     * Justin Coyne     * Mike Giarlo     * Peter Mangiafico     * Jeremy Nelson     * Justin Littman     * Naomi Dushay     * John Martin     * Aaron Collier * Date(s): <!-- required -->   * drafted: 2019-10-29  ## Context and Problem Statement <!-- required -->  Currently we are using JHOVE 1.x to generate voluminous technical metadata for every file of every object accessioned in SDR, and we do not use most of this metadata. This is problematic especially for large & many files: we cannot currently accessioning books with many pages because the technical metadata robot consumes all system memory which causes the virtual machine to kill the JHOVE process. We believe that only a small subset of the JHOVE output will ever be useful to SDR consumers.  Note: SMPL content ships with its own metadata typically from MediaInfo rather than JHOVE.  ## Decision Drivers <!-- optional -->  * Cannot accession large files (objects > 1GB or so) * Cannot accession objects with many pages, such as books * Blocker for Google Books project * Causes extreme delays accessioning other content  ## Open Questions  * What does SMPL do with technical metadata datastream. Should this be cross-walked or added as an attached binary? * How does this relate to the file identification as part of the contentMetadata? Should we persist the modeled data as a new type of datastream or as part of contentMetadatata?  ## Considered Options <!-- required -->  1. Do nothing 1. Identify, and only persist, *needed* technical metadata in a well-defined data model (*.e.g.*, the [one used in Princeton's figgy app](https://github.com/pulibrary/figgy/blob/main/app/resources/nested_resources/file_metadata.rb#L4-L35)), using a tool (or tools) other than JHOVE 1. Stop extracting technical metadata, though this may conflict with being considered a trusted digital repository 1. Add resources to worker machines 1. Run extraction using cloud-based processing  ## Decision Outcome <!-- required -->  **Preferred** (by Infrastructure Team) option: option 2, because:  * Option 1 is preventing us from accessioning books and other large objects, which is unacceptable to SDR customers * Option 3 is an unsound preservation strategy and does not meet SDR user needs * Option 4 has already been pursued a number of times already, and there's only so much we can toss at the worker machines * Option 5 has been rejected as a general deployment strategy for now  Thus, option 2 is the only option that currently meets the department's and its customers' needs.  As part of this work, we will move forward with a two-prong strategy in order to resolve the tension between the need to come up with a sound, community-oriented preservation practice and the need to accession large-scale content now.  In the short-term, we will come up with a short list of technical metadata attributes that will be extracted from all files and from all files of certain types. We will convene a “technical metadata strike team” in short order that will review attributes being used in Samvera and make recommendations based thereupon. The aim is for this group to finalize their recommendations in advance of the January 2020 Google Books work cycle.  In parallel, we will pursue a longer-term effort for determining what an ideal, community-oriented strategy is for doing this work building on best practices (which are currently murky/non-emergent). Along with this longer-term work, we will look into how to support on-demand regeneration of technical metadata so that we can iterate on the short-term work in the prior bullet. ",observability
ADR_166,https://github.com/budproj/architecture-decision-log.git,adl/0003-domain-driven-design.md,ADR 0003: Domain-Driven Design,"# ADR 0003: Domain-Driven Design  * [Table of contents](#)   * [Context](#context)   * [Decision](#decision)   * [Status](#status)   * [Consequences](#consequences)   * [More reading](#more-reading)  ## Context  During software development, the most common mistake is not speaking the same language with stakeholders and the product team. It is common having different points of view of the product architecture, and more significant gaps mean higher blockers and mistakes in the future.  Since we're creating a new product, we have the advantage of having the same language and architecture overview from day zero, meaning a more stable and robust infrastructure.  ## Decision  To prevent these issues, we decided to implement the Design-driven Development framework to architect our domain collaboratively with the product team and business owners.  With DDD, we can ensure proper refactors in the future, having a clear overview of our entities' relationships and domains. To learn more about DDD, [check this summary](https://medium.com/@ruxijitianu/summary-of-the-domain-driven-design-concepts-9dd1a6f90091).  We can't have a clear, detailed overview of our entire domain structure (that would be overwhelming), but we've organized our DDD in a way that we can have a high-level concept of the whole company and drill-down to see a more detailed view of each bounded context.  **The high-level concept of our Domain-Driven Design** is located in this repository, at [ADR#0015](0015-model-overview.md). You can take a look at that record to learn more about our architecture overview.  **Bounded context detailed domains** are located in the same folder (`records/domains`), but there is a single file for each bounded context. Those architectures focus only on the given bounded context and any entity that relates to it.  Each microservice we have are related to a single bounded context. So, to create new microservices, you must add a new bounded context to our domain architecture.  ## Status  Accepted.  ## Consequences  Domain-Driven Design is not a common topic. We must teach our developers to use it. But, this effort may reward us well. A good, stable, and reliable domain architecture is the key to a successful company.  We must ensure our onboarding process covers this paradigm to avoid any unwanted unrelated domain entity.  ---  ## More reading  * [Brief DDD summary](https://medium.com/@ruxijitianu/summary-of-the-domain-driven-design-concepts-9dd1a6f90091)  * [Book](https://www.amazon.com.br/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215/ref=asc_df_0321125215/?tag=googleshopp00-20&linkCode=df0&hvadid=379735814613&hvpos=&hvnetw=g&hvrand=12360278098423015108&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1001751&hvtargid=pla-449269547899&psc=1) ",architectural_patterns
ADR_167,https://github.com/status-im/status-react.git,doc/decisions/0009-release-process-mobile.md,0009. Mobile App Release Process,"# 0009. Mobile App Release Process  | Date | Tags | |---|---| | Tue Jul 24  | process, release |   ## Status  accepted   ## Context  Classical release approach: we cut off the release branch, stabilize it, fix every critical issue and release. Some changes are cherry-picked to the release branch.  It has a couple of downsides:  - Resources are scattered across two branches;  - Unique fixes and unique combination of commits in the release branch;  - Unpredictable release schedule (any day can be a release day!).  ## The Process  We do ""failable releases"" approach instead, when the release either happens on a specific day, or doesn't happen. We aim for a weekly cadence. But even more, we aim to be frank about the release state, give ourselves a permission to fail a release.   ### The Release Checklist _☝️  a release blocker is a GHI with “release” tag on it_  1. Do a release testing on a nightly. 1. If it is good enough (no release blockers), cut a release branch, like `releases/0.9.23`. If there is an existing branch from the previous unsuccessful release, rebase it to the current state of `develop`. 1. 🔄 Test the release branch, fix release blockers. 1. Cherry-pick only release blockers to the release branch. 1. Check-up with other teams (@go, #core-infra). 1. Mobile releases should not happen at the same time as cluster upgrades. 1. *After* the release branch is cut      - update status-go on `develop` (NOT the release branch);     - bump the app version on `develop` (NOT the release branch). 1. Update release notes, and app descriptions in GP and App Store (see [this section](#release-notes)). 1. If [“go/no go” assessment](#go-no-go) is negative (“no go”), just abandon the release branch.   ### Failable Releases Failable release philosophy: - We track potential release blockers as early as possible (based on testing of nightlies); - We cut off the release branch when there are no big blockers (wednesday morning the latest); - We fix remaining release blockers on `develop` and cherry-pick fixes to the release branch; - If we aren’t able to fix all release blockers in time™, leaving enough time for QA to thoroughly test the release, we mark this release as failed and focus on releasing next week; - Next week we just rebase the release branch on `develop`.  ### What is this failed release anyway? 1. Nothing is published to our users; 1. We don’t keep the release branch around, next week we force-rebase it to the latest state of `develop`.  ### Is it bad to fail a release? Nope. One of the nice side-effects of the failable release approach that it shows the real state of the develop branch.  Trying too hard to release anyway might paint a picture that is better than the reality. If there are too many failed releases, it is an indication that something is wrong with our `develop` or PR intakes, not with the release process. Don’t shoot the messenger :)  ### <a name=""go-no-go""></a> “Go/No-Go” decision The “no-go” decision can be make: 1. If there is a huge blocker on develop that we are not sure we will be able to fix in time; 1. If there is no time left for QA to make thorough testing; 1. If we don’t feel confident in some critical feature of the app, even if there is no critical issues found there; 1. If one of the teams (status-go, cluster, etc) isn’t ready for this release (check with the `#core-infra` or `@go`).  ### Schedule - We aim to submit an iTC build every Friday to have time for Apple to review it. - We aim to publish a release every Monday.  Note, that to aim is a key word there. If we fail to release in time, we just skip this week’s slot and try to release next week.  So, schedule might look like that: ``` May, 11: Release 1 May, 18: failed release, nothing is published May, 25: failed release, nothing is published June, 1: Release 2 ... ```  So, as you can see releases happen only on Mondays. They might or might not happen, but the schedule stays consistent.  ### <a name=""release-notes""></a>Release Notes We keep the file [`CHANGELOG.md`](../../CHANGELOG.md) in the repository. We also have an ongoing document with them.  #### iOS Test Flight Release **Upload to AppStore Connect** Use [this Jenkins job](https://jenkins.status.im/job/status-mobile/job/upload_release_ios/)  **“What to test” field** When AppStore Connect asks you to fill in the field called “what to test”, just copy the release notes there.  **Submitting to the review** We submit it on Friday, fix the compliance and add the group called “External Testers”. Don’t make the group name scare you, the real testers are in the group called “testflight-boarding”.   Don’t forget to update screenshots if necessary!  Then we submit it to Apple review.  **Releasing to our beta-testers** If reviewed successfully, we can share it to our users by adding “testflight-boarding” group to our build. As soon as it is added, invitations to upgrade are sent to our beta-testers!  ### GP Release The uploaded release is **immediately available**!  Do it only if the iOS build is approved by Apple!  Use [this Jenkins job](https://jenkins.status.im/job/status-mobile/job/upload_release_android/)  Don’t forget to update the screenshots if necessary!  **App Description** App description needs to be updated. It is much shorter than the release notes, so it is important to trim them down for GP.  ### Flexing & Planning With this approach we don’t plan features for release. We plan features for priorities. What is the difference? When we plan features for release, a feature A absolutely has to be included in release 0.2. That means, that if feature is not ready yet, the release 0.2 is not happening.  In features for priorities approach we plan which feature comes before or after which. Say, if we have features A, B and C and we know that A is more important than B and C then we will try to release it earlier.   In that case releases history might look like these: ``` Week 1: 0.1 - failed Week 2: 0.1 - feature A released Week 3: 0.2 - polishing of feature A and bugfixes Week 4: 0.3 - feature B Week 5: 0.4 - feature C ... ```  So we keep releasing cadence even if there are no features to release.  ### Retrospective Each release ends with a short retrospective/planning session for the next week.    ",others
ADR_168,https://github.com/cosmos/ibc-go.git,docs/architecture/adr-001-coin-source-tracing.md,ADR 001: Coin Source Tracing,"# ADR 001: Coin Source Tracing  ## Changelog  - 2020-07-09: Initial Draft - 2020-08-11: Implementation changes  ## Status  Accepted, Implemented  ## Context  The specification for IBC cross-chain fungible token transfers ([ICS20](https://github.com/cosmos/ibc/tree/master/spec/app/ics-020-fungible-token-transfer)), needs to be aware of the origin of any token denomination in order to relay a `Packet` which contains the sender and recipient addresses in the [`FungibleTokenPacketData`](https://github.com/cosmos/ibc/tree/master/spec/app/ics-020-fungible-token-transfer#data-structures).  The Packet relay sending works based in 2 cases (per [specification](https://github.com/cosmos/ibc/tree/master/spec/app/ics-020-fungible-token-transfer#packet-relay) and [Colin Axnér](https://github.com/colin-axner)'s description):  1. Sender chain is acting as the source zone. The coins are transferred to an escrow address (i.e locked) on the sender chain and then transferred to the receiving chain through IBC TAO logic. It is expected that the receiving chain will mint vouchers to the receiving address.  2. Sender chain is acting as the sink zone. The coins (vouchers) are burned on the sender chain and then transferred to the receiving chain through IBC TAO logic. It is expected that the receiving chain, which had previously sent the original denomination, will unescrow the fungible token and send it to the receiving address.  Another way of thinking of source and sink zones is through the token's timeline. Each send to any chain other than the one it was previously received from is a movement forwards in the token's timeline. This causes trace to be added to the token's history and the destination port and destination channel to be prefixed to the denomination. In these instances the sender chain is acting as the source zone. When the token is sent back to the chain it previously received from, the prefix is removed. This is a backwards movement in the token's timeline and the sender chain is acting as the sink zone.  ### Example  Assume the following channel connections exist and that all channels use the port ID `transfer`:  - chain `A` has channels with chain `B` and chain `C` with the IDs `channelToB` and `channelToC`, respectively - chain `B` has channels with chain `A` and chain `C` with the IDs `channelToA` and `channelToC`, respectively - chain `C` has channels with chain `A` and chain `B` with the IDs `channelToA` and `channelToB`, respectively  These steps of transfer between chains occur in the following order: `A -> B -> C -> A -> C`. In particular:  1. `A -> B`: sender chain is source zone. `A` sends packet with `denom` (escrowed on `A`), `B` receives `denom` and mints and sends voucher `transfer/channelToA/denom` to recipient. 2. `B -> C`: sender chain is source zone. `B` sends packet with `transfer/channelToA/denom` (escrowed on `B`), `C` receives `transfer/channelToA/denom` and mints and sends voucher `transfer/channelToB/transfer/channelToA/denom` to recipient. 3. `C -> A`: sender chain is source zone. `C` sends packet with `transfer/channelToB/transfer/channelToA/denom` (escrowed on `C`), `A` receives `transfer/channelToB/transfer/channelToA/denom` and mints and sends voucher `transfer/channelToC/transfer/channelToB/transfer/channelToA/denom` to recipient. 4. `A -> C`: sender chain is sink zone. `A` sends packet with `transfer/channelToC/transfer/channelToB/transfer/channelToA/denom` (burned on `A`), `C` receives `transfer/channelToC/transfer/channelToB/transfer/channelToA/denom`, and unescrows and sends `transfer/channelToB/transfer/channelToA/denom` to recipient.  The token has a final denomination on chain `C` of `transfer/channelToB/transfer/channelToA/denom`, where `transfer/channelToB/transfer/channelToA` is the trace information.  In this context, upon a receive of a cross-chain fungible token transfer, if the sender chain is the source of the token, the protocol prefixes the denomination with the port and channel identifiers in the following format:  ```typescript prefix + denom = {destPortN}/{destChannelN}/.../{destPort0}/{destChannel0}/denom ```  Example: transferring `100 uatom` from port `HubPort` and channel `HubChannel` on the Hub to Ethermint's port `EthermintPort` and channel `EthermintChannel` results in `100 EthermintPort/EthermintChannel/uatom`, where `EthermintPort/EthermintChannel/uatom` is the new denomination on the receiving chain.  In the case those tokens are transferred back to the Hub (i.e the **source** chain), the prefix is trimmed and the token denomination updated to the original one.  ### Problem  The problem of adding additional information to the coin denomination is twofold:  1. The ever increasing length if tokens are transferred to zones other than the source:  If a token is transferred `n` times via IBC to a sink chain, the token denom will contain `n` pairs of prefixes, as shown on the format example above. This poses a problem because, while port and channel identifiers have a maximum length of 64 each, the SDK `Coin` type only accepts denoms up to 64 characters. Thus, a single cross-chain token, which again, is composed by the port and channels identifiers plus the base denomination, can exceed the length validation for the SDK `Coins`.  This can result in undesired behaviours such as tokens not being able to be transferred to multiple sink chains if the denomination exceeds the length or unexpected `panics` due to denomination validation failing on the receiving chain.  2. The existence of special characters and uppercase letters on the denomination:  In the SDK every time a `Coin` is initialized through the constructor function `NewCoin`, a validation of a coin's denom is performed according to a [Regex](https://github.com/cosmos/cosmos-sdk/blob/a940214a4923a3bf9a9161cd14bd3072299cd0c9/types/coin.go#L583), where only lowercase alphanumeric characters are accepted. While this is desirable for native denominations to keep a clean UX, it presents a challenge for IBC as ports and channels might be randomly generated with special and uppercase characters as per the [ICS 024 - Host Requirements](https://github.com/cosmos/ibc/tree/master/spec/core/ics-024-host-requirements#paths-identifiers-separators) specification.  ## Decision  The issues outlined above, are applicable only to SDK-based chains, and thus the proposed solution are do not require specification changes that would result in modification to other implementations of the ICS20 spec.  Instead of adding the identifiers on the coin denomination directly, the proposed solution hashes the denomination prefix in order to get a consistent length for all the cross-chain fungible tokens.  This will be used for internal storage only, and when transferred via IBC to a different chain, the denomination specified on the packed data will be the full prefix path of the identifiers needed to trace the token back to the originating chain, as specified on ICS20.  The new proposed format will be the following:  ```go ibcDenom = ""ibc/"" + hash(trace path + ""/"" + base denom) ```  The hash function will be a SHA256 hash of the fields of the `DenomTrace`:  ```protobuf // DenomTrace contains the base denomination for ICS20 fungible tokens and the source tracing // information message DenomTrace {   // chain of port/channel identifiers used for tracing the source of the fungible token   string path = 1;   // base denomination of the relayed fungible token   string base_denom = 2; } ```  The `IBCDenom` function constructs the `Coin` denomination used when creating the ICS20 fungible token packet data:  ```go // Hash returns the hex bytes of the SHA256 hash of the DenomTrace fields using the following formula: // // hash = sha256(tracePath + ""/"" + baseDenom) func (dt DenomTrace) Hash() tmbytes.HexBytes {   return tmhash.Sum(dt.Path + ""/"" + dt.BaseDenom) }  // IBCDenom a coin denomination for an ICS20 fungible token in the format 'ibc/{hash(tracePath + baseDenom)}'.  // If the trace is empty, it will return the base denomination. func (dt DenomTrace) IBCDenom() string {   if dt.Path != """" {     return fmt.Sprintf(""ibc/%s"", dt.Hash())   }   return dt.BaseDenom } ```  ### `x/ibc-transfer` Changes  In order to retrieve the trace information from an IBC denomination, a lookup table needs to be added to the `ibc-transfer` module. These values need to also be persisted between upgrades, meaning that a new `[]DenomTrace` `GenesisState` field state needs to be added to the module:  ```go // GetDenomTrace retrieves the full identifiers trace and base denomination from the store. func (k Keeper) GetDenomTrace(ctx Context, denomTraceHash []byte) (DenomTrace, bool) {   store := ctx.KVStore(k.storeKey)   bz := store.Get(types.KeyDenomTrace(traceHash))   if bz == nil {     return &DenomTrace, false   }    var denomTrace DenomTrace   k.cdc.MustUnmarshalBinaryBare(bz, &denomTrace)   return denomTrace, true }  // HasDenomTrace checks if a the key with the given trace hash exists on the store. func (k Keeper) HasDenomTrace(ctx Context, denomTraceHash []byte)  bool {   store := ctx.KVStore(k.storeKey)   return store.Has(types.KeyTrace(denomTraceHash)) }  // SetDenomTrace sets a new {trace hash -> trace} pair to the store. func (k Keeper) SetDenomTrace(ctx Context, denomTrace DenomTrace) {   store := ctx.KVStore(k.storeKey)   bz := k.cdc.MustMarshalBinaryBare(&denomTrace)   store.Set(types.KeyTrace(denomTrace.Hash()), bz) } ```  The `MsgTransfer` will validate that the `Coin` denomination from the `Token` field contains a valid hash, if the trace info is provided, or that the base denominations matches:  ```go func (msg MsgTransfer) ValidateBasic() error {   // ...   return ValidateIBCDenom(msg.Token.Denom) } ```  ```go // ValidateIBCDenom validates that the given denomination is either: // //  - A valid base denomination (eg: 'uatom') //  - A valid fungible token representation (i.e 'ibc/{hash}') per ADR 001 https://github.com/cosmos/ibc-go/blob/main/docs/architecture/adr-001-coin-source-tracing.md func ValidateIBCDenom(denom string) error {   denomSplit := strings.SplitN(denom, ""/"", 2)    switch {   case strings.TrimSpace(denom) == """",     len(denomSplit) == 1 && denomSplit[0] == ""ibc"",     len(denomSplit) == 2 && (denomSplit[0] != ""ibc"" || strings.TrimSpace(denomSplit[1]) == """"):     return sdkerrors.Wrapf(ErrInvalidDenomForTransfer, ""denomination should be prefixed with the format 'ibc/{hash(trace + \""/\"" + %s)}'"", denom)    case denomSplit[0] == denom && strings.TrimSpace(denom) != """":     return sdk.ValidateDenom(denom)   }    if _, err := ParseHexHash(denomSplit[1]); err != nil {     return Wrapf(err, ""invalid denom trace hash %s"", denomSplit[1])   }    return nil } ```  The denomination trace info only needs to be updated when token is received:  - Receiver is **source** chain: The receiver created the token and must have the trace lookup already stored (if necessary *ie* native token case wouldn't need a lookup). - Receiver is **not source** chain: Store the received info. For example, during step 1, when chain `B` receives `transfer/channelToA/denom`.  ```go // SendTransfer // ...    fullDenomPath := token.Denom  // deconstruct the token denomination into the denomination trace info // to determine if the sender is the source chain if strings.HasPrefix(token.Denom, ""ibc/"") {   fullDenomPath, err = k.DenomPathFromHash(ctx, token.Denom)   if err != nil {     return err   } }  if types.SenderChainIsSource(sourcePort, sourceChannel, fullDenomPath) { //... ```  ```go // DenomPathFromHash returns the full denomination path prefix from an ibc denom with a hash // component. func (k Keeper) DenomPathFromHash(ctx sdk.Context, denom string) (string, error) {   hexHash := denom[4:]   hash, err := ParseHexHash(hexHash)   if err != nil {     return """", Wrap(ErrInvalidDenomForTransfer, err.Error())   }    denomTrace, found := k.GetDenomTrace(ctx, hash)   if !found {     return """", Wrap(ErrTraceNotFound, hexHash)   }    fullDenomPath := denomTrace.GetFullDenomPath()   return fullDenomPath, nil } ```  ```go // OnRecvPacket // ...  // This is the prefix that would have been prefixed to the denomination // on sender chain IF and only if the token originally came from the // receiving chain. // // NOTE: We use SourcePort and SourceChannel here, because the counterparty // chain would have prefixed with DestPort and DestChannel when originally // receiving this coin as seen in the ""sender chain is the source"" condition. if ReceiverChainIsSource(packet.GetSourcePort(), packet.GetSourceChannel(), data.Denom) {   // sender chain is not the source, unescrow tokens    // remove prefix added by sender chain   voucherPrefix := types.GetDenomPrefix(packet.GetSourcePort(), packet.GetSourceChannel())   unprefixedDenom := data.Denom[len(voucherPrefix):]   token := sdk.NewCoin(unprefixedDenom, sdk.NewIntFromUint64(data.Amount))    // unescrow tokens   escrowAddress := types.GetEscrowAddress(packet.GetDestPort(), packet.GetDestChannel())   return k.bankKeeper.SendCoins(ctx, escrowAddress, receiver, sdk.NewCoins(token)) }  // sender chain is the source, mint vouchers  // since SendPacket did not prefix the denomination, we must prefix denomination here sourcePrefix := types.GetDenomPrefix(packet.GetDestPort(), packet.GetDestChannel()) // NOTE: sourcePrefix contains the trailing ""/"" prefixedDenom := sourcePrefix + data.Denom  // construct the denomination trace from the full raw denomination denomTrace := types.ParseDenomTrace(prefixedDenom)  // set the value to the lookup table if not stored already traceHash := denomTrace.Hash() if !k.HasDenomTrace(ctx, traceHash) {   k.SetDenomTrace(ctx, traceHash, denomTrace) }  voucherDenom := denomTrace.IBCDenom() voucher := sdk.NewCoin(voucherDenom, sdk.NewIntFromUint64(data.Amount))  // mint new tokens if the source of the transfer is the same chain if err := k.bankKeeper.MintCoins(   ctx, types.ModuleName, sdk.NewCoins(voucher), ); err != nil {   return err }  // send to receiver return k.bankKeeper.SendCoinsFromModuleToAccount(   ctx, types.ModuleName, receiver, sdk.NewCoins(voucher), ) ```  ```go func NewDenomTraceFromRawDenom(denom string) DenomTrace{   denomSplit := strings.Split(denom, ""/"")   trace := """"   if len(denomSplit) > 1 {     trace = strings.Join(denomSplit[:len(denomSplit)-1], ""/"")   }   return DenomTrace{     BaseDenom: denomSplit[len(denomSplit)-1],     Trace:     trace,   } } ```  One final remark is that the `FungibleTokenPacketData` will remain the same, i.e with the prefixed full denomination, since the receiving chain may not be an SDK-based chain.  ### Coin Changes  The coin denomination validation will need to be updated to reflect these changes. In particular, the denomination validation function will now:  - Accept slash separators (`""/""`) and uppercase characters (due to the `HexBytes` format) - Bump the maximum character length to 128, as the hex representation used by Tendermint's   `HexBytes` type contains 64 characters.  Additional validation logic, such as verifying the length of the hash, the  may be added to the bank module in the future if the [custom base denomination validation](https://github.com/cosmos/cosmos-sdk/pull/6755) is integrated into the SDK.  ### Positive  - Clearer separation of the source tracing behaviour of the token (transfer prefix) from the original   `Coin` denomination - Consistent validation of `Coin` fields (i.e no special characters, fixed max length) - Cleaner `Coin` and standard denominations for IBC - No additional fields to SDK `Coin`  ### Negative  - Store each set of tracing denomination identifiers on the `ibc-transfer` module store - Clients will have to fetch the base denomination every time they receive a new relayed fungible token over IBC. This can be mitigated using a map/cache for already seen hashes on the client side. Other forms of mitigation, would be opening a websocket connection subscribe to incoming events.  ### Neutral  - Slight difference with the ICS20 spec - Additional validation logic for IBC coins on the `ibc-transfer` module - Additional genesis fields - Slightly increases the gas usage on cross-chain transfers due to access to the store. This should   be inter-block cached if transfers are frequent.  ## References  - [ICS 20 - Fungible token transfer](https://github.com/cosmos/ibc/tree/master/spec/app/ics-020-fungible-token-transfer) - [Custom Coin Denomination validation](https://github.com/cosmos/cosmos-sdk/pull/6755) ",others
ADR_169,https://github.com/nhsuk/sexual-health-service-finder.git,doc/adr/0005-calculate-distance-between-origin-and-result-items-within-the-application.md,5. Calculate distance between origin and result items within the application,"# 5. Calculate distance between origin and result items within the application  Date: 2018-12-05  ## Status  Accepted  ## Context  The move to Azure search has introduced the need to calculate the distance between the search point and each result item. Previously, when using Elasticsearch, the distance was returned within the query response. Azure search does not have this capability, it is currently a [feature request](https://feedback.azure.com/forums/263029-azure-search/suggestions/17760211-support-geo-distance-in-select-result).  ## Decision  The decision is to calculate the distance between the search point and each result item within the consuming application i.e. the web app. The calculation for [great-circle distance](https://en.wikipedia.org/wiki/Great-circle_distance) is well known and available in numerous languages.  ## Consequences  One of the consequences is the web app will contain additional code to perform the calculation. It is unlikely the code will need to be changed therefore the additional overhead will be minimal. Another consequence is that the web app will take some additional time in order to calculate the distance and serve requests. No specific timings have been recorded to compare the duration of the web app performing the calculation rather than the search API. The timing is small regardless of where it occurs and no appreciable difference to the overall request/response duration is expected. ",others
ADR_170,https://github.com/dennisseidel/saas-plaform-tenant-identity-provider.git,adr/0001-record-architecture-decisions.md,1. Record architecture decisions,"# 1. Record architecture decisions  Date: 2019-02-05  ## Status  Accepted  ## Context  We need to record the architectural decisions made on this project.  ## Decision  We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).  ## Consequences  See Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's [adr-tools](https://github.com/npryce/adr-tools). ",governance_and_process
ADR_171,https://github.com/DFE-Digital/buy-for-your-school.git,doc/architecture/decisions/0002-use-pull-request-templates.md,2. Use Pull Request Templates,"# 2. Use Pull Request Templates  Date: 2019-08-16  ## Status  ![Accepted](https://img.shields.io/badge/adr-accepted-green)  ## Context  The quality of information included in our pull requests varies greatly which can lead to code reviews which take longer and are harder for the person to understand the considerations, outcomes and consequences of a series of changes.  A couple of recent projects have found a GitHub pull request template to have been a positive change. Prompting what pull request descriptions should include has lead to better documented changes that have been easier to review on the whole.   ## Decision  Include a basic pull request template for GitHub so that every pull request prompts every author to fill it out.  ## Consequences  - a small overhead to every pull request which may prompt us to spend more time in creating a pull request. We accept the cost of this to gain the benefits of easier reviews, and better documented changes - writing good pull request descriptions is not to be done instead of commit messages, they are the primary source of storing rationale that will persist. Commit messages may be duplicated into the pull request description. - prompting authors to articulate their thought process and decision making can improve the quality of code before a reviewer becomes involved. Essentially you are following the process of [rubber ducking](https://en.wikipedia.org/wiki/Rubber_duck_debugging) which gives you the final opportunity to amend your proposal - this is not compatible when source code is hosted on other services eg. [GitLab requires a file of a different name](https://docs.gitlab.com/ee/user/project/description_templates.html#creating-merge-request-templates), as all of our Rails projects are on GitHub this should be a minor issue - asking for screenshots doesn't always make sense especially when production deployments are facilitated by pull requests from develop into master. We accept that in these cases the cost of editing the template is small enough to not be a problem. The first header that prompts to describe changes does still apply - not all pull requests result in a frontend change that requires screenshots, we believe that these prompts can be removed quickly with a minor cost to the author ",governance_and_process
ADR_172,https://github.com/openregister/openregister-java.git,doc/arch/adr-011-root-path-handling.md,Decision record: Root path handling,"# Decision record: Root path handling  ## Context  As part of the work to release the next major version, there are three routes that need special handling: `/`, `/v1` and `/v2`.  This ADR proposes a behaviour for each one of them.  ## Decision  ### Root (`/`)  When HTML is requested, it should return the exact same HTML as per now.  Otherwise it should redirect (301) to the Register resource (`/{version}/register`) for the requested version in JSON regardless of the format specified.  ### Version root `/v1`  When HTML is requested, it should redirect (301) to `/` (See above).  Otherwise it should redirect (301) to the Register resource (`/{version}/register`) for the requested version in JSON regardless of the format specified.  ### Version root `/v2`  It should redirect (301) to the Register resource (`/{version}/register`) for the requested version in JSON regardless of the format specified.  ## Status  Accepted. ",governance_and_process
ADR_173,https://github.com/edx/event-routing-backends.git,docs/decisions/0009-persistence-and-retries-for-events.rst,(sem título),"Persistence and retries for events ==================================  Status ------  Approved  Context -------  `event-routing-backends` transmits events to configured recipients (Learning Record Stores) via http protocol in near real-time. A strategy needs to be implemented to handle the case when an LRS's link is down.  Decision --------  1. A celery task will be created for each transformation (xAPI or Caliper) of an event. Once the transformation is complete, nested celery tasks will be created, one for each recipient, to route the event.  2. Retry attempts shall be made for each recipient, for all events types, and for a configured number of retries and delay between each retry.  3. A limited type of events (namely *business critical events*) shall be persisted even after all retry attempts have been exhausted. Celery tasks, that failed to route the transformed event to their intended recipients, will be stored in a database. Each of these tasks (persisted via `celery-utils`) will include just enough information about the event that it gets resent appropriately after persistence. Events that consumers of LRS may use for record keeping such as course enrollment and completion events, shall be classified as *business critical events*.  4. A scheduled process will retry transmitting all persisted events in the database to respective recipient(s) at a configured frequency (e.g. once a day). This process will also check if the number of persisted events is higher than a configured threshold. If so, it will generate an alert for the admin.  5. An interface shall be provided for admin to view the list of recipient(s) whose events are persisting in the database. The admin may choose to contact the recipient(s) to try and resolve the communication issue.  Consequences ------------  1. All but *business critical events*, will be lost after the time and number of retry attempts in decision # 2 expire.  2. Decision # 1 is necessary to enable decision # 3 but will also increase the number of celery workers in use.  3. The admin will need to respond to alert discussed in decision # 4 to avoid unnecessary utilisation of storage space. ",others
ADR_174,https://github.com/alphagov/verify-service-provider.git,docs/adr/0025-we-will-only-release-one-configuration-file.md,25. We will only release one configuration file,"# 25. We will only release one configuration file  Date: 2017-11-06  ## Status  Accepted  ## Context  Historically we have had ""two ways"" of configuring Verify Service Provider:  - Using environment variables - Using a YAML file  When using environment variables the application used the verify-service-provider-env.yml file from the resources directory (so inside the jar). When using the YAML file you would pass the path to a different file as a command line parameter - usually people would use the example one that's contained in the repo.  There were a couple of reasons for the extra complexity of managing two files, both due to restrictions with the java buildpack used by cloudfoundry:  - It's not possible to specify command line arguments through the java buildpack,   so you can't specify a path to your config file - We weren't confident in the way cloudfoundry manages static files, so we didn't want   to rely on one.  There was also a philosophical point that 12 factor applications should be configured through their environment. This made the ""hide the configuration in the .jar and do everything through env vars"" way appealing.  ## Decision  We will remove the verify-service-provider-env.yml file from src/main/resources  The application will default to the verify-service-provider.yml file that's included in the .zip if no command line arguments are provided.  If the application is started without command line arguments specifying a yml file AND no environment variables have been set, startup should error gracefully and tell the user that the configuration fields have not been specified for example:  ""ERROR - no configuration fields found, either set environment variables or specify a configuration file using command line arguments ```server <path/to/verify-service-provider.yml>```""  We will establish the path to verify-service-provider.yml by asking java for the path to the .jar file containing the Application class and looking in the parent folder.  ## Consequences  We will have to play a story to make the default configuration file work in a way that's compatible with the current environment variable based solution.  Going forward, we will only need to maintain one configuration file instead of two.  Users will not have to learn about the dichotomy between ""configure with env vars"" and ""configure with files"".  The application will still run on PaaS using the default java buildpack.  ",governance_and_process
ADR_175,https://github.com/huifenqi/arch.git,decisions/0049-moving-your-website-to-https.md,49. 全站 https,"# 49. 全站 https  Date: 2018-06-27  ## Status  Proposed  ## Context  0. 客户网络环境不佳，存在流量劫持；使用的第三方服务要求 https； 1. 10+ 主域名，50+ 二级域名； 2. 190+ 项目； 3. 10+ 数据库表； 4. 大量自行托管及第三方资源文件。  ## Decision  ### why  1. 防止流量劫持，插入广告； 2. 防止账号密码等隐私数据被盗取； 3. 支持 HTML5 API，如用户地理位置，音视频等隐私数据获取； 4. 支持 HTTP/2； 5. Apple，微信等有要求。  ### How  1. 了解整个系统，统计使用到的域名及购买什么类型的域名证书； 2. 相关资源支持 https，解决 Mixed Content 问题 	1. 针对此问题，浏览器会提示警告，Android 的 webview 直接无法打开； 	2. 前端页面的外链资源（CSS、JS、图片、音频、字体文件、异步接口、表单 action 地址等等）固定了协议引用(http://, https://)，需更新为(//)； 	3. 后端代码中协议是否为动态的； 		1. 返回接口协议应和请求协议保持一致； 	4. 数据库； 		1. 针对自有资源，应只保存路径信息，协议和域名建议动态补充； 		2. 针对第三方资源，默认保留原地址，跟进需要再做转换，实在不行需要做代理。 	5. 资源文件：确保支持 https。 3. 支持 https； 	1. 移动端适配 https 		1. 针对运营商 DNS 劫持(降低 https 请求成功率)，需支持两种协议，并有动态降级方案； 	 2. nginx proxy + backend server 		1. 需关注 scheme 获取是否正确，注意 log 记录。 	3. 跳转 		1. 先 302 测试 https 全站正常，再 301 跳转； 		2. POST 请求会丢失 body 信息。 	4. 测试 		1. [https://www.ssllabs.com/ssltest/][1] 4. 强制 https； 5. 所有环境均要升级 https； 	1. 除生产外，需要将开发、测试、预发布均进行升级，保持环境的一致性，减少不可预估的问题发生 6. 优化。  ## Consequences  1. 性能(访问速度)有降低； 2. 增加系统复杂性； 3. 证书及资源(CDN 等)成本；  Refs:  1. [为什么我们应该尽快升级到 HTTPS？][2] 2. [浅谈推进有赞全站 HTTPS 项目-工程篇][3] 3. [启用全站HTTPS后不仅更安全而且更快 看淘宝是如何做到的][4] 4. [更快更安全的HTTPS 优化总结][5]  [1]:	https://www.ssllabs.com/ssltest/ [2]:	https://imququ.com/post/moving-to-https-asap.html [3]:	https://juejin.im/post/5aa22db7518825558804fbac [4]:	https://mp.weixin.qq.com/s?__biz=MzAxNDEwNjk5OQ==&mid=402402866&idx=1&sn=f3fde8ece13d51397c12f1a08713ebeb [5]:	https://zhuanlan.zhihu.com/p/35233649",technology_choice
ADR_176,https://github.com/yldio/asap-hub.git,docs/decision/03-data-storage.md,Data Storage,"# Data Storage  Status: Draft  Date: 2020-05-22  Author: Filipe Pinheiro <filipe@yld.io>  Reviewed-by: Tim Seckinger <tim.seckinger@yld.io>  ## Context  The ASAP Hub has to store data about users and usage of the application. To decide how to implement our data storage, we need to take into consideration the data model and data access patterns the application needs.  ## Data Model  ### Entity Relational Diagram  ``` erDiagram   User ||..|| Invite : has   User ||..o{ Auth : has ```  https://mermaid-js.github.io/mermaid-live-editor/ [![](https://mermaid.ink/img/eyJjb2RlIjoiZXJEaWFncmFtXG4gIFVzZXIgfHwuLnx8IEludml0ZSA6IGhhc1xuICBVc2VyIHx8Li5veyBBdXRoIDogaGFzIiwibWVybWFpZCI6eyJ0aGVtZSI6ImRlZmF1bHQifSwidXBkYXRlRWRpdG9yIjpmYWxzZX0)](https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZXJEaWFncmFtXG4gIFVzZXIgfHwuLnx8IEludml0ZSA6IGhhc1xuICBVc2VyIHx8Li5veyBBdXRoIDogaGFzIiwibWVybWFpZCI6eyJ0aGVtZSI6ImRlZmF1bHQifSwidXBkYXRlRWRpdG9yIjpmYWxzZX0)  ### Potential data access patterns  **Invite (one time token)**  - Create a one time token related to a particular user - Fetch a one time token and the related user information  **Auth**  We need to correlate the authentication provided by the external service to the profiles of the user.  - Create a new identity and the connection to the user account - Fetch the user account associated with a particular identity  **User**  The user entity contains the information about a user of the platform:  - Create a user account and an invite used to join the platform - Fetch the account information based on the code on an invite  ### Data Storage Options  A decision on what storage solution to choose should consider the entities of the system, how they relate to each other, and the data access patterns to retrieve them. As you can see above, the ERD doesn't have many of the entities and relationships required later in the project, but we need to pick a first storage solution now.  An important decision is about managed or unmanaged services. A managed service is better in our context due to the size of the team and to enable focus on different areas.  Our options are:  - A SQL database. Our data is strongly relational. A relational database would be an excellent fit. - MongoDB. MongoDB is a document database with high adoption due to the flexibility of schemaless documents. - DynamoDB. We are using serverless in AWS for the backend, which is an attractive pairing with DynamoDB. DynamoDB is a key/value and document database.  A **relational database** allows us to map the ERD without too much consideration due to the possibility to join data on different tables. The tooling for the relational database is wide-spread, and the database gives us substantial flexibility in our data model.  **MongoDB** is the go-to solution when considering a document database due to the simple API. Modelling data in MongoDB isn't as straightforward as in a relational database, but you still have flexibility when querying your data and changing access patterns.  **DynamoDB** is a document database hosted by AWS. DynamoDB offers a simple API tailored for data sets with known access patterns. To ensure the scaling capabilities, you need to think about your data model in a more intentional way than for a MongoDB. As you can read in the Peter Parker principle ""with great power comes great responsibility"".  **Considerations**  - Due to the serverless nature of our application, databases that use connection pools to manage database connection aren't a good fit. A serverless architecture can exhaust the connection limit, creating zombie connections that may impact database performance.  ## Options  ### SQL  **Amazon Aurora Serverless**  Amazon Aurora Serverless is an on-demand database. It starts, stops, and scales according to the needs of the application. Amazon Aurora Serverless supports the Data API, removing the need to have a persistent connection to the cluster.  https://aws.amazon.com/rds/aurora/pricing/  Amazon Aurora is a MySQL and PostgreSQL-compatible relational database. It is priced per hour starting at \$0.082/h (~\$60/mo).  https://aws.amazon.com/rds/aurora/  ### MongoDB  **MongoDB Atlas**  MongoDB Atlas is a managed service, and we can deploy the cluster on AWS, GCP, or Azure. In our case, AWS is the most suitable option so that we can leverage VPC peering to added security. It it priced per hour starting at \$0.08/h (~\$58/mo).  https://www.mongodb.com/cloud/atlas/pricing/  ### DynamoDB  DynamoDB is a pay-per-request or pay-per-provisioned-capacity. Since we don't know the capacity needed for our application, the choice would need to be its on-demand mode. DynamoDB is a key-value and document database performant at scale. It's fully managed, and the HTTP API fits nicely with the serverless model.  https://aws.amazon.com/dynamodb/pricing/on-demand/  ## Decision ","data_persistence, technology_choice"
ADR_177,https://github.com/buildit/midashboard-infrastructure.git,docs/architecture/decisions/0002-use-aws-bare-metal-rig-approach.md,2. Use AWS Bare Metal Rig approach,"# 2. Use AWS Bare Metal Rig approach  Date: 2017-09-27  ## Status  Accepted  Amended by [3. Use AWS CodePipeline and CodeBuild instead of Travis](0003-use-aws-codepipeline-and-codebuild-instead-of-travis.md)  ## Context  We need to create a riglet for our new bookit project so that we practice what we preach.  ## Decision  We will use the AWS Bare Metal Riglet from bookit-riglet as a starting point for our riglet.  We will keep the previous bookit-riglet and create a new bookit-infrastructure project/repo. Technologies:  * AWS: CloudFormation, ECR, ECS, Route53, VPC, ALB * Deployment Mechanism: Docker images * Build: Travis  ## Consequences  * This will tie us to the AWS platform. * The bookit-riglet is not ""complete.""  There a number of improvements that can be made along the way that we will have to balance with feature work. ",technology_choice
ADR_178,https://github.com/MadDonkeySoftware/mdsCloudDocs.git,developer/ADRs/0002 - ORID.md,0002. ORID,"# 0002. ORID  Date: 2020-JUL-31  ## Status  Accepted  ## Decision  All services shall emit and consume, where applicable, a ORID. ORID shall conform to the [specification version 1](https://github.com/MadDonkeySoftware/OridNode/blob/master/docs/v1.md) where the custom fields are as follows:  | field    | mapping   | sample value | |----------|-----------|--------------| | custom-1 | N/A       |              | | custom-2 | N/A       |              | | custom-3 | accountId | 123          |  ## Context  The issue motivating this decision, and any context that influences or constrains the decision.  As the mdsCloud ecosystem expands it can no longer be assumed that consumers of the service know internal APIs to issue commands. For example the current state machine implementation has a Task definition where the attribute ""Resource"" is a URL to invoke the function. The internal IP addresses and ports should not be known to consumers of the service as it needlessly tightly couples the system. This is compounded by operational concerns where an operator may need to ""shuffle"" or reconfigure systems during maintenance operations.  ## Consequences  What becomes easier or more difficult to do and any risks introduced by the change that will need to be mitigated.  Moving to an ORID supported system will allow for many of these concerns to be mitigated.",governance_and_process
ADR_179,https://github.com/kgrzybek/modular-monolith-with-ddd.git,docs/architecture-decision-log/0006-create-facade-between-api-and-business-module.md,6. Create façade between API and business module,"# 6. Create façade between API and business module  Date: 2019-07-01  Log date: 2019-11-04  ## Status  Accepted  ## Context  Our API layer should communicate with business modules to fulfill client requests. To support the maximum level of autonomy, each module should expose a minimal set of operations (the module API/contract/interface).  ## Decision  Each module will provide implementation for one interface with 3 methods:</br>  ```csharp Task<TResult> ExecuteCommandAsync<TResult>(ICommand<TResult> command);  Task ExecuteCommandAsync(ICommand command);  Task<TResult> ExecuteQueryAsync<TResult>(IQuery<TResult> query); ```  This interface will act as a façade (Façade pattern) between API and module. Only Commands, Queries and returned objects (which are part of this interface) should be visible to the API. Everything else should be hidden behind the façade (module encapsulation).  ## Consequences - API can communicate with the module only by façade (the interface). - Implementation of API is simpler - We can change module implementation and API does not require change if the interface is not changed - We need to focus on module encapsulation, sometimes it involves additional work (like instantiation using internal constructors)","architectural_patterns, api_and_contracts"
ADR_180,https://github.com/alphagov/verify-onboarding-prototypes.git,prototypes/prototype-0/docs/adr/0004-users-will-be-able-to-provide-relay-state.md,4. Users will be able to provide relay state,"# 4. Users will be able to provide relay state  Date: 01/06/2017  ## Status  Pending (may want to change if the added complexity is high)  ## Context  In SAML RPs can provide some extra data along with the request. This is called RelayState. Some existing RPs use this, but we're not sure what they use it for.  We're not aware of any need for the service-provider to use relay state itself.  ## Decision  Users will be able to specify whatever relay state they want to and it will be provided in the response.  ## Consequences  * The service provider won't be able to use relay state itself * The user will need to be able to customize one of the form inputs * The client-side (node JS) code will need to provide some way of getting   relay state from the response. ",others
ADR_181,https://github.com/hugolhafner/analytics-server.git,docs/adr/004-minimally-transform-source-code-during-build.md,ADR-004: Minimally Transform Source Code During Build,"# ADR-004: Minimally Transform Source Code During Build  - Status: Accepted - People involved: @sublimesneaks  ## Issue  There are many ways to transform the source code during build, including:  - Bundle into a single file - Minify  What transformations should the build use?  ## Assumptions  The code is not consumed on client-side.  ## Constraints  As the code is written in TypeScript, there must be _some_ transformation (e.g. ESM to CJS).  ## Decision  Minimally transform the source code during build.  Has the following advantages:  - Easier to debug and trace problems to a specific file (even with source maps) - Allows the consumer to `require` only the file(s) they need within the directory tree ",others
ADR_182,https://github.com/pulibrary/dpul.git,architecture-decisions/0001-document-architecture-decisions.md,1. Record architecture decisions,"# 1. Record architecture decisions  Date: 2020-01-29  ## Status  Accepted  ## Context  We need to record the architectural decisions made on this project.  ## Decision  We will use Architecture Decision Records, as described at https://adr.github.io/  ## Consequences  * Pull requests that significantly change Pomegranate's architecture should be accompanied by an ADR. ",governance_and_process
ADR_183,https://github.com/apache/james-project.git,src/adr/0046-generalize-event-bus.md,46. Generalize EventBus,"#46. Generalize EventBus  Date: 2020-06-11  ## Status  Implemented, used for JMAP notifications, however not yet used on top of the user entity as  described in this document.  ## Context  User email storage usage is limited both in size and count via quotas (IMAP RFC-2087). In order to ease administrating large user bases, the quota search extension allows administrator to retrieve all users whose email usages are exceeding a given occupation ratio.  When searching for users by quota ratio if we set the value of the parameters to 0, for example: `/quotas/users?minOccupationRatio=0&maxOccupationRatio=0`, the search feature is supposed to return newly created users who have not received any email yet at that point. However, this is not the case because the quotas are currently being initialized only after a user has received the first email.  We need to initialize user quotas upon user creation time. The problem is: there is currently no event at user creation  and since the quota-search feature is a plugin of James, it cannot be hardwired into the domain logic of user management to initialize the quota for a just created user.  ## Decision  For quota-search to be initialized/removed for a given user while keeping this feature as a plugin, we decided to adopt the Event Driven pattern we already use in Mailbox-api.  We can create new events related to user management (UserCreated, UserRemoved and so on).  To achieve that, we will extract the EventBus out of mailbox-api in order to make it a utility component (eventbus-api), then we will make both mailbox-api and data-api depend on that new module.   ## Consequences  Mailbox-api would leverage the EventBus to keep exposing the mailbox-listener-api without changes on top of the generified EventBus. We need to define a common Event interface in eventbus-api,  then each EventBus usage will define its own sealed event hierarchy implementing Event.  DeadLetter storage needs to be reworked in order to store events of various EventBus separately (which is needed for knowing which EventBus the event should be reprocessed on  and knowing which sealed hierarchy an event belongs to.)  As a consequence, we will need a Cassandra data migration to add the EventBus name as part of the EventDeadLetter primary key.   We could rely on the EventBus reliability for building any feature in James.",architectural_patterns
ADR_184,https://github.com/mofeixiaobao/gatemint-sdk.git,docs/architecture/adr-008-dCERT-group.md,ADR 008: Decentralized Computer Emergency Response Team (dCERT) Group,"# ADR 008: Decentralized Computer Emergency Response Team (dCERT) Group  ## Changelog  - 2019 Jul 31: Initial Draft  ## Context  In order to reduce the number of parties involved with handling sensitive information in an emergency scenario, we propose the creation of a specialization group named The Decentralized Computer Emergency Response Team (dCERT).  Initially this group's role is intended to serve as coordinators between various actors within a blockchain community such as validators, bug-hunters, and developers.  During a time of crisis, the dCERT group would aggregate and relay input from a variety of stakeholders to the developers who are actively devising a patch to the software, this way sensitive information does not need to be publicly disclosed while some input from the community can still be gained.   Additionally, a special privilege is proposed for the dCERT group: the capacity to ""circuit-break"" (aka. temporarily disable)  a particular message path. Note that this privilege should be enabled/disabled globally with a governance parameter such that this privilege could start disabled and later be enabled through a parameter change proposal, once a dCERT group has been established.   In the future it is foreseeable that the community may wish to expand the roles of dCERT with further responsibilities such as the capacity to ""pre-approve"" a security update on behalf of the community prior to a full community wide vote whereby the sensitive information would be revealed prior to a vulnerability being patched on the live network.    ## Decision  The dCERT group is proposed to include an implementation of a `SpecializationGroup` as defined in [ADR 007](./adr-007-specialization-groups.md). This will include the  implementation of:   - continuous voting  - slashing due to breach of soft contract  - revoking a member due to breach of soft contract  - emergency disband of the entire dCERT group (ex. for colluding maliciously)   - compensation stipend from the community pool or other means decided by    governance  This system necessitates the following new parameters:   - blockly stipend allowance per dCERT member   - maximum number of dCERT members   - required staked slashable tokens for each dCERT member   - quorum for suspending a particular member   - proposal wager for disbanding the dCERT group   - stabilization period for dCERT member transition  - circuit break dCERT privileges enabled   These parameters are expected to be implemented through the param keeper such  that governance may change them at any given point.   ### Continuous Voting Electionator  An `Electionator` object is to be implemented as continuous voting and with the following specifications:  - All delegation addresses may submit votes at any point which updates their     preferred representation on the dCERT group.   - Preferred representation may be arbitrarily split between addresses (ex. 50%    to John, 25% to Sally, 25% to Carol)   - In order for a new member to be added to the dCERT group they must     send a transaction accepting their admission at which point the validity of    their admission is to be confirmed.     - A sequence number is assigned when a member is added to dCERT group.       If a member leaves the dCERT group and then enters back, a new sequence number      is assigned.    - Addresses which control the greatest amount of preferred-representation are    eligible to join the dCERT group (up the _maximum number of dCERT members_).     If the dCERT group is already full and new member is admitted, the existing    dCERT member with the lowest amount of votes is kicked from the dCERT group.    - In the split situation where the dCERT group is full but a vying candidate       has the same amount of vote as an existing dCERT member, the existing       member should maintain its position.     - In the split situation where somebody must be kicked out but the two      addresses with the smallest number of votes have the same number of votes,      the address with the smallest sequence number maintains its position.    - A stabilization period can be optionally included to reduce the    ""flip-flopping"" of the dCERT membership tail members. If a stabilization    period is provided which is greater than 0, when members are kicked due to    insufficient support, a queue entry is created which documents which member is    to replace which other member. While this entry is in the queue, no new entries    to kick that same dCERT member can be made. When the entry matures at the    duration of the  stabilization period, the new member is instantiated, and old    member kicked.  ### Staking/Slashing  All members of the dCERT group must stake tokens _specifically_ to maintain eligibility as a dCERT member. These tokens can be staked directly by the vying dCERT member or out of the good will of a 3rd party (who shall gain no on-chain benefits for doing so). This staking mechanism should use the existing global unbonding time of tokens staked for network validator security. A dCERT member can _only be_ a member if it has the required tokens staked under this mechanism. If those tokens are unbonded then the dCERT member must be automatically kicked from the group.    Slashing of a particular dCERT member due to soft-contract breach should be performed by governance on a per member basis based on the magnitude of the breach.  The process flow is anticipated to be that a dCERT member is suspended by the dCERT group prior to being slashed by governance.    Membership suspension by the dCERT group takes place through a voting procedure by the dCERT group members. After this suspension has taken place, a governance proposal to slash the dCERT member must be submitted, if the proposal is not approved by the time the rescinding member has completed unbonding their tokens, then the tokens are no longer staked and unable to be slashed.   Additionally in the case of an emergency situation of a colluding and malicious dCERT group, the community needs the capability to disband the entire dCERT group and likely fully slash them. This could be achieved though a special new proposal type (implemented as a general governance proposal) which would halt the functionality of the dCERT group until the proposal was concluded. This special proposal type would likely need to also have a fairly large wager which could be slashed if the proposal creator was malicious. The reason a large wager should be required is because as soon as the proposal is made, the capability of the dCERT group to halt message routes is put on temporarily suspended, meaning that a malicious actor who created such a proposal could then potentially exploit a bug during this period of time, with no dCERT group capable of shutting down the exploitable message routes.   ### dCERT membership transactions  Active dCERT members   - change of the description of the dCERT group  - circuit break a message route  - vote to suspend a dCERT member.   Here circuit-breaking refers to the capability to disable a groups of messages, This could for instance mean: ""disable all staking-delegation messages"", or ""disable all distribution messages"". This could be accomplished by verifying that the message route has not been ""circuit-broken"" at CheckTx time (in `baseapp/baseapp.go`).   ""unbreaking"" a circuit is anticipated only to occur during a hard fork upgrade meaning that no capability to unbreak a message route on a live chain is required.   Note also, that if there was a problem with governance voting (for instance a capability to vote many times) then governance would be broken and should be halted with this mechanism, it would be then up to the validator set to coordinate and hard-fork upgrade to a patched version of the software where governance is re-enabled (and fixed). If the dCERT group abuses this privilege they should all be severely slashed.  ## Status  > Proposed  ## Consequences  ### Positive   - Potential to reduces the number of parties to coordinate with during an emergency   - Reduction in possibility of disclosing sensitive information to malicious parties  ### Negative   - Centralization risks  ### Neutral  ## References     (Specialization Groups ADR)[./adr-007-specialization-groups.md] ",others
ADR_185,https://github.com/operate-first/blueprint.git,adr/0007-alerting-setup.md,Alerting Setup for Operate First Monitoring,"# Alerting Setup for Operate First Monitoring  * Status: accepted * Deciders: hemajv, 4n4nd, anishasthana, HumairAK, tumido, mhild  Technical Story: [issue-1](https://github.com/operate-first/SRE/issues/14), [issue-2](https://github.com/operate-first/SRE/issues/19), [issue-3](https://github.com/operate-first/blueprint/issues/16)  ## Context and Problem Statement  As we have multiple services/applications deployed and monitored in the Operate First environment (ex. Jupyterhub, Argo, Superset, Observatorium, Project Thoth, AICoE CI pipelines etc), we need to implement an incident reporting setup for handling outages/incidents related to these services.  All the services are being monitored by [Prometheus](https://prometheus.io/). Prometheus scrapes and stores time series data identified by metric key/value pairs for each of the available services. These metrics can be used for measuring the service performance and alerting on any possible service degradation such as basic availability, latency, durability and any other applicable SLI/SLOs. These SLI/SLOs for the various services are defined and documented in the [SRE repository](https://github.com/operate-first/SRE/tree/master/sli-slo).  Alerting with Prometheus is separated into two parts. Alerting rules in Prometheus servers send alerts to an [Alertmanager](https://prometheus.io/docs/alerting/latest/alertmanager/). The Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email, on-call notification systems, and chat platforms.  Whether its a major bug, capacity issues, or an outage, users depending on the services expect an immediate response. Having an efficient incident management process is critical in ensuring that the incidents are always communicated to the users (via on-call notification systems) and handled by the team immediately. An on-call notification system is a system/software that provides an automated means of contacting users and communicating pertinent information during an incident. It also has additional on-call scheduling features that can be used to ensure that the right people on the team are available to address a problem during an incident. There are multiple on-call notification systems such as [PagerDuty](https://www.pagerduty.com/), [JIRA](https://www.atlassian.com/software/jira) etc that can be used for incident reporting, but which of these tools are best suitable for reporting the outages/incidents to our users?  ## Decision Drivers <!-- optional -->  * Visibility of incident reporting   * How can the incidents be tracked and reported in an open and transparent manner for our users? * Compatibility with Prometheus   * Is the incident reporting tool compatible with Prometheus i.e. can it handle/receive Prometheus alerts? * Complexity and cost of incident reporting tool   * How easy/hard is it to manage and operate the incident reporting tool?   * Is it a free or paid tool?   * Is it open source?  ## Considered Options  * Option 1:   * Use [PagerDuty](https://www.pagerduty.com/) which is a popular paid on-call management and incident response platform * Option 2:   * Use open source tools like:     * [Cabot](https://github.com/arachnys/cabot) - Python/Djano based monitoring platform     * [OpenDuty](https://github.com/openduty/openduty) - Incident escalation tool similar to PagerDuty     * [Dispatch](https://github.com/Netflix/dispatch) - Incident management tool by Netflix     * [Response](https://github.com/monzo/response) - Django based incident management tool    which are free, self-hosted infrastructure that provides some of the best features of PagerDuty, Pingdom etc without their cost and complexity * Option 3:   * Use [GitHub Alertmanager receiver](https://github.com/m-lab/alertmanager-github-receiver) which is a Prometheus Alertmanager webhook receiver that creates GitHub issues from alerts  ## Decision Outcome Chosen option: **Option 3**, because:   * The [GitHub alertmanager receiver](https://github.com/m-lab/alertmanager-github-receiver) can easily be configured and operated to function with Prometheus alerts. It automatically creates issues in GitHub repositories for any active alerts being fired, making it visible for any user to track   * All communication/updates/concerns related to the incident can be easily handled by adding comments in the issues created by the GitHub receiver   * Unlike Option 1, there is no additional cost involved   * There is no requirement for using JIRA/Slack for incident tracking, which are the only supported options in some of the tools listed in Option 2 (such as [Dispatch](https://github.com/Netflix/dispatch) and [Response](https://github.com/monzo/response)) In any case that such a requirement surfaces, we can use GitHub bots for different platforms such as [GitHub for Slack](https://slack.github.com/) and [Google Chat](https://support.google.com/chat/answer/9632291?co=GENIE.Platform%3DAndroid&hl=en) to notify us of the issues immediately   * It is actively being maintained and supported compared to some of the tools in Option 1 (such as [Cabot](https://github.com/arachnys/cabot) and [OpenDuty](https://github.com/openduty/openduty)) which lack community support ","technology_choice, observability"
ADR_186,https://github.com/CorneliuPopescu/insight.git,_docs/adr/0002-tweepy.md,2. Tweepy,# 2. Tweepy  Date: 2018-12-12  ## Status  Proposed  ## Subject Matter Expert (SME)  Corneliu Popescu  ## Context  To programmatically access twitter we need to use a Python library able to support APIs we need. The chosen library should be well reviewed and have wide community support.  ## Decision  We choose [Tweepy](https://github.com/tweepy/tweepy) as our Twitter API Pyhon library.  ## Consequences  We expect to have available a large number of examples and use cases.  ## Dependencies  - NA  ## References  - NA,technology_choice
ADR_187,https://github.com/jmoratilla/devops-challenge.git,doc/adr/0008-feat-ideas-for-resilience.md,8. feat_ideas_for_resilience,"# 8. feat_ideas_for_resilience  Date: 2020-02-25  ## Status  Draft  ## Context  Here I'm adding some ideas to add as a cul-de-sac about resilience and   previous experiences in order to keep a production environment under  control.  ### Mechanisms for resilience and fault tolerance  #### What can go wrong?  To identify possible points of failure, I will go upstreams from production to development.  #### Production  Attending to Murphy's Law: Anything that can possibly go wrong, does, I should meter anything.  So first point of failure here is monitoring.  ##### Monitoring  If is not monitorized, it is not in production  Anything that goes in production is a result of an effort to earn money, so we should be consciuous of the benefits of the feature as soon as it is deployed.  For me, any new feature request should have a way to meter the success or failure rate of the solution, and that check must be defined before the feature runs in production, just to show it is not in production yet.  To handle this, I would add a requirement in the specs for any feature request that should establish what is a success and what is a failure for the product owner that requested it.  A product owner dashboard should exist so any product owner should see a way to measure the success rates of her decisions.  This monitoring comes from Business, but as technicians we need to measure how the platform is performing, and the capacity of the current resources in order to answer the following question:  Will the next feature get into the current platform?  You can use [RED method](https://thenewstack.io/monitoring-microservices-red-method/) to monitoring features.  ##### Name Resolution  What if you have a website nobody can reach because your nameservers are   down?  You should have a primary and secondary nameservers in different   providers.  ##### Name System Attacks  What if your nameservers have been poisoned to deliver different   addresses?  Your nameservers must verify and check their data consistency.  ##### Name Changes Response Time  Your servers must be quite fast to respond queries, and fast enough to   propagate changes when endpoints are affected.  ##### Network and Security  Your services should have enough bandwidth to avoid queueing requests and deliver quick responses.  Your network is vulnerable to several attacks, from DDoS to specific website malformed urls or known product vulnerabilities.  Monitor your network capacity and get a provider that can handle all your needs.  In cloud environments is easy if you have the money.  Monitor your traffic using Firewalls or Web Application Firewalls, that inspect the requests looking for known vulnerabilities.  If you are using web services, perform the following recomendations:  - Use HTTP2 - Use CDN solutions - Use WAF solutions that can automatically ban attackers IP addresses   ##### SSL certificates  Your web applications and sites won't work if your SSL certificates are not valid.  Valid certificates means they are in working conditions and they are recognized by the users as trusted certificates.  Monitor your SSL certificates expiration dates and set a prodedure to renew them and publish them automatically.  Use global solutions like [Cloudflare](https://cloudflare.com) that provide solutions for DNS, CDN and Certificate management.  But keep an eye on other providers to have a failover.  ##### Know your servers, avoid stupid downtimes  Some servers can create unexpected conditions when you don't know their limits.  For example: nginx is known for be a good load balancer and reverse proxy, but it's also true that it caches the IP addresses of the upstream servers.  If the IP address of an upstream server changes (because reprovisioning a node), then nginx can behaves unexpectedly and it can return error codes.  Other issues can be related to timeouts.  If a web request last more than 40 seconds, nginx automatically rejects the request with a 504 (Gateway Timeout) error code.  To solve this, use your preproduction / staging environments to reproduce possible scenarios, and get knowledge about the limits of the servers, services and platforms used.  ##### Invest in your logs  Get all the possible info from the servers and any service involved in the production environment.  Process all your logs in order to aggregate data and set a baseline to detect anomalies.  Use solutions like Splunk, Logentries, Devo, Airbrake  #### Preproduction  ##### Schedule backups from production to preproduction / staging  TBD  ##### Pseudoanonymize backups from production to upstream environments  TBD  ##### Perform load tests  TBD  ##### Perform releases in staging and take times  TBD  #### QA  ##### Match specs with tests  TBD  #### Development  TBD  ## Decision  Here are lots of ideas to explore for next discussions.  ## Consequences  To discuss.",others
ADR_188,https://github.com/CMSgov/easi-app.git,docs/adr/0003-use-golang-for-server.md,Use Go As Server Language,"# Use Go As Server Language  The EASi developers need a language to support the application server side.  ## Considered Alternatives  * Go * Language with widespread web framework   (ex. Ruby or Python) * JavaScript/Node  ## Decision Outcome  * Chosen Alternative: *Go*  The top reason for selecting Go are that the team has in house expertise. The application team is immediately ready to produce client value in Go due to their familiarity with the language and ecosystem.  It is also a modern strong typed language, which provides compile time safety, as opposed to common web app alternatives, such as Ruby, Python, or JavaScript.  The benefits of using languages with more mature web frameworks (such as Rails or Django), are outweighed by the flexibility of using a frontend framework driven UI with a JSON API for data retrieval. For the latter, Go provides excellent tooling in its standard libraries and ecosystem, for building mature, stable, maintainable APIs.  ## Pros and Cons of the Alternatives  ### Go  * `+` Familiarity with team. * `+` Common tools across Truss projects. * `+` Static typed. * `-` Relatively immature compared to Ruby/Python   in web app ecosystem.  ### Language with Widespread Web Framework (Ruby or Python)  * `+` Easy to bootstrap common application features. * `+` Strong, mature ecosystem and community. * `-` Dynamically typed * `-` Not in application team's toolset.  ### JavaScript with Node  * `+` Frontend/Server in same language. * `+` Single package system (NPM or yarn) * `-` Dynamically typed. * `-` Not in application team's toolset,   especially for server side applications. ",technology_choice
ADR_189,https://github.com/alphagov/gsp.git,docs/architecture/adr/ADR039-cloudhsm-namespace-network-policy.md,ADR039: Restricting CloudHSM network access to particular namespaces,"# ADR039: Restricting CloudHSM network access to particular namespaces  ## Status  Accepted  ## Context  [ADR036](ADR036-hsm-isolation-in-detail.md) described the network and credential isolation we use to ensure that unauthorised users cannot access the CloudHSM.  Recently in 3ea9de2ff, we introduced a GlobalNetworkPolicy object, which is a Calico feature that allows a cluster-wide network policy to be imposed.  This allows us to control network access to and from particular namespaces in a way which cannot be overridden by tenants.  In particular, currently access to the HSM is only allowed from pods annotated with a `talksToHsm=true` label.  When working in their own namespace, a developer has full control over what labels they put on their pods, so they can still choose to put the `talksToHsm=true` label on their pods.  But they do not have control over what labels the namespace itself has; to change this would require a change to the `gsp` or appropriate `cluster-config` repository, which would make such a change visible to many more people.  Therefore, if we extend the GlobalNetworkPolicy to require a `talksToHsm=true` label on *both* the pod *and* the namespace, we will prevent tenants from unilaterally opening up network access to the HSM from their namespaces.  ## Decision  We will augment the GlobalNetworkPolicy (previously described in ADR036) by:   - setting a `GlobalNetworkPolicy` that denies access to the    CloudHSM's IP address unless the pod carries a label    (`talksToHsm=true`) and the namespace also carries a label    (`talksToHsm=true`) and allows all other egress traffic  ## Consequences  Control of which namespaces get the `talksToHsm=true` label will be via the appropriate `-cluster-config` repo.  If a developer wants to allow a new namespace to talk to the HSM, they will need to issue a PR against that repo.  If we are confident in the GlobalNetworkPolicy's control of HSM access, we could consider reducing the technical controls required on non-HSM namespaces.  For example, we could consider allowing developers to run plain `kubectl apply` in unprivileged namespaces, for fast-feedback learning. ",security
ADR_190,https://github.com/guttih/island.is-glosur.git,docs/adr/0011-open-source-license.md,Open Source License,"# Open Source License  * Status: proposed * Deciders: dev, devops, managers * Date: 2020-06-07  ## Context and Problem Statement  It is the offical policy of the Digital Iceland and stated in the Techical Direction that it is to be implemented as free and open source. Open source software by definition is open to anyone to use, modify, distribute and study. These permissions are enforced an open source license. There are a number of well-known and widely used open source licenses available and we need to choose a license that best fits the goals of digital iceland.  There are two main types of open source licences:  more permissive licences that confer broad freedoms and minimal obligations (e.g., the MIT, BSD and the Apache 2.0 licences); and sharealike licences that require licensing adaptations with the same licence if they distribute them (e.g., the GNU GPL).  Development for Digital Iceland will be open and free with minimum complications for development for all involved. Reuse and transparency will be promoted.  ## Decision Drivers  * The primary motivation is to encourage, co-development, collabiration, transparency and reuse of the software. * It is important to build on the experience of similar government led inititives in other countries. * Digital Iceland has no patents or intellecatual property that needs to be protected or guarded by the license chosen. * It is not a concern for Digital Iceland that the license restricts usage in other projects, be it open or closed source.  ## Considered Options  The different licenses  * Apache * BSD * GNU GPL * MIT  ## Decision Outcome  The MIT license was chosen, for the following reasons:  * It is the least restrictive of the licenses. * It is very consise, simple and easy to understand and therefore should be clear to users and developers. * Digital Iceland does not require protection of patents or existing intelletual property. * Well known government lead initiatives like uk.gov and X-Road use the MIT license. * The MIT license is the best known and most widely used free and open-source license in the world.  ## Pros and Cons of the Options  ### Apache  * Good, because is well known and very permissive like the MIT license. * Bad, it is has restrictions around redistribution that do not apply for Digital Iceland. * Bad, is way very long and wordy and therefore requires more effort to understand.  ### BSD  * Good, because is well known and very permissive like the MIT license. * Bad, because it has restrictions about using the names of the copyright holder which is not a concern for digital Iceland.  ### GNU GPL  * Good, because it is very well known. * Bad, that it is not permissive and requires derived software to adopt the license as well.   ## Links  * [Stjórnarráðið - Umfjöllun um opinn hugbúnað](https://www.stjornarradid.is/verkefni/upplysingasamfelagid/stafraent-frelsi/opinn-hugbunadur) * [Ríkisendurskoðun - Frjáls og opinn hugbúnaðr](https://rikisendurskodun.is/wp-content/uploads/2016/01/Frjals_og_opinn_hugbunadur_01.pdf)  [Apache]: <https://www.apache.org/licenses/LICENSE-2.0> [BSD]: <https://opensource.org/licenses/bsd-license.php> [GNU GPL]: <https://www.gnu.org/licenses/gpl-3.0.html> [MIT]: <https://opensource.org/licenses/mit-license.php> ",governance_and_process
ADR_191,https://github.com/learnitmyway/my-notes.git,adr/cypress.md,Sentry,## Sentry  ### Context - e2e/UI testing framework - Was not able to get it to run on Netlify  ### Decision I won't be able to start using it until I can get it working with a CI tool.,others
ADR_192,https://github.com/geodocker/geodocker.git,docs/arch/adr-0001-deployment.md,(sem título),"0001 - GeoDocker deployment ===============================  Context ------- Currently, it is possible to start a distributed geodocker cluster manually. It is not yet clear how to bring them up automatically at scale. The ideal solution should be available not only for services with a rich API (like AWS or DigitalOcean) but also for hosting providers that lack this sophisticated API and tooling. Currently, support for providers lacking such an API is not a priority.  Decision -------- Was decided to look at popular docker orchestration tools:   * [ECS](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html)   * [DCOS](http://dcos.io)   * [Rancher](http://rancher.com)  ##### ECS  ECS is a highly scalable, fast, container management service, and it is a part of AWS ecosystem. It's incredibly easy to launch containers via ECS, however it is not clear from the docs or from the community (on forums) how to launch distributed applications using ECS. The problem is that it provides no internal docker network between EC2 nodes, and it is not possible to communicate with docker containers on different nodes by some internal address. In addition, there is no API (or such API was not found / mentioned in docs / on forums) to launch containers on different nodes. A common use case for ECS is to launch linked containers, to scale them (all linked containers would be just launched on separate node(s)), and to balance requests between these nodes using load balancer. If we want containers to communicate with each other, it is possible to forward all necessary ports to the host network manually (a fact which spark deployment impossible) and to talk with containers as with nodes.  ##### DCOS  DC/OS is a distributed operating system based on the Apache Mesos distributed systems kernel. It has a community version, though DC/OS is mostly (it's supposed) oriented on enterprise users. This explains the instability of its latest community version. It has quickstart templates for  [AWS](https://mesosphere.com/amazon/) and for [DigitalOcean](https://docs.mesosphere.com/1.7/administration/installing/cloud/digitalocean/), but is not simple to install it on [some other hosting provider](https://dcos.io/docs/1.7/administration/installing/custom/) (it is not a one-liner). For research purposes, an AWS template was used ([modified](https://gist.github.com/pomadchin/c898fb767ce4d8bb943c2794c565fa8c) to use spot instances). DC/OS operates with mesos DNS, and it enables docker container communication on separate nodes. [Marathon](https://mesosphere.github.io/marathon/) is used to manage docker containers. All docker containers start with their own internal IP addresses (available via mesos DNS), but they are not accessable by potentially exposed ports (marathon specification requires explicit ports forwarding at least to the mesos internal network). The upshot of this is that you can't start two containers on the same mesos node with the same exposed port and there is no in-built port forwarding (on some internal docker network). Because of this, it is not possible to start our own dockerized Spark using Marathon. As a fast and simple solution for deployment, it is possible to start DC/OS built-in Hadoop and Spark packages, and to start Accumulo using [this](https://gist.github.com/pomadchin/2193ed3a10808e9368d326a0cebe393f) Marathon job specification. To solve Marathon DNS restrictions (as a consequence of port auto forwarding), it is possible to use [Calico](https://www.projectcalico.org/) (though not in the current DC/OS AWS template due to old docker version), and [Weave](https://www.weave.works/) (still has no [Weave.Net](https://www.weave.works/products/weave-net/) package for DC/OS). Solutions are possible but require further investigation.  ##### Rancher  Rancher is a completely open source docker management system. It has an easy [insallation](http://docs.rancher.com/rancher/latest/en/installing-rancher/installing-server/) (one-liner, per machine) and may work with AWS and DigitalOcean using their APIs. It is possible to provide slave nodes for Rancher manually (just by running another one liner on potential slaves) and it takes control over all docker containers on them. Rancher includes support for multiple orchestration frameworks: Cattle (native orchestrator), Docker Swarm, Kubernetes, Mesos (beta support). It also provides its own DNS on top of docker bridges. Cattle supports a sort of modified docker-compose.yml (v1) file.  Launching a cluster using Cattle is possible via [rancher-compose](http://docs.rancher.com/rancher/v1.0/zh/rancher-compose/). However Rancher provides DNS on top of docker bridges that causes a following problem with Spark: Spark master listens to `localhost` / `container_name` and this name in terms of a master container is an internal _docker_ IP address (17x.xxx.xxx), and in terms of some other container master address is an internal _rancher_ ip address (10.xxx.xxx), which makes master not available for other containers. A similar thing happens with Accumulo: it writes the wrong ip addresses / DNS records into Zookeeper and Accumulo master just is not available for tablets / other Accumulo processes. A solution is possible but requires further investigation.  Consequences ------------ A clear deployment solution is still not obvious. ECS is an Amazon service and we can just await necessary functionality. DC/OS Community version is _very_ unstable, and has some non-trival dns problems requiring (at the current moment) third-party libraries (Calico, Weave) and a non-trival installation proccess (generally). Rancher looks more stable, and has a more user-friendly (simpler to understand) ui / tools. But Rancher has it's own specific features to be explored and probably requires more time to research. ","technology_choice, infrastructure_and_deployment"
ADR_193,https://github.com/lorenzo-deepcode/buildit-all.git,bookit-api/docs/architecture/decisions/0005-use-id-token-from-microsoft-as-bearer-token.md,5. Use id_token from Microsoft as Bearer token,"# 5. Use id_token from Microsoft as Bearer token  Date: 2017-12-01  ## Status  Accepted  Amends [4. Security](0004-security.md)  Alternative considered [6. Use Okta as Identity provider](0006-use-okta-as-identity-provider.md)  Alternative considered [7. Use Pac4J to validate tokens](0007-use-pac4j-to-validate-tokens.md)  ## Context  In the interest of time and getting something to work, we are going to break up the steps further  ## Decision  * Instead of exchanging id_token for opaque access_token, the client will always send the id_token as the Bearer token * Proper validation of the id_token will still occur  ## Consequences  * The security implications are uncertain.  This is definitely not what the id_token is inteded for but it's unclear after much googling what the security holes are.  * If/when we need actual delegated authorization, especially against the MS Graph API, we will need to revisit this and acquire access_tokens ",security
ADR_194,https://github.com/HFAnalyticsLab/HES_pipeline.git,doc/adr/storing_dates_in_database.md,Storing dates in SQLite database,"# Storing dates in SQLite database    ## Context    SQLite does not feature a date format data type. As such writing a date format  data object from R, results in conversion to an integer with no relevance to the  original date.    ## Decision    Incoming raw data will not be converted to date format in R, and instead   maintained as a string for full dates (Y-m-d) or part dates (Y-m) and as an   integer for years.    ## Status    Implemented, and previous code for date conversion has been rolled back.    ## Consequences    For date analyses, when using the HES database date columns must be converted to  date format after performing a query. A handy function has been created to do   this in a standardised way (see [here](src/clean.R#L28))",others
ADR_195,https://github.com/KonduitAI/dl4j-dev-tools.git,codegen/adr/0006-op_specific_enums.md,Op specific enums,"# Op specific enums  ## Status  ACCEPTED  Discussed by: Alex Black, Robert Altena and Paul Dubs on 26. November 2019  ## Context Some ops have an ordinal parameter which switches between a few possible modes. Giving those modes a proper name makes usage and documentation easier.    ## Decision We allow `Arg` sections to have an `ENUM` data type and add a `possibleValues` property to define the possible values for this arg. The ordinal number of the enum is the same as its position within the `possibleValues` list starting from  `0`.   A runtime check on op construction, will ensure that each enum arg has one or more possible values, and that default values match one of the possible values (if applicable).  On code generation, an appropriate representation of this enum will be generated in the target language. The name of  the generated enum will be derived from the name of the arg.  ### Example ```kotlin Arg(ENUM, ""padMode""){    possibleValues = listOf(""CONSTANT"", ""REFLECT"", ""SYMMETRIC"")   description = ""padding mode""   } ```  ## Consequences  ### Advantages * We get easily understandable names for otherwise in-transparent ordinal mode modifiers  ### Disadvantages * The defined enum can only be used for a single op * The defined enum is only usable with a single arg ",others
ADR_196,https://github.com/UST-MICO/docs.git,adr/0026-implementation-of-complex-eai-patterns-with-faas.md,Implementation of complex EAI-Patterns with FaaS,"# Implementation of complex EAI-Patterns with FaaS   ## Context and Problem Statement  Some [Enterprise Integration Patterns](https://www.enterpriseintegrationpatterns.com) have a complex structure where parts of the behaviour can be implemented generically while some parts need to be modifiable by the end user (in our case the system admin using MICO). We have already [decided to use a FaaS platform](0023-faas.md) to provide this modifiability in form of code as configuration. While this works well for most patterns, for some of the more complex patterns it is not easy to allow modifiability via FaaS. This is especially the case if the user want to write as little code as possible meaning the [generic part](0025-generic-component.md) of the component has to be implemented by the MICO team.   ## Decision Drivers  * Modifiability of the patterns must be provided via a FaaS function * The function should only have to contain as little code as possible * Generic code for the pattern should be provided by the MICO platform either as a library to import into the FaaS function or in the component that calls said function   ## Challenges  ### Where / how to implement generic logic  * Implement all logic in the FaaS function * Implement custom logic + some of the generic logic in the FaaS function * Implement only custom logic in the FaaS function  ### State of configuration channel  How can the FaaS function get the current state of the configuration (e.g. dynamic router).  * Let the FaaS function subscribe to Kafka * Have a separate DB * Send the current configuration together with the message  ### Sending to multiple destinations / unknown destinations  A router does not have a specific endpoint to send all messages to and may even send messages to many endpoints.  * Implement generic routing with `recipientList` / `routingSlip` only and let FaaS function write the route into the message header * Let the function send the messages directly to Kafka  ### Stateful components  Some patterns are inherently stateful.  * Store state in a DB * Store state in generic component and send to FaaS function with message  ### Performing intermediate request steps  A content enricher needs to perform requests to get the content to inject into the message.  * Offload implementation completely to the user * Split into two parts, one determining which requests are needed and one injecting the requested content into the message.   ## Affected patterns  ```eval_rst ======================= ================ ============== ==========  Pattern                 dynamic config   destinations   stateful ======================= ================ ============== ========== Router                  no               yes            maybe Dynamic Router          yes              yes            maybe Content based Router    maybe            yes            maybe Aggregator              no               no             yes Resequencer             no               no             yes Process Manager         maybe            yes            maybe Message Normalizer      maybe            no             no ======================= ================ ============== ========== ```  ## Decision Outcome  **Where to implement logic**: To be decided  **State of configuration channels**: To be decided  **Stateful functions**: To be decided  **Routing**: We will support custom routing decisions in the FaaS function by always interpreting a routing slip if it is present. The routing slip has to support multiple destinations for one routing step. This will also allow us to make more patterns possible (everything that is not stateful) with a single generic kafka to FaaS connector.  **Intermediate requests**: To be decided   ## Evaluation of Options  ### Where to implement generic logic  #### Implement generic logic in FaaS-Function  * Good because we can use Kafka as trigger for the function * Good because it allows easy extension and modification of the generic logic where needed * Bad because we need to implement the generic logic in all supported languages as a library/framework * Bad because the alternative would mean complex FaaS networks/dependencies if generic logic is capsuled in FaaS functions  #### Implementing generic logic in Kafka to FaaS Adapter  * Good because we only need one implementation in one language * Bad because generic logic needs to support all patterns => possibly more complex implementation  ### Stateful patterns  #### Store state in compacted Kafka topic  * Good because it uses no extra component * Bad because only useful for storing configuration channel state/state that does not change too often * Bad because it needs many assumptions * Bad because whole channel needs to be read to load state  #### Store state in DB  * Good because DBs are really good for storing and querying state * Bad because management of DBs (isolation of DBs for different FaaS functions) is difficult  #### Send state with the message to the FaaS function  * Good because FaaS function can be trivially stateless * Bad because state could be large * Bad because generic component needs to know what state to send (up priori assumption)  ### Sending to multiple or unknown destinations  #### Allow FaaS function to send messages directly to kafka  * Good because it allows complex routing patterns * Bad because FaaS function needs to understand and react to various header fields in the cloud event (routingSlip, route history)  #### Implement generic routing via header fields (routingSlip/recipientList)  * Good because we can ensure semantics of some header fields in our implementation * Bad because header fields need to be sufficiently complex to support complex routing decisions  ### Intermediate requests  #### Offload complete implementation to user  * Good because all requests can be controlled by user code * Bad because user may need to (re-)implement generic code * Bad because FaaS function needs to know about kafka topics  #### Split into request part and merge part  * Good because split implementation is easyer to understand * Good because split implementation allows for generic request handling implemented by MICO * Bad because split pattern implementation is  more complex ",architectural_patterns
ADR_197,https://github.com/StuPro-TOSCAna/TOSCAna.git,docs/dev/adr/0005-springfox.md,*Use Springfox to automatically generate API Documentation*,"# *Use Springfox to automatically generate API Documentation*  **User Story:** *As a developer i want to know how to use the provided REST API*  The documentation of the API is very important, we therefore have to decide on a way to document our REST API   ## Considered Alternatives  * [Springfox](https://springfox.github.io/springfox/) * Manually created [Swagger](https://swagger.io/) Documentation * Markdown based API documentation   ## Decision Outcome  * Chosen Alternative: *Springfox* * Springfox allows us to keep the documentation of the API within the java code, we do not need to create a seperate document for it. The Autogeneration is also nice because we don't have to do anything manually once the implementaion is done.  ## Pros and Cons of the Alternatives   ### *Springfox*  * `+` The generation of the API documentation can be fully automated `->` API docs can be always up to date * `+` Commenting of the API docs is done with annotations * `+` Allows access to the API using `swagger-ui` this results in a quite crude but usable ""web ui"" * `-` Natively documenting everything (without the use of workarounds is not possible) * `-` Annotations have to be learned * `-` Code will have dead classes (used to model parts of the API) * `-` Generates a JSON based swaggerfile (not really human readable)  ### *Manual Swagger*  * `+` Decent readability * `+` The resulting documentation can be complete (no workarounds needed) * `-` Manuall updates of the file needed * `-` A quite complex DSL (domain specific language) has to be learned * `-` A major API change will result in a lot of work  ### *Markdown*  * `+` Good readability * `+` The resulting documentation can be complete (no workarounds needed) * `-` Manuall updates of the file needed * `-` A major API change will result in a lot of work ",technology_choice
ADR_198,https://github.com/department-of-veterans-affairs/va.gov-team.git,docs/adr/0005-use-github-actions-for-vsp-platform-ci.md,Context,"# Context  https://dsva.slack.com/archives/C01CJV0L9PS/p1612651359029600?thread_ts=1612651359.029600&cid=C01CJV0L9PS  TL;DR: CircleCI as implemented with VA restrictions does not allow easy management of permissions on ‘Contexts’ which contain credentials which are available to the entire ‘department-of-veterans-affairs’ Github Org. Although they are not displayed in plaintext in the CircleCI UI, they are obtainable in plaintext by running a build in CircleCI and using SSH to connect to your in-progress build, and outputting ENV variables. If the entire Org has access to the context, there is no way to stop anyone in the Org from doing so.  In addition, CircleCI as implemented with VA restrictions does not allow use of ‘third-party’ Orbs (reusable code) which means that we cannot publish our own Orbs either.  These two items are not preventing the use of CircleCI completely, as it is still possible to manage per-project secrets (only available to those with Github permissions on that repo) and you are free to cut and paste code from one workflow to another to re-use code, however, it makes managing multiple repos that have similar functions very tedious, and error prone.  In addition, managing the secrets per-repo means that if the Operations team wanted to rotate the credentials for that repo, they would have to use the CircleCI UI to change all of those credentials, one project at a time.  I have been evaluating multiple options for CI at this point and I think the best use-case for Operations is Github Actions, however, I have come to the conclusion that there would be no reason an application team could not use CircleCI, and manage the secrets in their own repo level secrets. There is also no reason that an application team could not use their own CI server that they manage, like CMS Tugboat does, or if they love Jenkins and want to use it, they can run their own Jenkins server somewhere.     I think our platform will strive to be CI-agnostic, but we will 100% provide examples of how to interact with ArgoCD, which WILL be required in the workflow     this has the benefit of removing Kubernetes credentials and interaction from the application teams completely. the Operations team will document what ArgoCD wants to receive as input, and how the team can make that input, and then Argo is the only place outside of members of the Operations team that has credentials to make changes to EKS clusters.     the application teams will also be able to log into ArgoCD and see their applications in Argo, along with logs and events that will help them troubleshoot their applications and deployments.   Just to be clear, we're talking about CI for:      VSP      VFS teams building on VSP      Others: CMS, VA Notify, etc.  For 3 I can see being agnostic. For 1 and 2, I think we should be consistent so we can support the VFS teams. for 2, we will provide examples and best practices  and we will support VFS teams in building those but I don’t want to REQUIRE them to use a particular process  I will strongly recommend to them that they use the process we build for them and my team will help them build it, absolutely. what I’m saying is that if they love Circle and want to use it, we can support that  as long as they manage their own project-level credentials  the problems with CircleCI for the Operations team is that we have many repos and managing those credentials at the repo level across all of those, as well as managing them all as separate pieces of code (when in reality they are 99% similar) is tedious, error prone, and makes Circle a bad fit (as long as the VA maintains those limitations) Github Actions is, I think, the best option.     # Vision  Each application has it’s own repository, it’s own ECR repo, and it’s own AWS user, with limited permission scope (able to push and pull from the application’s ECR repo, able to read parameters from only their path in Parameter Store). That AWS user has a token which is in Github Encrypted Secrets, managed by the application team. Any further secrets needed and variables for their application, are pulled from Parameter Store.  the Operations team manages these AWS users, ECR repos, and Github repos (using the Github terraform provider) with Terraform, as IaC. (edited)   that eases the lift on the Operations team because we can manage all of this using the tool we already use, we manage the permissions on the github repos by the github teams that already exist, and we scope the credentials we put on the repo to limit any blast radius.     # Other tools that were considered     Drone.io is really cool, but in the end too simple for our need.  Concourse-ci is also a great tool but almost as complex as Jenkins, with a steep learning curve  I had a demo of GoCD from some folks at Thoughtworks that Karl Brown got me in touch with and it is an amazing CI tool, miles ahead of Jenkins, but also very complex, and overkill for our needs  Jenkins is not viable for a number of reasons, not the least of which is the maintenance of the tool   AWS CodePipeline/Codebuild was also considered, but in order to use that, the Operations team would have to write all the projects, etc, in Terraform, which would increase the management load for my team     # Why Github Actions?     Github Actions is something we already have, fills all the needs, along with allowing us to maintain permissions over credentials and in addition, we can run on-premise runners if we need to, for things like the drupal pull which need VAEC network access1 also on-prem runners dont count toward your minutes usage in Github Actions so we can mitigate any cost issues that way  we can also write our own github actions and share them, as you could with Orbs in CircleCI  which we can’t do in Circle because of VA restrictions ",governance_and_process
ADR_199,https://github.com/eclipse/winery.git,docs/adr/0031-reuse-refinement-code-for-pattern-detection.md,(sem título),"<!---~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~   ~ Copyright (c) 2021 Contributors to the Eclipse Foundation   ~   ~ See the NOTICE file(s) distributed with this work for additional   ~ information regarding copyright ownership.   ~   ~ This program and the accompanying materials are made available under the   ~ terms of the Eclipse Public License 2.0 which is available at   ~ http://www.eclipse.org/legal/epl-2.0, or the Apache Software License 2.0   ~ which is available at https://www.apache.org/licenses/LICENSE-2.0.   ~   ~ SPDX-License-Identifier: EPL-2.0 OR Apache-2.0   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~-->   # Reuse the pattern refinement implementation for pattern detection  ## Context and Problem Statement  To create an executable deployment model, the pattern refinement process replaces a matching subgraph with the Refinement Structure of a PRM. To create a PbDCM, the pattern detection process replaces a matching subgraph with the Detector of a PRM. The replacement procedure is identical for both processes, only the structures used for the replacement differ. Therefore, the implementation of the pattern refinement process should be reused to implement the pattern detection process.  ## Decision Drivers  * Avoid duplicate code * Avoid introducing errors and inconsistencies during reimplementation  ## Considered Options  * Swap the Detector of all PRMs with their Refinement Structures * Reimplementation * Use common interface  ## Decision Outcome  Chosen option: ""Swap the Detector of all PRMs with their Refinement Structures"", because reimplementation introduces too much duplicate code and a common interface requires a lot of boilerplate code while also decreasing readability.  ### Positive Consequences <!-- optional -->  * Complete pattern refinement implementation can be reused  ### Negative consequences <!-- optional -->  * Readability and understandability decreases  ## Pros and Cons of the Options  ### Swap the Detector of all PRMs with their Refinement Structures  In the backend, the elements of the PRMs retrieved from the repository are swapped, .i.e, the Detector of each PRM is set to its Refinement Structure, its Refinement Structure is set to its Detector, and all mappings are adapted accordingly.  * Good, because complete refinement code can be reused * Bad, because decreases readability and understandability  ### Reimplementation  The complete pattern refinement code is reimplemented for pattern detection, i.e., the reimplemented code considers the Detector during the replacement, redirection of Relations using the Relation Mappings, and retaining elements using the Stay Mappings.  * Good, because better readability * Bad, because results in a lot of duplicate code * Bad, because the reimplemented code can contain errors and inconsistencies already fixed in the refinement implementation  ### Use common interface  Implement an interface which returns the Refinement Structure of a PRM for the replacement procedure of the pattern refinement process and returns the Detector of a PRM during the pattern detection process.  * Good, because refinement code can be reused * Bad, because requires a lot of boilerplate code * Bad, because it decreases readability  ## License  Copyright (c) 2021 Contributors to the Eclipse Foundation  See the NOTICE file(s) distributed with this work for additional information regarding copyright ownership.  This program and the accompanying materials are made available under the terms of the Eclipse Public License 2.0 which is available at http://www.eclipse.org/legal/epl-2.0, or the Apache Software License 2.0 which is available at https://www.apache.org/licenses/LICENSE-2.0.  SPDX-License-Identifier: EPL-2.0 OR Apache-2.0 ",governance_and_process
ADR_200,https://github.com/kbase/sample_service.git,design/implementation_notes.md,Relevant ArangoDB documentation,"# Relevant ArangoDB documentation  * Transactions: https://www.arangodb.com/docs/stable/transactions.html * Write conflicts: https://github.com/arangodb/arangodb/issues/9430 * Transaction failures: https://github.com/arangodb/arangodb/issues/11424 * Sorting graph traversal results: https://github.com/arangodb/arangodb/issues/11260  # Data links  ## Deleted objects / workspaces  * Currently links to deleted objects can be returned from `get_links_from_sample`.   * Should this be changed? It means an extra workspace call.   * It also has reproducibility issues - calls to the method with the same `effective_time` may     not produce the same results.     * That being said, changing permissions to workspaces can also change what links are returned       over time.   * If we don't return links to deleted objects, should the links be autoexpired if they aren't     already?     * This assumes `get_object_info3` with `ignoreErrors: 1` will only return `null` for deleted       objects when called via `administer` - verify   * What about expired links to deleted objects with an `effective_time` in the past? Return them?  * Links to deleted objects can be expired as long as the user has write access to the workspace.   However, links to objects in deleted workspaces **cannot** be expired by anyone, including   admins, given the current implementation.   * This naively seems ok since the links aren't accessible by anyone other than admins.  ## Creating / updating links  ### 10k link limit  * The Sample Service allows no more than 10k non-expired links from any one version of a sample or   from any one version of an object. That means that a single object version can have no more   than 10k data IDs (e.g. column names for matrix data). * This requirement was originally agreed upon because there appears to be no way to   [efficiently sort and page results with graph queries in ArangoDB](https://github.com/arangodb/arangodb/issues/11260).   Allowing the collection to grow without limit means that eventually sorted graph queries will   OOM / hog CPU on the database (or be consistently killed if the DB has that capability). 10K   is small enough that the results can be sorted in a small amount of memory in the DB,   application, or UI. * Link lookups are currently implemented without graph traverals and so in theory the   10K limit could be lifted.     * Would need sort / paging / indexes. As usual, paging needs to be based on some aspect of       the link that is unique, which is tricky here * HOWEVER - if any of the query parameters or expectations change the limit may have to be   reinstated, e.g.   * Changes to how the search regards ACLs.   * Searching on additional properties, e.g.     * Workspace object properties     * Sample properties  ### Implementation notes  * Since multiple clients may be creating, updating, or expiring links on the same sample   or data object simultaneously, the code needs to account for possible race conditions on   those operations. * Links are immutable once created, *except* that they can be expired. * The unique ID of a link in the database is a fn of the data unit ID (e.g.   workspace UPA + data id) for an unexpired link, and the DUID + created time for expired links.   This ensures there's only one non-expired link per DUID in the DB.  ### Implementation  * Parameters: `new_link`, `update` boolean indicating `current_link` should be replaced if it   exists * Start a transaction with a collection exclusive lock.   * This is required since we're counting multiple documents. Allowing other writes while the     transaction is in progress will make those counts inaccurate. * Fetch the `current_link` for the data unit ID, if any * If `current_link`:   * If not `update`: fail   * If `current_link` == `new_link`: abort transaction and return (no-op)   * If `new_link` is to a different sample and count_links(`new_link.Sample`) > 10K: fail     * Link look up is based on the data, so we know it's to the same UPA, and since we're       expiring and replacing a link the link count for the UPA doesn't change.   * Expire and save `current_link`     * Creates a new ArangoDB document with a new `_key` * Else:   * If count_links(`new_link.UPA`) > 10K: fail   * If count_links(`new_link.Sample`) > 10K: fail * Save `new_link` * Complete the transaction.  ### Extant failure modes  * ArangoDB transactions can fail on one node and succeed on another. This will commit the   changes that succeeded but cause the transaction as a whole to fail client-side. This means   in theory the client could determine the current state of the DB and try to repair any   inconsistencies, but in practice this is very complicated.   * A link could be expired without the current link being updated, effectively leaving an     expired and current version of the same link. If this link were to be expired again an     error would occur and the DB would have to be manually corrected.   * A link could be updated without the prior link being expired, effectively causing the record     of the prior link to disappear.  ## Expiring links  ### Implementation notes  * See the implementation notes for creating / updating links above. * Arango does not support atomically changing a document's `_key`. Since expiring a link   means changing the `_key`, we use a transaction to reduce the possibility of inconsistent   database state.   * But does a transaction really help?  ### Implementation  * Parameters: `duid` - the data unit ID * Fetch `current_link` from the DB via the `duid`. * If not `current_link`: fail * Start a transaction. * Expire `current_link` and save.   * Creates a new ArangoDB document with a new `_key`   * Fail if a duplicate key error occurs, meaning the link was expired after fetching the document. * Delete the old `current_link` document. * Complete the transaction.  ### Extant failure modes  * ArangoDB transactions can fail on one node and succeed on another. This will commit the   changes that succeeded but cause the transaction as a whole to fail client-side. This means   in theory the client could determine the current state of the DB and try to repair any   inconsistencies, but in practice this is very complicated.   * A link could be expired without the current link being deleted, effectively leaving an     expired and current version of the same link. If this link were to be expired again an     error would occur and the DB would have to be manually corrected.   * A link could be deleted without the expired document being written, destroying the link's     history. * Since expiration does not use an exclusive lock, it is possible for other writes to collide   with the expired link document and cause the write to fail.   * It is not clear if this will     [cause the transaction as a whole to fail](https://github.com/arangodb/arangodb/issues/11424). * If the delete fails, an error will be raised and the transaction will be aborted.   * Funnily enough, there is an `ignore_missing` parameter, but from the `python-arango`     documentation:     ```     :param ignore_missing: Do not raise an exception on missing document.     This parameter has no effect in transactions where an exception is       always raised on failures.     ```   * This could be caused by:     * Another thread expiring and deleting the link, in which case no further action is necessary.     * Another thread expiring and updating the link, in which case the current operation should       not expire the new link.   * This error takes split second timing and is highly unlikely to occur, and should not     leave the database in an inconsistent state. ",others
ADR_201,https://github.com/elastic/cloud-on-k8s.git,docs/design/0008-volume-management.md,8. Volume Management in case of disruption,"# 8. Volume Management in case of disruption  **Update (2019-07-30)**: We decided to rely on StatefulSets to manage PersistentVolumes. While this ADR remains valid, our range of action is now limited to what is supported by the StatefulSet controller, responsible for creating and reusing PVCs.  * Status: proposed * Deciders: cloud-on-k8s team * Date: 2019-03-08  ## Context and Problem Statement  The aim of this document is to capture some scenarios where a pvc gets orphaned and define how the “reuse pvc” mechanism must behave. This document does not deal with the reuse of a PVC after the spec of the cluster has been updated _(that is, ""inline"" VS ""grow-and-shrink"" updates)_. It is a complex scenario which deserves its own ADR.   As a preamble before we dive into the different use-cases and scenarios here are some considerations about what can lead to a disruption and a reminder about some constraints raised by storage classes.  ### Disruptions A Pod does not disappear until a person or the controller deletes it or there is an unavoidable hardware or system software error. The reasons can be classified into 2 main categories:  * There is an **external involuntary** disruption:   * Hardware failure   * VM instance is deleted   * Kernel panic   * Any runtime panic (for example `containerd` crash)   * Eviction, but it is not supposed to happen as long as we use a [QoS class of Guaranteed](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed) * There is an **external voluntary** disruption   * The node hosting the Pod is drained because, some _(non exhaustive)_ examples are:     * The K8S node is about to be upgraded or repaired     * The K8S cluster is scaling down   * The pod is manually deleted by someone (not only as in error but also because sometimes a reboot can fix a problem)  ### Storage class constraints Storage classes do not all provide the same capabilities when it comes to reusing a volume, for instance:  * Google persistent disks can be attached from a single availability zone * Regional persistent disks replicate the data between 2 zones in the same region * A volume backed by our elastic-local storage class can only be reused on the same node  At this stage it is worth mentioning that even if the K8S scheduler uses some predicates to reschedule a pod on a node where the volume can be reused or attached, it does not preserve the capacity needed to reschedule the pod. For instance if a pod was using a local volume and if the node runs out of capacity while the pod is being recreated then it becomes impossible to reuse the volume until some capacity is freed.  When a disruption occurs either a volume is considered to be **recoverable** or it is considered **unrecoverable**.  #### Unrecoverable volume  `Unrecoverable` is a state that can be reached in two situations:  * The administrator knows that the volume can't be recovered and it must be abandoned. * The Elastic operator creates a new pod in an attempt to reuse the volume but the pod is still not scheduled after a given amount of time.  #### Recovering strategies  There are 2 possible strategies when it is time to try to recover the data from a PVC:  ##### Recoverable required  The Elastic operator **must not delete** a PVC that may hold the only copy of some data. `Recoverable required` is a state in which the volume **must** be recovered to get the missing data back online.  ##### Recoverable optional  `Recoverable optional` is a state where the missing data is available on some others nodes. For instance if a K8S node with a local volume is down and if data can be replicated from other nodes then it is not mandatory for the Elastic operator to wait forever.  It is a best effort scenario, we have to choose between:  * Wait for the node to be back online  VS  * Paying the cost of a replication from other nodes  It means that in such scenario we have to find a way to determine the time the operator will wait before it is decided that a new pod must be created. It may be hard to find this exact timeout, it must be user configurable but a sane default value should be set, based on some criteria like, for example: * the shard size * the number of replicas still available  TODO: check if the controller has access to PVC but not to PV or nodes, we can't watch nodes or PV, we can't only watch the claims  ## Decision Drivers The solution must be able to handle the following use cases:  ### UC1: The K8S cluster is suffering a external involuntary disruption and the volumes cannot be recovered  In this scenario we must consider the data as permanently lost _(for example vm with local storage has been destroyed)_.  We need to give a way to the user to instruct the Elastic operator that: * It should immediately move the volume into a `Recovering` strategy. * If the volume is in the `Recoverable required` state the user should be able to forcibly not reuse the PVC, even if there is no other replica available.  ### UC2: The K8S cluster is suffering a external involuntary or voluntary disruption but the volumes can be eventually recovered  The Elastic operator will create a new pod and according to the PV affinity the scheduler will hopefully find a new node where the data is available. If it takes to much time to schedule the pod then the volume is moved into one of the two `Recoverable` states.  ### UC3: As an admin I want to plan a voluntary disruption and the volumes cannot be recovered  In this scenario the administrator want to definitively evacuate a node and the data will not be available anymore (for example, a server with a local storage is definitively removed from the cluster)  It is usually done in two steps:  1. Cordon the node 1. Evict or delete the pods  ## Considered options  ### Option 1: Add a finalizer to the PVC  A PVC that is used by a pod will not be deleted immediately because of a finalizer set by the scheduler. We can add our own finalizer to: 1. Create a new pod 1. Migrate the data and delete the pod. Once the pod has been deleted the PVC can be deleted by K8S.  ### Option 2: handle PVC deletion with an annotation  A tombstone is set on the PVC as an annotation. The annotation `elasticsearch.k8s.elastic.co/delete` can be set on a PVC with the following values:  * `graceful`:  migrate the data, delete the node and the PVC. * `force`: discard the data, the operator does not try to reuse the PVC, the PVC is deleted by the Elastic operator.   ### Option 3: Add a kubectl plugin to add some domain specific commands  `kubectl` can be extended with new sub-commands: https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/  for example: ```bash $ kubectl elastic migrate elasticsearch-sample-es-qlvprlqnnk -n default # will migrate the data then delete the pod and the pvc $ kubectl elastic delete elasticsearch-sample-es-qlvprlqnnk -n default # will delete the pod **and** the pvc ```  ### Option 4  Try to handle pod eviction and PVC deletion with a webhook.  ## Pros and Cons of the Options  ### Option 1  Pros: * Looks like a simple approach, just do a `kubectl delete pvc/XXXXX` to migrate the data and delete the pod.  Cons: * If the volume can't be recovered the user will have to remove the `Finalizer` * Administrator has to `uncordon` the node and delete manually _(also known as error prone)_ the `PVC` if he wants to drain it.  ### Option 2 Pros: * The Elastic operator can figure out easily if it must try to migrate the data or abandon the volume because it can't be recovered.  Cons: * Admin has to `uncordon` the node and annotate the `PVC` manually _(still error prone)_ if he wants to drain it. * Admin must remember the annotations  ### Option 3 Pros: * Provides a meaningful interface  Cons: * Stable?: Even if plugins were introduced as an alpha feature in the v1.8.0 release it has been reworked in v1.12.0 * End user must install the plugin * Admins still have to evict nodes manually when the node is drained  ### Option 4 Pros: * Integrate smoothly in the `cordon` + `drain` scenario.  Cons: * It doesn't seem possible to handle a node eviction _(needs to be confirmed)_. * Setting a webhook requires some privileges at the cluster level. * Is it even possible to use a webhooks to safely migrate some data when an eviction occurs?  ## Links  * Strimzi: [Deleting Kafka nodes manually](https://strimzi.io/docs/master/#proc-manual-delete-pod-pvc-kafka-deployment-configuration-kafka) * Kubernetes [isBeingUsed](https://github.com/kubernetes/kubernetes/blob/a3ccea9d8743f2ff82e41b6c2af6dc2c41dc7b10/pkg/controller/volume/pvcprotection/pvc_protection_controller.go#L210) function: A PVC can be deleted *only* if it is not used by a scheduled pod _(including the `Unknown` state)_ ","infrastructure_and_deployment, technology_choice"
ADR_202,https://github.com/vwt-digital/operational-data-hub.git,architecture/adr/0034-2fa-on-all-user-identities.md,34. 2FA on all user identities,"# 34. 2FA on all user identities  Date: 2020-09-21  ## Status  Accepted  Implements [6. Implement Security by Design](0006-implement-security-by-design.md)  ## Context  Two-Factor Authentication (2FA) is sometimes called multiple factor authentication. In simple terms, it adds an extra layer of security to every online platform you access. The first layer is generally a combination of a username and password. Adding one more step of authenticating your identity makes it harder for an attacker to access your data. This drastically reduces the chances of fraud, data loss, or identity theft.  Passwords have been the mainstream form of authentication since the start of the digital revolution. But, this security measure is far from infallible. Here are some worrying facts about this traditional security measure: * 90% of passwords can be cracked in less than six hours. * Two-thirds of people use the same password everywhere. * Sophisticated cyber attackers have the power to test billions of passwords every second.  The vulnerability of passwords is the main reason for requiring and using 2FA.  ## Decision  We will use 2FA on all user identities.  ## Consequences  Despite the additional overhead of the second factor to authenticate, the additional protection of the user identity is worth the effort. Most identity providers facilitate 2FA using a mobile app, which limits the additional effort required.  ## References  * https://secureswissdata.com/two-factor-authentication-importance/, retrieved 21 October 2020 ",security
ADR_203,https://github.com/actions/runner.git,docs/adrs/0297-base64-masking-trailing-characters.md,ADR 0297: Base64 Masking Trailing Characters,"# ADR 0297: Base64 Masking Trailing Characters  **Date** 2020-01-21  **Status** Proposed  ## Context  The Runner registers a number of Value Encoders, which mask various encodings of a provided secret. Currently, we register a 3 base64 Encoders: - The base64 encoded secret - The secret with the first character removed then base64 encoded - The secret with the first two characters removed then base64 encoded  This gives us good coverage across the board for secrets and secrets with a prefix (i.e. `base64($user:$pass)`).  However, we don't have great coverage for cases where the secret has a string appended to it before it is base64 encoded (i.e.: `base64($pass\n))`).   Most notably we've seen this as a result of user error where a user accidentally appends a newline or space character before encoding their secret in base64.  ## Decision  ### Trim end characters  We are going to modify all existing base64 encoders to trim information before registering as a secret. We will trim: - `=` from the end of all base64 strings. This is a padding character that contains no information.    - Based on the number of `=`'s at the end of a base64 string, a malicious user could predict the length of the original secret modulo 3.      - If a user saw `***==`, they would know the secret could be 1,4,7,10... characters. - If a string contains `=` we will also trim the last non-padding character from the base64 secret.   - This character can change if a string is appended to the secret before the encoding.   ### Register a fourth encoder  We will also add back in the original base64 encoded secret encoder for four total encoders: - The base64 encoded secret - The base64 encoded secret trimmed - The secret with the first character removed then base64 encoded and trimmed - The secret with the first two characters removed then base64 encoded and trimmed  This allows us to fully cover the most common scenario where a user base64 encodes their secret and expects the entire thing to be masked. This will result in us only revealing length or bit information when a prefix or suffix is added to a secret before encoding.   ## Consequences  - In the case where a secret has a prefix or suffix added before base64 encoding, we may now reveal up to 20 bits of information and the length of the original string modulo 3, rather then the original 16 bits and no length information - Secrets with a suffix appended before encoding will now be masked across the board. Previously it was only masked if it was a multiple of 3 characters - Performance will suffer in a negligible way ",others
ADR_204,https://github.com/Branchout/branchout.git,doc/adr/0002-language.md,2. language,"# 2. language  Date: 2018-10-31  ## Status  Accepted  Testing [3. testing](0003-testing.md)  ## Context  A language should be universal, simple and easily testable  Options * Shell * Go * Java * JavaScript  There should be very few dependencies  ## Decision  Shell  * No dependencies * Installed pretty much everywhere developers are  ## Consequences  Testing will be a learning curve - bats Ensuring portability - shellcheck Async is a little awkward - xargs ",technology_choice
ADR_205,https://github.com/OpenTOSCA/container.git,docs/adr/0002-use-spring-dependency-incjection.md,Use Spring for Dependency Injection and discovery of plugins,"# Use Spring for Dependency Injection and discovery of plugins  * Status: accepted  ## Context and Problem Statement  To facilitate testing, a clean, object oriented architecture as well as the plugin systems for various components a configurable Inversion of Control (IOC) container is required. This container is responsible for plugin discovery, as well as injecting services required by the API to serve it's ""external"" customers.  ## Decision Drivers   * Support for a plugin system that can discover additional components not originally compiled into the deployed WAR * Support for minimal configuration, allowing easy modification and discovery by convention  ## Considered Options  * Spring * Guice  ## Decision Outcome  The chosen IoC container is Spring, because it supports plugin discovery at minimal configuration and has easy support for servlet-based injection with `spring-mvc` and `spring-web`.  ### Negative consequences  The Plugins loaded cannot be adjusted at runtime. At time of writing, no such capability is required or planned.  ## Pros and Cons  ### Spring  * Well-Maintained and diverse IoC container supporting various configuration mechanisms * No support for changes in registration during runtime * Support for `javax.Inject` annotations as well as injection-site adaption through custom annotations * No direct support for servlet-based injection, but available as `spring-web`  ### Guice  * No support for XML-Based configuration * No direct support for servlet-based injection * At time of writing seems to be discontinued by original maintainer Google ",technology_choice
ADR_206,https://github.com/marcusholmgren/crispy-dragon.git,docs/OPM-1-Decision-Tracking.md,OPM1: Use ADRS for decision tracking,"# OPM1: Use ADRS for decision tracking Date: 2021-01-28  ## Status Accepted  ## Context A microservices architecture is complex and we'll need to make many decisions. We'll need a way to keep track of the important decision we make, so that we can revisit and re-evalute them in the future. We'd prefer to use a lightweight, text-based solution so that we don't have to install any new software tools.  ## Decision We've decided to use [Michael Nygard's lightweight architectural decision record (LADR)](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) format. LADR is text based and is lightweight enough to meet our needs. We'll keep each LADR record in its own text file and manage the files like code.  We also considered the following alternative solutions: * Project management tooling (not selected, because we didn't want to install tools) * Informal or ""word of mouth"" record keeping (not reliable)  ## Consequences * We'll need to write decision records for key decisions * We'll need a source code management solution to manage decision record files ",governance_and_process
ADR_207,https://github.com/arachne-framework/architecture.git,adr-014-project-templates.md,Architecture Decision Record: Project Templates,"# Architecture Decision Record: Project Templates  ## Context  When starting a new project, it isn't practical to start completely from scratch, every time. We would like to have a varity of ""starting point"" projects, for different purposes.  ### Lein templates In the Clojure space, Leiningen Templates fill this purpose. These are sets of special string-interpolated files that are ""rendered"" into a working project using special tooling.  However, they have two major drawbacks:  - They only work when using Leiningen as a build tool. - The template files are are not actually valid source files, which makes them difficult to maintain. Changes need to be manually copied over to the templates.  ### Rails templates  Rails also provides a complete project templating solution. In Rails, the project template is a `template.rb` file which contains DSL forms that specify operations to perform on a fresh project. These operations include creating files, modifying a projects dependencies, adding Rake tasks, and running specific _generators_.   Generators are particularly interesting, because the idea is that they can generate or modify stubs for files pertaining to a specific part of the application (e.g, a new model or a new controller), and they can be invoked _at any point_, not just initial project creation.   ## Decision  To start with, Arachne templates will be standard git repositories containing an Arachne project. They will use no special syntax, and will be valid, runnable projects out of the box.  In order to allow users to create their own projects, these template projects will include a `rename` script. The `rename` script will recursively rename an entire project directory to something that the user chooses, and will delete `.git` and re-run `git init`,   Therefore, the process to start a new Arachne project will be:  1. Choose an appropriate project template. 2. Clone its git repository from Github 3. Run the `rename` script to rename the project to whatever you wish 4. Start a repl, and begin editing.  ### Maven Distribution  There are certain development environments where there is not full access to the open internet (particularly in certain governmental applications.) Therefore, accessing GitHub can prove difficult. However, in order to support developers, these organizations often run their own Maven mirrors.  As a convenience to users in these situations, when it is necessary, we can build a wrapper that can compress and install a project directory as a Maven artifact. Then, using standard Maven command line tooling, it will be possible to download and decompress the artifact into a local filesystem directory, and proceed as normal.  ## Status  PROPOSED  ## Consequences  - It will take only a few moments for users to create new Arachne projects. - It will be straightforward to build, curate, test and maintain multiple different types of template projects. - The only code we will need to write to support templates is the ""rename"" script. - The rename script will need to be capable of renaming all the code and files in the template, with awareness of the naming requirements and conventions for Clojure namespaces and code. - Template projects themselves can be built continuously using CI  ### Contrast with Rails  One way that this approach is inferior to Rails templates is that this approach is ""atomic""; templating happens once, and it happens for the whole project. Rails templates can be composed of many different generators, and generators can be invoked at any point over a project's lifecycle to quickly stub out new functionality.  This also has implications for maintenance; because Rails generators are updated along with each Rails release, the template itself is more stable, wheras Arachne templates would need to be updated every single time Arachne itself changes. This imposes a maintenance burden on templates maintained by the core team, and risks poor user experience for users who find and try to use an out-of-date third-party template.  However, there is is mitigating difference between Arachne and Rails, which relates directly to the philosophy and approach of the two projects.  In Rails, the project *is* the source files, and the project directory layout. If you ask ""where is a controller?"", you can answer by pointing to the relevant `*.rb` file in the `app/controllers` directory. So in Rails, the task ""create a new controller"" _is equivalent to_ creating some number of new files in the appropriate places, containing the appropriate code. Hence the importance of generators.  In Arachne, by contrast, the project is not ultimately defined by its source files and directory structure; it is defined by the config. Of course there *are* source files and a directory structure, and there will be some conventions about how to organize them, but they are not the very definition of a project. Instead, a project's _Configuration_ is the canonical definition of what a project is and what it does. If you ask ""where is a controller?"" in Arachne, the only meaningful answer is to point to data in the configuration. And the task ""create a controller"" means inserting the appropriate data into the config (usually via the config DSL.)   As a consequence, Arachne can focus less on code generation, and more on generating *config* data. Instead of providing a _code_ generator which writes source files to the project structure, Arachne can provide _config_ generators which users can invoke (with comparable effort) in their config scripts.  As such, Arachne templates will typically be very small. In Arachne, code generation is an antipattern. Instead of making it easy to generate code, Arachne focuses on building abstractions that let users specify their intent directly, in a terse manner.",technology_choice
ADR_208,https://github.com/vwt-digital/operational-data-hub.git,architecture/adr/0060-lock-pip-requirements.md,60. Lock pip requirements,# 60. Lock pip requirements  Date: 2021-06-08  ## Status  Accepted  Implements [4. Create software defined everything](0004-create-software-defined-everything.md)  ## Context  Code Injection is a specific type of injection attack where an executable program statement is constructed involving user input at an attack surface that becomes vulnerable when it can be manipulated in an unanticipated way to invoke functionality that can be used to cause harm.  ## Decision  To prevent dependency injection attacks we decided to have both a requirements.in file and a [pip-tools/pip-compile](https://github.com/jazzband/pip-tools) generated requirements.txt  ## Consequences  Please check the 'Way of Working' document on [Confluence](https://recognize.atlassian.net/wiki/spaces/DAT/pages),security
ADR_209,https://github.com/psu-libraries/scholarsphere.git,doc/architecture/decisions/0004-blacklight-for-search-only.md,4. blacklight-for-search-only,"# 4. blacklight-for-search-only  Date: 2020-01-15  ## Status  Accepted  ## Context  There are two ways we can display works and work versions in Scholarsphere: 1) using the record that is in the  Postgres database; or, 2) using the record that is in Solr.  ## Decision  We going to use the Postgres record for displaying individual records, leaving Blacklight's Solr record for displaying search results only. The Solr record, or SolrDocument, will not be used when displaying the detailed record for a work or work version. It will only be used within the context of a list of search results.  ## Consequences  Blacklight dependency footprint is reduced, i.e. we're relying on it to do less. However, we also loose some baked-in features such as json API displays and other methods of reformating existing records, such as xml views, etc. That doesn't mean we can't re-use them later. ",data_persistence
ADR_210,https://github.com/etienneleba/TDD-hexagonal-project.git,Docs/ADRS/2020_12_02_9_20_PROJECT.md,Context,### Context  This is the beginning of the project   ### Decision  I decide to create a _Docs_ folder to store all the documentation.  In this _Docs_ folder there will also have a _ADRS_ folder to store all the decision on this project.   ### Consequences   There will be a single point to store all the documentation,governance_and_process
ADR_211,https://github.com/Ensembl/ols-client.git,doc/adr/0003-looping-over-list.md,3. looping over list,"# 3. looping over list  Date: 2018-07-26  ## Status  Done  ## Context  We want to be able to loop simply over Ontologies / Terms results, without bothering if a new call is made to change page.    OLS API results are paginated, the page size is a parameter in Query. There is no simple way to loop over all elements, and returning all results is not a solution, considering amount of data The actual calls to API are hidden from final users.  ```python  from ebi.ols.api.client import OlsClient  client = OlsClient() ontology = client.ontology('fpo')  terms = ontology.terms() individuals = ontology.individuals() properties = ontology.properties()  # work with all 'list' item types for term in terms:     # do whatever     print(term)  # Direct List'like access on all list types term = terms[1254] individual = individuals[123] # ...  ```    ## Decision  To Implement  ## Consequences  - [~] The list must keep track of current loaded, therefore if initial request   - [~] ListMixin object are state-full.  ",others
ADR_212,https://github.com/raster-foundry/raster-foundry.git,docs/architecture/adr-0027-annotate-project-STAC-export.md,0027 Annotate Project STAC Export,"# 0027 Annotate Project STAC Export  ## Context  This is an ADR documenting decisions made on MVP features of exporting Raster Foundry/Annotate label data and scenes in STAC-compliant catalogs. This aims to make us one step closer towards integrating STAC to the path of interoperability among Azavea’s Machine Learning workflow.  In Raster Foundry, we support creating geospatial labels based on images and scenes added to project layers. This feature is further extended in Annotate App, which is an application backed by Raster Foundry APIs, so that teams can work on an image labeling projects simultaneously within status-tracked tasks. To enable downstream Machine Learning work better in interoperating these ground truth labels, we have decided to implement a feature for exporting images and labels in STAC catalogs in an asynchronous manner. The following sections will go into details about the export process, the structure of the exported catalog, and where they are stored.  ## Decision  ### CRUD endpoints  Similar to image and scene exports in Raster Foundry, we have created CRUD endpoints for scene and label exports in the form of STAC catalogs by `api/stac`. More details about the API are in the [spec](https://github.com/raster-foundry/raster-foundry/blob/1.27.0/docs/swagger/spec.yml#L4802).  1. Create  `POST` JSON in the following shape creates an export record and kicks off the catalog building batch process.  ```json {      ""name"": ""Annotate project export test"",      ""layerDefinitions"": [           {                ""projectId"": ""<UUID of a project>"",                ""layerId"": ""<UUID of a project layer>""           }      ],      ""taskStatuses"": [“<task status>”] } ```  One or multiple objects for `layerDefinitions` may be supplied so that the exported catalog contains images and labels from one or many project layers. The creation will succeed only if the operating user has `VIEW` access to the project, and the layer exists within the project.  `taskStatuses` should be one or multiple of `UNLABELED`, `LABELING_IN_PROGRESS`, `LABELED`, `VALIDATION_IN_PROGRESS`, and `VALIDATED`. These are enums marking statuses of tasks in the database according to Annotate frontend operations. Only labels spatially fall in tasks of these statuses are included in the exported catalog. The geometry of the exported STAC Label Collection and Item will exclude the unexported task areas.  2. Update  Update is only permitted for super users or export owners. Only `name`, `export_location`, and `export_status` are open for updates -- the latter two fields will be updated after the catalog is exported and stored on S3.  3. List, get, and delete  There is nothing too special about these three, except that only super users or export owners are able to have valid results from these actions.  ### STAC catalog builder  We have created a STAC catalog export builder that will build the catalog in the following structure:  ``` Exported Catalog:      |-> Layer collection      |   |-> Scene Collection      |   |   |-> Scene Item      |   |   |-> (One or more scene items)      |   |-> Label Collection      |   |   |-> Label Item (Only one)      |   |   |-> Label Data in GeoJSON Feature Collection      |-> (One or more Layer Collections) ```  One may think of an export as a snapshot of scenes and labels contained in the specified layers that fall into tasks marked with certain statuses at a certain time. So an exported Catalog may contain multiple Layer Collections with each layer being a project layer in Raster Foundry. A Layer Collection contains one Scene Collection and one Label Collection. A Scene collection has one or multiple Scene Items with assets pointing to `COG` resources on S3, and these resources are from the `ingest_location` field of scenes in Raster Foundry database. A Label collection contains one Label Item representing ground truth labels with an asset pointing to a GeoJSON Feature Collection of the label data and with links pointing to Scene items representing the labelled imagery.  The `id` of the export record is the `id` of the Catalog. A Layer Collection’s `id` maps to a layer in Raster Foundry database. Scene Items’ `id`s are also scene `id`s in Raster Foundry database. `id` field in each `Feature` of the `FeatureCollection` of the ground truth data are annotation IDs in Raster Foundry database. Other IDs are generated on the fly.  ### Async batch job  We have created jobs in AWS batch to build these exports in an asynchronous manner. A STAC export job is kicked off when a user successfully performs a `POST` with the above mentioned JSON to the `api/stac` endpoint from Raster Foundry.  ### Static catalogs on S3  The exported STAC catalogs live on environment specific Raster Foundry S3 buckets. The S3 locations are determined by the self links in all STAC resources from the export builder. Resources are linked to each other by absolute links currently, except that `root` links use relative links. In future work, we will update the links so that only `self` links are absolute, and the rest will use relative links.  ## Consequences  The STAC endpoints, export builder, and the export batch job transform scenes and labels from layers in Raster Foundry database to STAC resources stored on S3.  ## Future Work  In terms of areas of enhancement for future work and better interoperability, some of these may be considered: update absolute links to relative links wherever makes sense, reuse the email notification component and notify export creator about the resource on S3 when the export is ready, support providing masks on exports to create training, validation, test sets, etc. ",technology_choice
ADR_213,https://github.com/mlibrary/nebula.git,doc/adr/0001-dont-export-moku-init-execs.md,1. Don't export `moku init` execs,"# 1. Don't export `moku init` execs  Date: 2019-02-15  Status ------  Accepted  Context -------  Let's say we have ten applications (`app_1`, `app_2`, ..., `app_10`) defined in hiera such that the deploy host can read them. Furthermore, let's say we have named instances of some of them on four different production hosts, like this:  ```                            +-------------+                            | Deploy Host |                            +-------------+  +-------------+   +-------------+   +-------------+   +-------------+ | App Host A: |   | App Host B: |   | App Host C: |   | App Host D: | | - app_1     |   | - app_1     |   | - app_1     |   | - app_1     | | - app_2     |   | - app_2     |   | - app_3     |   | - app_4     | +-------------+   +-------------+   +-------------+   +-------------+ ```  So only apps 1–4 are actually instantiated, and a couple of them are on more than one host. In this setup, we need the deploy host to run `moku init` once (and only once) for each of those four apps (and not at all for the other apps which haven't yet been instantiated).  If the deploy host runs the command for each app it knows how to set up, then it'll only run `init` once per app, but it'll run it for apps 5–10, which we do not want it to do.  If the app hosts export `exec` resources, then, in this example, the deploy host would find 4 of the same command for `app_1`, 2 for `app_2`, and 1 each for `app_3` and `app_4`. It wouldn't find any for apps 5–10, but it'd find too many for apps 1 and 2.  If the deploy host runs a puppetdb query to find all named instances to get a list of unique instance names, then it will get a list of apps 1–4, which it could use to run `moku init` for each of them. This is the desired behavior, but our experience with puppetdb queries is negative: they are very hard to test, and they are very ugly and hard to read.  In addition to all this, the `moku init` command requires that a json file exist on the deploy host for each application (based on its hieradata). Among other things, this json file contains a hash, keyed on datacenter, of lists of application hosts.  It is unacceptable for the deploy host to run `moku init` with an incomplete list of application hosts.  Decision --------  We will run `moku init` by hand but have puppet manage the configuration json. Also, the init command will accept application hosts to be keyed on hostname instead of datacenter, so instead of:  ```yaml deploy:   sites:     hatcher:     - node_1     - node_2     macc:     - node_3     - node_4 ```  We'll have:  ```yaml deploy:   sites:     nodes:       node_1: hatcher       node_2: hatcher       node_3: macc       node_4: macc ```  That way, we don't need to supply separate order numbers for each datacenter or try and come up with a way to do anything clever with the way concat fragments handle json. Each named instance can simply export a line with its hostname and datacenter and be done.  Consequences ------------  We'll have to run moku init by hand, but subsequent configuration has to be done by hand anyway, and it's still helpful for puppet to set up the json with truthful information about which hosts are ready to serve an application. ",infrastructure_and_deployment
ADR_214,https://github.com/alphagov/verify-service-provider.git,docs/adr/0016-we-will-have-a-healthcheck-endpoint.md,16. We will have a healthcheck endpoint,# 16. We will have a healthcheck endpoint  Date: 2017-07-25  ## Status  Accepted  ## Context  In various user research sessions we've observed users start the MSA or the verify service provider and then want to check whether it's working correctly. There's also a need for users to be able to monitor the health of the system once it's deployed to their environment.  Dropwizard allows you to configure an HTTP endpoint as a healthcheck. This can perform some arbitrary actions that check the health of the system.  ## Decision  We will have a healthcheck endpoint that will check the verify-service-provider can read metadata from the hub and the MSA.  ## Consequences  Users will be able to check and monitor the health of the verify-service-provider.  We will have to add some healthchecks.,others
ADR_215,https://github.com/vwt-digital/operational-data-hub.git,architecture/adr/0028-a-solution-is-implemented-by-one-or-more-gcp-projects.md,28. A solution is implemented by one or more GCP projects,"# 28. A solution is implemented by one or more GCP projects  Date: 2020-09-21  ## Status  Accepted  Implements [17. DDD defines ubiquitous language](0017-ddd-defines-ubiquitous-language.md)  Implements [26. Solution facilitates a coherent set of business functions](0026-solution-facilitates-a-coherent-set-of-business-functions.md)  Related to [27. A GCP project belongs to a single domain](0027-a-gcp-project-belongs-to-a-single-domain.md)  ## Context  A [solution facilitates a coherent set of business functions](0026-solution-facilitates-a-coherent-set-of-business-functions.md). Those functions can originate from multiple domains. A [project always belongs to a single domain](0027-a-gcp-project-belongs-to-a-single-domain.md). Therefore, a solution can be implemented by multiple projects, either due to the fact that it requires functions from multiple domains, or because projects allow better modularization of the solution, or both.  ![Structure of projects, domains and solutions](solution_project_domain.png ""Projects in different domains implementing a solution"")  ## Decision  We implement a solution by one or more projects.  ## Consequences  ### Advantages  * The platform's project structure is used to facilitate separation of concerns and modularization. * Responsibility for functionality is easily defined along the project boundaries.  ### Disadvantages  * Additional overhead when multiple small projects need to be created as functionality from multiple domains is required. ",microservices_and_modularity
ADR_216,https://github.com/MITLibraries/thing.git,docs/architecture_decisions/0003-deploy-via-heroku-pipelines.md,3. Deploy via Heroku Pipelines,"# 3. Deploy via Heroku Pipelines  Date: 2017-12-01  ## Status  Accepted  ## Context  Initially this project was not appropriate for deploy on Heroku because it needed MIT Touchstone authentication. However, based on [ADR-0002 Authentication via Touchstone SAML](0002-authentication-via-touchstone-saml.md) we are now able to remove the `mod_shib` requirement that initially prevented us from using Heroku.  ## Decision  We will use Heroku Pipelines for Staging / Production and PR builds.  ## Consequences  We will have CI / CD for this application in an environment we have proven in production on previous applications. ",infrastructure_and_deployment
ADR_217,https://github.com/alphagov/monitoring-doc.git,documentation/architecture/decisions/0010-packaging-node-exporter-as-deb-for-verify.md,10. Packaging Node Exporter as .deb for Verify,"# 10. Packaging Node Exporter as .deb for Verify  Date: 2018-08-15  ## Status  Accepted  ## Context  Node Exporter needs to be installed on Verify's infrastructure so that machine metrics can be gathered. Verify runs Ubuntu Trusty which does not have an existing node exporter package. Verify has an existing workflow for packaging binaries which can be leveraged to package node exporter.  ## Decision  Node exporter will be packaged as a deb using FPM following Verify's exiting packaging workflow.  ## Consequences  The use of Verify's infrastructure ties the Node exporter package to Verify, the node exporter would need to be repackaged for other programs to be able to be used. ",technology_choice
ADR_218,https://github.com/nicfix/py-ddd-cqrs-microservice-boilerplate.git,docs/adr/tooling/004-license_checker.md,Use an automated license checker,"# Use an automated license checker  ## Status  proposed  ## Context  Working with OpenSource software for commercial purposes requires every developer to check the licenses of the packages that he uses.  Some licenses might be not-compatible with the project needs and cause some legal issue if not properly handled.  Using open-source libraries speeds up development in a  significant way but has a drawback, nested dependencies.  Each open-source library could depend on further open-source libraries that could have different licenses for usage and re-distribution.  ## Decision  Given the previous concerns I suggest to use an automated licenses checker on the installed packages.  In python you can use `pip-licenses` (which is distributed with MIT license)   ## Consequences  Anytime a new library is added to the project a developer can verify if a new/conflicting license has been added with that library or with a nested dependency.  This allows the developer to take counter measures that can vary from mentioning the library in the list of open source technologies  or deciding to not use that library at all.",technology_choice
ADR_219,https://github.com/raster-foundry/raster-foundry.git,docs/architecture/adr-0002-frontend-framework.md,(sem título),"Frontend Frameworks =================== Context ------- ### Background Based on the advice of the Phase I team, we will be starting largely from scratch for the frontend code for this project. Many design elements are expected to be carried over from Phase I, but they will largely be rewritten to be more maintainable and extensible going forward. Therefore, we need to make a decision about which technologies to use for the frontend code.  The overwhelming majority of our team's frontend experience is with Angular 1.x. However, there are strong indications that we will need to migrate away from Angular 1.x eventually. Angular 2 is currently in RC, and represents essentially a complete rewrite of the framework. Once Angular 2 is released, we expect the Angular 1.x ecosystem to gradually decline in size and quality as libraries and developers shift to Angular 2. If we wrote this project's frontend in Angular 1.5 to start, we would almost certainly need to migrate away from Angular 1.5 at some point in the lifetime of the project, although whether that point would be in one year or five years is currently unclear. We expect that migration to be a complex and time-consuming task for anything other than a trivial application. This adds significant expected maintenance costs to an Angular-1-based approach, which has been our default for the past two years or so. In addition, our experience with Angular has revealed pain points that have led many team members to express a desire to try something new.  Given that migrating to a new framework seems inevitable within the lifespan of this project, it makes sense to consider alternatives to Angular 1.x now. There seem to be four possible paths which we could take: - Write Angular 1.5, upgrade later (the default if we change nothing now) - Switch to Angular 2 now for new development - Switch to React - Switch to Ember  *Implicit in this is a rejection of more ""exotic"" approaches such as Elm, ScalaJS, and Aurelia, which are immature, have small ecosystems, and/or add significant complexity to our development pipelines at a time when we already expect to be learning some kind of new framework.*  ### Convergence Despite the multiplicity of frontend web frameworks, the story over the last year or so among the big three web frameworks (Angular, React, and Ember) has been one of convergence. All three are moving toward use of a ""next-generation"" syntax plus transpilation, with Angular defaulting to TypeScript, Ember selecting ES2015, and a variety of approaches, including Clojurescript, used with React. Similarly, Angular 2 will introduce a new Angular-CLI which mimics many of the scaffolding features available from Ember's Ember-CLI. Angular 2 will bring significant improvements in page update speed over Angular 1 by adopting an approach similar to React's diffing algorithm, and Ember recently rolled out a new update engine, called Glimmer, which takes a similar approach. Ember has also adopted a strong component-based convention and one-way data flow by default, mimicking React, which Angular 2 will also adopt.  Therefore, the differences between these frameworks are rapidly becoming less about the availability or lack of certain built-in features, and more about ecosystem quality and maturity, project goals and governance, and focus on a pleasant development experience.  ### Assessment #### 1. Angular 1.5 -> 2 This approach would have the lowest startup cost because we would be able to get started right away. In order to make the eventual transition to Angular 2 as easy as possible, we would have to adhere to the recommended Angular style guide during the course of Angular 1 development, although this largely matches our preferred Angular style already. The recommended way to migrate is then to introduce a 1-to-2 interoperability module into the project which upgrades or downgrades Angular 1 and 2 components so that they can talk to each other, and then to rewrite components piece by piece while using Angular 1 and 2 side-by-side.  The continuing support situation for Angular 1.5 after the release of Angular 2 is currently unclear. Google has provided assurances that they will not cease development for Angular 1.x until the ""vast majority"" of users have switched to Angular 2, but it's unclear when this will happen or what their definition of the ""vast"" majority is. We do not want to be left as members of the Angular 1.x-using minority when Google stops development for it.  #### 2. Angular 2 now A way to avoid the problems with migrating to Angular 2 later in the life of the project is to begin using Angular 2 immediately, even in its RC state. This would gain us all the benefits of Angular 2 right from the start, and would allow us to learn the framework while the project is still small. It would also give us first-class usage of TypeScript, something which the other frameworks in this list lack.  The primary downside to Angular 2 is that it is still a young project; it can be expected to have bugs, usability and documentation issues, and a lack of libraries and community literature for the near future. There's also an unlikely but real possibility that the framework suffers from unknown design flaws that won't be exposed until it has received more real-world usage. Given Angular's popularity and backing by Google, we can assume that these issues will be ironed out eventually, but they will place a cost on development for at least the next year or two.  #### 3. React The React ecosystem is mature, and other teams at Azavea use it. One key difference between React and Angular / Ember is that React itself is not a full-featured framework: it is a library for generating components that communicate. Other modules, such as Flux or Redux, are required (or at least almost universally used) to build up a full frontend architecture.  This modular architecture runs somewhat counter to this team's framework preferences. Historically, we have preferred full-featured, opinionated frameworks over assembling collections of components, because our experience is that reducing the number of components in our application stack usually improves reliability and eliminates development headaches.  That said, opinionated frameworks can sometimes get in the way when we try to do something unusual, but our experience has been that as long as this is a rare occurrence, it is a price worth paying.  Structural issues aside, React has a healthy ecosystem with numerous libraries and has been used by other teams at Azavea before, so we would be able to leverage some of that knowledge if we run into issues.  #### 4. Ember Ember is a full-featured, opinionated MVC web framework that is quite mature in terms of features. Indeed, as mentioned above, Angular 2's Angular-CLI is heavily inspired by Ember's Ember-CLI. Similar to Angular 2, Ember has been working to incorporate more React-inspired features, including one-way dataflows by default and a faster differential page update engine. However, in contrast with Angular, Ember has already completed and rolled out these updates in Ember 2, which was released about a year ago. In addition, Ember did this while largely maintaining backwards compatibility, rather than rewriting the entire framework. The current release is 2.6.  The Ember-CLI offers impressive scaffolding capabilities -- it provides scaffolding for all common application components, including routes and tests, and the default project comes with a testing environment already set up and working. Ember-CLI also includes a package manager for adding community modules to one's project.  Ember uses ES2015, which is a big step up from ES5, but which lacks the type safety possible with TypeScript in Angular 2. There is a plugin for using TypeScript with Ember but this would likely be imperfect.  Ember is less well known than React and Angular, which results in a smaller ecosystem of literature and libraries. However, it is not a niche framework by any means. StackOverflow has about 19,000 questions tagged with ""ember.js"", as compared to 18,800 for ""reactjs"". Angular, however, has 185,600 (!) questions tagged  for ""angularjs"".  Decision -------- This project carries with it a large number of complex technical issues and new technologies related to raster processing. At the initial stages of the project, we will be more productive if we do not have to learn a new frontend framework alongside development of a complex backend architecture. This weighs in favor of sticking with what we know to start out. However, the long-term support situation for Angular 1 is currently unclear; it seems to be at least partially tied to community usage of Angular 1, which is tough to predict. Additionally, Angular 2 and Ember (and potentially more exotic frameworks such as Elm) appear to offer significant benefits over Angular 1.x such as improved page rendering speed and better developer ergonomics.  Raster Foundry will initially use Angular 1.5, following the style recommended by the [upgrade guide](https://angular.io/docs/ts/latest/guide/upgrade.html). Additionally, we will invest in our long-term productivity and hedge against decay in the Angular 1.x ecosystem by devoting resources toward an incremental upgrade process starting around the beginning of the second quarter of development work (roughly, mid-November, 2016). This process is expected to initially consume no more than 10% of per-sprint development points. Although the exact first steps in an incremental upgrade process will need to be decided, some possibilities might include: - Introduce a TypeScript or ES2015 transpilation layer to the existing project and begin rewriting existing code in one of these two languages. - Identify existing functionality that is well compartmentalized, and rewrite it in another framework or frameworks to assess ergonomics and interoperability. - Identify a planned component that would benefit from the strengths of a different framework and write it in that framework.  Consequences ------------ All developers will need to thoroughly read and familiarize themselves with the [Angular 2 Upgrade Guide](https://angular.io/docs/ts/latest/guide/upgrade.html). We will need to track development of the Angular 2 and 1 ecosystems. We will need to work to strike a balance between investment in new features and migrating away from Angular 1 to avoid accruing technical debt. We will likely need to develop temporary systems for doing development on Angular 1 alongside Angular 2 or another framework within the same frontend application. ",technology_choice
ADR_220,https://github.com/michelezamuner/smjs.git,docs/system/decisions/2019020401-handling-generic-input-output.md,Handling generic Input/Output,"# Handling generic Input/Output   ## Context  Currently we use a single View from the Presenter, to render the normal output of the use case, that is bound to the View interface in the general provider. However, we're not handling different views that might be needed, nor error messages, which are directly written to the STDERR of the application.  Regarding the former, we're not allowing the user to choose the output representation she prefers to get. Regarding the latter, if we're decoupling the presenter from the controller to support segregation between input and output devices, we cannot then just assume that the input device will also be able to support output, like we're doing now.   ## Decision  ### Segregation of input and output devices  We assume that a generic application can be connected to several different input and output devices: this means that input may come from different input devices (for example keyboard and mouse), and the output may go to different output devices (for example `STDOUT`, a GUI, and the filesystem). Of course this requires us to drop the assumption that all usual Web applications make, that the input device is the same as the output device (a Web browser, or maybe a Web/application server to be more precise).  One of the objectives of modern architectures is exactly allowing multiple different output devices to be used by the same application, and also from the same input device. To allow this, it's necessary that the controller is oblivious of the specific output device that will be used: for example, the controller shouldn't know if the output is going to a GUI, a CLI, a log, an audio speaker or a combination of these.  It's also important to mention that the strict separation between input and output is an application-level concern, which expects input to come from an interface (input boundary), and to send output to a different interface (output boundary). This is important because the application doesn't need to care if the device providing the input is the same as the one receiving the output or not. However, in most cases the input device will also support some kind of output, at least to display errors of input validation, and it would be very weird if the input came from a device, and the output were sent to a different one, because the user using the first device would expect to see the output there as well. However, this kind of architecture easily supports also cases where the input device has no output capability, and the output is to be sent somewhere else (like a game controller and a monitor).  ### Controllers and presenters  While a use case interactor performs its application logic, it may not need to perform a single presentation, for two reasons: - there are many different kinds of presentations, that will likely need to be handled differently, like main successful output, and error output - there are some presentations that will be produced immediately, like input validation results, and some that may be produced asynchronously, like command execution results  For this reason, the interactor will in general have to define several different presenter interfaces, instead of just one, and juggle the various presenters according to the application logic that is unfolding.  Now, we're describing this on the grounds that the presenters will be injected into the interactor. Now, in theory it's possible to achieve the same by returning a single response from the interactor, instead of letting the interactor call the presenters. This would actually work, and keep the dependency inversion principle respected, since the interactor still knows nothing of the details of presentation, however: - we need to stuff all possible response cases into a single response object, instead of cleanly separate the various cases - the controller will need to parse the response, understand what case it's about, and call different presenters according to it, adding much more responsibility to the controller than just having to translate the input data from the adapter format to the application format; additionally, the controller would really be just unpacking the same information that was packaged by the application, i.e. that a certain presenter needs to be called in a certain output situation, and the controller cannot deviate from this logic either, otherwise it would be taking presentation responsibility - the single response object could be returning asynchronous output, in addition to synchronous one, so the controller will also need to check which part of the response should be treated as asynchronous (like attaching a callback to a promise), and which as synchronous - even if the interactor was a coroutine (avoiding the problem of creating a single response for all cases), the controller would still need to check the kind of output at each return, and do the job of the interactor of associating a presenter to that kind of output, while the interactor already knows this information in the first place, in addition to still having to handle asynchronous output  For these reason, although returning the response from the interactor is technically feasible (and in certain scenarios it's certainly the best solution), it cannot be chosen as the general approach to use: rather it should be regarded as a special case, that works best only in specific situations.  ### Presenters and views  Each specific presenter that can be used to implement the output port represents a specific output model, which is characterized by the set of view fields and data type conversions it supports, or in other words, the kind of view model it will produce. For example, a ""screen"" view model might convert the application response to a certain set of fields of certain type, with the intent of having them being displayed to a screen, while a ""print"" view model might produce from the same application response a different set of fields, of different types, specialized for being sent to a printer.  Still, the same view model, representing a certain output model, can be rendered on the selected output device in different ways: a ""screen"" output model, containing fields belonging to a screen representation, might still be rendered as an HTML page, or as TXT file, or again as a graph. All these alternatives are represented by different views. This means that a specific presenter component, for example the ""screen"" presenter component, will define a ""screen"" view interface, that will then be implemented by multiple actual ""screen"" view instances, like HTML screen view, graph screen view, etc., one of which will then be selected and injected in the presenter, to be used for rendering.  ### Input definition  We borrow a definition of ""input"" from the REST standard. According to this definition, an input will be made of: - a resource identifier (for example `/path/to/my/resource`, URI in the Web context) - some input parameters (for example `?some[]=value&some[]=other`) - a resource representation (for example `.html`, `.pdf`, etc.)  this means that: - the identifier is used to pick a couple of controller and action, that needs to be called to perform the requested use case - the parameters are just passed to the action when it's called - the representation is related to the specific view that will be used to render the output generated by the use case's presenter; additionally, once a view is selected, it's presenter is also chosen, because a presenter might have multiple views, but a view only belongs to one presenter  For example, let's assume we have an `entitiesList` control widget, which displays a specific representation of a list of entities. When we trigger the `update` event on the `entitiesList` control widget, a very specific input can be created: the identifier would reference the controller and the action, like `entitiesController.update`; the input parameters could perhaps be the name of a subset of entities that this specific widget is configured to represent; finally, the representation is related to the specific widget that is sending this input, meaning that the same resource needs different widgets to be represented with different views.  ### Using a router  The interesting problem that arises is that the view to be chosen is known at input time, because it's related to the representation: this may lead to think that it should be known by the controller. However, the controller should be independent from any choice regarding the output, including which representation is used.  A solution can be to introduce a *router* component, that takes an input, and chooses a combination of *controller*, *action* and *view* according to the given input. This is much like what happens with usual MVC Web framework, but generalized for any kind of adapter. This choice made by the router can be statically configured, since it does not depend on the user input.  The input part is quite easy, in that the router needs just to create the right controller object, and call the right action method on it, passing the right parameters. The output part is trickier though: since the controller is decoupled from the presenter, it doesn't return any output object that can be then handled by the router, rather it forwards the execution to the presenter object, in a way that isn't in control of the router. What the router can do, though, is to select the correct view to be used, and use the container to bind it to the generic view interface. This way, when the presenter will be built, the right view object will automatically be injected into it.  ### Main output and side output  A typical application can have many different kinds of output. One of these is used to talk back to the user that provided the input: we can call it *main output*, and it would be produced by the selected use case. The *side output* would be any other kind of output produced by the application, like logging, monitoring, etc., which is not directly related to the input, and usually is not expected to be received by the user who produced the input.  While the main output is handled with a presenter, side output can be handled by using a message bus to send it to a non-specified recipient from the application layer: inside the adapter, then, one or more configured handlers will receive the messages, and output them to the configured devices.  As a case study, we can consider the verbose output of a console application. At first, this may seem a case where some side output (info logs) need to be redirected to the main output (console STDOUT). What's really happening, though, is that the side output is being produced in a way that is completely unrelated to the presenter and views used by the main output: messages are sent containing the side output, and then in the adapter a new handler is configured to listen to these messages, and print them to the same output device used by the main output. This way we can print to the same output device, but keep the two kinds of output completely independent.  ### Handling errors  We can identify three broad categories of errors: application errors, input errors and system errors. Errors generated by domain or application services can be caught inside the use case, and passed back to the presenter in the response. Input errors are related to wrong input, or input that doesn't pass validation, and they are again checked inside the application interactors: they shouldn't be handled by controllers, because controllers have no way to select a specific output device to forward errors to, and additionally it's better if controllers stick to their single responsibility of translating data using adapter-specific format, into request data using application-specific format; on the other hand, we can say that the use case interactor is taking the role that so-called ""controllers"" have in traditional Web applications, meaning validating input, using proper services to produce an output, and send the output to the proper presenter/view. Finally, system errors are programming or configuration errors that may happen in the adapter-specific code, and that are generally caught during development, but which may still happen in production, and make the application crash: since these errors might happen outside of the application logic, they cannot be handled by the application service, like application and input errors, but still they should be caught and displayed on an output device to notify the user.  System errors, not being generated inside the use case, don't concern any use case, and as such they don't need to be handled by any presenter. The way to handle them is to let any piece of code that is catching them sending a message to a widget that is responsible for displaying error messages, without needing to go through any application logic. Errors generated by domain or application services can be caught inside the use case, and passed back to a specific error presenter, which in turn will be configured with a specific view to be used in that situation.  ### User interface widgets  Any user interface, whether graphical or not, is composed of widgets, representing specific contexts for user interaction and presentation of data.  User interfaces are meant to be frequently changed, so they need to be as flexible as possible. To support this, widgets have to be designed so that they are as independent, reusable and replaceable as possible. A key tool to achieve this is using an events mechanism: instead of knowing of the existence of a specific widget, and calling a specific method on it, we send an event, and widgets that are interested in that event will respond to it with some action. This way we can easily replace widgets, or add new ones, without disrupting existing functionality.  Events can be generated by the domain, the application, or the widgets themselves. Thus, widgets must be allowed to know about all existing events. Of course who generates an event depends on where the event belongs to: if an event represents the completion of a domain task, then the domain will need to send it, whereas if an event represents the change of state of a graphical widget (not related to any application or domain concern), then the widget should send it.  Sometimes, however, it's not possible, or not convenient, to interact with widgets using events. For example it can happen that we need to be sure that a specific widget is actually called, but the event system doesn't ensure that any widget will respond to the event: for example if we want to display an error message, it doesn't make sense to send an event, hoping that a message box widget will catch it, rather we want to make sure that the message is actually displayed the way we need. To support these cases, widgets' actions must also be callable directly.  Widgets can then be just regular objects, exposing a set of methods: these can both be registered as event handlers, to be automatically called when certain events are caught, and be called directly on the object itself. This way we can take into account any possible combination of input and output methods, without creating a taxonomy of widgets that can have input and output, or only input and not output, etc., which is not really relevant on the user interface layer.  ### Widgets and use cases  The model of the communication with the application layer (i.e. with a port) distinguishes controllers from presenters, where the first are responsible for handling input, and the second are responsible for handling output. The reason why the two must be separated is that the presenters need to be injected into the use case interactor, since it may need to call different kinds of presenters in different situations, and also in an asynchronous fashion, and these are concerns of the application layer, not of the adapter.  Of course the use case interactor knows nothing of which widgets are used in the adapter, nor of the kind of user interface there is, so it cannot decide to update widgets: rather, the interactor defines some presenter interfaces representing the kinds of output it needs to perform, and then it is injected with the presenters of the actual widgets that are selected to be used.  However, from the point of view of the adapter layer (user interface), we don't necessarily need to separate controller objects from presenter objects. For example, an ""entity list"" widget might have a ""load"" controller action, that can be called directly to load entity data from the application, or as response to an event: this action will delegate to the ""list entities"" use case interactor, which will go through the query model to fetch the entities, and then call the given presenter to display them. In this case, the presenter will be the ""entity list"" widget itself, having a ""fill"" method for example, to fill the list with data. From the point of view of the interactor, it's enough that the entity list (or an adapter of it) implements the interface required by the interactor.  In the previous example the use case interactor will likely need to be injected also with an error presenter to display possible errors: this presenter can perhaps be implemented by a message box widget. Now, since the list widget will be injected with the interactor, it won't be required to know of the existence of the message box widget, nor of the fact that the list widget is used as a presenter itself, because these things are decided in the application configuration (the provider).  Additionally, this is one of those situations where a method should only be called while going through the application layer: we cannot call the ""fill"" method with random data, because the list widget need to contain domain entities that have gone through the application logic. However, since this method is public (whether it is meant to be just an event handler or not), nothing prevents some other object, or widget, from directly calling this method passing in somer random data, so we are left to the discipline of the implementer to remember to never call this method directly, but only as a handler of the proper application event. This risk is however mitigated by the fact that we're anyway following the discipline of letting the controller be known only by those few objects that really need to call it directly, by virtue of dependency injection: for example, the main application widget will likely need to use the entity list widget to call ""load"" or ""show"" on it at startup, but this is likely the only object that will have a reference to it.  Regarding widgets that display data, each widget is a specific view, or representation, of the data: this means that we can have different widgets (even at the same time) representing the same data, and thus using the same presenter. This means that presenters and controllers are not really part of the widgets themselves, but are actually distinct objects, that are used by one or more widgets. For example, we may have multiple different widgets dealing with weather data: one would display a graph of temperatures over time, another will display a geographical map with different colors for different temperatures, etc. All these widgets will need to use the same presenter, which converts the entities data coming from the ""show temperatures"" use case, to simple values: then, each different view will use the same values differently. So, when the graph widget receives the signal that temperatures have been updated, it uses the shared ""show temperatures"" controller to trigger the use case, which will have the presenter injected: now this presenter must be able to communicate with multiple different views at the same time, and the actual views may change during the execution, because we may hide some widget for example. So, the view interface used by the presenter won't be an actual object interface, but just a messaging protocol, and the interactor will be injected with a message bus, and calling the interface would mean sending a ""present"" message, to which any number of actual views can respond.  A similar situation happens when the same controller is called by multiple event handlers, or methods, of the same widget, like ""load"" and ""update"" on the entities list: both will have to go through the ""list entities"" controller action, delegating to the same use case interactor.  The application has two ways to communicate with the widgets. The most direct one is just calling the presenters that have been injected. However, the application can also send messages, to which widgets may respond. However, since calling a presenter may also be done by sending messages (in case we need to support concurrent presenters), there's not much difference between the two from a technical perspective. The real difference lays in the fact that presentation methods are meant to communicate to the adapter that the main output has been produced, while any other message is meant to communicate side events that happened, and to which someone may want to respond. If we're injecting a presenter object, it's important that the main output is communicated through that object, and not through sending messages, to keep the intent of the communication clear. If we're using a message bus to communicate with presenters too, it's important that messages are properly named, to make it clear which are sending back the main output.  ### Examples of presentation situations  A common situation is the one where we create a new entity, from a view that doesn't need to be updated to display the updated entity data:  Use case, ""create order"": - given I'm in the checkout view - and the cart has valid information - when I request to create a new order - then a new order is created with the cart information - and I am notified of the successful order creation  Use case ""error creating order"": - given I'm in the checkout view - and the cart has invalid information - when I request to create a new order - then no new order is created - and I am notified of the error with the cart information  Let's say we have a GUI adapter, with a checkout window (view) and an orders window (view). The checkout window will have a ""create order"" control with a method that takes the data from the ""cart"" widget, crafts a use case request from it, and uses it to call the use case interactor, which will have been already injected with a ""message box"" widget as both error presenter and result presenter. When the interactor is called, it first does validation of the request data, immediately calling the error presenter if the validation failed, or using the command model to request the creation of a new order otherwise. The command model will be executed asynchronously, to avoid blocking the user interface: this will create the order, update the storage, and send a message when the operation is completed. This message is caught by the interactor again, which uses the result presenter (asynchronously here) to notify that the order has been succcessfully created. At this point, the user can use the ""list orders"" use case to open the orders window, whose interactor will just read the data from the denormalized storage and present it.  Let's see now a case where we create a new entity in the same view where all entities are listed:  Use case, ""create todo item"": - given I'm in the todo list view - and the item form has valid data - when I request to create a new todo item - then a new todo item is created with the given information - and the todo list is updated to show the new todo item as well  Use case, ""error creating todo item"": - given I'm in the todo list view - and the item form has invalid data - when I request to create a new todo item - then no view item is created - and I am notified of the error with the item information  Here we have a single window, which is the todo list view, with the following widgets: an ""item"" form, a ""create new item"" button, a todo list, an error box, and a loader widget. When the user loads the window, the ""list items"" use case is performed, meaning that an input is sent to the ""load"" action of the todo list, which will create a request for the ""list items"" interactor, which will be injected with the list presenter, and will use the query model to get all the currently stored items from the denormalized database, which will be used to create a response, which will be sent to the todo list presenter to be displayed in the widget. Then, when the user clicks the ""create new item"" button, its controller will read the data from the form, craft a use case request with it, and send it to the ""create item"" interactor, which will be pre-injected with the error presenter, and the result presenter: if the request data is invalid, the error presenter will be sent a response containing the validation errors, and the interactor will return, otherwise the interactor will asynchronously call the command model to create a new todo item, then call the result presenter to signal that the request has been accepted (this could perhaps trigger a loading animation to start), and finally call the result presenter again to signal that the request has been completed, from inside the event handler of the command model completion event. In the meanwhile, the todo list controller was already register to respond with the ""update"" action to the message that is sent by the command model after the creation of an item is completed, so as soon as the command model signals the termination of the job, the todo list controller is called, triggering the ""list items"" use case again to update the todo items in the list with the current values, containing also the new one. The same form widget can have a controller registered to respond to the creation started message: this way if the request fails because of validation issues, the information present in the form fields is maintained to allow the user to fix it without re-typing everything, but if instead the validation passes, the form can clean its fields to allow a new item to be added next. All of these are purely GUI concerns, and that's why they aren't handled by the interactor, which must be concerned only with what is explicitly mentioned in the use case scenarios.  A less common case is the one where input data comes from a source that doesn't support any output, or is not interested in getting any. In actuality, this should be described by the application layer, because we can't use an adapter that doesn't support an output, with a port that provides one, because the role of the adapter is to support the use cases defined by the port. Thus, we can think of a use case that doesn't produce any outout.  Use case, ""signal activates actuator"": - when the sensor sends a new signal - then the actuator is activated  Here the interactor receives an input, in the form of a signal, but then produces no output, because activating the actuator is a secondary concern, adn the sensor doesn't support any output, since it can't even receive data back from its adapter.   ### Sloth machine views  Being a command-line application, the Sloth machine uses the process as the input device, and the process itself, in addition perhaps to the filesystem, as output devices. In particular, we want to use the exit status and the STDOUT and STDERR as main output devices. It's important, though, to highlight that these same devices can be used also to render output coming from the program that is being executed by the virtual machine, in addition to the virtual machine itself, and they can also be used to display side output, for example information messages in a verbose configuration. For this reason, we should constantly keep in mind where some output is coming from, and not only on which device it's displayed.  The first thing to do is clearly understand what's the output that the use case is supposed to provide: in our case the output comprises two elements: - the exit status of the program execution - the error message that might possibly have happened  It's important to underline that we want errors to be part of the output, because they're still significant for the application. Including errors in the response means that they can be handled by a presenter, and the views attached to it. Alternatively, we could have chosen to send the errors to some kind of side output, for example events, but in that case the decision of whether to display these errors or not would have been taken by the adapter, and not by the application. By sending errors with the response, we are clearly stating that we want errors to be displayed, as an application rule.  Thus, our application will need to handle two output boundaries: `ExitStatusPresenter`, receiving an `ExitStatusResponse`, and `ErrorPresenter`, receiving an `ErrorResponse`. For both of them, concrete instances will need to be injected into the interactor. From the adapter point of view, we should decide which output models we want to support. For instance, we can think of a ""console output"", which is meant to be used when the application should be used as a standard console application, properly using the exit status, STDOUT and STDERR; additionally, we can think of a ""textual output"", which is meant to be used when all data produced by the application should be available as human-friendly textual information. These output models define two distinct sets of presenters: for the console output, we'll have a `ConsoleExitStatusPresenter` and a `ConsoleErrorPresenter`, while for the textual output we'll have a `TextualExitStatusPresenter` and a `TextualConsoleErrorPresenter`: - the `ConsoleExitStatusPresenter` converts the `ExitStatus` object of the response into an integer included between `0` and `255` (using a default value if needed), because it needs to be sent to a console application exit status, abiding by POSIX standard, and produces a `ConsoleExitStatusViewModel`; - the `TextualExitStatusPresenter` converts the `ExitStatus` object of the response into a string, avoiding changing its semantic at all, and produces a `TextualExitStatusViewModel`; - both the `ConsoleErrorPresenter` and the `TextualErrorPresenter` will just convert the `Error` object of the response into a string, producing a `ConsoleErrorViewModel` or a `TextualErrorViewModel`;  Each output model can then be rendered into different widgets (views): these will be organized into output configurations, that can be selected by the user, for example through command-line arguments.  The *integrated configuration* is used to run programs on the virtual machine, as if they were normal executables; this means that the program's exit status is returned as the application's own exit status, and the error message is printed to STDERR:  - the `ExitStatusWidget` is used to return a number as the exit status of the console application; - the `StderrWidget` is used to print a message to the STDERR of the console; - the `IntegratedExitStatusView` is used to render the `ConsoleExitStatusViewModel` to the `ExitStatusWidget`; - the `IntegratedErrorView` is used to render the `ConsoleErrorViewModel` to the `StderrWidget`;  The *clean configuration* is used to run programs on the virtual machine, but hiding the error message, so to keep the console clean; the exit status is still returned as the application's own exit status, but now the error message is written to a file; a possible use case of this is when the actual program is writing stuff to STDERR, and we don't want it to mix with errors written by the application (instead of the program): - the `ExitStatusWidget` is used to return a number as the exit status of the console application; - the `OutputFileWidget` is used to write messages to an output file, instead of to STDOUT or STDERR; - the `CleanExitStatusView` is used to render the `ConsoleExitStatusViewModel` to the `ExitStatusWidget` (here we could share a single `ConsoleExitStatusView` with the previous case instead perhaps); - the `CleanErrorView` is used to render the `ConsoleErrorViewModel` to the `OutputFileWidget`;  The *verbose configuration* is used to gather all information on a centralized place, immediately visible; both the exit status and the error message are printed to STDOUT: - the `StdoutWidget` is used to print messages to the STDOUT of the console; - the `VerboseExitStatusView` is used to render the `TextualExitStatusViewModel` to the `StdoutWidget`; - the `VerboseErrorView` is used to render the `TextualErrorViewModel` to the `StdoutWidget`;  The *archived configuration* is used to gather all information on a centralized place, but without clogging the console (notice that STDOUT and STDERR may still contain what the actual program is writing to them); both exit status and error message are printed to a file: - the `OutputFileWidget` is used to write messages to an output file, instead of to STDOUT or STDERR; - the `ArchivedExitStatusView` is used to render the `TextualExitStatusViewModel` to the `OutputFileWidget`; - the `ArchivedErrorView` is used to render the `TextualErrorViewModel` to the `OutputFileWidget`;  Here's what some pseudo-code would look like: ``` ConsoleExitStatusPresenter     ConsoleExitStatusPresenter(ConsoleExitStatusView view)     present(ExitStatusResponse response)         exitStatus = normalizeExitStatus(response.getExitStatus())         viewModel = new ConsoleExitStatusViewModel(exitStatus)         view.render(viewModel)  ConsoleErrorPresenter     ConsoleErrorPresenter(ConsoleErrorView view)     present(ErrorResponse response)         error = response.getError().getMessage()         viewModel = new ConsoleErrorViewModel(error)         view.render(viewModel)  IntegratedExitStatusView: ConsoleExitStatusView     IntegratedExitStatusView(ExitStatusWidget widget)         render(ConsoleExitStatusViewModel viewModel)             widget.setExitStatus(viewModel.getExitStatus())  IntegratedErrorView: ConsoleErrorView     IntegratedErrorView(StderrWidget widget)         render(ConsoleErrorViewModel viewModel)             widget.setError(viewModel.getError())  ConsoleUi     ConsoleUi(Container container)         this.console = container.make(Console)         this.exitStatus = new ExitStatusWidget()         this.stderr = new StderrWidget()         container.bind(ExitStatusWidget, exitStatus)         container.bind(StderrWidget, stderr)     getExitStatus(): ExitStatusWidget         return exitStatus     getStderr(): StderrWidget         return stderr     render()         console.writeError(stderr.getError())         console.exit(exitStatus.getExitStatus())  ConsoleUi ui = new ConsoleUi(container); // send input, render views... ui.render() ```  Here we encapsulate all UI related code into a single class, which binds specific instances of widgets to their classes, so when views are built by the container, they get the right widgets. This specific case is particularly interesting because we have to deal with the problem that when we call `console.exit()`, the application is terminated, so any other UI code that might need to run after that won't be able to. To avoid this, instead of directly call console methods inside views, we let the views populate the widgets, and then render the UI as the last thing. Had we allowed views to directly call the console, the order with which the various presenters were called inside the interactor would have been important for user interface concerns, because if the interactor called the exit status presenter before the error one (or a generic message one, since if there are errors, no exit status is produced in this specific case), then the application would have been terminated before the interactor even finished its execution, and this is of course unacceptable. Having had a graphical interactive UI, instead, we could've displayed the exit status in a widget as soon as it was produced, because it wouldn't have caused the application to terminate.    ## Status  Proposed   ## Consequences  Everything regarding input and output for adapters is following a generic and reusable standard.  Different output representations and output devices are supported.  Input is actually completely decoupled from output. ",governance_and_process
ADR_221,https://github.com/CSCfi/rems.git,docs/architecture/013-event-design.md,013: Command & event design,"# 013: Command & event design  Authors: @opqdonut @Macroz  This ADR outlines some discussions about event design that were had while implementing [#2040 Reviewer/decider invitation](https://github.com/CSCfi/rems/issues/2040).  The point of the feature is being able to invite reviewers and deciders by email. This is similar to the existing feature of inviting application members by email.  ## Existing commands & events  - Command: invite-member   - Produces event: member-invited - Command: accept-invitation   - Produces: member-joined - Command: request-review   - Produces: review-requested - Command: request-decision   - Produces: decision-requested  ### Design 1  First implementation. Trying to share decider and reviewer invitation logic using a generic invite-actor command. Reuses the existing accept-invitation command to reuse some frontend & backend routes. Commit 2049a3651 in [PR #2415][2415].  - Command: invite-actor with `:role :decider` or `:role :reviewer`   - Produces: actor-invited - Command: accept-invitation   - Produces: actor-joined with `:role :decider` or `:role :reviewer`   - (Or member-joined event if the invitation was for a member)  Problems:  - Naming is hard: actor is an already overloaded term (means the author of an event) - Actor-joined event needed to duplicate behaviour of existing review-requested event  [2415]: https://github.com/CSCfi/rems/pull/2415  ### Design 2  Trying to reuse even more code by reusing the review-requested event. Commit 33c3c79 in [PR #2415][2415].  - Command: invite-actor with `:role :decider` or `:role :reviewer`   - Produces: actor-invited - Command: accept-invitation   - Produces: actor-joined with `:role :decider` or `:role :reviewer`   - Produces: review-requested OR decision-requested  Benefits:  - Nice split of responsibilities between events   - actor-joined marks invitation as used   - existing review-requested event used for granting reviewer rights  Problems:  - After accepting the invitation, the review-requested event triggers a new, potentially misleading notification email to the reviewer - Event log is also a bit confusing:   1. ""Alice invited Bob to review""   2. ""Bob accepted the invitation to review""   3. ""Alice requested a review from Bob""  Possible solutions:  - Reword log message to be more passive, ""Bob is now a reviewer"" - Add some sort of `:notify false` option to the review-requested event to prevent emails (and possibly hide from event list) - Go back to design 1  ### Design 3  After discussions ended up with less code sharing but more explicit API & event structure. Commit 253b66d2e in [PR #2415][2415].  - Command: invite-reviewer   - Produces: reviewer-invited - Command: invite-decider   - Produces: decider-invited - Command: accept-invitation    - Produces: reviewer-joined OR decider-joined  Benefits:  - More consistent API: request-review, invite-reviewer; request-decision, invite-decider - Not reusing {request,decision}-requested events avoids pitfalls in event log & emails - Reusing the accept-invitation command makes sense because we don't want multiple /accept-invitation URLs  Problems:  - Some code duplication - Bots & external consumers interested in review requests now need to listen to two events instead of one  ## Discussion  Since REMS commands are part of our public API, it makes sense to keep commands large (that is, one command does a lot of things). This way the user's intent can usually be represented with one command, keeping the frontend simple. Also, since one command means one database transaction (see [ADR 010: Database transactions](010-transactions.md)), issuing a single command is safer than issuing multiple commands. API usage is also nicer when you can often just post a single command.  However, since commands and events are decoupled, we could have these commands produce multiple small events. So far REMS has favoured most commands producing just one nongeneric and large event (that is, an event that has lots of effects). Also commands haven't reused events (for example review-requested and decision-requested are separate events). This way the events are more explicit and mirror the user's intent just like our commands.  Design 1 was an attempt at using smaller, more decoupled events. However that immediately ran into problems with consuming events internally: both the event log & email generation would have needed work.  Perhaps it is best for now to stick to large events so that we can easily react to the users intent in other code, instead of trying to recombine smaller events to reproduce intent (e.g. not sending redundant email reminders)  However, the decision to show the internal event log to users as-is might make our life harder in the future. We might need to re-evaluate this later. ",architectural_patterns
ADR_222,https://github.com/alphagov/content-data-api.git,docs/arch/adr-005-split-audit-tool-cpm.md,ADR 005: Split Audit Tool and Content Performance Manager,"# ADR 005: Split Audit Tool and Content Performance Manager  18-01-2018  ## Context  Content Performance Manager and Content Audit Tool have lived on the same repo for 10 months as we didn't have a very clear picture of the process that was driving both tools, and the underlying user needs.  We decided to keep them together because: 1. As per user research, both tools were addressing similar needs the content publishing workflow 2. It is easier to split both apps once you know that they are different, than to join them later on.  In the last quarter, we noticed that both tools are addressing very different user needs, hence we need to split them.  ## Decision  [Split Content Audit Tool and Content Performance Manager][1]  ## Status  Accepted.   [1]: https://trello.com/c/l7fn1C1P/20-2-split-term-generation-tool     ","technology_choice, governance_and_process"
ADR_223,https://github.com/nhsuk/mongodb-updater.git,doc/adr/0003-host-two-mongodb-updaters-in-stack.md,3. Host two mongodb-updaters in a Single Stack,"# 3. Host two mongodb-updaters in a Single Stack  Date: 2017-06-28  ## Status  Accepted  ## Context  Two different MongoDBs need to be updated on a daily basis.  ## Decision  Given the small number of databases to be updated both services will be hosted in the same stack, rather than manually create a stack for each database updater.  ## Consequences  The repository will hold a docker-compose file to define a stack containing the Pharmacy and the GP database updaters.  Both mongodb-updaters will be automatically deployed using the existing infrastructure as a single unit. ",data_persistence
ADR_224,https://github.com/cloudfoundry/cf-k8s-networking.git,doc/architecture-decisions/0015-app-access-logs-from-ingressgateway.md,15. App Access Logs from IngressGateway,"# 15. App Access Logs from IngressGateway  Date: 2020-07-06  ## Status  Accepted  ## Context  `cf logs` for an app shows logs both emitted by that app, and access logs from the Gorouter. The access logs from Gorouter look like this in the logstream:  ``` 2020-06-25T23:42:19.00+0000 [<source_type>/<instance_id>] OUT <log> ```  In cf-for-k8s, an app developer should similarly be able to see access logs as requests for the app travel through the routing tier. One discussion we had was whether these access logs should come from the ingressgateway envoy and/or from the app sidecar envoys.   One important piece of functionality in cf-for-BOSH is that when an app process has been killed, healthcheck requests still show up in the access log stream. This is because those requests still make it to the Gorouter, even though they do not make it to the app itself.  This is an example of an access log of a healthcheck request to a killed app. The 503 is being returned directly from the Gorouter. ``` 2020-07-06T10:45:55.83-0700 [RTR/0] OUT dora.maximumpurple.cf-app.com - [2020-07-06T17:45:55.828757970Z] ""GET /health HTTP/1.1"" 503 0 24 ""-"" ""curl/7.54.0"" ""35.191.2.88:63168"" ""10.0.1.11:61002"" x_forwarded_for:""76.126.189.35, 34.102.206.8, 35.191.2.88"" x_forwarded_proto:""http"" vcap_request_id:""0cd79f32-3cde-4eea-5853-9a2ca401be40"" response_time:0.004478 gorouter_time:0.000433 app_id:""1e196708-3b2d-4edc-b5b8-bf6b1119d802"" app_index:""0"" x_b3_traceid:""7f470cc2fcf44cc6"" x_b3_spanid:""7f470cc2fcf44cc6"" x_b3_parentspanid:""-"" b3:""7f470cc2fcf44cc6-7f470cc2fcf44cc6"" ```  We've done some previous work exploring access logs on the Istio Envoy IngressGateway (see related stories section below) and have [documented some of the fields here](https://github.com/cloudfoundry/cf-k8s-networking/blob/37dabf7907ffa7b284980cfcb6813ebcd449736c/doc/access-logs.md).  ## Decision  We decided to have the access logs come from the ingressgateway to begin with, as we think those provide the most valuable information.  Imagine a scenario where an app has crashed and the Pod is being rescheduled. The Envoy on the ingressgateway will still log this failed request. The sidecar, on the other hand, would be unreachable so it would not be able to log anything. Having the failed request in the access logs in this scenario could be valuable information for a developer attempting to debug their app with `cf logs`.  We also decided that the access log format would be JSON with the [following fields](https://docs.google.com/spreadsheets/d/1CuvoUEkiizVKvSZ2IaLya40sgMbm5at78CqxB8uUe80/edit#gid=0)  The work to enable this was completed in [#173568724](https://www.pivotaltracker.com/story/show/173568724).  ## Consequences  - In order to enable this, we added fluent-bit sidecars to our ingressgateways.   Information on why we decided to add our own fluent-bit images can be found in   this [draft PR](https://github.com/cloudfoundry/cf-k8s-networking/pull/57).   The final iteration of this was merged in from [this   PR](https://github.com/cloudfoundry/cf-k8s-networking/pull/63) - Will need to do [some extra work](https://www.pivotaltracker.com/story/show/172732552) to get logs from the ingressgateway pods into the log stream corresponding to the destination app. See https://github.com/cloudfoundry/cf-k8s-logging/tree/main/examples/forwarder-fluent-bit for more information. - It is unclear how difficult it would be to add custom formatting to sidecar Envoy logs, we [know how to do it for the ingressgateway logs](https://www.pivotaltracker.com/story/show/169739120) - We may need to revisit the sidecar logs later if we want access logs for container-to-container (c2c) networking (this doesn't exist for c2c in CF for BOSH today)  ---  ## Related Stories For additional context, here are some stories our team has worked on in the past:  - [Emit JSON ingress gateway access logs](https://www.pivotaltracker.com/story/show/169739120) - [Adding fields into access logs for gorouter parity](https://www.pivotaltracker.com/story/show/169737156) - [Explore emitting ingress gateway access logs with Fluentd](https://www.pivotaltracker.com/story/show/170119094) ",others
ADR_225,https://github.com/johanhaleby/occurrent.git,doc/architecture/decisions/0004-mongodb-datetime-representation.md,4. MongoDB datetime representation,"# 4. MongoDB datetime representation  Date: 2020-07-28  ## Status  Accepted  ## Context  Representing RFC 3339 in MongoDB when using the CloudEvent Jackson SDK _and_ allow for queries is problematic.  This is because MongoDB internally represents a Date with only millisecond resolution (see [here](https://docs.mongodb.com/manual/reference/method/Date/#behavior)):  > Internally, Date objects are stored as a signed 64-bit integer representing the number of milliseconds since the Unix epoch (Jan 1, 1970).  Since the Java CloudEvent SDK uses `OffsetDateTime` we'll lose nanosecond precision and the timezone if converting the `OffsetDateTime`  to `Date`. This is really bad since it goes against the ""understandability"" and ""transparent"" goals of Occurrent. I.e. if you create a CloudEvent with a OffsetDateTime containing nanoseconds in timezone ""Europe/Stockholm"" you would expect to get the same value back on read  which is not possible if we convert it to a date.   ## Decision  We've thus decided to just store the `ZoneDateTime` as an RFC 3339 string in MongoDB. This means that range queries on ""time"" will be  horrendously slow and probably not work as expected (string comparision instead of date comparision). The `EventStoreQueries` API currently supports sorting events by natural order ascending/descending which will be much faster (since it's using the timestamp of the  generated mongodb object id).      Note that sort options `TIME_ASC` and `TIME_DESC` are still retained in the API. The reason for this that we may allow customizations to the serialization mechanism _if_ nanosecond resolution and is not required and timezone is always `UTC` in the future.    ## Alternatives  An alternative worth considering would be to add an additional field in the serialization process. For example retain the ""time"" as RFC 3339 string but add an additional field in MongoDB that stores the `Date` so it can be used for fast ""time"" queries.   I've decided not to do this though for the following reasons:  1. Code simplicity. We would have needed to handle ""time"" queries specially. For example if using ""eq"" we probably would like to compare ""time"" field     and if not using ""eq"" we want to compare if the `Date` field. Combinations such as ""gte"" becomes even more problematic. 1. For time queries to be fast an index would be needed. It would introduce additional complexity for users of the MongoDB EventStore that this index would need    to be created for fast queries. Creating the index automatically would not be a good idea since it might not be required for every user. 1. User may never which to query for ""time""! In these cases storing an extra field is simply unnecessary.    For these reasons we've decided that it's better for the user to simply add a custom extension field him-/herself and create custom queries for this field. The `EventStoreQueries` api event supports querying custom fields right now  (though we could expand the API to allow custom `SortBy` fields instead of hardcoding ""time"" and ""natural"").   ## Consequences  This decision is quite sad since it's still very common represent time in Java application as a `Date`. In these cases it would be perfectly  fine to use the native `ISODate` in MongoDB. If converting a `Date` to a `OffsetDateTime` it would be possible to get the best of both worlds and just store the `Date` in MongoDB and one of the nice benefits of using CloudEvents are lost. ","data_persistence, technology_choice"
ADR_226,https://github.com/aml-org/amf.git,adrs/0003-new-annotation-removal-stage-present-in-all-webapi-pipelines.md,3. New annotation removal stage present in all webapi pipelines,"# 3. New annotation removal stage present in all webapi pipelines  Date: 2020-10-21  ## Status  Accepted  ## Context  When referencing external files there are certain cases where AMF inlines the parsed content without creating a links (this is because the targeted elements are not present in the references of the base unit).  For these cases, when a emitting an unresolved model these references are being emitted inlined.   ## Decision  In order to avoid emitting these references inlined for an unresolved model, we must make use of annotation to save the original reference.  When saving this reference, we must make sure that if the model is resolved this annotation is no longer present so that the emitter does not render references for a flattened model.  This leads to the creation of a new resolution stage that removes specific annotations from the model that must not be present in a resolved base unit.  ## Consequences  References can be emitted when receiving an unresolved model with inlined content.  This stage iterates over the whole model removing specific annotations which can lead to more processing, however performance tests have been made showing that this processing is insignificant.  ",others
ADR_227,https://github.com/alphagov/govuk-infrastructure.git,docs/architecture/decisions/0004-use-aws-load-balancer-controller-for-edge-traffic-services.md,4. Use AWS Load Balancer Controller for edge traffic services,"# 4. Use AWS Load Balancer Controller for edge traffic services  Date: 2021-08-12  ## Status  Accepted  ## Context  We require a method of managing and directing external internet traffic into the cluster. Kubernetes provides [several options for handling inbound traffic](https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0).  We do not want to expose the cluster directly to the internet, and so require an AWS load balancer in front of the cluster. The load balancer must support TLS termination and integration with our DNS provider (AWS Route 53).  Historically Kubernetes has supported provisioning of ALBs and NLBs for `Service` resources of `type=LoadBalancer` via the in-tree (built-in) [AWS cloud provider](https://github.com/kubernetes/cloud-provider-aws), with out-of-tree controllers required for `Ingress` resources. Built-in cloud providers are now [considered deprecated overall, in favour of out-of-tree providers](https://kubernetes.io/blog/2019/04/17/the-future-of-cloud-providers-in-kubernetes/), so an [Ingress Controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/) with support for `Service` resources is required.  The primary and recommended ingress controller for AWS/EKS is the [AWS Load Balancer Controller](https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html), which can provision and manage [ALBs for `Ingress` resources and NLBs for `Service` resources](https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/#aws-load-balancer-controller).  We must also consider how Kubernetes edge services and AWS load balancers will interact with the existing [GOV.UK Router service](https://github.com/alphagov/router), as there is significant overlap in their functionality and responsibilities. This will require further investigation and likely experimentation, and so that end we should ensure that we're able to use both `Ingress` and `Service` Kubernetes resources so that we have the flexibility to support a wide range of use cases in the immediate term - L4 & L7 traffic, name-based routing, HTTP->HTTPS redirection, etc.  ## Decision  Use the [AWS Load Balancer Controller](https://github.com/kubernetes-sigs/aws-load-balancer-controller).  ## Consequences  The AWS Load Balancer Controller supports TLS certificates via AWS Certificate Manager only, so certificates must be managed there (to be covered in a future ADR).  The load balancer controller does not handle DNS for declared ingress hostnames - a solution to this will be covered in a future ADR.  An appropriate ALB/NLB topography (how many LBs routing to where) will need to be established - by default the controller will provision one ALB per `Ingress` resource, which may not be what we want. Ingresses [can be grouped however](https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/ingress/annotations/#ingressgroup).  The load balancer controller supports [AWS WAF and Shield](https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/guide/ingress/annotations/#addons), both of which are currently in use on GOV.UK.  Access control for Ingress rules must be investigated, likely in conjunction with Kubernetes `namespace` usage - if all of GOV.UK is deployed into a single namespace, and multiple users or service accounts have the same level of access to `Ingress` objects, then user or process for component A could modify or destroy ingress rules for component B. ",technology_choice
ADR_228,https://github.com/LBHackney-IT/remultiform.git,docs/adr/0003-use-rollup-to-build-distributables.md,3. Use Rollup to build distributables,"# 3. Use Rollup to build distributables  Date: 2019-10-29  ## Status  Accepted  ## Context  We want to be able to distribute this library to me ingested by TypeScript or plain JavaScript (both commonJS and module) applications.  [Rollup](https://rollupjs.org/guide/en/) is a popular JavaScript bundler with support for TypeScript and simple configuration.  ## Decision  We will build distributables using Rollup.js.  ## Consequences  With Rollup.js we can build type declarations, commonJS, and module code using a single simple command, and some simple config. This makes it easier for us to distribute our code. ",technology_choice
ADR_229,https://github.com/huifenqi/arch.git,decisions/0035-disaster-recovery.md,35. 关于灾难恢复,"# 35. 关于灾难恢复  Date: 2017-06-05  ## Status  Proposed  ## Context  当前我们使用的是华北2可用区A机房（即北京昌平区的一个机房）部署了所有服务，存在以下几个问题，出现概率逐级减少： 1. 服务本身部署在单台机器，单机的故障会导致服务的不可用，这个我们的业务服务频频出现； 2. 所有服务部署于一个机房，机房电力，网络出现故障，将导致服务完全不可用，这个 2016 年中旬我们使用的 Aliyun 机房网络设备出现过一次问题，导致服务停服 1 小时左右（官方），实际对我们的影响在 12 个小时左右； 3. 北京发生各种灾害，殃及所有机房，导致服务不可用。  ### 基础概念  灾难恢复（Disaster recovery，也称灾备），指自然或人为灾害后，重新启用信息系统的数据、硬件及软体设备，恢复正常商业运作的过程。灾难恢复规划是涵盖面更广的业务连续规划的一部分，其核心即对企业或机构的灾难性风险做出评估、防范，特别是对关键性业务数据、流程予以及时记录、备份、保护。  地域，即城市，不同的地域可以做到自然灾害级别的灾备，之间延迟较高  可用区，即机房，不同的可用区可以做到电力和网络设备互相独立，之间有少量延迟  两地三中心，业界目前最可靠的解决方案，即在两个城市共三个机房中部署服务  ### 灾备的两项指标  RTO - Recovery Time Objective，它是指灾难发生后，从 IT 系统宕机导致业务停顿之时开始，到 IT 系统恢复至可以支持各部门运作、恢复运营之时，此两点之间的时间段称为 RTO  RPO - Recovery Point Objective,是指从系统和应用数据而言，要实现能够恢复至可以支持各部门业务运作，系统及生产数据应恢复到怎样的更新程度，这种更新程度可以是上一周的备份数据，也可以是上一次交易的实时数据  ### 两地三中心  ![][image-1]  * 主系统 	* 即当前对外提供服务的机房 	* 首先要做到自身的高可用，所有服务不因单机故障导致服务不可用 	* 无资源浪费，多一些部署和维护成本 * 同城灾备 	* 单个城市多个机房，解决单机房电力或网络故障导致的服务不可用 	* 主备的方式会有一半的资源浪费，双活的方式对服务有一定的延迟 	* 有一定的部署和维护成本 * 异地灾备 	* 即跨城市部署服务，解决自然灾害等引起的服务不可用 	* 由于延迟的原因，这部分资源属于备用服务，仅在发生灾害是激活 	* 平时资源都是浪费，并且有较高的部署和维护成本  ## Decision  1. 同机房的服务高可用（进行中），这个是目前最高优先级； 2. 同城双活（提议中），可以解决大部分我们遇到的机房问题； 3. 异地灾备（暂不考虑），针对支付业务，当涉及合规性时，我们得考虑下； 4. 明确我们各个服务的重要程度，分服务针对性的做高可用及灾备策略。  ## Consequences  Refs:  * 灾难恢复 [https://zh.wikipedia.org/wiki/%E7%81%BE%E9%9A%BE%E6%81%A2%E5%A4%8D][1] * Aliyun 地域和可用区 [https://help.aliyun.com/knowledge\_detail/40654.html][2] * 阿里云华北2区网络故障导致业务中断1小时 [http://www.sohu.com/a/101817812\_401503][3] * 因电缆井被烧，京津宁骨干网中断 [http://www.sohu.com/a/131579749\_465914][4] * 经历不可抗力是一种什么体验 [https://zhuanlan.zhihu.com/p/26855422][5] * 金融云特性 [https://help.aliyun.com/document\_detail/29851.html][6] * 云上场景：众安保险，两地三中心容灾部署实践 [https://yq.aliyun.com/articles/6633][7]  [1]:	https://zh.wikipedia.org/wiki/%E7%81%BE%E9%9A%BE%E6%81%A2%E5%A4%8D [2]:	https://help.aliyun.com/knowledge_detail/40654.html [3]:	http://www.sohu.com/a/101817812_401503 [4]:	http://www.sohu.com/a/131579749_465914 [5]:	https://zhuanlan.zhihu.com/p/26855422 [6]:	https://help.aliyun.com/document_detail/29851.html [7]:	https://yq.aliyun.com/articles/6633  [image-1]:	files/dr.png",infrastructure_and_deployment
ADR_230,https://github.com/LBHackney-IT/remultiform.git,docs/adr/0007-use-dependabot-to-keep-dependencies-up-to-date.md,7. Use Dependabot to keep dependencies up to date,# 7. Use Dependabot to keep dependencies up to date  Date: 2019-10-30  ## Status  Accepted  ## Context  We want to ensure all dependencies stay up to date. Dependabot offers a service where a bot opens PRs on GitHub when new versions of dependencies are released.  ## Decision  We will use Dependabot to monitor dependency updates.  ## Consequences  Using Dependabot means we don't need to manually monitor dependencies for new versions.,technology_choice
ADR_231,https://github.com/linagora/linshare-mobile-flutter-app.git,doc/adr/0009-enhancement-retry-authen-interceptor.md,9. enhancement-retry-authen-interceptor,"# 9. enhancement-retry-authen-interceptor  Date: 2021-05-10  ## Status  Accepted  ## Context  - The current `retry` logic is limited in the number of `retry` for failed requests. For example, at a screen where more than 3 different requests fail, the chance of each request being retried is very little, since they share a variable that counts the number of retry times. - Using a (in memory) counter variable for `retry` is risky and difficult to control  ## Decision  1. Send the `retry` counter into request header for each request 2. Retrieve the `retry` count from request data 3. Validate with the maximum allowed. If `retry` in the limit, increment it then re-send a request network with updated `retry` counter  ``` {     ...     SET retriesCount from request extra map with key RETRY_KEY     IF _isAuthenticationError(dioError, retriesCount) THEN         INCREMENT retriesCount         ADD retriesCount to header with key RETRY_KEY         CALL request again     ENDIF     ... } ```  ## Consequences  - The chance of each request being retried is increased - `Retry` counter is controlled inside the request",others
ADR_232,https://github.com/ammarnajjar/reading-todo.git,adr/001-document-architecture-decisions-in-adrs.md,Document Architecture Decisions in ADRs,# Document Architecture Decisions in ADRs  Date: Thu Aug 15 22:48:16 CEST 2019  ## Milestone  0.1.0  ## Context  For the project it is useful to keep all the architectural decisions documented in one place.  ## Decision  All development related decisions must be documented in ADRs.  ## Consequences  ### Pros:  - Easy and fast to look up all architectural decisions. - Learn to stick to a decision until other proves being better.  ### Cons:  - Write for every time a new decision is made a new markdown file :-D  ## References:  - [wikipedia](https://en.wikipedia.org/wiki/Architectural_decision<Paste>) - [ADR](https://github.com/joelparkerhenderson/architecture_decision_record),governance_and_process
ADR_233,https://github.com/perforce/helix-authentication-service.git,docs/architecture/decisions/0004-use-memory-store.md,Use (only) Memory Store,"# Use (only) Memory Store  * Status: accepted * Deciders: Nathan Fiedler * Date: 2020-08-20  ## Context  This application handles several important pieces of information. First is mapping requests for a login to a particular login action, and second is the login results to the initial request. This allows for the client application to signal that a login will begin, get the login URL that should be used, and then query the results of that login action.  For the sake of reliability, this information could be written to a database on a storage device. However, that adds complexity to the deployment of the application, and most of this data is ephemeral at best, lasting only a few seconds and up to a few minutes. Alternatively, the application could simply use an in-memory data store, which avoids leaking data in the event of a system breach, at the expense of losing data if the application is restarted.  ## Decision  This application will use an **in-memory** store rather than files or a database. The particular Node.js module selected is named `memorystore` and supports time-to-live expiration of cached data, which limits the time and exposure of any sensitive user data.  ## Consequence  The application has been using an in-memory store since the beginning, with no issues.  ## Links  * memorystore [GitHub](https://github.com/roccomuso/memorystore) ",data_persistence
ADR_234,https://github.com/kbremner/read-more-api.git,ReadMoreAPI/doc/adr/0002-use-asp-net-core.md,2. Use ASP.NET Core,"# 2. Use ASP.NET Core  Date: 2017-09-24  ## Status  Accepted  ## Context  With the introduction of .NET Core, we need to decide whether to use ASP.NET with .NET v4.x or ASP.NET Core.  ## Decision  We will use ASP.NET Core.  ## Consequences  The application can on a machine running Linux, MacOS or Windows, rather than being restricted to Windows.  The .NET Core implementation and tooling is still evolving, which may result in certain .NET Core releases requiring changes to the project structure.  Commonly used libraries, such as Entity Framework, have specific versions for use with .NET Core. In some cases the APIs differ from the .NET v4.x versions.  Some libraries have not yet carried out the work necessary to be compatible with .NET Core. It will not be possible to use these libraries until the required work is completed. ",technology_choice
ADR_235,https://github.com/apache/james-project.git,src/adr/0029-Cassandra-mailbox-deletion-cleanup.md,29. Cassandra mailbox deletion cleanup,"# 29. Cassandra mailbox deletion cleanup  Date: 2020-04-12  ## Status  Accepted (lazy consensus) & implemented  ## Context  Cassandra is used within distributed James product to hold messages and mailboxes metadata.  Cassandra holds the following tables:  - mailboxPathV2 + mailbox allowing to retrieve mailboxes informations  - acl + UserMailboxACL hold denormalized information  - messageIdTable & imapUidTable allow to retrieve mailbox context information  - messageV2 table holds message metadata  - attachmentV2 holds attachments for messages  - References to these attachments are contained within the attachmentOwner and attachmentMessageId tables   Currently, the deletion only deletes the first level of metadata. Lower level metadata stay unreachable. The data looks  deleted but references are actually still present.  Concretely:  - Upon mailbox deletion, only mailboxPathV2 & mailbox content is deleted. messageIdTable, imapUidTable, messageV2,   attachmentV2 & attachmentMessageId metadata are left undeleted.  - Upon mailbox deletion, acl + UserMailboxACL are not deleted.  - Upon message deletion, only messageIdTable & imapUidTable content are deleted. messageV2, attachmentV2 &   attachmentMessageId metadata are left undeleted.  This jeopardize efforts to regain disk space and privacy, for example through blobStore garbage collection.  ## Decision  We need to cleanup Cassandra metadata. They can be retrieved from dandling metadata after the delete operation had been  conducted out. We need to delete the lower levels first so that upon failures undeleted metadata can still be reached.  This cleanup is not needed for strict correctness from a MailboxManager point of view thus it could be carried out  asynchronously, via mailbox listeners so that it can be retried.  ## Consequences  Mailbox listener failures lead to eventBus retrying their execution, we need to ensure the result of the deletion to be  idempotent.   ## References   - [JIRA](https://issues.apache.org/jira/browse/JAMES-3148)",governance_and_process
ADR_236,https://github.com/Crown-Commercial-Service/ReportMI-service-manual.git,docs/adr/0013-use-s3-for-storing-files.md,13. Use AWS S3 for storing files,"# 13. Use AWS S3 for storing files  Date: 2018-07-18  ## Status  Accepted  ## Context  The Data Submission Service will need to store various files both during and after the submission process.  1. **Submission files** - the service will be receiving uploaded files from suppliers each month. These files will need to be stored somewhere prior to processing, and will need to be retained for audit purposes for a period of time afterwards. 1. **Finance export files** - the service will be producing a daily set of files to be transferred to the CCS finance system (Coda) to allow it to generate invoices 1. **Data Warehouse export files** - the service will be producing a daily set of files to be transferred to the CCS Data Warehouse to allow the MI team to perform analysis of the data.  In [ADR-0008][adr-0008] we decided to use Amazon Web Services (AWS) for hosting the service. AWS offers object storage for this use-case: AWS [Simple Storage Service][service-s3] (S3).   ## Decision  We will store submission and export files in AWS S3.  ## Consequences  We will configure the S3 buckets using Terraform as outlined in [ADR-0006][adr-0006].  We will need to be careful with the configuration of the S3 buckets to avoid accidental leakage of data. AWS provides useful tools to check that buckets are not publicly accessible.  [adr-0006]: 0006-use-terraform-to-create-and-document-infrastructure.md [adr-0008]: 0008-use-aws-for-hosting.md [service-s3]: https://aws.amazon.com/s3 ","technology_choice, data_persistence"
ADR_237,https://github.com/libero/community.git,doc/adr/0008-libero-infrastructure.md,8. Limit scope of Libero infrastructure,"# 8. Limit scope of Libero infrastructure  Date: 2020-01-10  ## Status  Proposed  ## Context  Libero infrastructure (servers, Kubernetes, buckets) supports Libero development by providing demo environments.  Service providers need documentation to learn to run Libero products.  Service providers cater for the disparate, very specific needs of publishers.  Service providers may consolidate their infrastructure with the rest of the platforms.  ## Decision  Libero infrastructure should serve two purposes:  - provide *demo* environments to showcase Libero products in certain configurations - provide realistic *reference* environments that do not serve real users but can be forked and adapted by service providers to kick start their Libero offering  ## Consequences  Demo environments for Libero products are run on Libero infrastructure.  Operational aspects that can be added to an environment without changing its architecture (e.g. backups, log aggregation) are considered out of scope for Libero infrastructure.  Libero Infrastructure As Code is not directly runnable by a third party.  Libero Helm charts are not directly installable by a third party. ",infrastructure_and_deployment
ADR_238,https://github.com/guttih/island.is-glosur.git,docs/adr/0008-use-oauth-and-openid-connect.md,Use OAuth 2.0 and OpenID Connect as protocols for Authentication and Authorization,"# Use OAuth 2.0 and OpenID Connect as protocols for Authentication and Authorization  - Status: accepted - Date: 2020-06-02  ## Context and Problem Statement  What protocol(s) shall we use as the new standard for authentication and authorization. It would be supported by our new centralized authority server and should be implemented in all new clients and resource systems needing authentication or authorization. A requirement might be made that the authority service need to support other protocols for legacy systems but all new systems should be encourage to use the same protocol.  ## Decision Drivers  - Secure - Well defined and well reviewed standard - Easy to implement by client and resource systems - Support for non web client systems i.e. mobile devices  ## Considered Options  - OAuth 2.0 + OpenID Connect - SAML 2.0  ## Decision Outcome  Chosen option: ""OAuth 2.0 + OpenID Connect"", because it is secure and well examined and and has support libraries for our tech stack.  ## Pros and Cons of the Options  ### OAuth 2.0 + OpenID Connect  - Good, because the authentication protocal is designed specifically to work with the authorization protocol. - Good, because it supports non web clients i.e. native apps. - Good, because it has certified, open source libraries for relying parties for OpenID authentication that match our   tech stack (javascript with typescript defenitions). - Bad, because it could require large tokens for authorization for multiple services, or split up tokens complicating   the process.  ### SAML 2.0  - Good, because it is the currently used standard for legacy systems. - Bad, because it doesn't have good support for non web clients. - Bad, because main focus is on enterprise SSO, not centralized authorization. ","security, technology_choice"
ADR_239,https://github.com/climatescape/climatescape.org.git,backend/doc/decisions/1-use-heroku.md,(sem título),"Decided to implement automatic data scraping backend as a customly built system deployed on Heroku, rather than using a specialized platform for scraping such as Apify, because we think that the backend system will eventually outgrow mere data scraping.  See [this message](https://github.com/climatescape/climatescape.org/issues/40#issuecomment-583264142) and the preceding messages in the thread.",infrastructure_and_deployment
ADR_240,https://github.com/DFE-Digital/teaching-vacancies.git,documentation/adr/0001_get_postcode_from_coordinates.md,Use postcodes.io to get postcode from coordinates,"# Use postcodes.io to get postcode from coordinates Date: 10/10/2019  ## Status approved  ## Context We need to get a postcode from the coordinates we get from the browser.  ## Decision To use postcodes.io instead of geocoder gem and just make a simple AJAX call from the browser.  # Consequences We avoid creating an endpoint on the server, therefore reducing the load we have to manage. On the other side we rely on a service that is less trusted than Google Maps, but open source and based on Open Data. ",technology_choice
ADR_241,https://github.com/adamAllaround/CardsApp.git,src/main/java/com/allaroundjava/architecture/decisionrecord/cardops-module-architecture.md,Choice of application level architecture for cardops module,"#Choice of application level architecture for cardops module  ##Context Need to choose an applicable architectural approach to cardops module so that it can be consistently followed and the module can be developed and extended easily.  ##Drivers  * The module contains business rules * It should be easy to prototype the solution * Time estimated for completion is 2 months * The application should be a prototype - not an enterprise class solution * The database should be quick to set up, best if it's known already * This domain is treated as a core domain * We're pootentially looking into changing the business rules around withdrawing, maybe introducing  new card types  ##Decision This module is best realized with Hexagonal Architecture, aka ports and adapters. Since its the core of  our domain. This architecture should enable easy prototyping and easy testing.    ##Consequences * may be slightly more difficult to understand for newcomers  ##Other options * Standard three layer - harder to test the solution",architectural_patterns
ADR_242,https://github.com/Azure-ukgov-paas/paas-team-manual.git,docs/architecture_decision_records/ADR016-end-to-end-encryption.md,(sem título),"Context =======  In order to ensure the confidentiality of private tenant data processed on the platform we need to ensure that requests and responses for traffic between the user and application instances are encrypted so that it is not possible for a network eavesdropper to access private tenant data.  There are 3 main network sections between the user and the application:  * User to ELB * ELB to router * Router to cells  Decision ========  * The traffic between the user and the ELB is encrypted by using an TLS listener on the ELB. A certificate issued by a certificate authority is set on the ELB and access to the private key is very restricted. * The ELB connects to the router VM via TLS. The router VM must, in consequence, serve TLS. * The router to application instances traffic is plain HTTP because the Cloud Foundry doesn't support TLS between gorouter and the application instances and the application instances may not talk TLS. We've decided to use IPSec on router and cell so the traffic will be encrypted transparently.  Status ======  Accepted  Consequences ============  The traffic is encrypted end-to-end between the user and the applications. ",security
ADR_243,https://github.com/NERC-CEH/datalab.git,architecture/decisions/0026-auth0-for-authentication.md,26. Auth0 for authentication,"# 26. Auth0 for authentication  Date: 2017-11-04  ## Status  Accepted  ## Context  User Authentication is a complex problem, can be time consuming to implement and errors in implementation can lead to security vulnerabilities. We feel that authentication, while critical, is not a differentiating factor and want to offload the work to a managed service.  ## Decision  We have opted to use [Auth0](https://auth0.com/) as our Identify provider. This gives us a quick way to integrate authentication into our application with minimal effort and as an open source project we are able to use the service free of charge.  ## Consequences  We have a high quality identity provider with a rich management interface to get us started with user management workflows and identity.  Auth0 also provides an extensive management API which will allow us to build our own interfaces in the longer term if we choose.  Auth0 provides the capability to federate identity with other identity providers. We may want to allow user to authenticate using their CEDA accounts or Crowd accounts (in the case of CEH) in the longer term. ","security, technology_choice"
ADR_244,https://github.com/ASethi93/james.git,src/adr/0010-enable-elasticsearch-routing.md,10 Enable ElasticSearch routing,"# 10 Enable ElasticSearch routing  Date: 2019-10-17  ## Status  Accepted (lazy consensus)  Additional performance testing is required for adoption.  ## Context  Our queries are mostly bounded to a mailbox or a user. We can easily limit the number of ElasticSearch nodes involved in a given query by grouping the underlying documents on the same node using a routing key.  Without a routing key, each shard needs to execute the query. The coordinator needs also to be waiting for the slowest shard.  Using the routing key unlocks significant throughput enhancement (proportional to the number of shards) and also a possible high percentile latency enhancement.  As most requests are restricted to a single coordination, most search requests will hit a single shard, as opposed to non routed searches which would have hit each shards  (each shard would return the number of searched documents, to be ordered and limited  again in the coordination node). This allows to be more linearly scalable.  ## Decision  Enable ElasticSearch routing.  Messages should be indexed by mailbox.  Quota Ratio should be indexed by user.  ## Consequences  A data reindex is needed.  On a single ElasticSearch node with 5 shards, we noticed latency reduction for mailbox search (2x mean time and 3x 99  percentile reduction)  ## References   - https://www.elastic.co/guide/en/elasticsearch/reference/6.3/mapping-routing-field.html  - [JIRA](https://issues.apache.org/jira/browse/JAMES-2917) ","data_persistence, performance_and_scalability"
ADR_245,https://github.com/patshone-manifesto/hee-web-blueprint.git,pages/blueprint/adrs/0012-use-external-search-service.md,(sem título),"--- layout: layouts/page.njk title: ADR-0012 Use external search service pageTitle: ADR-0012 Use external search service pageDescription:  path: /blueprint permalink: /blueprint/adrs/ADR-0012-use-external-search-service.html eleventyNavigation:   parent: Architecture decisions   key: ADR-0012 Use external search service   order: 12 ---  # 12. Use external search service  Date: 2020-06-08  ## Status  Pending  ## Context  We sought to determine whether the native out of the box solution provided by the platform nativly would be fit for purpose, or if an external service would be required.   For the purpose of the decision, the following were considered as requirements:  * Flexible facet management * Synonym configuration  * Query suggestions * Configurable misspelling tolerance * Natual language query support * Ability to integrate with external sources  ### Options  #### Native Bloomreach Lucene Search  The OOTB search provided by the platform nativly is provided by Apache Lucene. This native search option provides standard search options, including free text search and faceted filters. Common query types are supported, such as wildcard searches.   Whilst the native search functionality is fine for simple content websites, it lacks some of the enhanced functionality and configurability that the final solution will be asked to provide, including synonym configuration.   #### Bloomreach Search & Merchandising (brSM)   Bloomreach offer an enhanced managed search service, called 'Search & Merchandising'  https://www.bloomreach.com/en/products/search-merchandising  This tool provides several additional pieces of functionality including semantic understanding and personalisation. The product however is geared in the first instance to product and merchansing and many of the features are directed towards that usecase.    NHS digital are undertaking some work to prove out the use of brSM for this scenario. This is one of the first POCs using this search tool outside of a comemerce functionality.   #### Algolia  Algolia is a decided search-as-a-service product that provides much of the functionality needed out of the box, including synonym support and filters and facets. It is also highly customisable through the UI, allowing for non-developers to configure, adjust and maintain the search offering.   Algolia also offers prebuilt configurable front end components, which make implementing the search experience quick and easily.  #### Azure Cognative Search  Microsoft offer a cloud search service called Azure Cognative Search. This is a scalable search-as-a-service product, with a focus on machine learning powered capabilities such as optical character regognition   #### Amazon Kendra/ Elasticsearch   Amazon has long provided a well regarded open source search solution called Elasticsearch, which can be run on premises or on an EC2 or managed search instance. Amazon also offered Kendra, which is a machine learning based search service.   ## Decision  We believe that an external search service will be required to provide all of the capabailities that will ultimatly be needed to meet the complex user experience needs. Further to that, using an external search service will better suit the service based architectural model, where the search service will likely need to ultimatly take data from a variety of services such as the LKS document and colberation platform, and in future potentially other services such as Oriel and TIS.   Using a managed service such as Algolia provides a good balance between powerful and user friendly functionality and implementation complexity - Algolia was chosen as the basis for the POC in part owing to its comprehensive service offering combined with its prebuilt react component library offering fast and efficient implementation.   Bloomreach Search and Merchandising is an interesting option that provides advantages being tightly integrated into the core CMS project, however using it outside of commerce is somewhat unproven in the market.   Search as a service options such as Elastic or Azure Cognative search would also be viable candidates (assuming the organisational goal of aligning more functionality to MS's offerings, Azure would likely be recommended ahead of Elasticsearch), and the cost models of these offer likely cost savings, the trade off is more complexity to implement and maintain.    ## Consequences  An integration with an external search provider brings additional complexity to the architectural stack, and requires additional considerations during development, as well as an additional ongoing cost for procuring the service (if a managed service is selected).  ",technology_choice
ADR_246,https://github.com/ministryofjustice/opg-modernising-lpa-docs.git,src/adr/articles/0001-record-architecture-decisions.md,(sem título),"--- layout: layouts/adr.njk title: ""Record architecture decisions"" weight: 1 date: 2021-06-27 review_in: 12 months tags:      - adr     - open_source     - open_data areas_of_coverage: [""Digital Service""] status: ""accepted"" contributors: [""John Nolan""] adr_number: ""0001"" ---  ## Context  We need to record the architectural decisions made on this project.  ## Technical  ### Interoperability - How does this enable the exchange of information  Allows anyone to be able to follow and contribute to ongoing decisions made on the service.  ### Developer Knowledge - How well known is this in our current skill sets  **Overall**: 8/10 Developers are aware of ADRs, but their experience of doing them brings this score down a little.  ### Support/Open Source - Is it well supported  Fully Open Source.  ### Scalability  Uses Markdown and git to achieve scalability.  ## Ethics  ### Mitigate against being tech deterministic  Giving visibility to our decisions and allowing contributions ensures Citizens and colleagues are able to have a voice and be aware of changes over time.  ### Ensure you conduct inclusive research  The design of the pages ensures the site is accessible to the highest standard. This is achieved by using the GDS pattern library.  In the future we should look at including a Welsh version to be more inclusive.  Language within the ADRs will contain technical language so may not be accessible to persons not familiar with the terminology.  ### Think big and imagine what the impact of your work can be  Decisions can be read and commented on inside and outside of government. Encouraging conversation with others exploring these technologies will enable better communities and feed back into our own decisions.  This will allow us to get a wider range of opinions on our decisions which we could not have got before.  ### Interrogate your data decisions  N/A  ### Decision  We will use Architecture Decision Records, as [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions).  ### Consequences  See Michael Nygard's article, linked above.  ADRs will be public for visibility and collaboration.  Pull requests and Github issues can be used to drive conversations. ",governance_and_process
ADR_247,https://github.com/LBHackney-IT/Data-Platform-Playbook.git,docs/architecture-decisions/records/002-ingest-google-sheets-data.md,(sem título),"--- id: ingest-google-sheets-data title: ""Ingest Google Sheets Data - ADR 002"" description: """" tags: [adr] number: ""2"" date-issued: ""2021-03-23"" status: ""Accepted"" ---  ## Context  Hackney currently have datasets distributed over a number of different Google Sheets Documents due their data recovery efforts. We need to get this information pulled into the data platform for processing.  ## Decision  We will use a python based AWS Glue Job in conjunction with the gspread python library to pull the data onto platform  ## Consequences  Having the code sit with AWS Glue makes the data import easier to keep consistent with the rest of the platform but leaves us dependent on being able to import the gspread library into the AWS Glue job and requires that we access credentials from Google Workspace ",technology_choice
ADR_248,https://github.com/chef/automate.git,dev-docs/adr/adr-2018-10-04.md,ADR 2018-10-04: Names Not UUIDs for IAM API,"# ADR 2018-10-04: Names Not UUIDs for IAM API  ## Status  * ACCEPTED (2018-10-24) * PROPOSED (2018-10-04)  ## Context  In our current IAM implementation, some services identify entities using human-readable names and other services identify entities using UUIDs (which we'll refer to as IDs in the following discussion).   Using IDs makes ensuring entity uniqueness easier. With UUIDs we can allow independent and concurrent creation of identifiers across multiple instances of a given service. IDs have performance advantages for use in data stores since comparisons are faster and indices are smaller (in general) than those that use a natural key.  APIs that can be accessed using human-readable names tend to provide a more self-documenting protocol whereas those that deal only in IDs result in message payloads that require additional tooling to map into a human readable form. A named-based approach requires verifying uniqueness within a data store to maintain data consistency.  When users are the one to select a name, they often want the ability to change their mind and change the name. Rename functionality improves usability by letting users reflect their updated understanding into the system. Renames also introduce implementation challenges as one needs to decide when and how to cascade renames throughout the system.  When we consider building aggregate functionality combinging the capabilities of multiple services, we want to be able to share an entity identifier across the bounded context in which the entity resides. Once this occurs, changing the identifier (rename) becomes more complicated and, definitionally, will be outside of a transaction. Strategies to handle this distributed cascade problem include event driven architecture where such updates are published to the event bus and providing long-lived aliases such that previous identifiers continue to work (e.g. github repo rename behavior).  Looking across the A2 API, today most messages use an `id` field as the name for the entity's unique identifier. This does not align with Google's API design guidance (https://cloud.google.com/apis/design/resource_names) the requires the field to be called `name`. The proposed decision is to make use of `name` following Google's API design guide for new APIs and to commit to evolving existing APIs to support `name` as well. The alternative is to agree that our identifiers are called `id` regardless of their form. Either way, a goal of this decision is to define a standard that will result in increased consistency across the A2 API.  ## Decision  For IAMv2 we will use human-readable resource identifiers for the user visible protocol. This means that policies, roles, scopes, teams, and users will all be identified using unique ""names"" in the public GRPC API and resulting HTTP/JSON API that are intended to be human readable ""friendly"" names.  We will disallow renames to an entity's identifying name so that the names can be shared outside of the IAMv2 system.  We will support a ""display name"" field for entities in IAMv2 that users can use to describe the item and that we can use for UI display. The display name can be renamed. The UX is undecided, but we can consider flows where resource name is derrived from display name or vice versa to reduce the number of inputs a user needs to provide when creating things. The GCP projects API serves as a good example. In general, uniqueness should be enforced for `display_name` since the intention is to provide a human readable and changable label for distinct items.  In the IAMv2 data store (postgresql), we will use UUIDs and the core IAM services will handle UUID/name mapping and uniqueness constraint enforcement.  We will use the `id` field to store the unique, unchanging identifier in our GRPC protocol message definitions. We will use `display_name` for the can-be-edited name. For now, we are deciding not to adopt `name` as the field name for an entity's unique identifier as suggested in Google's [design guidance](https://cloud.google.com/apis/design/resource_names). While we'd like to follow those guidelines, the change would cause more disruption than benefit at this time. We can reconsider in future.  When using names instead of UUIDs, special consideraton needs to be given to handling delete operations for entities that may be referenced outside of their originating bounded context. The system cannot rely on cascading delete behavior when a given identifier is stored across different bounded contexts in the system. In these situations, we have to consider what happens when an item is deleted and then a new item is created with the same id. If the system allows this re-use of id after delete, then it must ensure that all references are removed or invalidated. Alternatively, the system can implement a delete marker (aka ""tombstone"") approach and disallow deleted identifiers from ever being reused.  ## Consequences  Once this decision is implemented, we will assess the resulting context by 2019-03-10.  ",api_and_contracts
ADR_249,https://github.com/navikt/laundromat.git,docs/adr/0003-choice-of-entities.md,3. Choice of entities for the Named Entity Recognizer,"# 3. Choice of entities for the Named Entity Recognizer  Date: 08-07-2020  ## Status  Accepted  ## Context The choice of entities, i.e. what categories of information we want to anonymise and the specificity of these categories, will greatly impact the performance of our model. Additionally, time constraints mean that there is an upper limit to how many entities can be included. As such, we concluded that those entities which fulfill at least two of the following three categories should be included:  * It is either directly identifying or a close proxy (e.g. names, phone numbers, etc.)  * It is a so-called “special categories” of information (e.g. medical information)  * It is present in the data in non-trivial quantities   ## Decision We have chosen the following NER entities:  * ORG (Organisation)  * LOC (Location)  * PER (Person)  * FNR (Personal number)  * MONEY  * DATE_TIME (Dates, time of day, name of day, and name of month)  * MEDICAL_CONDITIONS    Entities that will be left purely to RegEx are:  * NAV_YTELSE and NAV_OFFICE  * AGE  * TLF (Telephone number)  * BACC (Bank account number)  We believe this list strikes the right balance between performance (fewer entities are better) and coverage.  ## Consequences  Since data will be labeled with these entities, changing this list will require substantial resources and possible relabeling of data. Increasing granularity especially will be difficult to achieve if desired. These will most likely be the entities used going forward.  ",others
ADR_250,https://github.com/button-inc/service-development-toolkit.git,docs/decisions/7_directory-structure.md,Design Decision 7 - Directory Structure,"# Design Decision 7 - Directory Structure  ## Background  Decision for how to organize compiled npm package.  ## Findings  We considered module-support (See decision 6), bundler tools, and general structure. 3 major react libraries were evaluated for comparison:  - [react-bootstrap](https://www.npmjs.com/package/react-bootstrap) - [semantic-ui react](https://github.com/Semantic-Org/Semantic-UI-React) - [material-ui](https://github.com/mui-org/material-ui)  | React Bootstrap                                                                                                     | Semantic UI                                                                | Material UI                                                                                                                                             | | ------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- | | Supports cjs, ems, and umd. Uses webpack to bundle. Matches our current structure. Builds components in typescript. | Supports cjs, ems, and umd. Structures components into individual folders. | Supports cjs, ems, and umd. Uses rollup as bundler. Organizes components into folders. Builds components in js, and exports type support for typescript |  ## Decision  After evaluating existing structures and options, decided to use a structure similar to react bootstrap, with directories organized as:  ### Component-library  ``` └───src │   └───Component1 │       │   index.ts │       │   component1.ts │       │   ... │   └───Component2 │       │   index.ts │       │   component2.ts │       │   ... ```  And structure the compiled npm module as:  ### Output package  ``` └───Component-library |   └───lib │       └───Component1 │           │   index.ts │           │   component.ts │           │   package.json │           │   ... │       └───Component2 │           │   index.ts │           │   component.ts │           │   package.json │           │   ... |       ... │       └───esm |           |   Component1.js |           |   Component1.d.ts |           |   Component2.js |           |   Component2.d.ts |           |   ... │       └───cjs |           |   Component1.js |           |   Component1.d.ts |           |   Component2.js |           |   Component2.d.ts |           |   ... │       └───dist ```  Example component level package.json  ``` {   ""name"": ""component-library/Button"",   ""private"": true,   ""main"": ""../cjs/Button.js"",   ""module"": ""../esm/Button.js"",   ""types"": ""../esm/Button.d.ts"" } ```  This will meet the requirement for decision 6 to support esm, umd, and cjs support, and also support importing individual components. ",governance_and_process
ADR_251,https://github.com/ammarnajjar/reading-todo.git,adr/003-postgres-as-database.md,PostgreSQL as the RDBMS,"# PostgreSQL as the RDBMS  Date: Thu Sep 19 21:42:04 CEST 2019  ## Milestone  0.1.0  ## Context  For the project a RDBMS need to be chosen and adopted.  ## Decision  - [PostrgreSQL](https://www.postgresql.org/) will be the relational database management system of choice.  ## Consequences  Why:  - Open source. - Free. - Available for many operating systems. - Its SQL implementation closely follows ANSI standards. - Widely used and well documented, so finding help is no issue. - Supported by many platforms for deployment (Amazon, Azure). - Official docker support. - For my use case, a very simple database, this looks so appealing, fast and easy to use.  Why not:  - Not the bleeding edge technology. - Not the fastest DBMS - Not so much adopted in the commercial world.  ## References (optional)  - [Official Website](https://www.postgresql.org/) - [SQLite vs MySQL vs PostgreSQL](https://www.digitalocean.com/community/tutorials/sqlite-vs-mysql-vs-postgresql-a-comparison-of-relational-database-management-systems) - [Wikipedia](https://en.wikipedia.org/wiki/PostgreSQL) ","technology_choice, data_persistence"
ADR_252,https://github.com/joejag/wikiindex.git,doc/arch/adr-006-config.md,Config,"# Config  ## Context  * We need to set the port and other config values for our application * We do not know the deployment environment yet  ## Decisions  * We will use environment variables to configure the app, rather than a file  ## Alternatives Considered  * A YAML file would allow for structured config","technology_choice, security"
ADR_253,https://github.com/edinella/micro.git,docs/adr/0002-have-a-single-repository-for-all-microservices.md,Have a single repository for all microservices,"# Have a single repository for all microservices  ## Context and Problem Statement  A repository for each microservice or only one for all of them?  ## Decision Drivers  * Development agility * Microservices decoupling * Big-picture scenario visibility  ## Considered Options  * A repository for each microservice, and another for service definitions * A single repository with decoupled microservices including service definitions  ## Decision Outcome  Chosen option: A single repository for all microservices and service definitions this time, because this way its easy to get a big picture of the approach, for the purposes of this example.  In production, I'll consider separating microservices and service definitions on repos, in order to have separate triggers on CI for them.  I think [this approach](https://medium.com/namely-labs/how-we-build-grpc-services-at-namely-52a3ae9e7c35) will be a good fit for address this concern in production. ",microservices_and_modularity
ADR_254,https://github.com/zooniverse/front-end-monorepo.git,docs/arch/adr-15.md,ADR 15: Drawing tools API,"# ADR 15: Drawing tools API  Created: June 19, 2019 Updated: October 31, 2019  ## Context  The way the drawing tools currently function on Panoptes-Front-End (PFE) have numerous issues including:  - Updating the classification annotation on each touch or pointer event which causes unnecessary re-rendering of the DOM - The separation concerns are not clear between components and stores. Multiple components can update the annotation making it hard to debug or add new features to.    - Example: The `MarkingsRenderer` and the `FrameAnnotator` both call change handlers that update the classification annotation? Can the drawing annotation be updated by both or is one solely responsible? It is unclear by reading the code. Why does something named `MarkingsRenderer` update the annotation? - Drawing tools have a complex API that involves exposing static methods to be called by their parent component - Annotation / classification payloads have no consistent standards for describing data: some tools mark rotation in differing directions, for example.  ## Decision  What we do not want to do: - Re-render on every pointer or touch event. - update annotation state while drawing is in progress. - support more than one drawing task in a step. - Use D3.js since it has its own internal data store and it would be complicated to integrate that with a observable stream.  What we do want to do: - Have a component, the interactive layer, that manages the interaction with the marks and pointer and touch events.   - The interactive layer should not allow events to bubble so the events are encapsulated to just the interaction with the subject. This is to help prevent browser scroll during drawing. An attempted fix on PFE for reference: zooniverse/Panoptes-Front-End#5411   - Events will be observed and be streamed via an observable. We will use rx.js to create an observer/observable event stream.   - The last state of the event stream will be used to add an annotation to the classification when next or done is clicked.   - The interactive layer will have local component state to keep track of which mark is selected for editing as well as handle the edit and delete events.  - Have a component, the markings renderer, that manages the rendering of previous annotation marks as read only. It will hide them if hide previous marks is toggled. - These two components will initially use separate SVG layers that connect to the same stores. Later any duplicated code will be refactored into a higher order component. - Have a multi-image subject viewer. We will have to support projects that want each frame to have independent drawing annotations as well as projects that want each frame to have the same drawing annotations. Each frame should have the same pan/zoom function. We have two proposed options in implementation:   -  Create an interactive layer and markings renderer for each frame. Each interactive layer will initialize its own event stream.   - Create a single interactive layer and markings renderer and filter what is rendered by the frame index.   - Projects have requested each frame to have the same pan/zoom function, but we were unable to implement in PFE: zooniverse/Panoptes-Front-End#3465     - Are there any cases where projects want separate pan/zoom function for each frame? - Have a schema, or set of schemas, describing annotations.  ## Status  Superseded by [ADR-22](adr-22.md).  ## Consequences  - This is the first time rx.js will be used in our code and there will be a learning curve. - The current feedback code may need refactoring as it is written only for the D3.js interactive plot subject viewers.  ### rx.js use  We had an early prototype at the start of this proposal using rx.js. This library is an implementation of the proposed observable specification for javascript and has an API for use with browser DOM events. After several months of experimentation, we have decided that we will proceed with implementing the drawing tools just with the browser pointer events API and potentially integrate rx.js at a later date as an enhancement.  In retrospect, trying to incorporate rx.js increased the complexity for implementation and for learning and contributed to delays. For future experiments, we should be sure to structure how we'll go about the experiment including evaluation milestones for its use from the start.  ",technology_choice
ADR_255,https://github.com/Ensembl/gti-genesearch.git,doc/adr/adr-016.md,ADR 015: Single type indices,"# ADR 015: Single type indices  ## TL;DR Switch from one index with multiple types to multiple indices.  ## Context Elastic now recommend that there should only be one document type per index, and this feature will be dropped in future: https://www.elastic.co/guide/en/elasticsearch/reference/current/removal-of-types.html  To this end, we will make this change now to avoid future pain. This is particularly important as we bring in variants and other types into Elastic.  ## Status Under development  ## Consequences more config needed more flexibility  ## Tags ",data_persistence
ADR_256,https://github.com/dxw/support-rota.git,doc/architecture/decisions/0002-use-a-changelog-for-tracking-changes-in-a-release.md,2. Use a changelog for tracking changes in a release,"# 2. Use a changelog for tracking changes in a release  Date: 2019-09-13  ## Status  Accepted  ## Context  Documenting changes for a release can be challenging. It often involves reading back through commit messages and PRs, looking for and classifying changes, which is a time consuming and error prone process.  ## Decision  We will use a changelog (`CHANGELOG.md`) in the [Keep a Changelog 1.0.0](https://keepachangelog.com/en/1.0.0/) format to be updated when code changes happen, rather than at release time.  ## Consequences  This will make compiling releases much simpler, as the process for determining changes in a release is simply a matter of looking at the changelog.  This does add some overhead to making code changes, and requires that releases update the changelog as part of their process, but those overheads are small. ",governance_and_process
ADR_257,https://github.com/zooniverse/galaxy-zoo-touch-table.git,docs/arch/adr-6.md,ADR 6: Choosing a Test Framework,"## ADR 6: Choosing a Test Framework July 26, 2019  ### Context The app needs tests. This is a must. Although test driven development is best, the app added tests after development began to test logic and encourage future changes do not break functionality. It would be best to test all aspects of the MVVM architecture, but view models are where the majority of the app logic lives.  ### Decision Several testing frameworks exist for .NET apps, with the main three being xUnit, NUnit and MSTest. The app uses xUnit to test view models, but any of the other two frameworks would've also been suitable. I chose xUnit because the testing syntax looked cleaner, the framework was a bit newer, and I found some nice tutorials that recommended xUnit above the other two. Perhaps most importantly, I wanted to get started with testing instead of spending too much time debating the merits of which one to chose.  Perhaps just as important as which testing framework to use is which mocking package to use when faking API or other function calls in my tests. For that, Moq seems to be the clear winner. It's easy to find documentation online about using Moq with each testing framework.  ### Status Accepted  ### Consequences Getting started with testing is an undertaking, and it is very easy to let tests fall by the wayside when the touch table has a deadline and new features need to be added quickly. However, many xUnit tutorials seem incomplete and only test the view models. I'm curious if this is a sufficient amount of test coverage.  _In Retrospect:_ Unfortunately, I added tests later in the development process, and that caused some unnecessary delay while I was restructuring the architecture. This could have been avoided if I was a bit more familiar with how [Moq](https://www.nuget.org/packages/moq/) worked from the outset of building the app. I also had to explore using [Unity](https://www.nuget.org/packages/Unity/) to make sure dependency injection was in place for my unit tests. For more information in using Unity, refer to the `ContainerHelper` class in the `Lib` folder of the app. ","technology_choice, testing_strategy"
ADR_258,https://github.com/opentransportmanager/otm-docs.git,docs/adr/simulator_language.md,Simulator Language,# Simulator Language  ## Status  Accepted  ## Context  Simulator should do multiple calculations in a short time. Choice should be dictated by the speed of code execution.  ## Decision  Golang  ## Consequences  - Multithreading - Dynamic typed and compiled,technology_choice
ADR_259,https://github.com/CleverCloud/clever-components.git,docs/adr/adr-0005-how-should-we-implement-copy-to-clipboard.md,(sem título),"--- kind: '📌 Architecture Decision Records' ---  # ADR 0005: How should we implement copy to clipboard?  🗓️ 2019-10-05 · ✍️ Hubert Sablonnière  This ADR tries to explain the choices we made to add a copy to clipboard feature on our `<cc-input-text>` component.  ## Technical choices?  ### Is there a standard way to achieve this?  We use to use [document.execCommand()](https://developer.mozilla.org/en-US/docs/Web/API/document/execCommand) to do so but there is now a standard way to do it which is based on promises. It's called [clipboard.writeText()](https://developer.mozilla.org/en-US/docs/Web/API/Clipboard/writeText). The problem is that it does not work on Safari 12+.  ### Are there any polyfills?  Yes, there is [clipboard-polyfill](https://github.com/lgarron/clipboard-polyfill). It has most of the `Clipboard` object API but it also means lots of things we don't need. It's 2.3 KB (min+gzip).  ### Alternative?  This polyfill is small and working but we found a smaller lib that only does that: [clipboard-copy](https://github.com/feross/clipboard-copy). It's basically a ""use `clipboard.writeText()` and fallback to `document.execCommand()`"" in 433 B (min+gzip). It's made by [feross](https://github.com/feross) ;-).  ## UI/UX choices?  We looked at how others were doing ""copy to clipboard"" like the copy URL to clone in GitHub and GitLab. We also looked at 1password and a few others. Most of them have an icon button on the right which is grouped with the text input. The button has the same behaviour and states as other buttons in the site. Sometimes the icon is a clipboard, sometimes it's the classic copy icon (2 files on top of each other).  We tried to adapt our existing `<cc-button>` to support icons and then embed it in the `<cc-input-text>` but it was easier just to use a `<button>` directly. We also tried to apply the same behaviour and states (focus, active, hover...) we had on our `<cc-buttons>` but it was not a good idea:  * The box shadow thing we have on hover that disappears when active was not working with the rest of the design. * The whole concept of a button as we have with `<cc-button>` grouped visually with `<cc-input-text>` would not work in `multi` mode with lots of lines since the height can be quite big.  Then we got inspiration from Slack's buttons/actions that are in the main input text of the chat. Those are just icons at first but they have hover, focus and active states that make sense.  In the end, we decided to:  * Have an icon button like Slack does * Align it to top right, whatever the height * Make it display a green tick for a second after the click (just like GitHub does in Pull Request copy branch) ",others
ADR_260,https://github.com/zooniverse/tove.git,docs/arch/adr-02.md,ADR 02: Credential Storage,"# ADR 02: Credential Storage  * Status: accepted * Interested parties: @zwolf, @camallen, @adammcmaster * Date: October 21, 2019  ## Context  Rails apps need access to environment-based credentials (API keys, db URLs and passwords, etc). We do this a few different ways across all of our Rails apps. This ADR is a chance to take everyone's temperature on using a neat but new bit of Rails 6 and inform similar decisions later.  ## Considered Options  * Kubernetes secret storing encoded environment variables * Rails internal credential storage solution * Kubernetes mounted dotenv volume  ### Option 1: Kubernetes encoded env variables  A list of environment variables are base64-encoded and piped into a k8s secret. Loaded by being added individually to the templates.  Pros: * Our current standard (Caesar, PRNAPI)--or as close to one as we have. * Each var exists in template, so the contents are clearly defined.  Cons: * Whole base64 encoding thing makes reading/editing credentials a chore * Credentials are stored entirely seperate from the app, tying their values to deployment/k8s instead of to the app.  ### Option 2: Rails internal credentials  As of Rails 5.1, Rails supports storing its own credentials. Rails 6 includes support for this feature across multiple environments. A `config/[environment].yml.enc` file is encrpyted with a `config/[environment].key`. The latter is stored as a k8s secret and mounted in `/config`. The encrypted yml file can then theoretically be included in version control, but could also be stored in the same volume mount if that makes people nervous. Development key+creds can be kept in git and are used by default (via `RAILS_ENV`).  Syntax for the Rails helpers is as follows: `rails edit:credentials --environment staging`  The `--environment` arg looks for `config/[environment].key` to decode `config/[environment].yml.enc`.  Pros: * Simpler templates, since every var doesn't have to be included to still be accessible * Follows new conventions, built into Rails. * Keeps the app's requirements within the context of the app. A record is kept (potentially versioned, even) and redeployment (say, to Azure) has less steps.  Cons: * Rails 6 only (for multiple envs). Our other apps will need upgrade all the way to use the same functionality. * Different. This already an issue with our various other Rails apps, so it would be yet another strategy, but a fairly self-documenting one. * Rails 6 is released and stable, but this feature is kind of new. 5.1 was a while ago, though.   ### Option 3: k8s Mounted secrets volumes Used by old Rails apps deployed to k8s (eduapi, for instance). Roughly the same as Option 1, since it's a list of envvars that is loaded by k8s into the environment, only with a mounted volume that completely obfuscates the contents everywhere. So it's like the first one, only worse. Including for reference, but not the direction we want to go.  ## Decision Outcome  Decided to go with Option 2, Rails credentials. It's the most forward-looking option and isn't terribly different from existing setups. There's even a precedent in the graphql stats API. Also, as it's already being done in the aforementioned API, we're going to store encoded credentials in the repo.  ### Links * rails docs: https://edgeguides.rubyonrails.org/security.html#custom-credentials * PR that added environment specificity: https://github.com/rails/rails/issues/31349 * quick blog post on use: https://blog.saeloun.com/2019/10/10/rails-6-adds-support-for-multi-environment-credentials.html ",technology_choice
ADR_261,https://github.com/kbremner/read-more-api.git,ReadMoreAPI/doc/adr/0007-deploy-to-heroku.md,7. Deploy to Heroku,"# 7. Deploy to Heroku  Date: 2017-09-24  ## Status  Accepted  ## Context  The application needs to be deployed somewhere.  There are many container orchestration platforms for configuring a cluster of machines running Docker containers and managing the deployment of images on to these containers. Setting up these tools involves managing the cluster of machines and the hosting costs associated with these machines.  Heroku is a Platform as a Service (PaaS) provider which helps with the deployment of applications. They have a [Container Registry](https://devcenter.heroku.com/articles/container-registry-and-runtime) solution that handles the deployment of Docker images in to suitable containers.  Heroku has several pricing tiers for machines that the application will run on, including a free tier.  Heroku provides a free hosted PostgreSQL option. It will handle setting a ""DATABASE_URL"" environment variable, containing the information required to connect to this database. The free tier database is limited to 10,000 rows.  We want the setup process to be as simple as possible.  ## Decision  We will host the application on Heroku, using their Container Registry solution with a hosted PostgreSQL database.  ## Consequences  We will have no costs associated with hosting the application.  The suitability of the free tier will need to be investigated if performance becomes an issue.  A database will not need to be setup seperately.  The suitability of the free database tier will need to be investigate if performance or the quantity of data becomes an issue.",infrastructure_and_deployment
ADR_262,https://github.com/cljdoc/cljdoc.git,doc/adr/0019-use-custom-search.md,Implement our own indexing and artifacts search,"# Implement our own indexing and artifacts search  ## Status  Proposed  ## Context  We need to be able to search (documented) projects, whether  they come from Clojars or Maven Central.  See https://github.com/cljdoc/cljdoc/issues/85  ## Decision  Implement our own search, using direct integration with Lucene. Download and  index artifact list from Clojars and ""org.clojure"" artifacts from Maven Central.  We will use Lucene as that is the absolutely prevailing solution for search in Java. Direct Java interop is quite idiomatic in Clojure; it isn't too much work as we only need to implement the parts relevant for us and not a generic Lucene wrapper. We avoid the risk of depending on incomplete and potentially abandoned library (as happend to Clojars with clucy). And to be able to use Lucene efficiently we need to understand it sufficiently anyway.  ## Consequences  * Our search results will be different from those you get from Clojars, we won't   benefit from any improvements on Clojars' side * We will be able to search also for non-Clojars projects (and thus also make    their docset available in Dash) * Cljdoc will become slightly more complex and possibly expensive to operate    (due to downloading the ~10MB index from Clojars regularly) * New projects will not appear immediately in the search until the next scheduled   indexing (unless we mitigate that somehow) * We can fine-tune the search to prioritize the results we want",technology_choice
ADR_263,https://github.com/Azure-ukgov-paas/paas-team-manual.git,docs/architecture_decision_records/ADR013-building-bosh-releases.md,(sem título),"Context ======= We use [Bosh](https://bosh.io/) to create and manage our cloudfoundry deployment on AWS. To deploy software, Bosh needs certain binary dependencies available. These are known as bosh [releases](https://bosh.io/docs/release.html).  Before this decision, we usually built and uploaded releases to Bosh as part of our [concourse](https://concourse.ci/) pipeline. Occasionally, we would manually build a release, store it on GitHub, and point Bosh to it there.   ### Building Bosh Releases  We investigated different approaches to creating bosh releases, in particular  * Multiple pipelines created dynamically using [branch manager](https://github.com/alphagov/paas-concourse-branch-manager) * A single pipeline using [pullrequest-resource](https://github.com/jtarchie/pullrequest-resource)  The work on these spikes was recorded in https://www.pivotaltracker.com/n/projects/1275640/stories/115142265 https://www.pivotaltracker.com/n/projects/1275640/stories/128937731  Decision ======== We will use the [pullrequest-resource](https://github.com/jtarchie/pullrequest-resource) approach to build all our Bosh releases in a consistent way.  Status ====== Accepted  Consequences ============ We must gradually migrate all our Bosh release builds to their own build pipelines. We will need separate jobs to build from master - this already has a proof of concept in the spike. We may have to add additional config in projects we fork to allow us to create final builds. ",technology_choice
ADR_264,https://github.com/alphagov/paas-team-manual.git,source/architecture_decision_records/ADR002-concourse-pool-resource.html.md,(sem título),"--- title: ADR002 - Concourse pool resource ---  # ADR002: Concourse pool resource  ## Context  When building pipelines using concourse, we investigated using the [pool resource](https://github.com/concourse/pool-resource) in order to control flow through jobs. This was an alternative to the use of the [semver resource](https://github.com/concourse/semver-resource).  These 2 resources are both workarounds to solve the problem of triggering jobs when we haven't made changes to a resource.  The problem is that the pool resource relies on write access to a github repo, which means we must pass public keys that allow this access into the pipeline and deployed concourse instance - we want to minimise the number of credentials we pass, and the semver resource relies on AWS credentials that are already passed.  ## Decision  We will not use the pool resource for flow between jobs - instead we will use the semver resource  ## Status  Accepted  ## Consequences  This was an investigation into a different approach, so no consequences ",technology_choice
ADR_265,https://github.com/opinionated-digital-center/python-library-project-generator.git,docs/adr/0002-use-gnu-make-to-centralise-all-tasks.md,Use GNU Make to centralise all tasks,"# Use GNU Make to centralise all tasks  * Status: accepted * Date: 2020-05-07  ## Context and Problem Statement  We want to be able to centralise in a single tool all tasks to be called: * During the development cycle. * During the build cycle. * In the CI/CD pipelines.  ## Decision Drivers  * Must have a significant user base and community. * Must not require significant installation. * Must be sufficiently simple to learn and use.  ## Considered Options  * [`GNU Make`](https://www.gnu.org/software/make/). * [`Invoke`](http://www.pyinvoke.org/). * [`Rake`](https://github.com/ruby/rake). * [`SCons`](https://scons.org/).  ## Decision Outcome  Chosen option: `GNU Make`, because compared to the other evaluated tools (see [Pros and Cons](#pros-and-cons-of-the-options)), it fits the bill where as: * `Invoke` is not well maintained, nor well documented, nor a de facto standard, nor has   a sufficient community. * `Rake` required to install and learn Ruby. * `SCons` is more a build tool and seems difficult to apprehend/get to grips with.  ## Pros and Cons of the Options  ### [`GNU Make`](https://www.gnu.org/software/make/)  `GNU Make` [manual](https://www.gnu.org/software/make/manual/).  * Good, because:   * It is a well known and widely used standard.   * It has a huge existing community.   * It is pre-installed by default on most systems, and can easily be installed on     most existing systems.   * It does not require to install a specific language (python, ruby, etc.)   * It does enough for the purpose of writing and using tasks.   * It has plenty of documentation.   * It has plenty of existing information sources. * Bad, because:   * Some concepts can be difficult to grasp for our use.   * Some default behaviors can be unclear and cause scripts not to work.   * It can be difficult to debug when using complex features.  ### [`Invoke`](http://www.pyinvoke.org/)  * Good, because:   * It is Python based.   * It promises:     * Like Ruby’s Rake, a clean, high level API.     * Like GNU Make, an emphasis on minimal boilerplate for common patterns       and the ability to run multiple tasks in a single invocation. * Bad, because:   * It is not a de facto standard for Python.   * It has a large number of     [unresolved and untriaged issues](https://github.com/pyinvoke/invoke/issues)     (202 at time of writing) and a large number of     [opened Pull Requests](https://github.com/pyinvoke/invoke/pulls), some dating back     2014, which suggests a slack in maintenance.   * It does not seem to have a large enough community.   * Previous points seem to disqualify this solution.  ### [`Rake`](https://github.com/ruby/rake)  * Good, because:   * It is a proven tool.   * It is a de facto standard in the Ruby world.   * It has an extensive community.   * It has extensive documentation. * Bad, because:   * It requires to know/learn another language than Python (Ruby).   * It requires to install a Ruby stack.  ### [`SCons`](https://scons.org/)  * Good, because:   * Written in Python.   * Seems fairly active. * Bad, because:   * A build tool more than a task tool.   * Getting to grip the tool through the documentation is cumbersome.   * It has a large number of     [unresolved and untriaged issues](https://github.com/pyinvoke/invoke/issues)     (604 at time of writing), which suggests a slack in maintenance.   * Limited number of stars on github. ",technology_choice
ADR_266,https://github.com/corgibytes/cukeness.git,doc/architecture/decisions/0003-api-server-responsible-for-interaction-with-external-services.md,3. API Server Responsible for Interaction with External Services,"# 3. API Server Responsible for Interaction with External Services  Date: 2018-08-27  ## Status  Accepted  ## Context  There are a couple external services that need to be communicated with. These are expected to be a source control system and a server running the [Cucumber Wire Protocol](1987e2349b14ca0fe93e879d762df09f1a9b3934). There are other functions that are needed such as authentication and authorization, and an abstraction around the storage and organization of Gherkin-based executable specifications  ## Decision  An API server will be built to handle the following:  * Integration with source control systems (initially just `git` but the addition of others needs to be possible) * Communication with Cucumber Wire Protocol service * Required abstractions for creating, modifying, organizing, and executing Gherkin-based executable specifications * Abstractions for authentication and authorization (initially fulfilled by a simple database authentication mechanism, but eventually allowing other authentication sources such as OAuth)  These functions will be independent of any user interface that's presented to facilitate carrying out these actions.  ## Consequences  This will drastically simplify the user interface, because it will only need to concern itself with communication with one entity to function. However, this also makes the API server rather complex, in that it has to juggle many different responsibilities. These responsibilities can be decomposed into ancillary services and libraries as needed to mitigate this extra complexity. ",api_and_contracts
ADR_267,https://github.com/mozilla/sre-adrs.git,decisions/websre/0002-web-sre-service-documentation.md,2. Web SRE Service Documentation,"# 2. Web SRE Service Documentation  Date: 2021-06-01 Scope: Web SRE Team  ## Status  Approved  ## Context  The Web SRE Team manages a number of services that vary in users, technologies used, setup, our commitment, infrastructure, process, etc.   Pre-existing documentation for these services, if it exists, also varies - in location, level of detail, scope, structure, presumed audience, etc.  To help share knowledge between the Web SRE Team about these services' unique situations, a consistent baseline for our services documentation is a required first step.  ## Decision  A Web SRE service here is defined as any codebase where we have some ownership over an aspect of this codebase being functional - e.g. the deployment process, automation, infrastructure running the service, observability of some aspect(s), or authoring the code ourselves. Web SRE services could be externally or internally-facing, including infrastructure services only Web SRE are aware of and leverage. Web SRE services also include non-prod environments for testing infrastructure code (like staging Kubernetes clusters to test building new clusters) and supporting services (log aggregation, alerting, etc) as well as non-prod application code.   For each Web SRE Service, we will author a Service Documentation page in Mana as a child of https://mana.mozilla.org/wiki/display/SRE/Service+Documentation.  Service Documentation pages: * use the Web SRE Services Docs confluence page template for its structure; * have a primary audience of the Web SRE Team itself, with a secondary audience of other SRE teams; * are open to Mozilla internally, however not all teams of Mozilla are the intended audience of these pages (SRE-external teams will focus on the escalation path section primarily); * have Runbook pages (what to do when a specific symptom of an issue is recognized) as child pages to the relevant Service Documentation page; * have How-to pages (how to perform specific tasks for a service) as child pages to the relevant Service Documentation page; * are maintained to reflect the current state - to the best of our knowledge - of a service's context, including a service being decommissioned (the Service Documentation page should note that but be left up as a tombstone marker of the decision); * cover Web SRE Services as defined above. * replace SRE_INFO.md files in Web SRE team-managed source code GitHub repositories. When an SRE_INFO.md file is encountered, it should be reviewed for any information that can be added to a Service Documentation page, removed, and an link to the Service Documentation page added to the codebase's REAMDE.md.  Service Documentation pages do not: * include all possible services Web SRE might work with - e.g. we don't document AWS Services generically or services owned and maintained by other teams;  * include all possible details for all audiences beyond SRE - when such documentation requests come up, they can be added as a How To as a child to a service page, or a generic How To if not limited to one service; * live anywhere other than Mana. Web SRE Service documentation managed elsewhere, e.g. GitHub, Google Docs, should be migrated & then deprecated via links pointing readers to the Mana page; * replace other forms of documentation living elsewhere - e.g. codebase-specific documentation within a git repository, decision records in this repository, collaboration / draft developer notes in Google documents, Mana pages in other places and formats walking through shared infrastructure or processes, etc..  Service Documentation pages optionally have Runbook & How-To child pages. For these, there are the following expectatiosn:  Runbook pages should: * include ways to validate what state a system is in (e.g. how to reproduce the problem); * be as self-contained as is feasible; * explain what problem a given state indicates; * give commands to resolve the problem as clearly as possible; * outline a fallback plan (who to call, what to do next).  How-to pages should: * repeat existing documentation as little as possible. Linking to external docs is encouraged, perhaps augmented with our specific contexts; * assume a high level of competence from the audience (don't explain how to download a csv file, though giving an example command to establish context is great); * explain the decision points in a process, and how to make them; * outline what is needed to perform the work as early as possible in the document (what access, what tools); * outline who needs to approve the work, or how to decide if it's safe to do the work as early as possible.  ## Consequences  This will require a fair amount of work in getting the Web SRE Services portfolio adequately covered by the guidelines above.  This will also require maintenance work ensuring the drift between service state and documentation state is as minimal as is feasible.  ## Resources  * [Service Documentation Folder](https://mana.mozilla.org/wiki/display/SRE/Service+Documentation) * [Web SRE Service Documentation Template (Web SRE Team & Jira Space admins only can view)](https://mana.mozilla.org/wiki/pages/templates2/viewpagetemplate.action?entityId=131596432&key=SRE) ",governance_and_process
ADR_268,https://github.com/LBHackney-IT/lbh-adrs.git,Platform/Accepted/Frontend-Tech-Stack.md,Frontend Tech Stack,"# Frontend Tech Stack  ### **Date:** 30th March 2021  ### **Status:** ACCEPTED  ## **Context**  Hackney has several frontends applications using different programming languages and frameworks:  - Angualr/JS - Ruby - React/TS  Hackney has got a  microservices architecture exposing APIs to be used by different consumers and so each frontend can be developed with its own programming language and framework. Anyway, unless an application requires a particular programming language, it’s still good to agree on a common language/framework for mtfh-T&L workstream. This will make it easier to shift developers from different teams.   ## **Decision**  **React with TS (Type Script)**  React with TypeScript is the most common framework/language used for the majority of Hackney frontend application, it’s the programming language that Hackney frontend developers are more familiar with, plus it has got the following well known advantages:  - It’s open source - Easy to learn because of its simple design and less time worrying about the framework specific code - Better user experience and very fast performance compared with other FE frameworks  ## **Consequences**  No consequences as all frontends of each team are more familiar with React, plus, before the Cyber attack, Hackney started to migrate in React the old Angular and Ruby apps.",technology_choice
ADR_269,https://github.com/ministryofjustice/offender-management-architecture-decisions.git,decisions/0007-use-ruby-for-new-applications-for-manage-offenders-in-custody.md,7. Use Ruby for new applications for Manage Offenders in Custody,"# 7. Use Ruby for new applications for Manage Offenders in Custody  Date: 2018-10-19  ## Status  Accepted  ## Context  ### Language use across HMPPS  HMPPS has active development in four languages, including services with significant prison-staff-facing components in all four: Ruby, Python, JavaScript and Java.  HMPPS has live services (which have passed service assessments) built in Ruby and Python. Ensuring that we support our existing users should be our top priority, so it is essential to maintain our skills in the languages used in our live services.  One of the advantages of a microservices approach is that teams can work on separate services in different languages, using HTTP APIs to share data and functionality. There is no need for all services to be built in the same language. We are already using this approach across HMPPS.  There is no clear vision or strategy at the moment for changing the number of languages in use across HMPPS. We are not in a position to decide that for all of HMPPS.  ### Team skills  All four languages in active use across HMPPS are represented to varying degrees in the skill sets of the current members of the team, but only Ruby is common to all of them. The team have worked together on a live service built in Ruby for all of their time at MOJ/HMPPS. We still own that service and continuously improve it alongside our work on Manage Offenders in Custody, although we are spending the majority of our time on the latter.  The primary language skills of HMPPS's civil servant developers and technical architect (a significant proportion of whom are on this team) are in Ruby and Python. It is unrealistic to expect people to be equally proficient in many languages at the same time.  The team have already committed to learning about Kubernetes for the new Cloud Platform (see [ADR 0002](0002-use-cloud-platform-for-hosting.md)) and to learning Java so that we can collaborate on the APIs which are being built in Sheffield (see [ADR 0006](0006-use-the-custody-api-to-access-nomis-data.md)). This is already a significant proportion of unfamiliar technologies for the team to learn and use.  Developing applications involves much more than using the standard library of a language. The ecosystem of libraries tools around that language often takes much more work and time to become familiar with than the basics of the language itself. Although all of the team know some JavaScript, we do not all have experience of using it for building server-side applications. We would therefore have a lot more to learn if we were to choose to use JavaScript, as some related but separate services do.  ### Time constraints  The Offender Management in Custody programme has fixed timelines for its national rollout in the next year. Although we are not committing to delivering particular services at set dates months in advance, we will reduce our opportunity for learning from a smaller set of real users before the national rollout if we are not ready to take advantage of the Wales pilot which begins in January.  We know that allocations is only the first of several areas of the programme which are likely to need support from us, so timescales are tight for us.  We anticipate that the complexity of building this service lies in managing the quality of the data available across NOMIS, Delius and OASys, rather than in representing that data to users.  Choosing to use a less familar language for developing our applications, on top of what we already need to learn, would put us at significant risk of not delivering working software until several months after our first users need it.  ### Code reuse  Using the same language for groups of similar services can make it easier to provide a coherent experience for users by allowing presentation code to be shared more easily between services. However, the same HTML structure of pages can be produced by services written in different languages. Since we are committed to progressive enhancement (see [ADR 0003](0003-use-progressive-enhancement.md)), we will use client-side JavaScript solely to enhance the functionality of those pages, and that JavaScript can be reused across services regardless of the language used on the server.  As an example of this approach, there is a strong and active cross-government community which develops, researches and supports design patterns, styles and components which are used on services built in many different languages: https://design-system.service.gov.uk/  We will base our user-facing applications on this established design system in any case. There is already a variety of design approaches in use across the prison-staff-facing services we have, and our best chance of standardising that well is to align ourselves with the cross-government approach.  That approach is supported by extensive user research over several years and across many services and departments. Using it as our starting point reduces the need for us to undertake duplicate research ourselves to understand the effectiveness of alternatives to those existing patterns. We expect that we will need to extend those patterns and develop others inspired by them to meet our users' needs, and we will contribute what we learn back to the HMPPS and cross-government communities.  Since it has been agreed that all services which need to use a NOMIS API should migrate to the Custody API (see [ADR 0006](0006-use-the-custody-api-to-access-nomis-data.md)), any API client library which we build in Ruby can be reused by other Ruby services to ease their migration.  ### Operational considerations  The team has considerable experience of operating live services built in Ruby at scale.  We do not anticipate scaling to be a significant concern for allocation - we expect to have a couple of hundred users a day at most for it.  The new Cloud Platform makes it easy, quick and cheap for us to scale up if we need to.  ## Decision  We will use Ruby for the new applications we build in London as part of Manage Offenders in Custody.  ## Consequences  We will build on the knowledge the team already has of the Ruby ecosystem.  We will not have to significantly deepen our knowledge of a third language (as well as Ruby and Java), familarise ourselves with a different ecosystem of libraries and decide on and learn another set of tools in order to make progress.  We will be able to use the GOV.UK Design System as the basis for making our services look consistent with other government and HMPPS services.  We may not be able to reuse libraries which are built by teams in Sheffield if they are intended for use with particular JavaScript frameworks which we do not need to use.  We will write client code in Ruby for the Custody API (and any other APIs we use) which we could extract into libraries to be used by other Ruby services when they migrate to use those APIs.  We will maintain a strong level of Ruby knowledge within HMPPS, which will help us ensure that we can continue to support a significant proportion of our live services in the future.  If HMPPS wants to ensure that we have civil servants with strong skills in the other languages currently used across all its services, we will need to focus on hiring in those areas rather than expecting our existing developers to be able to work equally productively across all those languages. ",technology_choice
ADR_270,https://github.com/nhsuk/mongodb-updater.git,doc/adr/0001-record-architecture-decisions.md,1. Record architecture decisions,"# 1. Record architecture decisions  Date: 2017-06-19  ## Status  Accepted  ## Context  We need to record the architectural decisions made on this project.  ## Decision  We will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions  ## Consequences  See Michael Nygard's article, linked above. For a lightweight ADR toolset, see Nat Pryce's _adr-tools_ at https://github.com/npryce/adr-tools. ",governance_and_process
ADR_271,https://github.com/elifesciences/elife-xpub.git,docs/adr/0002-pubsweet.md,2. Use of PubSweet,"# 2. Use of PubSweet  Date: 2018-06-05  ## Status  Accepted  ## Context  eLife needs to develop their own online journal submission and peer review system.  There were three options available:  - Build a system from scratch. - Use [PubSweet](https://pubsweet.org/) an open source project built by the [Coko Foundation](https://coko.foundation/technology/) - Consider using / licensing a closed-source alternative.  Building a system from scratch would be ruled out if there was a suitable existing project that could be used.  The option to use a closed-source was ruled out, even though one of these alternatives had plans to go open source this was considered too great a risk to the project.  ## Decision  The decision was taken on Sept 2016 to use PubSweet. This decision was taken in-part due to another publisher [Hindawi](hindawi.com) also participating in the collaboration.  We can keep track of dependencies on PubSweet components, all components used are to be recorded in: [PubSweet.md](../developing/PubSweet.md)  ## Consequences  - The opportuinty to contribute to PubSweet, thus the [wider community](https://coko.foundation/community/partners-projects/) - PubSweet mandates the use of [React](https://reactjs.org/) ",technology_choice
ADR_272,https://github.com/cloudfoundry/cf-k8s-networking.git,doc/architecture-decisions/0010-route-crd-and-kubebuilder-instead-of-metacontroller.md,10. Create Route CRD and use Kubebuilder Instead of cfroutesync/Metacontroller,"# 10. Create Route CRD and use Kubebuilder Instead of cfroutesync/Metacontroller  Date: 2020-05-05  ## Status  Accepted  ## Context  ### Proposed Design The proposal and discussion for the Route CRD and design can be found [here](https://docs.google.com/document/d/1DF7eTBut1I74w_sVaQ4eeF74iQes1nG3iUv7iJ7E35U/edit?usp=sharing).  ![Proposed RouteControllerDesign](../assets/routecontroller-design.png)  ### Summary In order to achieve our scaling targets for the cf-for-k8s networking control plane, we need a ""fast path"" for networking changes to propagate from Cloud Controller to Kubernetes. The periodic sync loop implementation of `cfroutesync` and Metacontroller adds around 7 seconds of latency **on top** of the ~7 seconds of Istio control plane latency which in total exceed our scaling targets<sup>1</sup>.  We propose: 1. Replacing our sync loop with a `Route` [Kubernetes custom resource definition (CRD)](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) and a controller that manages networking resources based on these Routes. 2. Cloud Controller updates Route resources directly on the Kubernetes API as a result of user actions (e.g. `cf map-route`) 3. Keeping a periodic sync loop will between Cloud Controller's database (CCDB) and the Kubernetes API to ensure consistency.   ## Decision  We will introduce a `Route` CRD and build a `RouteController` using the Kubebuilder v2 controller framework.  Cloud Controller will be updated to perform CRUD operations on Kubernetes`Route` resources in both its v2 and v3 APIs.  We will remove `cfroutesync` and dependencies on Metacontroller.   ## Consequences This reverses the [previous decision (ADR 2)](https://github.com/cloudfoundry/cf-k8s-networking/blob/develop/doc/architecture-decisions/0002-directly-create-istio-resources.md) to use Metacontroller and directly create Kubernetes and Istio resources.  #### Route CRD * Decouples cf-k8s-networking from Cloud Controller. Additional routing control planes could be added in the future or developers could use `kubectl` directly. * Abstracts the underlying ingress implementation away from Cloud Controller. We could replace Istio with an alternative ingress solution without requiring CC changes. * Moves us closer to a more ""Kubernetes native"" design. * Potential downside of adding yet another CRD is that it may put more load on the Kubernetes API / etcd. Could become an issue as other teams also move to CRD-based designs.  #### Using Kubebuilder * Provides Community buy-in; the `kubebuilder` framework is the encouraged way to engineer a CRD * Provides built-in best practices for writing a controller, including: shared caching, retries, back-offs, leader election for high availability deployments, etc...  #### Removal of Metacontroller Removal of Metacontroller alleviates some future problems: * As discussed in [ADR 2](https://github.com/cloudfoundry/cf-k8s-networking/blob/develop/doc/architecture-decisions/0002-directly-create-istio-resources.md), Metacontroller did not support the many Route to one VirtualService object relationship which required us to aggregate Routes from Cloud Controller ourselves. With Kubebuilder we can support this relationship and keep the data representations consistent across both the Cloud Controller and Kubernetes APIs. * The Metacontroller design, most likely, does not provide necessary metrics for GA * Metacontroller itself is [no longer supported](https://github.com/GoogleCloudPlatform/metacontroller/issues/184) and currently presents issues with Kubernetes `v1.16+`   ### Footnotes _<sup>1</sup> Routing changes should take effect in under 10 seconds on an environment with 2000 app instances and 1000 routes._  ",api_and_contracts
ADR_273,https://github.com/Vodurden/CrossyToad.git,docs/architecture/decisions/adr-0001-record-architecture-decisions.md,ADR 0001: Record architecture decisions,"# ADR 0001: Record architecture decisions  Date: 2018-09-30  ## Status  Accepted  ## Context  I want to be able to remember the reason I chose a particular architecture so I can change my mind!  I also hope the records will be useful to others trying to understand how someone might approach creating a game in Haskell.  Normally I would use this style for a team codebase, but I'm hoping the approach will have value even though this is a solo project.  Also I like architecture decision records and want more projects to use them, so let's lead by example!  ## Decision  I will use Architecture Decision Records, as described by Michael Nygard in this article: http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions  ## Consequences  - If I make a decision of consequence. I'll write an ADR outlining why I made that decision. - See Michael Nygard's article, linked above, for further consequences of this approach. ",governance_and_process
ADR_274,https://github.com/alphagov/verify-service-provider.git,docs/adr/0023-we-will-report-the-version-in-a-saml-extension.md,23. We will report the version in a saml extension,"# 23. We will report the version in a saml extension  Date: 2017-10-10  ## Status  Accepted  ## Context  The verify-service-provider will be deployed to a number of relying parties. Which relying party is using which version could potentially become difficult to keep track of.  To make it as easy as possible for us to manage this we'd like the verify-service-provider to report its version in some way.  Because the verify-service-provider is not intended to be accessible to the internet we can't simply expose an endpoint that reports the version number. Also, since the SAML messages go via the browser we can't use a custom HTTP header.  There's also a concern about the security implications of reporting a version number in cleartext.  We considered a couple of options:  - Requesting metadata from Verify with a custom user-agent string - Sending the version in an unencrypted saml extension - Sending the version in an encrypted saml extension  ## Decision  We decided to send the version number in the SAML AuthnRequests as an encrypted SAML extension. The XML will look roughly like this:  ``` <saml:AuthnRequest>   <saml:Issuer>...</saml:Issuer>   <saml:Signature>...</saml:Signature>   <saml:Extensions>     <saml:EncryptedAttribute>...</saml:EncryptedAttribute>   </saml:Extensions> </saml:AuthnRequest> ```  Once decrypted, the Attribute in the Extensions will look like:  ``` <saml:Attribute Name=""Versions"">   <saml:AttributeValue xsi:type=""metrics:VersionsType"">     <metrics:ApplicationVersion>3.4.1</metrics:ApplicationVersion>   </saml:AttributeValue> </saml:Attribute> ```  ## Consequences  Verify will be able to monitor the versions of connected instances of the verify-service-provider.  AuthnRequests sent by the verify-service-provider will be approximately 2,500 bytes longer than they would be without the version extension. They should still be short enough that this will not cause any validation issues.  Version numbers will not be visible to malicious third parties as the extension will be encrypted with Verify's public key.  ",others
ADR_275,https://github.com/nulib/meadow.git,doc/architecture/decisions/0029-npm.md,29. npm,"# 29. npm  Date: 2021-11-03  ## Status  Accepted  ## Context  The latest upgrade of Yarn has introduced issues that we're finding difficult to overcome.  Supersedes [11. Yarn](0011-yarn.md)  ## Decision  Switch back to `npm` instead of `yarn` in all dev, test, and build environments.  ## Consequences  The instructions have changed, but overall the complexity involved in installing and maintaining  meadow's JavaScript dependencies should be about the same. ",technology_choice
ADR_276,https://github.com/openkfw/TruBudget.git,docs/developer/architecture/0002-access-control-model.md,(sem título),"--- sidebar_position: 2 ---  # Access Control Model  Date: 03/04/2018  ## Status  Accepted  ## Context  We need to define our approach to access-control/authorization.  ## Decision  Since our users' organizations differ a lot in terms of structures and policies, we need to have a very versatile access control mechanism in place. In the industry, the most used technique employed is role-based access control (RBAC). We base our mandatory access control (MAC) mechanism on RBAC, but use the notion of _intent_ instead of role (an intent can be executed by exactly one role, thus effectively replacing the role concept).  ![Intent + Resource = Permission](./img/0002-access-control-model.png)  ### Intents  An intent is what the user is trying to achieve, for example, ""add a workflow to a subproject"". By using intents rather than roles, we side-step the problem found in many projects, where over time developers create similar roles on-the-fly, as from their point of view the implications of re-using a role are not always clear. Conversely, intents are always specific to a use case (examples: ""create project"" or ""list a project's subprojects"" rather than ""admin"" or ""user"").  While roles _could_ be used to bundle intent-execution rights (e.g. have one role that is allowed to execute all ""view"" intents), we think that those roles would have to be managed by organizations themselves (as it will depend on their structure). Since in most cases this would mean a 1:1 mapping from role to user group, we skip roles altogether.  ### User Groups  Organizations group their users into _user groups_. For any given projects, each (resource-specific) intent has a list of user groups assigned to it; all users in the assigned groups are then allowed to execute the respective intent.  ```plain +-------+               +---------------------+                +---------------+ | User: | is member of  | Group:              | is allowed to  | Intent:       | | Alice +-------------->+ Project Maintainers +--------------->+ Add workflow  | |       |               |                     |                | to subproject | +-------+               +---------------------+                +---------------+ ```  ### Implementation Pattern  The goal is to enable us to follow a clear pattern for our access control needs:  - HTTP controllers call the domain modules and the authorization module (perhaps using a   middleware), but do not deal with intents or groups. - Domain modules may interact with the chain to fetch domain objects, and/or prepare   closures to be authorized and executed later on. They deal with intents, but not with   users or groups. - Finally, the authorization module ensures that the _user_ executing the _intent_   belongs to a _group_ that is allowed to do that. In order to decide that, the module   has to fetch resource-specific ACLs from the chain.  Modifying ACLs is done in the same way: each resource's ACL specifies the groups that may execute the ""change this ACL"" intent (to be renamed). This hints at the necessity to provide _defaults_ for ACLs when creating resources.  ### Resource-specific Access Control Lists (ACLs)  With each resource/stream on the chain, an ACL stream-item is stored that lists for each intent the groups allowed to execute that intent:  ```json {   ""acl"": {     ""view project"": [""all users""],   },   ... } ```  ## Consequences  With the proposed changes in place, our users will be able to impose their respective organizational structures onto TruBudget's resources in a way that should be flexible enough and straight-forward to integrate with their existing directory servers. ",security
ADR_277,https://github.com/robertlcs/react-native-app.git,doc/adr/0006-make-tabs-swipeable.md,6. Make tabs swipeable,"# 6. Make tabs swipeable  Date: 2018-09-12  ## Status  Accepted  ## Context  To navigate between tabs, users can swipe left or right or click a tab.  ## Decision  We will use NativeBase's component ""Tabs"". Previously used [react-native-tab-view](https://github.com/react-native-community/react-native-tab-view) since it's a cross-platform component that works perfectly on iOS and Android.  ## Consequences  It renders inactive tabs. ",technology_choice
ADR_278,https://github.com/buildit/digitalrig-metal-aws.git,docs/architecture/decisions/0006-create-reference-implementation-repository.md,6. Create Reference Implementation Repository,"# 6. Create Reference Implementation Repository  Date: 2019-01-14  ## Status  Accepted  ## Context  The rig defined at [Bookit Infrastructure](https://github.com/buildit/bookit-infrastructure) is an instance of the AWS Bare Metal Rig.   Whilst it's rather generic as it is, it is specific to Bookit's needs.   The AWS Bare Metal Rig is also intended to offer choices for the different components (Compute - ECS EC2 vs ECS Fargate vs EKS, RDS - Aurora MySQL vs Aurora Postgres vs Aurora Serverless, etc).   The only way to capture that is via branches which can be hard to discover.   Finally, there is not a single repo that represents the latest and greatest version of the AWS Bare Metal Rig.  As instances of Rigs diverge, it is difficult to instantiate a new one that includes all of the latest features  ## Decision  Create a digitalrig-metal-aws repo (https://github.com/buildit/digitalrig-metal-aws) that demonstrates all of the options and latest features of the AWS Bare Metal Rig and removes any Bookit specific wording/concepts.  ## Consequences  Projects looking to instantiate new AWS Bare Metal Rigs shall be able to clone this reference implementation, make choices nad changes specific to that project, and instantiate their project specific Rig. Changes and enhancements will need to be implemented in 2 places:  1) the project specific Rig and 2) the AWS Bare Metal reference implementation Rig so that future implementations will contain the latest features ",others
ADR_279,https://github.com/jmoratilla/devops-challenge.git,doc/adr/0004-feat-about-orchestration.md,4. feat-about-orchestration,"# 4. feat-about-orchestration  Date: 2020-02-17  ## Status  Accepted  ## Context  Goals:  1. To select an orchestration technology suitable for this type of architecture. 2. The solution must be scalable according to the load.  Here I'm going to talk about Infrastructure as Code (IaC), the way to define a system  architecture for running applications in the cloud.  By defining resources in a text file and process them using specific API engines, a   cloud company can create a set of computing units, networks, users and services in a   repeatable and under version control way.  That allows to create new ways to manage  infrastructure, network, storage.  To read the file we use tools that will contact the APIs and engines to control the  order and the execution of each task performed by the engines.  These tools are called  orchestrators.  Orchestration tools exist for creating the resources needed to deploy  applications.  But there are two models here:  - Those that reuse resources (mutable servers) - Those that don't reuse resources (inmutable servers)  For a microservices architecture, our main purpose is to quickly replace those  services, with no downtime.  For this, the inmutable servers philosophy is better, as you can launch an  instance of the new version, test it, and decide to enable it in production   without risking the rest of services.  Well, not so valid if we consider database migrations, but it requires to set  some requirements about database management that are far from the scope of   this test.  To scale apps you can upgrade computing resources (vertical scaling), or  adding more computing instances provisioned with the app (horizontal scaling).    Both can be performed easily with inmutable servers.  Which orchestration tools use the inmutable servers perspective?  - [AWS CloudFormation](https://aws.amazon.com/cloudformation) - [Terraform](https://www.terraform.io)  AWS cloud formation uses a json-like text file where resources like computing  units, vpc, load balancers, databases, etc., are defined to create and   provision the declared resources.  This file can be versioned and stored in   a version control system.  This allows you to recreate the resources used in   each stage of the development of the applications.  The main issue here is   that is only valid for AWS.  Terraform, by hashicorp, uses a DSL (Domain Specific Language) that makes easy  to define resources.  Also, it allows to define providers for several   platforms and services, so it's not limited to AWS, but it can work with   Google, Azure, IBM, bare metal, vmware, etc.  But for Kubernetes, there is another tool that works as orchestrator:   * [kops](https://github.com/kubernetes/kops)  This tool, that it is used to manage and orchestrate clusters, groups and   secrets in AWS and other cloud platforms, uses terraform internally.  So it   can be used to generate a terraform set of files and store them in a version  control system to do the same.  ## Decision  I'm going to use kops for the management of the clusters, and I'm going to  generate templates that can be used to deploy new clusters depending on the  requirements of an environment like development, staging or production.  ## Consequences  The first thing I change here is that it is not possible to keep using my own virtual servers to create the kubernetes cluster.  Everything now will be  running on AWS.  ","technology_choice, infrastructure_and_deployment"
ADR_280,https://github.com/budproj/unfinished-design-system.git,docs/adl/004-tokenization-and-static-assets.md,ADR 4: Tokenization and Static Assets,"# ADR 4: Tokenization and Static Assets  * [Table of contents](#)   * [Context](#context)   * [Decision](#decision)   * [Status](#status)   * [Consequences](#consequences)   * [More reading](#more-reading)   * [Updates](#updates)  ## Context  Tokens have a significant role in theming. They're responsible for defining the primitives of our theme, such as color codes, font names, and others. Tokens are relevant to determining asset locations too.  We must find a proper way to handle and maintain our tokens.  ## Decision  We've decided to use [Style Dictionary](https://amzn.github.io/style-dictionary/#/) as our framework to handle tokens. It is easy to use since we can define our tickets in standard JSON but empowering them with string interpolation, variables, and other features.  For our static assets, we're going to host them at [AWS S3](https://aws.amazon.com/s3/), defining the proper CORS rules, and refer the location of those as tokens for our applications to use.  Instead of increasing the size of our Javascript bundle with static assets, we prefer to keep it simple and light by hosting those in an S3 bucket and asking for the application to download it.  ## Status  **DEPRECATED** _check [update 1](#update-1)_  ## Consequences  Tokenization is a complex process. We need to look out and keep it simple. The logic (like mapping those tokens to theme properties) must happen in the design-system itself, keeping the token package just for constant's definition.  ---  ## More reading  * [Style Dictionary's docs](https://amzn.github.io/style-dictionary/#/)  ## Updates  ### Update 1  After [business/ADR#001](https://github.com/budproj/architecture-decision-log/blob/main/records/business/001-reducing-initial-technical-complexity.md), we've decided to stop the development of a decoupled design system. ","technology_choice, data_persistence"
ADR_281,https://github.com/nhsuk/connecting-to-services.git,doc/adr/0015-add-info-page.md,15. Add info page,"# 15. Add info page  Date: 2019-10-16  ## Status  Accepted  ## Context  When debugging it is useful to be sure of what the deployed version of the application is. In addition, when debugging date and time issues (e.g openness) then it is useful to know the time in UTC from the point of view of the application.  ## Decision  The application will provide an info page.  ## Consequences  Debugging depoyment and time related issues will be made easier. ",others
ADR_282,https://github.com/guttih/island.is-glosur.git,docs/adr/0009-naming-files-and-directories.md,Unified naming strategy for files and directories,"# Unified naming strategy for files and directories  * Status: accepted * Deciders: devs * Date: 2020-07-03  ## Context and Problem Statement   As of the date of this writing, there are multiple different naming styles used in the monorepo, mostly because NX has defaults that differ between schematic types.  In order for navigating the monorepo in a consistent rational manner, we should align on naming strategy for files  and directories.    ## Decision Drivers  * Provide consistency when navigating the codebase * The earlier we decide on this, the better   ## Considered Options Some mixture of these: * kebab-case * PascalCase * camelCase * snake_case  ## Decision Outcome  Chosen option: Name files after their default export. If that default export is a React Component, or a class, then the file name should be in PascalCase. Otherwise, the filename should be in camelCase. Basically, for naming files avoid using kebab-case and snake_case and make sure the name follows the default export of the file.  Naming directories should follow these guidelines: Only use kebab-case when naming NX apps and libraries, or folders containing apps and libraries, e.g. `island-ui` instead of `islandUi`: `import { Box } from '@island.is/island-ui/core'`  Use PascalCase for directories only containing React components: ```` components/CtaButton/index.ts import 'components/CtaButton' ```` or: ```` components/CtaButton/CtaButton.tsx import 'components/CtaButton/CtaButton' ```` rather than ```` components/cta-button/CtaButton.tsx ````  In all other cases, use camelCase.  ### Positive Consequences  * Easier to navigate the codebase * File names are more readable, and developers know what to expect * This approach is the most common practice, and something most JS and TS developers are familiar with. ",governance_and_process
ADR_283,https://github.com/Midnighter/structurizr-python.git,docs/development/adr/0009-use-pydantic-for-json-de-serialization.md,9. Use pydantic for JSON (de-)serialization,"# 9. Use pydantic for JSON (de-)serialization  Date: 2020-06-09  ## Status  Accepted  ## Context  In order to interact with a remote workspace, for example, at structurizr.com. The remote or local workspace has to be (de-)serialized from or to JSON.  ## Decision  In order to perform these operations we choose [pydantic](https://pydantic-docs.helpmanual.io/) which has a nice API, active community, good data validation, helpful documentation, and good performance.  ## Consequences  We separate the models representing Structurizr entities and their business logic from how those models are (de-)serialized. That means that for each model we have a corresponding IO pydantic model describing the JSON data model.  ",technology_choice
ADR_284,https://github.com/apache/james-project.git,src/adr/0019-reactor-netty-adoption.md,19. Reactor-netty adoption for JMAP server implementation,"# 19. Reactor-netty adoption for JMAP server implementation  Date: 2020-02-28  ## Status  Accepted (lazy consensus) & implemented  ## Context  After adopting the last specifications of JMAP (see  [new JMAP specifications adoption ADR](https://github.com/apache/james-project/blob/master/src/adr/0018-jmap-new-specs.md)),  it was agreed that we need to be able to serve both `jmap-draft` and the new `jmap` with a reactive server.   The current outdated implementation of JMAP in James is currently using a non-reactive [Jetty server](https://www.eclipse.org/jetty/).  There are many possible candidates as reactive servers. Among the most popular ones for Java:  * [Spring](https://spring.io) * [Reactor-netty](https://github.com/reactor/reactor-netty) * [Akka HTTP](https://doc.akka.io/docs/akka-http/current/introduction.html) * ...  ## Decision  We decide to use `reactor-netty` for the following reasons:  * It's a reactive server * It's using [Reactor](https://projectreactor.io/), which is the same technology that we use in the rest of our codebase * Implementing JMAP does not require high level HTTP server features  ## Consequences  * Porting current `jmap-draft` to use a `reactor-netty` server instead of a Jetty server * The `reactor-netty` server should serve as well the new `jmap` implementation * We will be able to refactor and get end-to-end reactive operations for JMAP, unlocking performance gains  ## References  * JIRA: [JAMES-3078](https://issues.apache.org/jira/browse/JAMES-3078) * JMAP new specifications adoption ADR: https://github.com/apache/james-project/blob/master/src/adr/0018-jmap-new-specs.md",technology_choice
ADR_285,https://github.com/XLab-Tongji/PerformanceTestDocs.git,failures-adr/0004-sprout-fails-when-transaction-in-progress.md,4. Sprout Fails When Transaction In Progress,"# 4. Sprout Fails When Transaction In Progress  Date: 2018-06-17  ## Status  Accepted  ## Context  A sprout node fails while a transaction is in progress, the transaction fails.  ## Decision  Either the UE will retry automatically or it will display an error to the user, who should retry. Transaction fail can't be routed to sprout2 (As from that point on the P-CSCF will not retry the transaction).  ## Consequences  UE/user retries, or transaction fails  ## Flow Chart  ![sprout-pending-transaction](http://www.projectclearwater.org/wp-content/uploads/2014/02/sprout-pending-transaction.png)",others
ADR_286,https://github.com/ahitrin/SiebenApp.git,doc/adr/0003-dsl-for-creation-and-representation-of-goal-trees.md,3. DSL for creation and representation of goal trees,"# 3. DSL for creation and representation of goal trees  Date: 2017-05-14  ## Status  Accepted  ## Context  We have to work with a lot of goal tree examples during SiebenApp testing. Current API allows only to create goaltree step-by-step. It makes hard to point a border between test setup and test actions.  ## Decision  Create a declarative [DSL][DSL] that allows to define a goal tree that ""exists before test actions"". Use it in all unit tests.  ## Consequences  Tests will became more readable and clean.  [DSL]: https://en.wikipedia.org/wiki/Domain-specific_language ",testing_strategy
ADR_287,https://github.com/karlmdavis/fhir-benchmarks.git,dev/architecture/0003-test-output.md,Architecture Decision: What Should the Test Output Look Like?,"# Architecture Decision: What Should the Test Output Look Like?  I suspect that a very common workflow for this project later on will be:  1. FHIR server implementor decides to add or improve benchmark for their FHIR server. 1. Implemetor downloads this project,      points it at a local copy of their server,      and then starts working to implement and/or improve the benchmarks for their server. 1. After each change to their server and/or the benchmark,      implementor will need to find, view, and understand the results. 1. Implementor will use results to guide next steps,      often re-running the benchmarks and going through this loop for a while.  I'd been thinking it'd be possible for implementors to just review a single   JSON output file to inspect the results, and iterate on their servers' performance. That's... likely not true, though? It's entirely likely that some servers will support dozens of benchmarks. It's unlikely that a single JSON file for that much data will be human-readable.  What if the benchmark tooling makes it simple for implementors to run only one test at a time, though? Even still, I think it's fair to say that they'll eventually need a way to visualize the complete results. In particular, they'll likely often want to compare their server to another one.  I'm going to need to spend time on the visual design of the results. I mean... no shit, right? What I'm realizing now, though, is that getting useful output and comparisons   will need to be an **early** concern for the project -- won't be able to defer it for long.  TODO: Not sure I have a concrete answer to how to represent HDR Histogram data in output files.",others
ADR_288,https://github.com/home-assistant/architecture.git,adr/0010-integration-configuration.md,0010. Integration configuration,"# 0010. Integration configuration  Date: 2020-04-14  ## Status  Accepted  ## Context  Home Assistant has been relying on YAML for its configuration format for a long time. However, in certain cases, this caused issues or did not work at all. These cases are best explained by listing the different categories of integrations that we have in Home Assistant:  - Integrations that integrate devices. Examples include Hue, TP-Link. - Integrations that integrate services. Examples include AdGuard, Snapcast. - Integrations that integrate transports. These integrations allow users to   define their own protocol. Examples include MQTT, serial, GPIO. - Integrations that process Home Assistant data and make this available to   other integrations. Examples are template, stats, derivative, utility meter. - Integrations that provide automations. Examples include automation,   device_sun_light_trigger, alert. - Integrations that help controlling devices and services.   Examples include script, scene. - Integrations that expose Home Assistant data to other services.   Examples include Google Assistant, HomeKit.  In all but the first two cases, YAML does just fine. The configuration is static, is not discovered and relies on the user setting it up. These cases have been solved by providing a hybrid approach. We offer YAML with a reload service and we offer Storage Collections, which allows the user to create/manage these integrations via the UI.  However, in the first two cases, it doesn’t work. Integrations are discovered. Integrations require users logging in on the vendor’s website and authorize linking (OAuth2) or users are required to press buttons on the hub to authorize linking (i.e. Hue).  In the cases that people can authorize an integration by just putting their username and password in the YAML file, they don’t want to, because it prevents them from sharing their configuration. This is solved currently by using YAML secrets that are substituted during load. This results in one file that provides the structure of your configuration and one file that provides the values. See below for an anonymized example as can be found on GitHub:  ```yaml camera:  platform: onvif  name: bedroom  host: !secret camera_onvif_bedroom_host  port: !secret camera_onvif_bedroom_port  username: !secret camera_onvif_bedroom_username  password: !secret camera_onvif_bedroom_password ```  So to solve these first two cases, we’ve introduced config entries (a centralized config object) and config flows. Config flows handle creating config entries with data from different sources. It can handle a new entry created via the user interface, automatic discovery, but also is able to handle importing configuration from YAML. Config entries allow for migrations during upgrades, limiting the breaking changes we have to induce on our users.  Config flows empower users of all knowledge levels, to use and enjoy Home Assistant. Since the introduction of config flows we’ve kept it open to contributors of individual integrations to decide if they want to implement YAML and/or a user-facing config flow.  Some contributors have decided to drop the YAML import to reduce their maintenance and support burden. A burden that they volunteer to do in their spare time. This has sadly resulted in a few pretty de-motivating comments, towards the contributors and the project in general. These comments often violate the code of conduct we have in place to protect the Home Assistant community.  This induces the risk of losing contributors and maintainers, halts our project goals and slows down innovation. As an open source project, maintaining our contributors is our highest priority as they are the creators of the project in the first place. They should be highly admired and valued for their contributions.  From a project perspective, we have not provided the necessary guidelines on this matter for our contributors to work with and have therefore not managed the expectations of our users to a full extent.  ## Decision  To protect project goals and to provide clarity to our users and contributors, we’re introducing the following rules on how integrations need to be configured:  - Integrations that communicate with devices and/or services are only configured via   the UI. In rare cases, we can make an exception. - All other integrations are configured via YAML or via the UI.  These rules apply to all new integrations. Existing integrations that should not have a YAML configuration, are allowed and encouraged to implement a configuration flow and remove YAML support. Changes to existing YAML configuration for these same existing integrations, will no longer be accepted.  ## Consequences  - Power to our contributors! ❤ - Removes confusion and questions around the future of the YAML configuration   for all users and contributors. - Builds upon the goals that have been set out, as presented at the   State of the Union 2019. - This might impact the number of integrations contributed. This requires   configuration flows, which require tests. However, we do provide scaffolding   scripts for this (`python3 -m script.scaffold`). ",api_and_contracts
ADR_289,https://github.com/BMMRO-tech/BMMRO.git,architectural-decision-records/2019-11-20_firestore.md,Firestore,"# Firestore  **Status:** Accepted  ## Context  We need a database in order to store the data provided by the user in the different forms.  ## Decision  To reduce the amount of integrations and complexity, we've decided to use one of the databases that Firebase offers. Under the Firebase umbrella are Real-time database and Firestore which are similar in many features. Google recommends Firestore for new developers, as it is built upon “the successes of Real-time database”. Also, for our purposes, Firestore is a better choice because:  - It offers offline support for iOS, Android and Web clients whereas real-time DB supports only iOS and Android clients. - The one-big-JSON-tree structure of real-time can become difficult to maintain as the DB grows. Document-based Firestore offers better organisation provisions using collections. - Scaling on firestore is automatic whereas scaling on real-time database requires sharding.  ## Links  - [Differences between the Firestore and Real-time Database](https://firebase.google.com/docs/database/rtdb-vs-firestore) ",data_persistence
ADR_290,https://github.com/alphagov/verify-service-provider.git,docs/adr/0002-how-do-we-secure-the-api.md,2. Use dropwizard functionality to secure API,"# 2. Use dropwizard functionality to secure API  Date: 2017-06-01  ## Status  Accepted  ## Context  We need to secure the interaction between the ""client"" code (e.g. node JS) and the server side code (which will be a dropwizard app).  Depending on how the users want to run the service provider we may need different security solutions.  ## Decision  If possible users can talk to the service provider on the loopback (127.0.0.1) If that doesn't work for some reason then they can use the dropwizard config to set up basic auth or tls or something.  See http://www.dropwizard.io/1.1.0/docs/manual/configuration.html#connectors  ## Consequences  We'll deliver a prototype that allows users to configure dropwizard things.  ","security, technology_choice"
ADR_291,https://github.com/ministryofjustice/opg-use-an-lpa.git,docs/architecture/decisions/0011-the-same-zend-application-will-be-used-for-both-viewer-and-actor-components.md,11. The same Zend application will be used for both Viewer and Actor components,"# 11. The same Zend application will be used for both Viewer and Actor components  Date: 2019-05-01  ## Status  Accepted  ## Context  Use an LPA will be made up of two components - those for use by LPA _actors_, and those used by third  party groups who are the _viewers_ of the LPA.  At present it is expected that these two components will be hosted on two different domains.  ## Decision  That both `Viewer` and `Actor` will both be separate modules of the same Zend application.  Note: it is still expected that they will be deployed separately into two containers.  ## Consequences  * The two services will be able to share code much more easily. * There will only be a single Zend codebase to manage. * The application will be slightly larger overall as the dependencies for both modules will be included. ",microservices_and_modularity
ADR_292,https://github.com/alphagov/paas-team-manual.git,source/architecture_decision_records/ADR028-move-logs-to-logit.html.md,(sem título),"--- title: ADR028 - Migrate CF platform logs to Logit ---  # ADR028: Migrate CF platform logs to Logit  ## Context The work to ship cloud foundry platform logs to Logit was started in 2018 Q1. It was paused because some IA issues with Logit were not resolved. At one point RE recommended that PaaS should host our own logstash as this part of the service was not widely available by market Elastic SaaS providers. The PaaS team was also considering to use Elasticsearch on AWS with our accounts.  ## Decision An updated conversation with the RE tool team has confirmed that the IA issues had been resolved, and that GDS can continue to use Logit for now.  It is a GDS strategy to use a consistent logging solution. Hence, we should continue our migration of platform logs to logit, including our logstash filters.  There is considerably less maintenance work for us if we use Logit's logstash filter rather than hosting the bosh release one. In the future if GDS choose to use another vendor that do not have a hosted logstash solution, they would need to provide a migration strategy for all the current logstash users.  ## Status Approved  ## Consequences We will continue the migration of platform logs to logit including logstash, and take a risk that we may need to spin up our logstash in the future if GDS choose a different platform logs provider. ",observability
ADR_293,https://github.com/VirtualProgrammingLab/viplab-websocket-api.git,docs/adr/0002-use-sha256-with-base64url-encoding.md,Use SHA256 and Base64Url encoding for verifying json,"# Use SHA256 and Base64Url encoding for verifying json  * Status: proposed * Deciders: Leon Kiefer * Date: 2019-12-13  ## Context and Problem Statement  We have to transfer json data and verify the integrity of the data. The transfer involves an Authorization server which provides the json, a client which gets the data form that server and pass it to the WebSocket API. The WebSocket API must able to verify the integrity of the json data.  ## Decision Drivers <!-- optional -->  * Use standard encodings  ## Considered Options  * Transfer complete json via secure channel * Send SHA256 hash of Base64Url encoded json * Send SHA256 hash of formatted json  ## Decision Outcome  Chosen option: ""Send SHA256 hash of Base64Url encoded json"", because this method is platform independent and not much session state is required.  ### Positive Consequences <!-- optional -->  * The JWT really function as a verification token for the other requests. * Can be applied to all json data that must be verified.  ### Negative Consequences <!-- optional -->  * The json must be transferred in Base64Url encoding  ## Pros and Cons of the Options <!-- optional -->  ### Transfer complete json via secure channel  The complete json data is transferred using a secure channel, like a JWT Token Claim. The secure channel takes care of the Integrity and the transport.  * Good, because well known solutions can be used. * Good, because very secure. * Bad, because depending on selected secure channel many data have to be stored in a session state. * Bad, because there are limitations how many data can be transferred at once using a secure channel.  ### Send SHA256 hash of Base64Url encoded json  The json is encoded with Base64Url encoding (RFC 4648). The Base64 String is then transferred instead of the raw json. For the Base64 string the message digest using SHA256 (FIPS PUB 180-4) is computed and used to verify the integrity. The message digest is a sequence of bytes and should be encoded to a lower-case hex string to transfer it over a secure channel like JWT.  * Good, because the Integrity of the json can be verified simply. * Good, because only the small hex encoded message digest must be transferred over a secure channel. * Bad, because the json is transferred Base64 encoded and must be decoded before use. * Bad, because this must be implemented by hand.  ### Send SHA256 hash of formatted json  The json can be transferred in any format. To verify the integrity, the json is formatted in a way, that the same json info set result in the same json string representation on all platforms. This json string is then hashed using SHA256 (FIPS PUB 180-4) and the message digest can be used the verify the integrity. The message digest is a sequence of bytes and can be Base64Url encoded to transferred it over a secure channel like JWT.  * Good, because only the small Base64 encoded message digest must be transferred over a secure channel. * Good, because json can be be transferred as is. * Bad, because difficult to get a consistent formatting across platforms. * Bad, because heavily depends on the json formatting library * Bad, because this must be implemented by hand.  ## Links <!-- optional -->  * [ADR-0001](0001-use-json-web-tokens.md) * [ADR-0003](0003-transfere-hash-in-jwt-claim.md) ","security, technology_choice"
ADR_294,https://github.com/dennisseidel/saas-platform-frontend.git,adr/0008-use-launchaco-com-to-generate-the-logo-for-free.md,8. Use launchaco.com to generate the logo for free,# 8. Use launchaco.com to generate the logo for free  Date: 2019-02-09  ## Status  Accepted  ## Context  The landing page need a logo for the name. This could be design by hand but this requires skill and time or an online service could be used.  ## Decision  Generate the logo with [launchaco.com](https://www.launchaco.com/logo/editor).   ## Consequences  This speeds up the logo building process and at the time of writing the service was free. The logos are not very creative but to bootstrap a business this should be enough.    ## Links * https://www.launchaco.com/logo/editor,others
ADR_295,https://github.com/alphagov/verify-service-provider.git,docs/adr/0021-we-will-use-http-200-for-valid-saml.md,21. We will put verified status inside json object,"# 21. We will put verified status inside json object  Date: 2017-08-16  ## Status  Accepted  ## Context  When communicating with the Verify Service Provider API, we need to decide what status code to respond with for correctly formatted SAML that represents some kind of authentication failure (eg. NO_MATCH).   ## Decision  Any valid SAML will return a 200 OK response and can be deserialized as a <code>TranslatedRepsonseBody</code>. We will have to define an enum of possible SAML outcomes (<code>Scenario</code>) as we can't use HTTP codes Invalid JSON/SAML or internal errors will use a relevant, different HTTP status code.  ## Consequences  API consumers such as passport-verify will have to handle authentication failures as 200 codes. We will have to document the possible error scenarios. We can add API authentication such as HTTP Basic Auth at a later date without worrying about clashing on HTTP status codes. ",api_and_contracts
ADR_296,https://github.com/cljdoc/cljdoc.git,doc/adr/0004-utilize-codox-to-read-clojure-script-sources.md,4. Utilize Codox to Read Clojure/Script Sources,"# 4. Utilize Codox to Read Clojure/Script Sources  Date: 2018-01-14  ## Status  Accepted  Supercedes [2. Don't build on top of Codox](0002-don-t-build-on-top-of-codox.md)  ## Context  I initially thought reading metadata from source files is built into Grimoire but it is not and has been implemented separately in projects like [lein-grim](https://github.com/clojure-grimoire/lein-grim). The implementation in `lein-grim` did not work with `.cljs` or `.cljc` files and so copying that was not an option.  In a previous ADR I decided not to build on top of codox to generate documentation. I still believe Codox is not what I want to generate final artifacts (HTML, JS Apps) but has relatively solid features when it comes to reading source files and extracting metadata.  Codox' `:writer` option allows us to easily retrieve the raw data in a plain format that is easy to understand.   ## Decision  We will use Codox to retrieve metadata from source files for now. Storage of metadata will be stored in Grimoire as before.  ## Consequences  Codox does not provide means to read the source of vars. This will need to be implemented separately, perhaps using `clojure.repl/source-fn` which however has it's own issues. ",technology_choice
ADR_297,https://github.com/Alfresco/SearchServices.git,search-services/alfresco-search/doc/architecture/decisions/0002-search-morelikethis-adr.md,2. Search More Like This,"# 2. Search More Like This  Date: 09/01/2019  ## Status  Investigation Complete  ## Context  *Intro (Lucene)*  The More Like This (MLT from now on) functionality is implemented in Lucene and made available through Solr Rest API. The main implementation code is currently in the Lucene library : org.apache.lucene.queries.mlt.MoreLikeThis_ class . In that class there is the logic to take a document Id (or the document itself) in input and calculate a MLT query based on the significant terms extracted from the document in relation to the corpus. Currently it operates extracting the document from the local Lucene index when standalone(Terms vectors or stored content must be available) or using the realtime GET to fetch the input document from adjacent shard. The MLT query is built on the assumption that it is possible to identify significant terms from a document based on the term frequencies of them in document, compared to their occurrence in the corpus. After the significant terms are extracted, a score is assigned to each one of them and the most important are used to build a boosted query. Which fields to take into account when extracting the terms and building the query is one of the MLT parameters. I attach the slides from a presentation I made in 2017[1] detailing the internal of the functionality and some proposed refactor.  *Apache Solr*  Apache Solr exposes various ways to interact with the MLT library.  * MoreLikeThisHandler -> a dedicate request handler to return similar results to an input one * MoreLikeThisComponent -> to automatically execute more like this query on each document in the result set * MoreLikeThisQueryParser -> I tend to consider this the modern approach, that allow you to build MLT queries and debug them more easily  *More Like These*  Implementing the More Like These can be vital to offer advanced functionalities and reccomndetation to the users. The proposed implementation approach will cover different software areas : Lucene, Solr, Alfresco APIs .  I attached an High Level T-Shirt sizing estimation to each part of the developments.  *Lucene - M*  The Lucene implementation will be the biggest part. It will require to extend the More Like This class with: * Additional facade methods to process list of documents or list of document Ids * Significant term extraction and scoring from the input set of documents In particular the second bit (significant term extraction and scoring) will be the most important. Various algorithms are available to provide such capability [2] , I recommend a first implementation based on: JHL. After a first investigation KLIP [3] seems a promising implementation as well. Being Alfresco use case very generic additional variants can be added later. Each variant could be specific to a specific scenario (big collection, kind of the collection ect)  *Solr - S*  Out of the various ways Apache Solr serves the functionality as a beginning I recommend to extend the 3) MoreLikeThisQueryParser. We’ll re-use all the Solr MLT parameters and in addition we’ll support a different input. These are the classes:  org.apache.solr.search.mlt.CloudMLTQParser org.apache.solr.search.mlt.SimpleMLTQParser  The More Like These Functionality will be compatible out of the box with SolrCloud but for the Alfresco distribution model it will require some work.  *Solr - Alfresco Customisation - M*  To have the More Like These fully distributed in the Alfresco use case: The Interesting terms extraction and query building is effectively run locally on a single Solr. To have it properly distributed it is needed to: * Enable distributed IDF * customise the query parser to use the shards parameter to fetch the documents through the get request handler (that already supports it) the customised query parser needs to be compatible with Alfresco name mapping and locale functionalities * Acl needs to be preserved * the seed document must be fetched from the solr content store, potentially though the realtime GET * field parameters must be appropriately rewritten according to Alfresco mappings  *Alfresco Side - Configuration - S*  A specific request handler will be configured in the Alfresco solrconfig.xml to expose an endpoint that will use the More Like This query parser by default. It will take in input as request parameters a set of document Ids and the document fields to use for the document similarity. The rest of the parameters will be hardcoded in the config, only expert admin are invited to touch them (such as the algorithm for term scoring, and all the other MLT params)  *Alfresco Repository and Rest API - M*  A specific query language must be implemented: org.alfresco.rest.api.search.impl.SearchMapper org.alfresco.repo.search.impl.lucene.LuceneQueryLanguageSPI . The implementation should follow Alfresco best practice and allow to interact with the dedicated request handler for the More Like These  [1] https://www.slideshare.net/AlessandroBenedetti/advanced-document-similarity-with-apache-lucene  [2] https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-significantterms-aggregation.html#_parameters_5  [3] Tomokiyo, T., Hurst, M. (2003). A language model approach to keyphrase extraction. In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment (Vol. 18, pp. 33–40) Association for Computational Linguistics.  ## Decision Based on the results of the investigations the most critical points have been identified in:  1) Alfresco custom storing approach 2) Alfresco custom ACL filtering 3) Alfresco sharding model  The decision is to structure the developments in 3 sequential macro areas: 1) provide the More Like This functionality via API 2) provide the More Like These functionality via API 3) provide the automatic query expansion through API and configuration  The investigation was done just at API level which means no front end tasks have been taken under consideration. You find the detailed tasks under consequences.  ## Consequences Appropriate Jiras have been created: https://issues.alfresco.com/jira/browse/SEARCH-1385 https://issues.alfresco.com/jira/browse/SEARCH-1386 https://issues.alfresco.com/jira/browse/SEARCH-1387 ",others
ADR_298,https://github.com/mofeixiaobao/gatemint-sdk.git,docs/architecture/adr-009-evidence-module.md,ADR 009: Evidence Module,"# ADR 009: Evidence Module  ## Changelog  - 2019 July 31: Initial draft - 2019 October 24: Initial implementation  ## Status  Accepted  ## Context  In order to support building highly secure, robust and interoperable blockchain applications, it is vital for the Cosmos SDK to expose a mechanism in which arbitrary evidence can be submitted, evaluated and verified resulting in some agreed upon penalty for any misbehavior committed by a validator, such as equivocation (double-voting), signing when unbonded, signing an incorrect state transition (in the future), etc. Furthermore, such a mechanism is paramount for any [IBC](https://github.com/cosmos/ics/blob/master/ibc/2_IBC_ARCHITECTURE.md) or cross-chain validation protocol implementation in order to support the ability for any misbehavior to be relayed back from a collateralized chain to a primary chain so that the equivocating validator(s) can be slashed.  ## Decision  We will implement an evidence module in the Cosmos SDK supporting the following functionality:  - Provide developers with the abstractions and interfaces necessary to define   custom evidence messages, message handlers, and methods to slash and penalize   accordingly for misbehavior. - Support the ability to route evidence messages to handlers in any module to   determine the validity of submitted misbehavior. - Support the ability, through governance, to modify slashing penalties of any   evidence type. - Querier implementation to support querying params, evidence types, params, and   all submitted valid misbehavior.  ### Types  First, we define the `Evidence` interface type. The `x/evidence` module may implement its own types that can be used by many chains (e.g. `CounterFactualEvidence`). In addition, other modules may implement their own `Evidence` types in a similar manner in which governance is extensible. It is important to note any concrete type implementing the `Evidence` interface may include arbitrary fields such as an infraction time. We want the `Evidence` type to remain as flexible as possible.  When submitting evidence to the `x/evidence` module, the concrete type must provide the validator's consensus address, which should be known by the `x/slashing` module (assuming the infraction is valid), the height at which the infraction occurred and the validator's power at same height in which the infraction occurred.  ```go type Evidence interface {   Route() string   Type() string   String() string   Hash() HexBytes   ValidateBasic() error    // The consensus address of the malicious validator at time of infraction   GetConsensusAddress() ConsAddress    // Height at which the infraction occurred   GetHeight() int64    // The total power of the malicious validator at time of infraction   GetValidatorPower() int64    // The total validator set power at time of infraction   GetTotalPower() int64 } ```  ### Routing & Handling  Each `Evidence` type must map to a specific unique route and be registered with the `x/evidence` module. It accomplishes this through the `Router` implementation.  ```go type Router interface {   AddRoute(r string, h Handler) Router   HasRoute(r string) bool   GetRoute(path string) Handler   Seal() } ```  Upon successful routing through the `x/evidence` module, the `Evidence` type is passed through a `Handler`. This `Handler` is responsible for executing all corresponding business logic necessary for verifying the evidence as valid. In addition, the `Handler` may execute any necessary slashing and potential jailing. Since slashing fractions will typically result from some form of static functions, allow the `Handler` to do this provides the greatest flexibility. An example could be `k * evidence.GetValidatorPower()` where `k` is an on-chain parameter controlled by governance. The `Evidence` type should provide all the external information necessary in order for the `Handler` to make the necessary state transitions. If no error is returned, the `Evidence` is considered valid.  ```go type Handler func(Context, Evidence) error ```  ### Submission  `Evidence` is submitted through a `MsgSubmitEvidence` message type which is internally handled by the `x/evidence` module's `SubmitEvidence`.  ```go type MsgSubmitEvidence struct {   Evidence }  func handleMsgSubmitEvidence(ctx Context, keeper Keeper, msg MsgSubmitEvidence) Result {   if err := keeper.SubmitEvidence(ctx, msg.Evidence); err != nil {     return err.Result()   }    // emit events...    return Result{     // ...   } } ```  The `x/evidence` module's keeper is responsible for matching the `Evidence` against the module's router and invoking the corresponding `Handler` which may include slashing and jailing the validator. Upon success, the submitted evidence is persisted.  ```go func (k Keeper) SubmitEvidence(ctx Context, evidence Evidence) error {   handler := keeper.router.GetRoute(evidence.Route())   if err := handler(ctx, evidence); err != nil {     return ErrInvalidEvidence(keeper.codespace, err)   }    keeper.setEvidence(ctx, evidence)   return nil } ```  ### Genesis  Finally, we need to represent the genesis state of the `x/evidence` module. The module only needs a list of all submitted valid infractions and any necessary params for which the module needs in order to handle submitted evidence. The `x/evidence` module will naturally define and route native evidence types for which it'll most likely need slashing penalty constants for.  ```go type GenesisState struct {   Params       Params   Infractions  []Evidence } ```  ## Consequences  ### Positive  - Allows the state machine to process misbehavior submitted on-chain and penalize   validators based on agreed upon slashing parameters. - Allows evidence types to be defined and handled by any module. This further allows   slashing and jailing to be defined by more complex mechanisms. - Does not solely rely on Tendermint to submit evidence.  ### Negative  - No easy way to introduce new evidence types through governance on a live chain   due to the inability to introduce the new evidence type's corresponding handler  ### Neutral  - Should we persist infractions indefinitely? Or should we rather rely on events?  ## References  - [ICS](https://github.com/cosmos/ics) - [IBC Architecture](https://github.com/cosmos/ics/blob/master/ibc/1_IBC_ARCHITECTURE.md) - [Tendermint Fork Accountability](https://github.com/tendermint/spec/blob/7b3138e69490f410768d9b1ffc7a17abc23ea397/spec/consensus/fork-accountability.md) ",architectural_patterns
ADR_299,https://github.com/jchaudhu/SAP-Cloud.git,doc/architecture/decisions/support-kubernetes.md,Support autoscaling using Kubernetes,"### Support autoscaling using Kubernetes  The pipeline can leverage certain Kubernetes features to support dynamic scalability.    #### Solutions assessed - Create pod per step - Create pod per stage - Create pod per pipeline  ##### Create pod per step The idea was to create a pod whenever `dockerExecute` step is invoked.  Though it was a simple solution, it came with a huge overhead of additional stashing and unstashing.  This noticeably delayed the pipeline execution.  Hence this solution is not appropriate in our use case.  ##### Create pod per stage Here we follow the approach of creating a pod per each stage. This addresses the problem we had in the first approach.  But, we need to know the `dockerImage`s used in each stage in advance so that a pod can be created with those images.  Since the list of `dockerImage`s of an individual stage is derived from the `pipeline_config.yaml`, there needs to be an approach which can support the pipeline extensions.  ##### Create pod per pipeline We have also analyzed the idea of using a pod per pipeline.  However, this would contradict our basic principle of isolation between stages, because the file system is shared by every container that exists in a pod.  #### Conclusion We have decided to use the second approach where a pod is created per stage.  To support the same when pipeline extensions are used, we will create a pod per step for extensions. ","performance_and_scalability, technology_choice"
ADR_300,https://github.com/bbc/digital-paper-edit-storybook.git,docs/ADR/adr-28-08.md,Relationship between Storybook and DPE Client,"# Relationship between Storybook and DPE Client  - Status: accepted - Deciders: Alli and Eimi - Date: 2019-08-28  Technical Story: N/A  ## Context and Problem Statement  We needed to clarify the relationship between the DPE Client repository, where the components we are using to populate the Storybook repo have already been written, and the Storybook repository.  - From which repo would components be published? - Which repos would consume components from NPM? - Should the Storybook live inside the Client repo?  ## Decision Drivers  N/A  ## Considered Options  1. Publish components to NPM from the DPE Client repo, and then consume those components in the Storybook repo 2. Publish components to NPM from the Storybook repo, and then consume those components in the DPE client 3. Include the Storybook config and setup within the Client repo  ## Decision Outcome  Chosen option: Option 2, because this allows us to refactor components' code and preview changes within the Storybook locally — before publishing the component to the hosted Storybook and NPM.  This means that our workflow for populating the Storybook and refactoring the Client code is as follows:  1. Duplicate component code to Storybook repo 2. Publish completed components to NPM 3. Remove the original component code from the Client and import via NPM  ### Positive Consequences  ### Negative consequences  Caveat: If more than one person is working on the Storybook and DPE Client, they'll need to sync up to ensure that details in code refactors are not lost due to overlapping work.  If possible, also avoid having people working simultaneously on a component that consumes / is consumed by another component (i.e., one person working on a card component and another person working on a list component that consumes card components).  ## Pros and Cons of the Options  ### Option 1: Publish components to NPM from the Client and consume via Storybook  This workflow would mean that we would need to be refactoring code in the client before publishing individual components to NPM. To modify the components in the Storybook, we would need to re-publish to NPM from the client. This is gross.  ### Option 2: Publish components to NPM from the Storybook and refactor to consume in the Client  Although this workflow means we are essentially copy-pasting code over from the Client repo to the Storybook, it allows us to:  - Refactor component code while previewing the Storybook locally - Reflect changes to the code in the Client by refactoring to import components from NPM  This is a more sensible workflow than option one.  ### Option 3: Same repo for both Storybook and Client  We didn't discuss this option in detail — mainly because it would likely introduce too many moving parts to the same repo.  ## Links  N/A ",governance_and_process